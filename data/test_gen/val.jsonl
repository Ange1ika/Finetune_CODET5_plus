{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004431", "source": "def is_valid_privateattr_name(name: str) -> bool:\n    return name.startswith('_') and not name.startswith('__')", "target": "def test_str_config():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(fields={'field_a': core_schema.model_field(schema=core_schema.str_schema())}),\n        config=CoreConfig(str_max_length=5),\n    )\n    assert v.validate_python({'field_a': 'test'}) == ({'field_a': 'test'}, None, {'field_a'})\n    with pytest.raises(ValidationError, match='String should have at most 5 characters'):\n        v.validate_python({'field_a': 'test long'})"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004432", "source": "def pydantic_encoder(obj: Any) -> Any:\n    from dataclasses import asdict, is_dataclass\n    from pydantic.v1.main import BaseModel\n    if isinstance(obj, BaseModel):\n        return obj.dict()\n    elif is_dataclass(obj):\n        return asdict(obj)\n    for base in obj.__class__.__mro__[:-1]:\n        try:\n            encoder = ENCODERS_BY_TYPE[base]\n        except KeyError:\n            continue\n        return encoder(obj)\n    else:\n        raise TypeError(f\"Object of type '{obj.__class__.__name__}' is not JSON serializable\")", "target": "def test_json_or_python():\n    def s1(v: int) -> int:\n        return v + 1\n    def s2(v: int) -> int:\n        return v + 2\n    s = SchemaSerializer(\n        core_schema.json_or_python_schema(\n            core_schema.int_schema(serialization=core_schema.plain_serializer_function_ser_schema(s1)),\n            core_schema.int_schema(serialization=core_schema.plain_serializer_function_ser_schema(s2)),\n        )\n    )\n    assert s.to_json(0) == b'1'\n    assert s.to_python(0) == 2"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004433", "source": "def dataclass(\n        *,\n        init: bool = True,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool = False,\n        config: Union[ConfigDict, Type[object], None] = None,\n        validate_on_init: Optional[bool] = None,\n        use_proxy: Optional[bool] = None,\n        kw_only: bool = ...,\n    ) -> Callable[[Type[_T]], 'DataclassClassOrWrapper']:\n        ...", "target": "def test_extra_custom_serializer():\n    @dataclasses.dataclass\n    class Model:\n        pass\n    schema = core_schema.dataclass_schema(\n        Model,\n        core_schema.dataclass_args_schema(\n            'Model',\n            [],\n            extra_behavior='allow',\n        ),\n        [],\n    )\n    s = SchemaSerializer(schema)\n    v = SchemaValidator(schema)\n    m = v.validate_python({'extra': 'extra'})\n    assert s.to_python(m) == {'extra': 'extra bam!'}"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004434", "source": "def head_hash(self) -> str:\n        return self._run_git(\"show-ref\", \"--hash\", \"HEAD\").strip()", "target": "def test_compute_diff(self) -> None:\n        diff = self.repo.diff(\"HEAD\")\n        sha = _shasum(diff)\n        self.assertEqual(len(sha), 64)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004435", "source": "def dataclass(\n    _cls: type[_T] | None = None,\n    *,\n    init: Literal[False] = False,\n    repr: bool = True,\n    eq: bool = True,\n    order: bool = False,\n    unsafe_hash: bool = False,\n    frozen: bool | None = None,\n    config: ConfigDict | type[object] | None = None,\n    validate_on_init: bool | None = None,\n    kw_only: bool = False,\n    slots: bool = False,\n) -> Callable[[type[_T]], type[PydanticDataclass]] | type[PydanticDataclass]:\n    assert init is False, 'pydantic.dataclasses.dataclass only supports init=False'\n    assert validate_on_init is not False, 'validate_on_init=False is no longer supported'\n    if sys.version_info >= (3, 10):\n        kwargs = {'kw_only': kw_only, 'slots': slots}\n    else:\n        kwargs = {}\n    def create_dataclass(cls: type[Any]) -> type[PydanticDataclass]:\n        from ._internal._utils import is_model_class\n        if is_model_class(cls):\n            raise PydanticUserError(\n                f'Cannot create a Pydantic dataclass from {cls.__name__} as it is already a Pydantic model',\n                code='dataclass-on-model',\n            )\n        original_cls = cls\n        has_dataclass_base = any(dataclasses.is_dataclass(base) for base in cls.__bases__)\n        if not has_dataclass_base and config is not None and hasattr(cls, '__pydantic_config__'):\n            warn(\n                f'`config` is set via both the `dataclass` decorator and `__pydantic_config__` for dataclass {cls.__name__}. '\n                f'The `config` specification from `dataclass` decorator will take priority.',\n                category=UserWarning,\n                stacklevel=2,\n            )\n        config_dict = config if config is not None else getattr(cls, '__pydantic_config__', None)\n        config_wrapper = _config.ConfigWrapper(config_dict)\n        decorators = _decorators.DecoratorInfos.build(cls, replace_wrapped_methods=True)\n        decorators.update_from_config(config_wrapper)\n        original_doc = cls.__doc__\n        if _pydantic_dataclasses.is_stdlib_dataclass(cls):\n            original_doc = None\n            bases = (cls,)\n            if issubclass(cls, Generic):\n                generic_base = Generic[cls.__parameters__]\n                bases = bases + (generic_base,)\n            cls = types.new_class(cls.__name__, bases)\n        if frozen is not None:\n            frozen_ = frozen\n            if config_wrapper.frozen:\n                warn(\n                    f'`frozen` is set via both the `dataclass` decorator and `config` for dataclass {cls.__name__!r}.'\n                    'This is not recommended. The `frozen` specification on `dataclass` will take priority.',\n                    category=UserWarning,\n                    stacklevel=2,\n                )\n        else:\n            frozen_ = config_wrapper.frozen or False\n        cls_anns = _typing_extra.safe_get_annotations(cls)\n        for field_name in cls_anns:\n            field_value = getattr(cls, field_name, None)\n            if isinstance(field_value, FieldInfo):\n                setattr(cls, field_name, _pydantic_dataclasses.as_dataclass_field(field_value))\n        with _pydantic_dataclasses.patch_base_fields(cls):\n            cls = dataclasses.dataclass(\n                cls,\n                init=True,\n                repr=repr,\n                eq=eq,\n                order=order,\n                unsafe_hash=unsafe_hash,\n                frozen=frozen_,\n                **kwargs,\n            )\n        if config_wrapper.validate_assignment:\n            original_setattr = cls.__setattr__\n            @functools.wraps(cls.__setattr__)\n            def validated_setattr(instance: PydanticDataclass, name: str, value: Any, /) -> None:\n                if frozen_:\n                    return original_setattr(instance, name, value)\n                inst_cls = type(instance)\n                attr = getattr(inst_cls, name, None)\n                if isinstance(attr, property):\n                    attr.__set__(instance, value)\n                elif isinstance(attr, functools.cached_property):\n                    instance.__dict__.__setitem__(name, value)\n                else:\n                    inst_cls.__pydantic_validator__.validate_assignment(instance, name, value)\n            cls.__setattr__ = validated_setattr.__get__(None, cls)\n            if slots and not hasattr(cls, '__setstate__'):\n                def _dataclass_getstate(self: Any) -> list[Any]:\n                    return [getattr(self, f.name) for f in dataclasses.fields(self)]\n                def _dataclass_setstate(self: Any, state: list[Any]) -> None:\n                    for field, value in zip(dataclasses.fields(self), state):\n                        object.__setattr__(self, field.name, value)\n                cls.__getstate__ = _dataclass_getstate\n                cls.__setstate__ = _dataclass_setstate\n        cls.__is_pydantic_dataclass__ = True\n        cls.__pydantic_decorators__ = decorators\n        cls.__doc__ = original_doc\n        firstlineno = getattr(original_cls, '__firstlineno__', None)\n        cls.__module__ = original_cls.__module__\n        if sys.version_info >= (3, 13) and firstlineno is not None:\n            original_cls.__firstlineno__ = firstlineno\n            cls.__firstlineno__ = firstlineno\n        cls.__qualname__ = original_cls.__qualname__\n        cls.__pydantic_fields_complete__ = classmethod(_pydantic_fields_complete)\n        cls.__pydantic_complete__ = False\n        _pydantic_dataclasses.complete_dataclass(cls, config_wrapper, raise_errors=False)\n        return cls\n    return create_dataclass if _cls is None else create_dataclass(_cls)", "target": "def test_slots_dataclass_subclass(revalidate_instances, input_value, expected):\n    schema = core_schema.dataclass_schema(\n        FooDataclassSlots,\n        core_schema.dataclass_args_schema(\n            'FooDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n            ],\n            extra_behavior='forbid',\n        ),\n        ['a', 'b'],\n        revalidate_instances=revalidate_instances,\n        slots=True,\n    )\n    v = SchemaValidator(schema)\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=expected.message) as exc_info:\n            print(v.validate_python(input_value))\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        dc = v.validate_python(input_value)\n        assert dataclasses.is_dataclass(dc)\n        assert dataclasses.asdict(dc) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004436", "source": "def write_line(*args):\n        nonlocal out\n        out += f\"| {' | '.join(str(a) for a in args)} |\\n\"", "target": "def test_pr_with_not_user_facing_label(\n        self, mocked_rn_labels: Any, mocked_gql: Any\n    ) -> None:\n        \"Test PR with 'topic: not user facing' label\"\n        pr = GitHubPR(\"pytorch\", \"pytorch\", 75095)\n        self.assertTrue(has_required_labels(pr))"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004437", "source": "def rebuild_dataclass_fields(\n    cls: type[PydanticDataclass],\n    *,\n    config_wrapper: ConfigWrapper,\n    ns_resolver: NsResolver,\n    typevars_map: Mapping[TypeVar, Any],\n) -> dict[str, FieldInfo]:\n    FieldInfo_ = import_cached_field_info()\n    rebuilt_fields: dict[str, FieldInfo] = {}\n    with ns_resolver.push(cls):\n        for f_name, field_info in cls.__pydantic_fields__.items():\n            if field_info._complete:\n                rebuilt_fields[f_name] = field_info\n            else:\n                existing_desc = field_info.description\n                ann = _typing_extra.eval_type(\n                    field_info._original_annotation,\n                    *ns_resolver.types_namespace,\n                )\n                ann = _generics.replace_types(ann, typevars_map)\n                new_field = FieldInfo_.from_annotated_attribute(\n                    ann,\n                    field_info._original_assignment,\n                    _source=AnnotationSource.DATACLASS,\n                )\n                new_field.description = new_field.description if new_field.description is not None else existing_desc\n                update_field_from_config(config_wrapper, f_name, new_field)\n                rebuilt_fields[f_name] = new_field\n    return rebuilt_fields", "target": "def test_missing_error(pydantic_version):\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'field_a': core_schema.model_field(schema=core_schema.str_schema()),\n                'field_b': core_schema.model_field(schema=core_schema.int_schema()),\n            }\n        )\n    )\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python({'field_a': b'abc'})\n    assert (\n        str(exc_info.value)\n        ==\n        + (\n            f'\\n    For further information visit https://errors.pydantic.dev/{pydantic_version}/v/missing'\n            if os.environ.get('PYDANTIC_ERRORS_INCLUDE_URL', '1') != 'false'\n            else ''\n        )\n    )"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004438", "source": "def _decorator(cls: type) -> type:\n        block = generate_dataclass_help(params_cls)\n        cls.__doc__ = (cls.__doc__ or \"\") + f\"\\n\\n{title}:\\n{block}\"\n        return cls", "target": "def test_env_path_returns_path_when_present(self):\n        tmp = Path(\"./b\").resolve()\n        with patch.dict(os.environ, {\"P\": str(tmp)}, clear=True):\n            p = m.env_path(\"P\", None, resolve=True)\n            self.assertEqual(p, tmp)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004439", "source": "def dataclass(\n        *,\n        init: bool = True,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool = False,\n        config: Union[ConfigDict, Type[object], None] = None,\n        validate_on_init: Optional[bool] = None,\n        use_proxy: Optional[bool] = None,\n    ) -> Callable[[Type[_T]], 'DataclassClassOrWrapper']:\n        ...", "target": "def test_dataclass_initvar_not_required_on_union_ser() -> None:\n    @dataclasses.dataclass\n    class Foo:\n        x: int\n        init_var: dataclasses.InitVar[int] = 1\n    @dataclasses.dataclass\n    class Bar:\n        x: int\n    schema = core_schema.union_schema(\n        [\n            core_schema.dataclass_schema(\n                Foo,\n                core_schema.dataclass_args_schema(\n                    'Foo',\n                    [\n                        core_schema.dataclass_field(name='x', schema=core_schema.int_schema()),\n                        core_schema.dataclass_field(\n                            name='init_var',\n                            init_only=True,\n                            schema=core_schema.with_default_schema(core_schema.int_schema(), default=1),\n                        ),\n                    ],\n                ),\n                ['x'],\n                post_init=True,\n            ),\n            core_schema.dataclass_schema(\n                Bar,\n                core_schema.dataclass_args_schema(\n                    'Bar', [core_schema.dataclass_field(name='x', schema=core_schema.int_schema())]\n                ),\n                ['x'],\n            ),\n        ]\n    )\n    s = SchemaSerializer(schema)\n    assert s.to_python(Foo(x=1), warnings='error') == {'x': 1}\n    assert s.to_python(Foo(x=1, init_var=2), warnings='error') == {'x': 1}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004440", "source": "def dataclass(\n        _cls: type[_T],\n        *,\n        init: Literal[False] = False,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool | None = None,\n        config: ConfigDict | type[object] | None = None,\n        validate_on_init: bool | None = None,\n    ) -> type[PydanticDataclass]: ...", "target": "def test_properties():\n    @dataclasses.dataclass\n    class FooProp:\n        a: str\n        b: bytes\n        @property\n        def c(self) -> str:\n            return f'{self.a} {self.b.decode()}'\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'FooProp',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bytes_schema()),\n            ],\n            computed_fields=[core_schema.computed_field('c', core_schema.str_schema())],\n        ),\n        ['a', 'b'],\n    )\n    s = SchemaSerializer(schema)\n    assert s.to_python(FooProp(a='hello', b=b'more')) == IsStrictDict(a='hello', b=b'more', c='hello more')\n    assert s.to_python(FooProp(a='hello', b=b'more'), mode='json') == IsStrictDict(a='hello', b='more', c='hello more')\n    j = s.to_json(FooProp(a='hello', b=b'more'))\n    if on_pypy:\n        assert json.loads(j) == {'a': 'hello', 'b': 'more', 'c': 'hello more'}\n    else:\n        assert j == b'{\"a\":\"hello\",\"b\":\"more\",\"c\":\"hello more\"}'\n    assert s.to_python(FooProp(a='hello', b=b'more'), exclude={'b'}) == IsStrictDict(a='hello', c='hello more')\n    assert s.to_json(FooProp(a='hello', b=b'more'), include={'a'}) == b'{\"a\":\"hello\"}'"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004441", "source": "def is_new_type(type_: Type[Any]) -> bool:\n    return isinstance(type_, test_type.__class__) and hasattr(type_, '__supertype__')", "target": "def test_error_details() -> None:\n    def act_on_error_details(_: ErrorDetails) -> None:\n        pass\n    v = SchemaValidator({'type': 'int'})\n    try:\n        v.validate_python('not an int')\n    except ValidationError as err:\n        for details in err.errors(include_url=False):\n            act_on_error_details(details)"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004442", "source": "def secs_to_ms(time_s):\n    return time_s * 1e3", "target": "def test_pr_with_release_notes_label(\n        self, mocked_rn_labels: Any, mocked_gql: Any\n    ) -> None:\n        \"Test PR with 'release notes: nn' label\"\n        pr = GitHubPR(\"pytorch\", \"pytorch\", 71759)\n        self.assertTrue(has_required_labels(pr))"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004443", "source": "def cherry_pick(self, ref: str) -> None:\n        self._run_git(\"cherry-pick\", \"-x\", ref)", "target": "def test_is_iterable(self) -> None:\n        from collections.abc import Iterator\n        iter_ = PeekableIterator(\"\")\n        self.assertTrue(isinstance(iter_, Iterator))"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004444", "source": "def _set_nested_attr(obj: nn.Module, names: list[str], value: Tensor) -> None:\n    if len(names) == 1:\n        setattr(obj, names[0], value)\n    else:\n        _set_nested_attr(getattr(obj, names[0]), names[1:], value)", "target": "def test_merged_lastfailed_content_with_overlap(self) -> None:\n        last_failed_source = {\n            \"tools/tests/test_foo.py::test_num1\": True,\n            \"tools/tests/test_foo.py::test_num2\": True,\n            \"tools/tests/test_bar.py::test_num1\": True,\n        }\n        last_failed_dest = {\n            \"tools/tests/test_foo.py::test_num1\": True,\n            \"tools/tests/test_car.py::test_num1\": True,\n            \"tools/tests/test_car.py::test_num2\": True,\n        }\n        last_failed_merged = {\n            \"tools/tests/test_foo.py::test_num1\": True,\n            \"tools/tests/test_foo.py::test_num2\": True,\n            \"tools/tests/test_bar.py::test_num1\": True,\n            \"tools/tests/test_car.py::test_num1\": True,\n            \"tools/tests/test_car.py::test_num2\": True,\n        }\n        merged = _merged_lastfailed_content(last_failed_source, last_failed_dest)\n        self.assertEqual(merged, last_failed_merged)"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004445", "source": "def clone_repo(username: str, password: str, org: str, project: str) -> GitRepo:\n    path = tempfile.mkdtemp()\n    _check_output(\n        [\n            \"git\",\n            \"clone\",\n            f\"https://{username}:{password}@github.com/{org}/{project}\",\n            path,\n        ]\n    ).strip()\n    return GitRepo(path=path)", "target": "def test_is_iterable(self) -> None:\n        from collections.abc import Iterator\n        iter_ = PeekableIterator(\"\")\n        self.assertTrue(isinstance(iter_, Iterator))"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004446", "source": "def _apply_alias_generator_to_field_info(\n    alias_generator: Callable[[str], str] | AliasGenerator, field_name: str, field_info: FieldInfo\n):\n    if (\n        field_info.alias_priority is None\n        or field_info.alias_priority <= 1\n        or field_info.alias is None\n        or field_info.validation_alias is None\n        or field_info.serialization_alias is None\n    ):\n        alias, validation_alias, serialization_alias = None, None, None\n        if isinstance(alias_generator, AliasGenerator):\n            alias, validation_alias, serialization_alias = alias_generator.generate_aliases(field_name)\n        elif callable(alias_generator):\n            alias = alias_generator(field_name)\n            if not isinstance(alias, str):\n                raise TypeError(f'alias_generator {alias_generator} must return str, not {alias.__class__}')\n        if field_info.alias_priority is None or field_info.alias_priority <= 1:\n            field_info.alias_priority = 1\n        if field_info.alias_priority == 1:\n            field_info.serialization_alias = get_first_not_none(serialization_alias, alias)\n            field_info.validation_alias = get_first_not_none(validation_alias, alias)\n            field_info.alias = alias\n        if field_info.alias is None:\n            field_info.alias = alias\n        if field_info.serialization_alias is None:\n            field_info.serialization_alias = get_first_not_none(serialization_alias, alias)\n        if field_info.validation_alias is None:\n            field_info.validation_alias = get_first_not_none(validation_alias, alias)", "target": "def test_from_attributes(input_value, expected, from_attributes_mode):\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'a': core_schema.model_field(schema=core_schema.int_schema()),\n                'b': core_schema.model_field(schema=core_schema.int_schema()),\n                'c': core_schema.model_field(schema=core_schema.str_schema()),\n            },\n            from_attributes=from_attributes_mode == 'schema',\n        )\n    )\n    kwargs = {}\n    if from_attributes_mode == 'validation':\n        kwargs['from_attributes'] = True\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            val = v.validate_python(input_value, **kwargs)\n            print(f'UNEXPECTED OUTPUT: {val!r}')\n    else:\n        output = v.validate_python(input_value, **kwargs)\n        assert output == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004447", "source": "def get_origin(t: Type[Any]) -> Optional[Type[Any]]:\n        if type(t).__name__ in AnnotatedTypeNames:\n            return cast(Type[Any], Annotated)\n        return getattr(t, '__origin__', None)", "target": "def test_schema_typing_error() -> None:\n    _: CoreSchema = {'type': 'wrong'}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004448", "source": "def dataclass(\n        *,\n        init: Literal[False] = False,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool = False,\n        config: ConfigDict | type[object] | None = None,\n        validate_on_init: bool | None = None,\n        kw_only: bool = ...,\n        slots: bool = ...,\n    ) -> Callable[[type[_T]], type[PydanticDataclass]]:\n        ...", "target": "def test_dataclass_args_init_with_default(input_value, extra_behavior, expected):\n    @dataclasses.dataclass\n    class Foo:\n        a: str\n        b: str\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(\n                    name='b',\n                    schema=core_schema.with_default_schema(schema=core_schema.str_schema(), default='HELLO'),\n                    init=False,\n                ),\n            ],\n            extra_behavior=extra_behavior,\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)) as exc_info:\n            v.validate_python(input_value)\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        assert dataclasses.asdict(v.validate_python(input_value)) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004449", "source": "def set_dataclass_fields(\n    cls: type[StandardDataclass],\n    config_wrapper: _config.ConfigWrapper,\n    ns_resolver: NsResolver | None = None,\n) -> None:\n    typevars_map = get_standard_typevars_map(cls)\n    fields = collect_dataclass_fields(\n        cls, ns_resolver=ns_resolver, typevars_map=typevars_map, config_wrapper=config_wrapper\n    )\n    cls.__pydantic_fields__ = fields", "target": "def test_dataclass_wrap_json():\n    schema = core_schema.no_info_wrap_validator_function(\n        lambda v, handler: handler(v),\n        core_schema.dataclass_schema(\n            FooDataclass,\n            core_schema.dataclass_args_schema(\n                'FooDataclass',\n                [\n                    core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                    core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n                ],\n            ),\n            ['a', 'b'],\n        ),\n    )\n    v = SchemaValidator(schema)\n    assert v.validate_json('{\"a\": \"hello\", \"b\": true}') == FooDataclass(a='hello', b=True)\n    assert v.validate_json('{\"a\": \"hello\", \"b\": true}', strict=True) == FooDataclass(a='hello', b=True)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004450", "source": "def make_dataclass_validator(dc_cls: Type['Dataclass'], config: Type[BaseConfig]) -> 'CallableGenerator':\n    yield from _get_validators(dataclass(dc_cls, config=config, use_proxy=True))", "target": "def test_slots() -> None:\n    @dataclasses.dataclass(slots=True)\n    class Model:\n        x: int\n    schema = core_schema.dataclass_schema(\n        Model,\n        core_schema.dataclass_args_schema(\n            'Model', [core_schema.dataclass_field(name='x', schema=core_schema.int_schema())]\n        ),\n        ['x'],\n        slots=True,\n    )\n    val = SchemaValidator(schema)\n    m: Model\n    m = val.validate_python({'x': 123})\n    assert m == Model(x=123)\n    with pytest.raises(ValidationError):\n        val.validate_python({'x': 'abc'})\n    val.validate_assignment(m, 'x', 456)\n    assert m.x == 456\n    with pytest.raises(ValidationError):\n        val.validate_assignment(m, 'x', 'abc')"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004451", "source": "def rebuild_dataclass_fields(\n    cls: type[PydanticDataclass],\n    *,\n    config_wrapper: ConfigWrapper,\n    ns_resolver: NsResolver,\n    typevars_map: Mapping[TypeVar, Any],\n) -> dict[str, FieldInfo]:\n    FieldInfo_ = import_cached_field_info()\n    rebuilt_fields: dict[str, FieldInfo] = {}\n    with ns_resolver.push(cls):\n        for f_name, field_info in cls.__pydantic_fields__.items():\n            if field_info._complete:\n                rebuilt_fields[f_name] = field_info\n            else:\n                existing_desc = field_info.description\n                ann = _typing_extra.eval_type(\n                    field_info._original_annotation,\n                    *ns_resolver.types_namespace,\n                )\n                ann = _generics.replace_types(ann, typevars_map)\n                new_field = FieldInfo_.from_annotated_attribute(\n                    ann,\n                    field_info._original_assignment,\n                    _source=AnnotationSource.DATACLASS,\n                )\n                new_field.description = new_field.description if new_field.description is not None else existing_desc\n                update_field_from_config(config_wrapper, f_name, new_field)\n                rebuilt_fields[f_name] = new_field\n    return rebuilt_fields", "target": "def test_only_validate_by_name(py_and_json) -> None:\n    v = py_and_json(\n        {\n            'type': 'model-fields',\n            'fields': {'field_a': {'validation_alias': 'FieldA', 'type': 'model-field', 'schema': {'type': 'int'}}},\n        },\n        config=CoreConfig(validate_by_name=True, validate_by_alias=False),\n    )\n    assert v.validate_test({'field_a': '123'}) == ({'field_a': 123}, None, {'field_a'})\n    with pytest.raises(ValidationError, match=r'field_a\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'FieldA': '123'})"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004452", "source": "def generate_dataclass_help(cls) -> str:\n    if not is_dataclass(cls):\n        raise TypeError(f\"{cls} is not a dataclass\")\n    def get_value(f):\n        if f.default is not MISSING:\n            return f.default\n        if f.default_factory is not MISSING:\n            try:\n                return f.default_factory()\n            except Exception as e:\n                return f\"<error: {e}>\"\n        return \"<required>\"\n    lines = [f\"{f.name:<22} = {repr(get_value(f))}\" for f in fields(cls)]\n    return indent(\"\\n\".join(lines), \"    \")", "target": "def test_env_path_optional_unset_returns_default_path_no_resolve(self):\n        d = Path(\"z\")\n        with patch.dict(os.environ, {}, clear=True):\n            p = m.env_path_optional(\"P\", default=d, resolve=False)\n            self.assertEqual(p, d)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004453", "source": "def _apply_field_title_generator_to_field_info(\n    title_generator: Callable[[str, FieldInfo], str],\n    field_name: str,\n    field_info: FieldInfo,\n):\n    if field_info.title is None:\n        title = title_generator(field_name, field_info)\n        if not isinstance(title, str):\n            raise TypeError(f'field_title_generator {title_generator} must return str, not {title.__class__}')\n        field_info.title = title", "target": "def test_from_attributes_error_error():\n    class BadError(Exception):\n        def __str__(self):\n            raise RuntimeError('intentional error inside error')\n    class Foobar:\n        @property\n        def x(self):\n            raise BadError('intentional error')\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={'x': core_schema.model_field(schema=core_schema.int_schema())}, from_attributes=True\n        )\n    )\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python(Foobar())\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'get_attribute_error',\n            'loc': ('x',),\n            'msg': IsStr(regex=r'Error extracting attribute: \\S+\\.<locals>\\.BadError: <exception str\\(\\) failed>'),\n            'input': HasRepr(IsStr(regex='.+Foobar object at.+')),\n            'ctx': {'error': IsStr(regex=r'\\S+\\.<locals>\\.BadError: <exception str\\(\\) failed>')},\n        }\n    ]\n    class UnInitError:\n        @property\n        def x(self):\n            raise RuntimeError\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python(UnInitError())\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'get_attribute_error',\n            'loc': ('x',),\n            'msg': 'Error extracting attribute: RuntimeError',\n            'input': HasRepr(IsStr(regex='.+UnInitError object at.+')),\n            'ctx': {'error': 'RuntimeError'},\n        }\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004454", "source": "def is_finalvar(ann_type: Type[Any]) -> bool:\n    return _check_finalvar(ann_type) or _check_finalvar(get_origin(ann_type))", "target": "def test_schema_validator() -> None:\n    SchemaValidator({'type': 'int'})"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004455", "source": "def current_branch(self) -> Optional[str]:\n        try:\n            return self._run_git(\"symbolic-ref\", \"--short\", \"HEAD\").strip()\n        except RuntimeError:\n            return None", "target": "def test_double_asterisks(self) -> None:\n        allowed_patterns = [\n            \"aten/src/ATen/native/**LinearAlgebra*\",\n        ]\n        patterns_re = patterns_to_regex(allowed_patterns)\n        fnames = [\n            \"aten/src/ATen/native/LinearAlgebra.cpp\",\n            \"aten/src/ATen/native/cpu/LinearAlgebraKernel.cpp\",\n        ]\n        for filename in fnames:\n            self.assertTrue(patterns_re.match(filename))"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004456", "source": "def _dataclass_setstate(self: Any, state: list[Any]) -> None:\n                    for field, value in zip(dataclasses.fields(self), state):\n                        object.__setattr__(self, field.name, value)", "target": "def test_extra_custom_serializer():\n    @dataclasses.dataclass\n    class Model:\n        pass\n    schema = core_schema.dataclass_schema(\n        Model,\n        core_schema.dataclass_args_schema(\n            'Model',\n            [],\n            extra_behavior='allow',\n        ),\n        [],\n    )\n    s = SchemaSerializer(schema)\n    v = SchemaValidator(schema)\n    m = v.validate_python({'extra': 'extra'})\n    assert s.to_python(m) == {'extra': 'extra bam!'}"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004457", "source": "def extract_weights(mod: nn.Module) -> tuple[tuple[Tensor, ...], list[str]]:\n    orig_params = tuple(mod.parameters())\n    names = []\n    for name, p in list(mod.named_parameters()):\n        _del_nested_attr(mod, name.split(\".\"))\n        names.append(name)\n    params = tuple(p.detach().requires_grad_() for p in orig_params)\n    return params, names", "target": "def test_fails(self) -> None:\n        @retries_decorator(rc=0)\n        def foo(x: int, y: int) -> int:\n            return x + y\n        self.assertEqual(foo(\"a\", 4), 0)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004458", "source": "def _apply_alias_generator_to_field_info(\n    alias_generator: Callable[[str], str] | AliasGenerator, field_name: str, field_info: FieldInfo\n):\n    if (\n        field_info.alias_priority is None\n        or field_info.alias_priority <= 1\n        or field_info.alias is None\n        or field_info.validation_alias is None\n        or field_info.serialization_alias is None\n    ):\n        alias, validation_alias, serialization_alias = None, None, None\n        if isinstance(alias_generator, AliasGenerator):\n            alias, validation_alias, serialization_alias = alias_generator.generate_aliases(field_name)\n        elif callable(alias_generator):\n            alias = alias_generator(field_name)\n            if not isinstance(alias, str):\n                raise TypeError(f'alias_generator {alias_generator} must return str, not {alias.__class__}')\n        if field_info.alias_priority is None or field_info.alias_priority <= 1:\n            field_info.alias_priority = 1\n        if field_info.alias_priority == 1:\n            field_info.serialization_alias = get_first_not_none(serialization_alias, alias)\n            field_info.validation_alias = get_first_not_none(validation_alias, alias)\n            field_info.alias = alias\n        if field_info.alias is None:\n            field_info.alias = alias\n        if field_info.serialization_alias is None:\n            field_info.serialization_alias = get_first_not_none(serialization_alias, alias)\n        if field_info.validation_alias is None:\n            field_info.validation_alias = get_first_not_none(validation_alias, alias)", "target": "def test_from_attributes_extra_ignore_no_attributes_accessed() -> None:\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={'a': core_schema.model_field(schema=core_schema.int_schema())},\n            from_attributes=True,\n            extra_behavior='ignore',\n        )\n    )\n    accessed: list[str] = []\n    class Source:\n        a = 1\n        b = 2\n        def __getattribute__(self, name: str, /) -> Any:\n            accessed.append(name)\n            return super().__getattribute__(name)\n    assert v.validate_python(Source()) == ({'a': 1}, None, {'a'})\n    assert 'a' in accessed and 'b' not in accessed"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004459", "source": "def evaluate_forwardref(type_: ForwardRef, globalns: Any, localns: Any) -> Any:\n        return type_._evaluate(globalns, localns)", "target": "def test_schema_validator_wrong() -> None:\n    try:\n        SchemaValidator({'type': 'bad'})\n    except SchemaError:\n        pass\n    else:\n        raise AssertionError('SchemaValidator did not raise SchemaError')"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004460", "source": "def parse_fuller_format(lines: Union[str, list[str]]) -> GitCommit:\n    if isinstance(lines, str):\n        lines = lines.split(\"\\n\")\n    if len(lines) > 1 and lines[1].startswith(\"Merge:\"):\n        del lines[1]\n    assert len(lines) > 7\n    assert lines[0].startswith(\"commit\")\n    assert lines[1].startswith(\"Author: \")\n    assert lines[2].startswith(\"AuthorDate: \")\n    assert lines[3].startswith(\"Commit: \")\n    assert lines[4].startswith(\"CommitDate: \")\n    assert len(lines[5]) == 0\n    return GitCommit(\n        commit_hash=lines[0].split()[1].strip(),\n        author=lines[1].split(\":\", 1)[1].strip(),\n        author_date=datetime.fromtimestamp(int(lines[2].split(\":\", 1)[1].strip())),\n        commit_date=datetime.fromtimestamp(int(lines[4].split(\":\", 1)[1].strip())),\n        title=lines[6].strip(),\n        body=\"\\n\".join(lines[7:]),\n    )", "target": "def test_ghstack_branches_not_in_sync(self) -> None:\n        head_ref = \"gh/clee2000/1/head\"\n        self._skip_if_ref_does_not_exist(head_ref)\n        self.assertFalse(are_ghstack_branches_in_sync(self.repo, head_ref))"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004461", "source": "def custom_pydantic_encoder(type_encoders: dict[Any, Callable[[type[Any]], Any]], obj: Any) -> Any:\n    warnings.warn(\n        '`custom_pydantic_encoder` is deprecated, use `BaseModel.model_dump` instead.',\n        category=PydanticDeprecatedSince20,\n        stacklevel=2,\n    )\n    for base in obj.__class__.__mro__[:-1]:\n        try:\n            encoder = type_encoders[base]\n        except KeyError:\n            continue\n        return encoder(obj)\n    else:\n        return pydantic_encoder(obj)", "target": "def test_json_bytes_base64_round_trip():\n    data = b'\\xd8\\x07\\xc1Tx$\\x91F%\\xf3\\xf3I\\xca\\xd8@\\x0c\\xee\\xc3\\xab\\xff\\x7f\\xd3\\xcd\\xcd\\xf9\\xc2\\x10\\xe4\\xa1\\xb01e'\n    encoded_std = b'\"2AfBVHgkkUYl8/NJythADO7Dq/9/083N+cIQ5KGwMWU=\"'\n    encoded_url = b'\"2AfBVHgkkUYl8_NJythADO7Dq_9_083N-cIQ5KGwMWU=\"'\n    assert to_json(data, bytes_mode='base64') == encoded_url\n    v = SchemaValidator(core_schema.bytes_schema(), config=CoreConfig(val_json_bytes='base64'))\n    assert v.validate_json(encoded_url) == data\n    assert v.validate_json(encoded_std) == data\n    with pytest.raises(ValidationError) as exc:\n        v.validate_json('\"wrong!\"')\n    [details] = exc.value.errors()\n    assert details['type'] == 'bytes_invalid_encoding'\n    assert to_json({'key': data}, bytes_mode='base64') == b'{\"key\":' + encoded_url + b'}'\n    v = SchemaValidator(\n        core_schema.dict_schema(keys_schema=core_schema.str_schema(), values_schema=core_schema.bytes_schema()),\n        config=CoreConfig(val_json_bytes='base64'),\n    )\n    assert v.validate_json(b'{\"key\":' + encoded_url + b'}') == {'key': data}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004462", "source": "def get_args(t: Type[Any]) -> Tuple[Any, ...]:\n        if type(t).__name__ in AnnotatedTypeNames:\n            return t.__args__ + t.__metadata__\n        if isinstance(t, _GenericAlias):\n            res = t.__args__\n            if t.__origin__ is Callable and res and res[0] is not Ellipsis:\n                res = (list(res[:-1]), res[-1])\n            return res\n        return getattr(t, '__args__', ())", "target": "def test_wrong_function_signature() -> None:\n    def wrong_validator(value: Any) -> Any:\n        return value\n    v = SchemaValidator(core_schema.with_info_plain_validator_function(wrong_validator))\n    try:\n        v.validate_python(1)\n    except TypeError as exc:\n        assert 'takes 1 positional argument but 2 were given' in str(exc)\n    else:\n        raise AssertionError('v.validate_python(1) did not raise TypeError')"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004463", "source": "def is_literal_type(type_: Type[Any]) -> bool:\n    return Literal is not None and get_origin(type_) in LITERAL_TYPES", "target": "def test_schema_typing_error() -> None:\n    _: CoreSchema = {'type': 'wrong'}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004464", "source": "def is_valid_privateattr_name(name: str) -> bool:\n    return name.startswith('_') and not name.startswith('__')", "target": "def test_validate_assignment():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(fields={'field_a': core_schema.model_field(schema=core_schema.str_schema())})\n    )\n    assert v.validate_python({'field_a': 'test'}) == ({'field_a': 'test'}, None, {'field_a'})\n    data = {'field_a': 'test'}\n    assert v.validate_assignment(data, 'field_a', b'abc') == ({'field_a': 'abc'}, None, {'field_a'})\n    assert data == {'field_a': 'abc'}"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004465", "source": "def _run_git(self, *args: Any) -> str:\n        if self.debug:\n            print(f\"+ git -C {self.repo_dir} {' '.join(args)}\")\n        return _check_output([\"git\", \"-C\", self.repo_dir] + list(args))", "target": "def test_double_asterisks(self) -> None:\n        allowed_patterns = [\n            \"aten/src/ATen/native/**LinearAlgebra*\",\n        ]\n        patterns_re = patterns_to_regex(allowed_patterns)\n        fnames = [\n            \"aten/src/ATen/native/LinearAlgebra.cpp\",\n            \"aten/src/ATen/native/cpu/LinearAlgebraKernel.cpp\",\n        ]\n        for filename in fnames:\n            self.assertTrue(patterns_re.match(filename))"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004466", "source": "def rebuild_dataclass_fields(\n    cls: type[PydanticDataclass],\n    *,\n    config_wrapper: ConfigWrapper,\n    ns_resolver: NsResolver,\n    typevars_map: Mapping[TypeVar, Any],\n) -> dict[str, FieldInfo]:\n    FieldInfo_ = import_cached_field_info()\n    rebuilt_fields: dict[str, FieldInfo] = {}\n    with ns_resolver.push(cls):\n        for f_name, field_info in cls.__pydantic_fields__.items():\n            if field_info._complete:\n                rebuilt_fields[f_name] = field_info\n            else:\n                existing_desc = field_info.description\n                ann = _typing_extra.eval_type(\n                    field_info._original_annotation,\n                    *ns_resolver.types_namespace,\n                )\n                ann = _generics.replace_types(ann, typevars_map)\n                new_field = FieldInfo_.from_annotated_attribute(\n                    ann,\n                    field_info._original_assignment,\n                    _source=AnnotationSource.DATACLASS,\n                )\n                new_field.description = new_field.description if new_field.description is not None else existing_desc\n                update_field_from_config(config_wrapper, f_name, new_field)\n                rebuilt_fields[f_name] = new_field\n    return rebuilt_fields", "target": "def test_extra_behavior_ignore(\n    config: Union[core_schema.CoreConfig, None],\n    schema_extra_behavior_kw: dict[str, Any],\n    validate_fn_extra_kw: Union[ExtraBehavior, None],\n):\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            {'f': core_schema.model_field(core_schema.str_schema())}, **schema_extra_behavior_kw\n        ),\n        config=config,\n    )\n    m, model_extra, fields_set = v.validate_python({'f': 'x', 'extra_field': 123}, extra=validate_fn_extra_kw)\n    assert m == {'f': 'x'}\n    assert model_extra is None\n    assert fields_set == {'f'}\n    v.validate_assignment(m, 'f', 'y', extra=validate_fn_extra_kw)\n    assert m['f'] == 'y'\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_assignment(m, 'not_f', 'xyz', extra=validate_fn_extra_kw)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'no_such_attribute',\n            'loc': ('not_f',),\n            'msg': \"Object has no attribute 'not_f'\",\n            'input': 'xyz',\n            'ctx': {'attribute': 'not_f'},\n        }\n    ]\n    assert 'not_f' not in m"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004467", "source": "def _add_pydantic_validation_attributes(\n    dc_cls: Type['Dataclass'],\n    config: Type[BaseConfig],\n    validate_on_init: bool,\n    dc_cls_doc: str,\n) -> None:\n    init = dc_cls.__init__\n    @wraps(init)\n    def handle_extra_init(self: 'Dataclass', *args: Any, **kwargs: Any) -> None:\n        if config.extra == Extra.ignore:\n            init(self, *args, **{k: v for k, v in kwargs.items() if k in self.__dataclass_fields__})\n        elif config.extra == Extra.allow:\n            for k, v in kwargs.items():\n                self.__dict__.setdefault(k, v)\n            init(self, *args, **{k: v for k, v in kwargs.items() if k in self.__dataclass_fields__})\n        else:\n            init(self, *args, **kwargs)\n    if hasattr(dc_cls, '__post_init__'):\n        try:\n            post_init = dc_cls.__post_init__.__wrapped__\n        except AttributeError:\n            post_init = dc_cls.__post_init__\n        @wraps(post_init)\n        def new_post_init(self: 'Dataclass', *args: Any, **kwargs: Any) -> None:\n            if config.post_init_call == 'before_validation':\n                post_init(self, *args, **kwargs)\n            if self.__class__.__pydantic_run_validation__:\n                self.__pydantic_validate_values__()\n                if hasattr(self, '__post_init_post_parse__'):\n                    self.__post_init_post_parse__(*args, **kwargs)\n            if config.post_init_call == 'after_validation':\n                post_init(self, *args, **kwargs)\n        setattr(dc_cls, '__init__', handle_extra_init)\n        setattr(dc_cls, '__post_init__', new_post_init)\n    else:\n        @wraps(init)\n        def new_init(self: 'Dataclass', *args: Any, **kwargs: Any) -> None:\n            handle_extra_init(self, *args, **kwargs)\n            if self.__class__.__pydantic_run_validation__:\n                self.__pydantic_validate_values__()\n            if hasattr(self, '__post_init_post_parse__'):\n                initvars_and_values: Dict[str, Any] = {}\n                for i, f in enumerate(self.__class__.__dataclass_fields__.values()):\n                    if f._field_type is dataclasses._FIELD_INITVAR:\n                        try:\n                            initvars_and_values[f.name] = args[i]\n                        except IndexError:\n                            initvars_and_values[f.name] = kwargs.get(f.name, f.default)\n                self.__post_init_post_parse__(**initvars_and_values)\n        setattr(dc_cls, '__init__', new_init)\n    setattr(dc_cls, '__pydantic_run_validation__', ClassAttribute('__pydantic_run_validation__', validate_on_init))\n    setattr(dc_cls, '__pydantic_initialised__', False)\n    setattr(dc_cls, '__pydantic_model__', create_pydantic_model_from_dataclass(dc_cls, config, dc_cls_doc))\n    setattr(dc_cls, '__pydantic_validate_values__', _dataclass_validate_values)\n    setattr(dc_cls, '__validate__', classmethod(_validate_dataclass))\n    setattr(dc_cls, '__get_validators__', classmethod(_get_validators))\n    if dc_cls.__pydantic_model__.__config__.validate_assignment and not dc_cls.__dataclass_params__.frozen:\n        setattr(dc_cls, '__setattr__', _dataclass_validate_assignment_setattr)", "target": "def test_extra_behavior_ignore(\n    config: Union[core_schema.CoreConfig, None],\n    schema_extra_behavior_kw: dict[str, Any],\n    validate_fn_extra_kw: Union[ExtraBehavior, None],\n):\n    @dataclasses.dataclass\n    class MyModel:\n        f: str\n    v = SchemaValidator(\n        core_schema.dataclass_schema(\n            MyModel,\n            core_schema.dataclass_args_schema(\n                'MyModel', [core_schema.dataclass_field('f', core_schema.str_schema())], **schema_extra_behavior_kw\n            ),\n            ['f'],\n        ),\n        config=config,\n    )\n    m: MyModel = v.validate_python({'f': 'x', 'extra_field': 123}, extra=validate_fn_extra_kw)\n    assert m.f == 'x'\n    assert not hasattr(m, 'extra_field')\n    v.validate_assignment(m, 'f', 'y', extra=validate_fn_extra_kw)\n    assert m.f == 'y'\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_assignment(m, 'not_f', 'xyz', extra=validate_fn_extra_kw)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'no_such_attribute',\n            'loc': ('not_f',),\n            'msg': \"Object has no attribute 'not_f'\",\n            'input': 'xyz',\n            'ctx': {'attribute': 'not_f'},\n        }\n    ]\n    assert not hasattr(m, 'not_f')"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004468", "source": "def make_dataclass_validator(dc_cls: Type['Dataclass'], config: Type[BaseConfig]) -> 'CallableGenerator':\n    yield from _get_validators(dataclass(dc_cls, config=config, use_proxy=True))", "target": "def test_dataclass_self_init_alias_field_name():\n    @dataclasses.dataclass(init=False)\n    class Foo:\n        a: str\n        b: bool\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), validation_alias='aAlias'),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), validation_alias=['bAlias', 0]),\n            ],\n        ),\n        ['a', 'b'],\n        config={'loc_by_alias': False},\n    )\n    v = SchemaValidator(schema)\n    def __init__(self, *args, **kwargs):\n        v.validate_python(ArgsKwargs(args, kwargs), self_instance=self)\n    Foo.__init__ = __init__\n    foo = Foo(aAlias=b'hello', bAlias=['True'])\n    assert dataclasses.asdict(foo) == {'a': 'hello', 'b': True}\n    with pytest.raises(ValidationError) as exc_info:\n        Foo(aAlias=b'hello', bAlias=['wrong'])\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'bool_parsing',\n            'loc': ('b',),\n            'msg': 'Input should be a valid boolean, unable to interpret input',\n            'input': 'wrong',\n        }\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004469", "source": "def dataclass(\n        *,\n        init: Literal[False] = False,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool = False,\n        config: ConfigDict | type[object] | None = None,\n        validate_on_init: bool | None = None,\n        kw_only: bool = ...,\n        slots: bool = ...,\n    ) -> Callable[[type[_T]], type[PydanticDataclass]]:\n        ...", "target": "def test_dataclass_args_init_only(py_and_json: PyAndJson, input_value, expected):\n    schema = core_schema.dataclass_args_schema(\n        'MyDataclass',\n        [\n            core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), kw_only=False),\n            core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), kw_only=False, init_only=True),\n        ],\n        collect_init_only=True,\n    )\n    v = py_and_json(schema)\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)) as exc_info:\n            v.validate_test(input_value)\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        assert v.validate_test(input_value) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004470", "source": "def new_post_init(self: 'Dataclass', *args: Any, **kwargs: Any) -> None:\n            if config.post_init_call == 'before_validation':\n                post_init(self, *args, **kwargs)\n            if self.__class__.__pydantic_run_validation__:\n                self.__pydantic_validate_values__()\n                if hasattr(self, '__post_init_post_parse__'):\n                    self.__post_init_post_parse__(*args, **kwargs)\n            if config.post_init_call == 'after_validation':\n                post_init(self, *args, **kwargs)", "target": "def test_dataclass_self_init_alias():\n    @dataclasses.dataclass(init=False)\n    class Foo:\n        a: str\n        b: bool\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), validation_alias='aAlias'),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), validation_alias=['bAlias', 0]),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    def __init__(self, *args, **kwargs):\n        v.validate_python(ArgsKwargs(args, kwargs), self_instance=self)\n    Foo.__init__ = __init__\n    foo = Foo(aAlias=b'hello', bAlias=['True'])\n    assert dataclasses.is_dataclass(foo)\n    assert dataclasses.asdict(foo) == {'a': 'hello', 'b': True}\n    with pytest.raises(ValidationError) as exc_info:\n        Foo(aAlias=b'hello', bAlias=['wrong'])\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'bool_parsing',\n            'loc': ('bAlias', 0),\n            'msg': 'Input should be a valid boolean, unable to interpret input',\n            'input': 'wrong',\n        }\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004471", "source": "def dataclass(\n        _cls: Type[_T],\n        *,\n        init: bool = True,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool = False,\n        config: Union[ConfigDict, Type[object], None] = None,\n        validate_on_init: Optional[bool] = None,\n        use_proxy: Optional[bool] = None,\n    ) -> 'DataclassClassOrWrapper':\n        ...", "target": "def test_alias_allow_pop(py_and_json: PyAndJson):\n    schema = core_schema.dataclass_schema(\n        BasicDataclass,\n        core_schema.dataclass_args_schema(\n            'BasicDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), validation_alias='FieldA'),\n            ],\n        ),\n        ['a'],\n        config=core_schema.CoreConfig(validate_by_name=True, validate_by_alias=True),\n    )\n    v = py_and_json(schema)\n    assert v.validate_test({'FieldA': 'hello'}) == BasicDataclass(a='hello')\n    assert v.validate_test({'a': 'hello'}) == BasicDataclass(a='hello')\n    assert v.validate_test(\n        {\n            'FieldA': 'hello',\n            'a': 'world',\n        }\n    ) == BasicDataclass(a='hello')\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'foobar': 'hello'})"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004472", "source": "def rebuild_model_fields(\n    cls: type[BaseModel],\n    *,\n    config_wrapper: ConfigWrapper,\n    ns_resolver: NsResolver,\n    typevars_map: Mapping[TypeVar, Any],\n) -> dict[str, FieldInfo]:\n    rebuilt_fields: dict[str, FieldInfo] = {}\n    with ns_resolver.push(cls):\n        for f_name, field_info in cls.__pydantic_fields__.items():\n            if field_info._complete:\n                rebuilt_fields[f_name] = field_info\n            else:\n                new_field = _recreate_field_info(\n                    field_info, ns_resolver=ns_resolver, typevars_map=typevars_map, lenient=False\n                )\n                update_field_from_config(config_wrapper, f_name, new_field)\n                rebuilt_fields[f_name] = new_field\n    return rebuilt_fields", "target": "def test_on_error_bad_default(self):\n        with pytest.raises(SchemaError, match=\"'on_error = default' requires a `default` or `default_factory`\"):\n            SchemaValidator(\n                schema=core_schema.model_fields_schema(\n                    fields={\n                        'x': core_schema.model_field(\n                            schema=core_schema.with_default_schema(schema=core_schema.str_schema(), on_error='default')\n                        )\n                    }\n                )\n            )"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004473", "source": "def flatten(schema: Definition) -> FlatIntermediateDefinition:\n    result: FlatIntermediateDefinition = {}\n    _flatten(key_prefix=(), sub_schema=schema, result=result)\n    for k, v in result.items():\n        assert isinstance(k, tuple)\n        assert all(isinstance(ki, str) for ki in k)\n        assert isinstance(v, (TimerArgs, GroupedBenchmark))\n    return result", "target": "def test_double_asterisks(self) -> None:\n        allowed_patterns = [\n            \"aten/src/ATen/native/**LinearAlgebra*\",\n        ]\n        patterns_re = patterns_to_regex(allowed_patterns)\n        fnames = [\n            \"aten/src/ATen/native/LinearAlgebra.cpp\",\n            \"aten/src/ATen/native/cpu/LinearAlgebraKernel.cpp\",\n        ]\n        for filename in fnames:\n            self.assertTrue(patterns_re.match(filename))"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004474", "source": "def rebuild_dataclass_fields(\n    cls: type[PydanticDataclass],\n    *,\n    config_wrapper: ConfigWrapper,\n    ns_resolver: NsResolver,\n    typevars_map: Mapping[TypeVar, Any],\n) -> dict[str, FieldInfo]:\n    FieldInfo_ = import_cached_field_info()\n    rebuilt_fields: dict[str, FieldInfo] = {}\n    with ns_resolver.push(cls):\n        for f_name, field_info in cls.__pydantic_fields__.items():\n            if field_info._complete:\n                rebuilt_fields[f_name] = field_info\n            else:\n                existing_desc = field_info.description\n                ann = _typing_extra.eval_type(\n                    field_info._original_annotation,\n                    *ns_resolver.types_namespace,\n                )\n                ann = _generics.replace_types(ann, typevars_map)\n                new_field = FieldInfo_.from_annotated_attribute(\n                    ann,\n                    field_info._original_assignment,\n                    _source=AnnotationSource.DATACLASS,\n                )\n                new_field.description = new_field.description if new_field.description is not None else existing_desc\n                update_field_from_config(config_wrapper, f_name, new_field)\n                rebuilt_fields[f_name] = new_field\n    return rebuilt_fields", "target": "def test_from_attributes_path_error():\n    class PropertyError:\n        @property\n        def foo(self):\n            raise RuntimeError('intentional error')\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'my_field': core_schema.model_field(\n                    validation_alias=[['foo', 'bar', 'bat'], ['foo', 3], ['spam']], schema=core_schema.int_schema()\n                )\n            },\n            from_attributes=True,\n        )\n    )\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python(PropertyError())\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'get_attribute_error',\n            'loc': ('my_field',),\n            'msg': 'Error extracting attribute: RuntimeError: intentional error',\n            'input': HasRepr(IsStr(regex='.+PropertyError object at.+')),\n            'ctx': {'error': 'RuntimeError: intentional error'},\n        }\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004475", "source": "def _update_fields_from_docstrings(cls: type[Any], fields: dict[str, FieldInfo], use_inspect: bool = False) -> None:\n    fields_docs = extract_docstrings_from_cls(cls, use_inspect=use_inspect)\n    for ann_name, field_info in fields.items():\n        if field_info.description is None and ann_name in fields_docs:\n            field_info.description = fields_docs[ann_name]", "target": "def test_fields_required_by_default():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'x': core_schema.model_field(schema=core_schema.str_schema()),\n                'y': core_schema.model_field(schema=core_schema.str_schema()),\n            }\n        )\n    )\n    assert v.validate_python({'x': 'pika', 'y': 'chu'}) == ({'x': 'pika', 'y': 'chu'}, None, {'x', 'y'})\n    with pytest.raises(ValidationError) as exc_info:\n        assert v.validate_python({'x': 'pika'})\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'missing', 'loc': ('y',), 'msg': 'Field required', 'input': {'x': 'pika'}}\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004476", "source": "def _dataclass_validate_assignment_setattr(self: 'Dataclass', name: str, value: Any) -> None:\n    if self.__pydantic_initialised__:\n        d = dict(self.__dict__)\n        d.pop(name, None)\n        known_field = self.__pydantic_model__.__fields__.get(name, None)\n        if known_field:\n            value, error_ = known_field.validate(value, d, loc=name, cls=self.__class__)\n            if error_:\n                raise ValidationError([error_], self.__class__)\n    object.__setattr__(self, name, value)", "target": "def test_dataclass_wrap_json():\n    schema = core_schema.no_info_wrap_validator_function(\n        lambda v, handler: handler(v),\n        core_schema.dataclass_schema(\n            FooDataclass,\n            core_schema.dataclass_args_schema(\n                'FooDataclass',\n                [\n                    core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                    core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n                ],\n            ),\n            ['a', 'b'],\n        ),\n    )\n    v = SchemaValidator(schema)\n    assert v.validate_json('{\"a\": \"hello\", \"b\": true}') == FooDataclass(a='hello', b=True)\n    assert v.validate_json('{\"a\": \"hello\", \"b\": true}', strict=True) == FooDataclass(a='hello', b=True)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004477", "source": "def new_init(self: 'Dataclass', *args: Any, **kwargs: Any) -> None:\n            handle_extra_init(self, *args, **kwargs)\n            if self.__class__.__pydantic_run_validation__:\n                self.__pydantic_validate_values__()\n            if hasattr(self, '__post_init_post_parse__'):\n                initvars_and_values: Dict[str, Any] = {}\n                for i, f in enumerate(self.__class__.__dataclass_fields__.values()):\n                    if f._field_type is dataclasses._FIELD_INITVAR:\n                        try:\n                            initvars_and_values[f.name] = args[i]\n                        except IndexError:\n                            initvars_and_values[f.name] = kwargs.get(f.name, f.default)\n                self.__post_init_post_parse__(**initvars_and_values)", "target": "def test_leak_dataclass(validator):\n    def fn():\n        @dataclasses.dataclass\n        class Dataclass:\n            a: int\n            @classmethod\n            def _validator(cls, v, info):\n                return v\n            @classmethod\n            def _wrap_validator(cls, v, validator, info):\n                return validator(v)\n        field_schema = core_schema.int_schema()\n        if validator == 'field':\n            field_schema = core_schema.with_info_before_validator_function(Dataclass._validator, field_schema)\n            field_schema = core_schema.with_info_wrap_validator_function(Dataclass._wrap_validator, field_schema)\n            field_schema = core_schema.with_info_after_validator_function(Dataclass._validator, field_schema)\n        dataclass_schema = core_schema.dataclass_schema(\n            Dataclass,\n            core_schema.dataclass_args_schema('Dataclass', [core_schema.dataclass_field('a', field_schema)]),\n            ['a'],\n        )\n        if validator == 'dataclass':\n            dataclass_schema = core_schema.with_info_before_validator_function(Dataclass._validator, dataclass_schema)\n            dataclass_schema = core_schema.with_info_wrap_validator_function(\n                Dataclass._wrap_validator, dataclass_schema\n            )\n            dataclass_schema = core_schema.with_info_after_validator_function(Dataclass._validator, dataclass_schema)\n        Dataclass.__pydantic_validator__ = SchemaValidator(dataclass_schema)\n        return Dataclass\n    klass = fn()\n    ref = weakref.ref(klass)\n    assert ref() is not None\n    del klass\n    assert_gc(lambda: ref() is None)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004478", "source": "def is_builtin_dataclass(_cls: Type[Any]) -> bool:\n    return (\n        dataclasses.is_dataclass(_cls)\n        and not hasattr(_cls, '__pydantic_model__')\n        and set(_cls.__dataclass_fields__).issuperset(set(getattr(_cls, '__annotations__', {})))\n    )", "target": "def test_dataclass_args_init_with_default(input_value, extra_behavior, expected):\n    @dataclasses.dataclass\n    class Foo:\n        a: str\n        b: str\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(\n                    name='b',\n                    schema=core_schema.with_default_schema(schema=core_schema.str_schema(), default='HELLO'),\n                    init=False,\n                ),\n            ],\n            extra_behavior=extra_behavior,\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)) as exc_info:\n            v.validate_python(input_value)\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        assert dataclasses.asdict(v.validate_python(input_value)) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004479", "source": "def rebuild_dataclass_fields(\n    cls: type[PydanticDataclass],\n    *,\n    config_wrapper: ConfigWrapper,\n    ns_resolver: NsResolver,\n    typevars_map: Mapping[TypeVar, Any],\n) -> dict[str, FieldInfo]:\n    FieldInfo_ = import_cached_field_info()\n    rebuilt_fields: dict[str, FieldInfo] = {}\n    with ns_resolver.push(cls):\n        for f_name, field_info in cls.__pydantic_fields__.items():\n            if field_info._complete:\n                rebuilt_fields[f_name] = field_info\n            else:\n                existing_desc = field_info.description\n                ann = _typing_extra.eval_type(\n                    field_info._original_annotation,\n                    *ns_resolver.types_namespace,\n                )\n                ann = _generics.replace_types(ann, typevars_map)\n                new_field = FieldInfo_.from_annotated_attribute(\n                    ann,\n                    field_info._original_assignment,\n                    _source=AnnotationSource.DATACLASS,\n                )\n                new_field.description = new_field.description if new_field.description is not None else existing_desc\n                update_field_from_config(config_wrapper, f_name, new_field)\n                rebuilt_fields[f_name] = new_field\n    return rebuilt_fields", "target": "def test_model_fields_deep():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'field_a': core_schema.model_field(schema=core_schema.str_schema()),\n                'field_b': core_schema.model_field(\n                    schema=core_schema.model_fields_schema(\n                        fields={\n                            'field_c': core_schema.model_field(schema=core_schema.str_schema()),\n                            'field_d': core_schema.model_field(\n                                schema=core_schema.model_fields_schema(\n                                    fields={\n                                        'field_e': core_schema.model_field(schema=core_schema.str_schema()),\n                                        'field_f': core_schema.model_field(schema=core_schema.int_schema()),\n                                    }\n                                )\n                            ),\n                        }\n                    )\n                ),\n            }\n        )\n    )\n    model_dict, model_extra, fields_set = v.validate_python(\n        {'field_a': '1', 'field_b': {'field_c': '2', 'field_d': {'field_e': '4', 'field_f': 4}}}\n    )\n    assert model_dict == {\n        'field_a': '1',\n        'field_b': (\n            {'field_c': '2', 'field_d': ({'field_e': '4', 'field_f': 4}, None, {'field_f', 'field_e'})},\n            None,\n            {'field_d', 'field_c'},\n        ),\n    }\n    assert model_extra is None\n    assert fields_set == {'field_a', 'field_b'}\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python({'field_a': '1', 'field_b': {'field_c': '2', 'field_d': {'field_e': '4', 'field_f': 'xx'}}})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('field_b', 'field_d', 'field_f'),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'xx',\n        }\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004480", "source": "def is_valid_privateattr_name(name: str) -> bool:\n    return name.startswith('_') and not name.startswith('__')", "target": "def test_empty_model():\n    v = SchemaValidator(core_schema.model_fields_schema(fields={}))\n    assert v.validate_python({}) == ({}, None, set())\n    with pytest.raises(\n        ValidationError, match=re.escape('Input should be a valid dictionary or instance of Model [type=model_type,')\n    ):\n        v.validate_python('x')"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004481", "source": "def pydantic_encoder(obj: Any) -> Any:\n    from dataclasses import asdict, is_dataclass\n    from pydantic.v1.main import BaseModel\n    if isinstance(obj, BaseModel):\n        return obj.dict()\n    elif is_dataclass(obj):\n        return asdict(obj)\n    for base in obj.__class__.__mro__[:-1]:\n        try:\n            encoder = ENCODERS_BY_TYPE[base]\n        except KeyError:\n            continue\n        return encoder(obj)\n    else:\n        raise TypeError(f\"Object of type '{obj.__class__.__name__}' is not JSON serializable\")", "target": "def test_error_loc():\n    v = SchemaValidator(\n        core_schema.typed_dict_schema(\n            fields={\n                'field_a': core_schema.typed_dict_field(\n                    schema=core_schema.list_schema(items_schema=core_schema.int_schema())\n                )\n            },\n            extras_schema=core_schema.int_schema(),\n            extra_behavior='allow',\n        )\n    )\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_json('{\"field_a\": [1, 2, \"wrong\"]}')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('field_a', 2),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'wrong',\n        }\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004482", "source": "def benchmark_using_throughput_benchmark(config, module):\n    print(\"Benchmarking via ThroughputBenchmark\")\n    bench = ThroughputBenchmark(module.module)\n    bench.add_input(*module.tensor_inputs)\n    stats = bench.benchmark(1, config.num_warmup_iters, config.num_iters)\n    return stats.latency_avg_ms / NUM_LOOP_ITERS", "target": "def test_ghstack_branches_in_sync(self) -> None:\n        head_ref = \"gh/SS-JIA/206/head\"\n        self._skip_if_ref_does_not_exist(head_ref)\n        self.assertTrue(are_ghstack_branches_in_sync(self.repo, head_ref))"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004483", "source": "def create_pydantic_model_from_dataclass(\n    dc_cls: Type['Dataclass'],\n    config: Type[Any] = BaseConfig,\n    dc_cls_doc: Optional[str] = None,\n) -> Type['BaseModel']:\n    field_definitions: Dict[str, Any] = {}\n    for field in dataclasses.fields(dc_cls):\n        default: Any = Undefined\n        default_factory: Optional['NoArgAnyCallable'] = None\n        field_info: FieldInfo\n        if field.default is not dataclasses.MISSING:\n            default = field.default\n        elif field.default_factory is not dataclasses.MISSING:\n            default_factory = field.default_factory\n        else:\n            default = Required\n        if isinstance(default, FieldInfo):\n            field_info = default\n            dc_cls.__pydantic_has_field_info_default__ = True\n        else:\n            field_info = Field(default=default, default_factory=default_factory, **field.metadata)\n        field_definitions[field.name] = (field.type, field_info)\n    validators = gather_all_validators(dc_cls)\n    model: Type['BaseModel'] = create_model(\n        dc_cls.__name__,\n        __config__=config,\n        __module__=dc_cls.__module__,\n        __validators__=validators,\n        __cls_kwargs__={'__resolve_forward_refs__': False},\n        **field_definitions,\n    )\n    model.__doc__ = dc_cls_doc if dc_cls_doc is not None else dc_cls.__doc__ or ''\n    return model", "target": "def test_dataclass_slots_field_before_validator():\n    @dataclasses.dataclass(slots=True)\n    class Foo:\n        a: int\n        b: str\n        @classmethod\n        def validate_b(cls, v: bytes, info: core_schema.ValidationInfo) -> bytes:\n            assert v == b'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return b'hello world!'\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(\n                    name='b',\n                    schema=core_schema.with_info_before_validator_function(Foo.validate_b, core_schema.str_schema()),\n                ),\n            ],\n        ),\n        ['a', 'b'],\n        slots=True,\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 1, 'b': b'hello'})\n    assert dataclasses.asdict(foo) == {'a': 1, 'b': 'hello world!'}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004484", "source": "def is_builtin_dataclass(_cls: Type[Any]) -> bool:\n    return (\n        dataclasses.is_dataclass(_cls)\n        and not hasattr(_cls, '__pydantic_model__')\n        and set(_cls.__dataclass_fields__).issuperset(set(getattr(_cls, '__annotations__', {})))\n    )", "target": "def test_dataclass_self_init():\n    @dataclasses.dataclass(init=False)\n    class Foo:\n        a: str\n        b: bool\n        def __init__(self, *args, **kwargs):\n            v.validate_python(ArgsKwargs(args, kwargs), self_instance=self)\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), kw_only=False),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), kw_only=False),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    foo = Foo(b'hello', 'True')\n    assert dataclasses.is_dataclass(foo)\n    assert dataclasses.asdict(foo) == {'a': 'hello', 'b': True}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004485", "source": "def push(self, config_wrapper: ConfigWrapper | ConfigDict | None):\n        if config_wrapper is None:\n            yield\n            return\n        if not isinstance(config_wrapper, ConfigWrapper):\n            config_wrapper = ConfigWrapper(config_wrapper, check=False)\n        self._config_wrapper_stack.append(config_wrapper)\n        try:\n            yield\n        finally:\n            self._config_wrapper_stack.pop()", "target": "def test_field_priority_model():\n    v = SchemaValidator(\n        cs.model_schema(\n            cls=MyModel,\n            config=CoreConfig(str_max_length=10),\n            schema=cs.model_fields_schema(fields={'f': cs.model_field(schema=cs.str_schema(max_length=5))}),\n        )\n    )\n    assert 'max_length:Some(5)' in plain_repr(v)\n    assert v.isinstance_python({'f': 'test'}) is True\n    assert v.isinstance_python({'f': 'test long'}) is False"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004486", "source": "def takes_validated_data_argument(\n    default_factory: Callable[[], Any] | Callable[[dict[str, Any]], Any],\n) -> TypeIs[Callable[[dict[str, Any]], Any]]:\n    try:\n        sig = signature(default_factory)\n    except (ValueError, TypeError):\n        return False\n    parameters = list(sig.parameters.values())\n    return len(parameters) == 1 and can_be_positional(parameters[0]) and parameters[0].default is Parameter.empty", "target": "def test_by_alias_and_name_config_interaction(\n    config_by_alias: Union[bool, None],\n    config_by_name: Union[bool, None],\n    runtime_by_alias: Union[bool, None],\n    runtime_by_name: Union[bool, None],\n) -> None:\n    if config_by_alias is False and config_by_name is False and runtime_by_alias is False and runtime_by_name is False:\n        pytest.skip(\"Can't have both by_alias and by_name as effectively False\")\n    class Model:\n        def __init__(self, my_field: int) -> None:\n            self.my_field = my_field\n    core_config = {\n        **({'validate_by_alias': config_by_alias} if config_by_alias is not None else {}),\n        **({'validate_by_name': config_by_name} if config_by_name is not None else {}),\n    }\n    schema = core_schema.model_schema(\n        Model,\n        core_schema.model_fields_schema(\n            {\n                'my_field': core_schema.model_field(core_schema.int_schema(), validation_alias='my_alias'),\n            }\n        ),\n        config=core_schema.CoreConfig(**core_config),\n    )\n    s = SchemaValidator(schema)\n    alias_allowed = next(x for x in (runtime_by_alias, config_by_alias, True) if x is not None)\n    name_allowed = next(x for x in (runtime_by_name, config_by_name, False) if x is not None)\n    if alias_allowed:\n        assert s.validate_python({'my_alias': 1}, by_alias=runtime_by_alias, by_name=runtime_by_name).my_field == 1\n    if name_allowed:\n        assert s.validate_python({'my_field': 1}, by_alias=runtime_by_alias, by_name=runtime_by_name).my_field == 1"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004487", "source": "def get_origin(tp: Type[Any]) -> Optional[Type[Any]]:\n        if type(tp).__name__ in AnnotatedTypeNames:\n            return cast(Type[Any], Annotated)\n        return _typing_get_origin(tp) or getattr(tp, '__origin__', None)", "target": "def test_correct_function_signature() -> None:\n    def my_validator(value: Any, info: Any) -> str:\n        return str(value)\n    v = SchemaValidator(core_schema.with_info_plain_validator_function(my_validator))\n    assert v.validate_python(1) == '1'"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004488", "source": "def as_dataclass_field(pydantic_field: FieldInfo) -> dataclasses.Field[Any]:\n    field_args: dict[str, Any] = {'default': pydantic_field}\n    if sys.version_info >= (3, 14) and pydantic_field.description is not None:\n        field_args['doc'] = pydantic_field.description\n    if sys.version_info >= (3, 10) and pydantic_field.kw_only:\n        field_args['kw_only'] = True\n    if pydantic_field.repr is not True:\n        field_args['repr'] = pydantic_field.repr\n    return dataclasses.field(**field_args)", "target": "def test_dataclass_post_init():\n    @dataclasses.dataclass\n    class Foo:\n        a: str\n        b: bool\n        def __post_init__(self):\n            self.a = self.a.upper()\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n            ],\n        ),\n        ['a', 'b'],\n        post_init=True,\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 'hello', 'b': True})\n    assert foo.a == 'HELLO'\n    assert foo.b is True"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004489", "source": "def rebuild_dataclass_fields(\n    cls: type[PydanticDataclass],\n    *,\n    config_wrapper: ConfigWrapper,\n    ns_resolver: NsResolver,\n    typevars_map: Mapping[TypeVar, Any],\n) -> dict[str, FieldInfo]:\n    FieldInfo_ = import_cached_field_info()\n    rebuilt_fields: dict[str, FieldInfo] = {}\n    with ns_resolver.push(cls):\n        for f_name, field_info in cls.__pydantic_fields__.items():\n            if field_info._complete:\n                rebuilt_fields[f_name] = field_info\n            else:\n                existing_desc = field_info.description\n                ann = _typing_extra.eval_type(\n                    field_info._original_annotation,\n                    *ns_resolver.types_namespace,\n                )\n                ann = _generics.replace_types(ann, typevars_map)\n                new_field = FieldInfo_.from_annotated_attribute(\n                    ann,\n                    field_info._original_assignment,\n                    _source=AnnotationSource.DATACLASS,\n                )\n                new_field.description = new_field.description if new_field.description is not None else existing_desc\n                update_field_from_config(config_wrapper, f_name, new_field)\n                rebuilt_fields[f_name] = new_field\n    return rebuilt_fields", "target": "def test_frozen_field():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'name': core_schema.model_field(schema=core_schema.str_schema()),\n                'age': core_schema.model_field(schema=core_schema.int_schema()),\n                'is_developer': core_schema.model_field(\n                    schema=core_schema.with_default_schema(schema=core_schema.bool_schema(), default=True), frozen=True\n                ),\n            }\n        )\n    )\n    r1, model_extra, fields_set = v.validate_python({'name': 'Samuel', 'age': '36'})\n    assert r1 == {'name': 'Samuel', 'age': 36, 'is_developer': True}\n    assert model_extra is None\n    assert fields_set == {'name', 'age'}\n    v.validate_assignment(r1, 'age', '35')\n    assert r1 == {'name': 'Samuel', 'age': 35, 'is_developer': True}\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_assignment(r1, 'is_developer', False)\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'frozen_field', 'loc': ('is_developer',), 'msg': 'Field is frozen', 'input': False}\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004490", "source": "def benchmark_using_throughput_benchmark(config, module):\n    print(\"Benchmarking via ThroughputBenchmark\")\n    bench = ThroughputBenchmark(module.module)\n    bench.add_input(*module.tensor_inputs)\n    stats = bench.benchmark(1, config.num_warmup_iters, config.num_iters)\n    return stats.latency_avg_ms / NUM_LOOP_ITERS", "target": "def test_peek(self, input_: str = \"abcdef\") -> None:\n        iter_ = PeekableIterator(input_)\n        for idx, c in enumerate(iter_):\n            if idx + 1 < len(input_):\n                self.assertEqual(iter_.peek(), input_[idx + 1])\n            else:\n                self.assertTrue(iter_.peek() is None)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004491", "source": "def _pydantic_fields_complete(cls: type[PydanticDataclass]) -> bool:\n    return all(field_info._complete for field_info in cls.__pydantic_fields__.values())", "target": "def test_dataclass_initvar_not_required_on_union_ser() -> None:\n    @dataclasses.dataclass\n    class Foo:\n        x: int\n        init_var: dataclasses.InitVar[int] = 1\n    @dataclasses.dataclass\n    class Bar:\n        x: int\n    schema = core_schema.union_schema(\n        [\n            core_schema.dataclass_schema(\n                Foo,\n                core_schema.dataclass_args_schema(\n                    'Foo',\n                    [\n                        core_schema.dataclass_field(name='x', schema=core_schema.int_schema()),\n                        core_schema.dataclass_field(\n                            name='init_var',\n                            init_only=True,\n                            schema=core_schema.with_default_schema(core_schema.int_schema(), default=1),\n                        ),\n                    ],\n                ),\n                ['x'],\n                post_init=True,\n            ),\n            core_schema.dataclass_schema(\n                Bar,\n                core_schema.dataclass_args_schema(\n                    'Bar', [core_schema.dataclass_field(name='x', schema=core_schema.int_schema())]\n                ),\n                ['x'],\n            ),\n        ]\n    )\n    s = SchemaSerializer(schema)\n    assert s.to_python(Foo(x=1), warnings='error') == {'x': 1}\n    assert s.to_python(Foo(x=1, init_var=2), warnings='error') == {'x': 1}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004492", "source": "def set_dataclass_fields(\n    cls: type[StandardDataclass],\n    config_wrapper: _config.ConfigWrapper,\n    ns_resolver: NsResolver | None = None,\n) -> None:\n    typevars_map = get_standard_typevars_map(cls)\n    fields = collect_dataclass_fields(\n        cls, ns_resolver=ns_resolver, typevars_map=typevars_map, config_wrapper=config_wrapper\n    )\n    cls.__pydantic_fields__ = fields", "target": "def test_dataclass_self_init_alias_field_name():\n    @dataclasses.dataclass(init=False)\n    class Foo:\n        a: str\n        b: bool\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), validation_alias='aAlias'),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), validation_alias=['bAlias', 0]),\n            ],\n        ),\n        ['a', 'b'],\n        config={'loc_by_alias': False},\n    )\n    v = SchemaValidator(schema)\n    def __init__(self, *args, **kwargs):\n        v.validate_python(ArgsKwargs(args, kwargs), self_instance=self)\n    Foo.__init__ = __init__\n    foo = Foo(aAlias=b'hello', bAlias=['True'])\n    assert dataclasses.asdict(foo) == {'a': 'hello', 'b': True}\n    with pytest.raises(ValidationError) as exc_info:\n        Foo(aAlias=b'hello', bAlias=['wrong'])\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'bool_parsing',\n            'loc': ('b',),\n            'msg': 'Input should be a valid boolean, unable to interpret input',\n            'input': 'wrong',\n        }\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004493", "source": "def _general_metadata_cls() -> type[BaseMetadata]:\n    from annotated_types import BaseMetadata\n    class _PydanticGeneralMetadata(PydanticMetadata, BaseMetadata):\n        def __init__(self, metadata: Any):\n            self.__dict__ = metadata\n    return _PydanticGeneralMetadata", "target": "def test_json_error():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'field_a': core_schema.model_field(\n                    schema=core_schema.list_schema(items_schema=core_schema.int_schema())\n                )\n            }\n        )\n    )\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_json('{\"field_a\": [123, \"wrong\"]}')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('field_a', 1),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'wrong',\n        }\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004494", "source": "def _pydantic_fields_complete(cls: type[PydanticDataclass]) -> bool:\n    return all(field_info._complete for field_info in cls.__pydantic_fields__.values())", "target": "def test_dataclass_self_init():\n    @dataclasses.dataclass(init=False)\n    class Foo:\n        a: str\n        b: bool\n        def __init__(self, *args, **kwargs):\n            v.validate_python(ArgsKwargs(args, kwargs), self_instance=self)\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), kw_only=False),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), kw_only=False),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    foo = Foo(b'hello', 'True')\n    assert dataclasses.is_dataclass(foo)\n    assert dataclasses.asdict(foo) == {'a': 'hello', 'b': True}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004495", "source": "def dataclass(\n        _cls: Type[_T],\n        *,\n        init: bool = True,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool = False,\n        config: Union[ConfigDict, Type[object], None] = None,\n        validate_on_init: Optional[bool] = None,\n        use_proxy: Optional[bool] = None,\n        kw_only: bool = ...,\n    ) -> 'DataclassClassOrWrapper':\n        ...", "target": "def test_dataclass():\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bytes_schema()),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    s = SchemaSerializer(schema)\n    assert s.to_python(Foo(a='hello', b=b'more')) == IsStrictDict(a='hello', b=b'more')\n    assert s.to_python(Foo(a='hello', b=b'more'), mode='json') == IsStrictDict(a='hello', b='more')\n    j = s.to_json(Foo(a='hello', b=b'more'))\n    if on_pypy:\n        assert json.loads(j) == {'a': 'hello', 'b': 'more'}\n    else:\n        assert j == b'{\"a\":\"hello\",\"b\":\"more\"}'\n    assert s.to_python(Foo(a='hello', b=b'more'), exclude={'b'}) == IsStrictDict(a='hello')\n    assert s.to_json(Foo(a='hello', b=b'more'), include={'a'}) == b'{\"a\":\"hello\"}'"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004496", "source": "def _set_nested_attr(obj: nn.Module, names: list[str], value: Tensor) -> None:\n    if len(names) == 1:\n        setattr(obj, names[0], value)\n    else:\n        _set_nested_attr(getattr(obj, names[0]), names[1:], value)", "target": "def test_double_asterisks(self) -> None:\n        allowed_patterns = [\n            \"aten/src/ATen/native/**LinearAlgebra*\",\n        ]\n        patterns_re = patterns_to_regex(allowed_patterns)\n        fnames = [\n            \"aten/src/ATen/native/LinearAlgebra.cpp\",\n            \"aten/src/ATen/native/cpu/LinearAlgebraKernel.cpp\",\n        ]\n        for filename in fnames:\n            self.assertTrue(patterns_re.match(filename))"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004497", "source": "def _check_finalvar(v: Optional[Type[Any]]) -> bool:\n    if v is None:\n        return False\n    return v.__class__ == Final.__class__ and (sys.version_info < (3, 8) or getattr(v, '_name', None) == 'Final')", "target": "def test_type_error():\n    try:\n        PydanticKnownError('foobar')\n    except KeyError as exc:\n        assert str(exc) == '\"Invalid error type: \\'foobar\\'\"'\n    else:\n        raise AssertionError(\"PydanticKnownError('foobar') did not raise KeyError\")\n    e = PydanticKnownError('recursion_loop')\n    assert isinstance(e, PydanticKnownError)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004498", "source": "def tail(self) -> ConfigWrapper:\n        return self._config_wrapper_stack[-1]", "target": "def test_on_field():\n    v = SchemaValidator(cs.str_schema(min_length=2, max_length=5))\n    r = plain_repr(v)\n    assert 'min_length:Some(2)' in r\n    assert 'max_length:Some(5)' in r\n    assert v.isinstance_python('test') is True\n    assert v.isinstance_python('test long') is False"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004499", "source": "def display_as_type(v: Type[Any]) -> str:\n    if not isinstance(v, typing_base) and not isinstance(v, WithArgsTypes) and not isinstance(v, type):\n        v = v.__class__\n    if is_union(get_origin(v)):\n        return f'Union[{\", \".join(map(display_as_type, get_args(v)))}]'\n    if isinstance(v, WithArgsTypes):\n        return str(v).replace('typing.', '')\n    try:\n        return v.__name__\n    except AttributeError:\n        return str(v).replace('typing.', '')", "target": "def test_schema_typing() -> None:\n    schema: CoreSchema = {\n        'type': 'union',\n        'choices': [{'type': 'int'}, {'type': 'int', 'ge': 1}, {'type': 'float', 'lt': 1.0}],\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = {\n        'type': 'tagged-union',\n        'discriminator': 'type',\n        'choices': {\n            'apple': {\n                'type': 'typed-dict',\n                'fields': {'pips': {'type': 'typed-dict-field', 'schema': {'type': 'int'}}},\n            },\n            'banana': {\n                'type': 'typed-dict',\n                'fields': {'curvature': {'type': 'typed-dict-field', 'schema': {'type': 'float'}}},\n            },\n        },\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'int', 'ge': 1}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'float', 'lt': 1.0}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'str', 'pattern': r'http://.*'}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'bool', 'strict': False}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'literal', 'expected': [1, '1']}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'any'}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'none'}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'bytes'}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'list', 'items_schema': {'type': 'str'}, 'min_length': 3}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'set', 'items_schema': {'type': 'str'}, 'max_length': 3}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'tuple', 'items_schema': [{'type': 'str'}], 'variadic_item_index': 0, 'max_length': 3}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'tuple', 'items_schema': [{'type': 'str'}, {'type': 'int'}]}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'frozenset', 'items_schema': {'type': 'str'}, 'max_length': 3}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'dict', 'keys_schema': {'type': 'str'}, 'values_schema': {'type': 'any'}}\n    SchemaValidator(schema)\n    schema: CoreSchema = {\n        'type': 'typed-dict',\n        'fields': {'bar': {'type': 'typed-dict-field', 'schema': {'type': 'str'}}},\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = {\n        'type': 'model',\n        'cls': Foo,\n        'schema': {'type': 'model-fields', 'fields': {'bar': {'type': 'model-field', 'schema': {'type': 'str'}}}},\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = {\n        'type': 'typed-dict',\n        'fields': {\n            'a': {'type': 'typed-dict-field', 'schema': {'type': 'str'}},\n            'b': {'type': 'typed-dict-field', 'schema': {'type': 'str'}, 'validation_alias': 'foobar'},\n            'c': {\n                'type': 'typed-dict-field',\n                'schema': {'type': 'str'},\n                'validation_alias': [['foobar', 0, 'bar'], ['foo']],\n            },\n            'd': {\n                'type': 'typed-dict-field',\n                'schema': {'type': 'default', 'schema': {'type': 'str'}, 'default': 'spam'},\n            },\n        },\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = {\n        'type': 'function-wrap',\n        'function': {'type': 'with-info', 'function': wrap_validator, 'field_name': 'foobar'},\n        'schema': {'type': 'str'},\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = core_schema.with_info_plain_validator_function(validator)\n    SchemaValidator(schema)\n    schema: CoreSchema = {\n        'type': 'definitions',\n        'schema': {'type': 'definition-ref', 'schema_ref': 'Branch'},\n        'definitions': [\n            {\n                'type': 'typed-dict',\n                'fields': {\n                    'name': {'type': 'typed-dict-field', 'schema': {'type': 'str'}},\n                    'sub_branch': {\n                        'type': 'typed-dict-field',\n                        'schema': {\n                            'type': 'default',\n                            'schema': {\n                                'type': 'nullable',\n                                'schema': {'type': 'definition-ref', 'schema_ref': 'Branch'},\n                            },\n                            'default': None,\n                        },\n                    },\n                },\n                'ref': 'Branch',\n            }\n        ],\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'date', 'le': date.today()}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'time', 'lt': time(12, 13, 14)}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'datetime', 'ge': datetime.now()}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'is-instance', 'cls': Foo}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'callable'}\n    SchemaValidator(schema)\n    schema: CoreSchema = {\n        'type': 'arguments',\n        'arguments_schema': [\n            {'name': 'a', 'mode': 'positional_only', 'schema': {'type': 'int'}},\n            {'name': 'b', 'schema': {'type': 'str'}},\n            {'name': 'c', 'mode': 'keyword_only', 'schema': {'type': 'bool'}},\n        ],\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'call', 'arguments_schema': {'type': 'any'}, 'function': foo}\n    SchemaValidator(schema)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004500", "source": "def dataclass(\n        *,\n        init: Literal[False] = False,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool | None = None,\n        config: ConfigDict | type[object] | None = None,\n        validate_on_init: bool | None = None,\n    ) -> Callable[[type[_T]], type[PydanticDataclass]]:\n        ...", "target": "def test_dataclass_json_duplicate_keys():\n    @dataclasses.dataclass\n    class MyDataclass:\n        name: str\n        age: int\n    schema = core_schema.dataclass_schema(\n        MyDataclass,\n        core_schema.dataclass_args_schema(\n            'MyDataclass',\n            [\n                core_schema.dataclass_field(name='name', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='age', schema=core_schema.int_schema()),\n            ],\n        ),\n        ['name', 'age'],\n    )\n    v = SchemaValidator(schema)\n    json_with_duplicates = '{\"name\": \"Alice\", \"age\": 30, \"name\": \"Bob\", \"age\": 25}'\n    result = v.validate_json(json_with_duplicates)\n    assert result.name == 'Bob', \"Last value for 'name' should win\"\n    assert result.age == 25, \"Last value for 'age' should win\"\n    assert dataclasses.asdict(result) == {'name': 'Bob', 'age': 25}\n    json_multiple_duplicates = '{\"name\": \"First\", \"age\": 1, \"name\": \"Second\", \"name\": \"Third\", \"age\": 3}'\n    result2 = v.validate_json(json_multiple_duplicates)\n    assert result2.name == 'Third', 'Last value among multiple duplicates should win'\n    assert result2.age == 3\n    assert dataclasses.asdict(result2) == {'name': 'Third', 'age': 3}"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004501", "source": "def load_weights(mod: nn.Module, names: list[str], params: tuple[Tensor, ...]) -> None:\n    for name, p in zip(names, params):\n        _set_nested_attr(mod, name.split(\".\"), p)", "target": "def test_pr_with_release_notes_label(\n        self, mocked_rn_labels: Any, mocked_gql: Any\n    ) -> None:\n        \"Test PR with 'release notes: nn' label\"\n        pr = GitHubPR(\"pytorch\", \"pytorch\", 71759)\n        self.assertTrue(has_required_labels(pr))"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004502", "source": "def set_dataclass_fields(\n    cls: type[StandardDataclass],\n    config_wrapper: _config.ConfigWrapper,\n    ns_resolver: NsResolver | None = None,\n) -> None:\n    typevars_map = get_standard_typevars_map(cls)\n    fields = collect_dataclass_fields(\n        cls, ns_resolver=ns_resolver, typevars_map=typevars_map, config_wrapper=config_wrapper\n    )\n    cls.__pydantic_fields__ = fields", "target": "def test_serialization_alias():\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bytes_schema(), serialization_alias='BAR'),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    s = SchemaSerializer(schema)\n    assert s.to_python(Foo(a='hello', b=b'more'), by_alias=True) == IsStrictDict(a='hello', BAR=b'more')\n    assert s.to_python(Foo(a='hello', b=b'more'), mode='json', by_alias=True) == IsStrictDict(a='hello', BAR='more')\n    j = s.to_json(Foo(a='hello', b=b'more'), by_alias=True)\n    if on_pypy:\n        assert json.loads(j) == {'a': 'hello', 'BAR': 'more'}\n    else:\n        assert j == b'{\"a\":\"hello\",\"BAR\":\"more\"}'"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004503", "source": "def display_as_type(v: Type[Any]) -> str:\n    if not isinstance(v, typing_base) and not isinstance(v, WithArgsTypes) and not isinstance(v, type):\n        v = v.__class__\n    if is_union(get_origin(v)):\n        return f'Union[{\", \".join(map(display_as_type, get_args(v)))}]'\n    if isinstance(v, WithArgsTypes):\n        return str(v).replace('typing.', '')\n    try:\n        return v.__name__\n    except AttributeError:\n        return str(v).replace('typing.', '')", "target": "def test_schema_validator_wrong() -> None:\n    try:\n        SchemaValidator({'type': 'bad'})\n    except SchemaError:\n        pass\n    else:\n        raise AssertionError('SchemaValidator did not raise SchemaError')"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004504", "source": "def new_type_supertype(type_: Type[Any]) -> Type[Any]:\n    while hasattr(type_, '__supertype__'):\n        type_ = type_.__supertype__\n    return type_", "target": "def test_schema_typing_error() -> None:\n    _: CoreSchema = {'type': 'wrong'}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004505", "source": "def _is_field_cached_property(obj: 'Dataclass', k: str) -> bool:\n        return False", "target": "def test_serialization_exclude():\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(\n                    name='a', schema=core_schema.str_schema(), serialization_exclude_if=lambda x: x == 'bye'\n                ),\n                core_schema.dataclass_field(name='b', schema=core_schema.bytes_schema(), serialization_exclude=True),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    s = SchemaSerializer(schema)\n    assert s.to_python(Foo(a='hello', b=b'more')) == {'a': 'hello'}\n    assert s.to_python(Foo(a='hello', b=b'more'), mode='json') == {'a': 'hello'}\n    assert s.to_python(Foo(a='bye', b=b'more'), mode='json') == {}\n    j = s.to_json(Foo(a='hello', b=b'more'))\n    if on_pypy:\n        assert json.loads(j) == {'a': 'hello'}\n    else:\n        assert j == b'{\"a\":\"hello\"}'\n    j = s.to_json(Foo(a='bye', b=b'more'))\n    if on_pypy:\n        assert json.loads(j) == {}\n    else:\n        assert j == b'{}'"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004506", "source": "def get_arch_name() -> str:\n    if torch.cuda.is_available():\n        return torch.cuda.get_device_name()\n    else:\n        return platform.machine()", "target": "def testNonContiguous(self):\n        x = torch.tensor([100, 200, 300])[::2]\n        assert not x.is_contiguous()\n        assert x[0] == 100\n        assert x[1] == 300\n        return x"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004507", "source": "def convert_generics(tp: Type[Any]) -> Type[Any]:\n        return tp", "target": "def test_ser_function_plain():\n    def f(input: Any, info: core_schema.SerializationInfo, /) -> str:\n        return str(info)\n    s = SchemaSerializer(\n        core_schema.any_schema(\n            serialization=core_schema.plain_serializer_function_ser_schema(\n                f, info_arg=True, return_schema=core_schema.str_schema()\n            )\n        )\n    )\n    assert s.to_python(123) == (\n        \"SerializationInfo(include=None, exclude=None, context=None, mode='python', by_alias=False, exclude_unset=False, \"\n        'exclude_defaults=False, exclude_none=False, exclude_computed_fields=False, round_trip=False, serialize_as_any=False)'\n    )"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004508", "source": "def rebuild_dataclass(\n    cls: type[PydanticDataclass],\n    *,\n    force: bool = False,\n    raise_errors: bool = True,\n    _parent_namespace_depth: int = 2,\n    _types_namespace: MappingNamespace | None = None,\n) -> bool | None:\n    if not force and cls.__pydantic_complete__:\n        return None\n    for attr in ('__pydantic_core_schema__', '__pydantic_validator__', '__pydantic_serializer__'):\n        if attr in cls.__dict__ and not isinstance(getattr(cls, attr), _mock_val_ser.MockValSer):\n            delattr(cls, attr)\n    cls.__pydantic_complete__ = False\n    if _types_namespace is not None:\n        rebuild_ns = _types_namespace\n    elif _parent_namespace_depth > 0:\n        rebuild_ns = _typing_extra.parent_frame_namespace(parent_depth=_parent_namespace_depth, force=True) or {}\n    else:\n        rebuild_ns = {}\n    ns_resolver = _namespace_utils.NsResolver(\n        parent_namespace=rebuild_ns,\n    )\n    return _pydantic_dataclasses.complete_dataclass(\n        cls,\n        _config.ConfigWrapper(cls.__pydantic_config__, check=False),\n        raise_errors=raise_errors,\n        ns_resolver=ns_resolver,\n        _force_build=True,\n    )", "target": "def test_dataclass_subclass_subclass_revalidate():\n    v = SchemaValidator(\n        core_schema.dataclass_schema(\n            FooDataclass,\n            core_schema.dataclass_args_schema(\n                'FooDataclass',\n                [\n                    core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                    core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n                ],\n            ),\n            ['a', 'b'],\n            revalidate_instances='subclass-instances',\n            strict=True,\n        )\n    )\n    foo = FooDataclass(a='hello', b=True)\n    assert v.validate_python(foo) is foo\n    sub_foo = FooDataclassSame(a='hello', b='True')\n    sub_foo2 = v.validate_python(sub_foo)\n    assert sub_foo2 is not sub_foo\n    assert type(sub_foo2) is FooDataclass\n    assert dataclasses.asdict(sub_foo2) == dict(a='hello', b=True)"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004509", "source": "def from_markdown_table(data: str) -> TimingResultType:\n    out = data.strip().split(\"\\n\")\n    out = out[2:]\n    res: TimingResultType\n    res = defaultdict(defaultdict)\n    for line in out:\n        model, task, mean, var = (f.strip() for f in line.strip().split(\"|\") if f)\n        res[model][task] = (float(mean), float(var))\n    return res", "target": "def test_restores_on_exception(self):\n        start = Path.cwd()\n        with tempfile.TemporaryDirectory() as td:\n            target = Path(td) / \"wd_exc\"\n            target.mkdir()\n            with self.assertRaises(ValueError):\n                with working_directory(str(target)):\n                    self.assertEqual(Path.cwd().resolve(), target.resolve())\n                    raise ValueError(\"boom\")\n        self.assertEqual(Path.cwd().resolve(), start.resolve())"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004510", "source": "def extract_weights(mod: nn.Module) -> tuple[tuple[Tensor, ...], list[str]]:\n    orig_params = tuple(mod.parameters())\n    names = []\n    for name, p in list(mod.named_parameters()):\n        _del_nested_attr(mod, name.split(\".\"))\n        names.append(name)\n    params = tuple(p.detach().requires_grad_() for p in orig_params)\n    return params, names", "target": "def test_sets_and_restores_new_var(self):\n        var = \"TEST_TMP_ENV_NEW\"\n        self.assertNotIn(var, os.environ)\n        with temp_environ({var: \"123\"}):\n            self.assertEqual(os.environ[var], \"123\")\n        self.assertNotIn(var, os.environ)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004511", "source": "def patch_base_fields(cls: type[Any]) -> Generator[None]:\n    original_fields_list: list[tuple[DcFields, DcFields]] = []\n    for base in cls.__mro__[1:]:\n        dc_fields: dict[str, dataclasses.Field[Any]] = base.__dict__.get('__dataclass_fields__', {})\n        dc_fields_with_pydantic_field_defaults = {\n            field_name: field\n            for field_name, field in dc_fields.items()\n            if isinstance(field.default, FieldInfo)\n            and (field.default.description is not None or field.default.kw_only or field.default.repr is not True)\n        }\n        if dc_fields_with_pydantic_field_defaults:\n            original_fields_list.append((dc_fields, dc_fields_with_pydantic_field_defaults))\n            for field_name, field in dc_fields_with_pydantic_field_defaults.items():\n                default = cast(FieldInfo, field.default)\n                new_dc_field = copy.copy(field)\n                if sys.version_info >= (3, 10) and default.kw_only:\n                    new_dc_field.kw_only = True\n                if default.repr is not True:\n                    new_dc_field.repr = default.repr\n                dc_fields[field_name] = new_dc_field\n    try:\n        yield\n    finally:\n        for fields, original_fields in original_fields_list:\n            for field_name, original_field in original_fields.items():\n                fields[field_name] = original_field", "target": "def test_dataclass_args_init_only(py_and_json: PyAndJson, input_value, expected):\n    schema = core_schema.dataclass_args_schema(\n        'MyDataclass',\n        [\n            core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), kw_only=False),\n            core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), kw_only=False, init_only=True),\n        ],\n        collect_init_only=True,\n    )\n    v = py_and_json(schema)\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)) as exc_info:\n            v.validate_test(input_value)\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        assert v.validate_test(input_value) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004512", "source": "def env_path_field(\n    name: str,\n    default: Union[str, Path] = \"\",\n    *,\n    resolve: bool = True,\n) -> Path:\n    return field(default_factory=lambda: env_path(name, default, resolve=resolve))", "target": "def test_dataclass_path_field_with_default_value(self):\n        @dataclass\n        class C2:\n            out: Path = m.env_path_field(\"OUT\", default=\"some/dir\", resolve=False)\n        with patch.dict(os.environ, {}, clear=True):\n            c = C2()\n            self.assertEqual(c.out, Path(\"some/dir\"))"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004513", "source": "def to_markdown_table(\n    res: TimingResultType, header: Optional[tuple[str, ...]] = None\n) -> str:\n    if header is None:\n        header = (\"model\", \"task\", \"mean\", \"var\")\n    out = \"\"\n    def write_line(*args):\n        nonlocal out\n        out += f\"| {' | '.join(str(a) for a in args)} |\\n\"\n    write_line(*header)\n    write_line(*[\"--\"] * len(header))\n    for model, tasks in res.items():\n        for task, line in tasks.items():\n            write_line(*(model, task) + line)\n    return out", "target": "def test_noop_when_empty_path(self):\n        start = Path.cwd()\n        with working_directory(\"\"):\n            self.assertEqual(Path.cwd(), start)\n        self.assertEqual(Path.cwd(), start)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004514", "source": "def _dataclass_validate_assignment_setattr(self: 'Dataclass', name: str, value: Any) -> None:\n    if self.__pydantic_initialised__:\n        d = dict(self.__dict__)\n        d.pop(name, None)\n        known_field = self.__pydantic_model__.__fields__.get(name, None)\n        if known_field:\n            value, error_ = known_field.validate(value, d, loc=name, cls=self.__class__)\n            if error_:\n                raise ValidationError([error_], self.__class__)\n    object.__setattr__(self, name, value)", "target": "def test_dataclass():\n    schema = core_schema.dataclass_schema(\n        FooDataclass,\n        core_schema.dataclass_args_schema(\n            'FooDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 'hello', 'b': True})\n    assert dataclasses.is_dataclass(foo)\n    assert foo.a == 'hello'\n    assert foo.b is True\n    assert dataclasses.asdict(v.validate_python(FooDataclass(a='hello', b=True))) == {'a': 'hello', 'b': True}\n    with pytest.raises(ValidationError, match='Input should be an instance of FooDataclass') as exc_info:\n        v.validate_python({'a': 'hello', 'b': True}, strict=True)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'dataclass_exact_type',\n            'loc': (),\n            'msg': 'Input should be an instance of FooDataclass',\n            'input': {'a': 'hello', 'b': True},\n            'ctx': {'class_name': 'FooDataclass'},\n        }\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004515", "source": "def ip_v6_interface_validator(input_value: Any, /) -> IPv6Interface:\n    if isinstance(input_value, IPv6Interface):\n        return input_value\n    try:\n        return IPv6Interface(input_value)\n    except ValueError:\n        raise PydanticCustomError('ip_v6_interface', 'Input is not a valid IPv6 interface')", "target": "def test_validators_build(benchmark) -> None:\n    class Base1(BaseModel):\n        a: int\n        @field_validator('a', mode='after')\n        @classmethod\n        def val_a(cls, value: int) -> int: ...\n        @computed_field\n        def prop(self) -> int: ...\n    class Bare:\n        @computed_field\n        def prop_bare(self) -> int: ...\n    class Sub1(Base1):\n        @computed_field\n        def prop_2(self) -> int: ...\n        @computed_field\n        def prop_3(self) -> int: ...\n        @computed_field\n        def prop_4(self) -> int: ...\n    @benchmark\n    def bench() -> None:\n        class SubS(Sub1, Bare, defer_build=True):\n            @computed_field\n            def prop_5(self) -> int: ...\n            @computed_field\n            def prop_6(self) -> int: ...\n            @computed_field\n            def prop_7(self) -> int: ..."}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004516", "source": "def dataclass(\n        _cls: Type[_T],\n        *,\n        init: bool = True,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool = False,\n        config: Union[ConfigDict, Type[object], None] = None,\n        validate_on_init: Optional[bool] = None,\n        use_proxy: Optional[bool] = None,\n        kw_only: bool = ...,\n    ) -> 'DataclassClassOrWrapper':\n        ...", "target": "def test_frozen():\n    @dataclasses.dataclass\n    class MyModel:\n        f: str\n    v = SchemaValidator(\n        core_schema.dataclass_schema(\n            MyModel,\n            core_schema.dataclass_args_schema('MyModel', [core_schema.dataclass_field('f', core_schema.str_schema())]),\n            ['f'],\n            frozen=True,\n        )\n    )\n    m = v.validate_python({'f': 'x'})\n    assert m.f == 'x'\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_assignment(m, 'f', 'y')\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'frozen_instance', 'loc': (), 'msg': 'Instance is frozen', 'input': 'y'}\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004517", "source": "def _set_nested_attr(obj: nn.Module, names: list[str], value: Tensor) -> None:\n    if len(names) == 1:\n        setattr(obj, names[0], value)\n    else:\n        _set_nested_attr(getattr(obj, names[0]), names[1:], value)", "target": "def test_fails(self) -> None:\n        @retries_decorator(rc=0)\n        def foo(x: int, y: int) -> int:\n            return x + y\n        self.assertEqual(foo(\"a\", 4), 0)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004518", "source": "def custom_pydantic_encoder(type_encoders: dict[Any, Callable[[type[Any]], Any]], obj: Any) -> Any:\n    warnings.warn(\n        '`custom_pydantic_encoder` is deprecated, use `BaseModel.model_dump` instead.',\n        category=PydanticDeprecatedSince20,\n        stacklevel=2,\n    )\n    for base in obj.__class__.__mro__[:-1]:\n        try:\n            encoder = type_encoders[base]\n        except KeyError:\n            continue\n        return encoder(obj)\n    else:\n        return pydantic_encoder(obj)", "target": "def test_float(input_value, expected):\n    v = SchemaValidator(core_schema.float_schema())\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            v.validate_json(input_value)\n    else:\n        assert v.validate_json(input_value) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004519", "source": "def benchmark_module(config, module, use_throughput_benchmark=False):\n    if use_throughput_benchmark:\n        return benchmark_using_throughput_benchmark(config, module)\n    module.forward(config.num_warmup_iters)\n    print(f\"Running module for {config.num_iters} iterations\")\n    start = time.time()\n    module.forward(config.num_iters)\n    end = time.time()\n    time_elapsed_s = end - start\n    return secs_to_ms(time_elapsed_s) / config.num_iters / NUM_LOOP_ITERS", "target": "def test_gh_get_labels_raises_with_no_pages(\n        self,\n        mock_request_for_labels: Any,\n        get_last_page_num_from_header: Any,\n    ) -> None:\n        with self.assertRaises(AssertionError) as err:\n            gh_get_labels(\"foo\", \"bar\")\n        self.assertIn(\"number of pages of labels\", str(err.exception))"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004520", "source": "def collect_dataclass_fields(\n    cls: type[StandardDataclass],\n    *,\n    config_wrapper: ConfigWrapper,\n    ns_resolver: NsResolver | None = None,\n    typevars_map: dict[Any, Any] | None = None,\n) -> dict[str, FieldInfo]:\n    FieldInfo_ = import_cached_field_info()\n    fields: dict[str, FieldInfo] = {}\n    ns_resolver = ns_resolver or NsResolver()\n    dataclass_fields = cls.__dataclass_fields__\n    for base in reversed(cls.__mro__):\n        if not dataclasses.is_dataclass(base):\n            continue\n        with ns_resolver.push(base):\n            for ann_name, dataclass_field in dataclass_fields.items():\n                base_anns = _typing_extra.safe_get_annotations(base)\n                if ann_name not in base_anns:\n                    continue\n                globalns, localns = ns_resolver.types_namespace\n                ann_type, evaluated = _typing_extra.try_eval_type(dataclass_field.type, globalns, localns)\n                if _typing_extra.is_classvar_annotation(ann_type):\n                    continue\n                if (\n                    not dataclass_field.init\n                    and dataclass_field.default is dataclasses.MISSING\n                    and dataclass_field.default_factory is dataclasses.MISSING\n                ):\n                    continue\n                if isinstance(dataclass_field.default, FieldInfo_):\n                    if dataclass_field.default.init_var:\n                        if dataclass_field.default.init is False:\n                            raise PydanticUserError(\n                                f'Dataclass field {ann_name} has init=False and init_var=True, but these are mutually exclusive.',\n                                code='clashing-init-and-init-var',\n                            )\n                        continue\n                    field_info = FieldInfo_.from_annotated_attribute(\n                        ann_type, dataclass_field.default, _source=AnnotationSource.DATACLASS\n                    )\n                    field_info._original_assignment = dataclass_field.default\n                else:\n                    field_info = FieldInfo_.from_annotated_attribute(\n                        ann_type, dataclass_field, _source=AnnotationSource.DATACLASS\n                    )\n                    field_info._original_assignment = dataclass_field\n                if not evaluated:\n                    field_info._complete = False\n                    field_info._original_annotation = ann_type\n                fields[ann_name] = field_info\n                update_field_from_config(config_wrapper, ann_name, field_info)\n                if field_info.default is not PydanticUndefined and isinstance(\n                    getattr(cls, ann_name, field_info), FieldInfo_\n                ):\n                    setattr(cls, ann_name, field_info.default)\n    if typevars_map:\n        for field in fields.values():\n            field.apply_typevars_map(typevars_map)\n    if config_wrapper.use_attribute_docstrings:\n        _update_fields_from_docstrings(\n            cls,\n            fields,\n            use_inspect=not hasattr(cls, '__is_pydantic_dataclass__'),\n        )\n    return fields", "target": "def test_extra_behavior_allow(\n    config: Union[core_schema.CoreConfig, None],\n    schema_extra_behavior_kw: dict[str, Any],\n    extras_schema_kw: dict[str, Any],\n    expected_extra_value: Any,\n):\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            {'f': core_schema.model_field(core_schema.str_schema())}, **schema_extra_behavior_kw, **extras_schema_kw\n        ),\n        config=config,\n    )\n    m, model_extra, fields_set = v.validate_python({'f': 'x', 'extra_field': '123'})\n    assert m == {'f': 'x'}\n    assert model_extra == {'extra_field': expected_extra_value}\n    assert fields_set == {'f', 'extra_field'}\n    v.validate_assignment(m, 'f', 'y')\n    assert m == {'f': 'y'}\n    new_m, new_model_extra, new_fields_set = v.validate_assignment({**m, **model_extra}, 'not_f', '123')\n    assert new_m == {'f': 'y'}\n    assert new_model_extra == {'extra_field': expected_extra_value, 'not_f': expected_extra_value}\n    assert new_fields_set == {'not_f'}"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004521", "source": "def get_wheels(\n    output_dir: Path,\n    max_depth: Optional[int] = None,\n) -> list[str]:\n    root = Path(output_dir)\n    if not root.exists():\n        return []\n    items = []\n    for dirpath, _, filenames in os.walk(root):\n        depth = Path(dirpath).relative_to(root).parts\n        if max_depth is not None and len(depth) > max_depth:\n            continue\n        for fname in sorted(filenames):\n            if fname.endswith(\".whl\"):\n                pkg = fname.split(\"-\")[0]\n                relpath = str((Path(dirpath) / fname).relative_to(root))\n                items.append({\"pkg\": pkg, \"relpath\": relpath})\n    return items", "target": "def test_overwrites_and_restores_existing_var(self):\n        var = \"TEST_TMP_ENV_OVERWRITE\"\n        os.environ[var] = \"orig\"\n        with temp_environ({var: \"override\"}):\n            self.assertEqual(os.environ[var], \"override\")\n        self.assertEqual(os.environ[var], \"orig\")"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004522", "source": "def collect_dataclass_fields(\n    cls: type[StandardDataclass],\n    *,\n    config_wrapper: ConfigWrapper,\n    ns_resolver: NsResolver | None = None,\n    typevars_map: dict[Any, Any] | None = None,\n) -> dict[str, FieldInfo]:\n    FieldInfo_ = import_cached_field_info()\n    fields: dict[str, FieldInfo] = {}\n    ns_resolver = ns_resolver or NsResolver()\n    dataclass_fields = cls.__dataclass_fields__\n    for base in reversed(cls.__mro__):\n        if not dataclasses.is_dataclass(base):\n            continue\n        with ns_resolver.push(base):\n            for ann_name, dataclass_field in dataclass_fields.items():\n                base_anns = _typing_extra.safe_get_annotations(base)\n                if ann_name not in base_anns:\n                    continue\n                globalns, localns = ns_resolver.types_namespace\n                ann_type, evaluated = _typing_extra.try_eval_type(dataclass_field.type, globalns, localns)\n                if _typing_extra.is_classvar_annotation(ann_type):\n                    continue\n                if (\n                    not dataclass_field.init\n                    and dataclass_field.default is dataclasses.MISSING\n                    and dataclass_field.default_factory is dataclasses.MISSING\n                ):\n                    continue\n                if isinstance(dataclass_field.default, FieldInfo_):\n                    if dataclass_field.default.init_var:\n                        if dataclass_field.default.init is False:\n                            raise PydanticUserError(\n                                f'Dataclass field {ann_name} has init=False and init_var=True, but these are mutually exclusive.',\n                                code='clashing-init-and-init-var',\n                            )\n                        continue\n                    field_info = FieldInfo_.from_annotated_attribute(\n                        ann_type, dataclass_field.default, _source=AnnotationSource.DATACLASS\n                    )\n                    field_info._original_assignment = dataclass_field.default\n                else:\n                    field_info = FieldInfo_.from_annotated_attribute(\n                        ann_type, dataclass_field, _source=AnnotationSource.DATACLASS\n                    )\n                    field_info._original_assignment = dataclass_field\n                if not evaluated:\n                    field_info._complete = False\n                    field_info._original_annotation = ann_type\n                fields[ann_name] = field_info\n                update_field_from_config(config_wrapper, ann_name, field_info)\n                if field_info.default is not PydanticUndefined and isinstance(\n                    getattr(cls, ann_name, field_info), FieldInfo_\n                ):\n                    setattr(cls, ann_name, field_info.default)\n    if typevars_map:\n        for field in fields.values():\n            field.apply_typevars_map(typevars_map)\n    if config_wrapper.use_attribute_docstrings:\n        _update_fields_from_docstrings(\n            cls,\n            fields,\n            use_inspect=not hasattr(cls, '__is_pydantic_dataclass__'),\n        )\n    return fields", "target": "def test_extra_behavior_allow_keys_validation() -> None:\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            {}, extra_behavior='allow', extras_keys_schema=core_schema.str_schema(max_length=3)\n        )\n    )\n    m, model_extra, fields_set = v.validate_python({'ext': 123})\n    assert m == {}\n    assert model_extra == {'ext': 123}\n    assert fields_set == {'ext'}\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python({'extra_too_long': 123})\n    assert exc_info.value.errors()[0]['type'] == 'string_too_long'"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004523", "source": "def isoformat(o: Union[datetime.date, datetime.time]) -> str:\n    return o.isoformat()", "target": "def test_inf_nan_allow():\n    v = SchemaValidator(core_schema.float_schema(allow_inf_nan=True))\n    assert v.validate_json('Infinity') == float('inf')\n    assert v.validate_json('-Infinity') == float('-inf')\n    assert v.validate_json('NaN') == IsFloatNan()"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004524", "source": "def _update_fields_from_docstrings(cls: type[Any], fields: dict[str, FieldInfo], use_inspect: bool = False) -> None:\n    fields_docs = extract_docstrings_from_cls(cls, use_inspect=use_inspect)\n    for ann_name, field_info in fields.items():\n        if field_info.description is None and ann_name in fields_docs:\n            field_info.description = fields_docs[ann_name]", "target": "def test_allow_extra_invalid():\n    with pytest.raises(SchemaError, match='extras_schema can only be used if extra_behavior=allow'):\n        SchemaValidator(\n            schema=core_schema.model_fields_schema(\n                fields={}, extras_schema=core_schema.int_schema(), extra_behavior='ignore'\n            )\n        )\n    with pytest.raises(SchemaError, match='extras_keys_schema can only be used if extra_behavior=allow'):\n        SchemaValidator(\n            schema=core_schema.model_fields_schema(\n                fields={}, extras_keys_schema=core_schema.int_schema(), extra_behavior='ignore'\n            )\n        )"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004525", "source": "def set_dataclass_fields(\n    cls: type[StandardDataclass],\n    config_wrapper: _config.ConfigWrapper,\n    ns_resolver: NsResolver | None = None,\n) -> None:\n    typevars_map = get_standard_typevars_map(cls)\n    fields = collect_dataclass_fields(\n        cls, ns_resolver=ns_resolver, typevars_map=typevars_map, config_wrapper=config_wrapper\n    )\n    cls.__pydantic_fields__ = fields", "target": "def test_aliases(py_and_json: PyAndJson):\n    schema = core_schema.dataclass_args_schema(\n        'MyDataclass',\n        [\n            core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), validation_alias='Apple'),\n            core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), validation_alias=['Banana', 1]),\n            core_schema.dataclass_field(\n                name='c', schema=core_schema.int_schema(), validation_alias=['Carrot', 'v'], init_only=True\n            ),\n        ],\n        collect_init_only=True,\n    )\n    v = py_and_json(schema)\n    assert v.validate_test({'Apple': 'a', 'Banana': ['x', 'false'], 'Carrot': {'v': '42'}}) == (\n        {'a': 'a', 'b': False},\n        (42,),\n    )"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004526", "source": "def _call_initvar(*args: Any, **kwargs: Any) -> NoReturn:\n        raise TypeError(\"'InitVar' object is not callable\")", "target": "def test_dataclass_args_init(input_value, extra_behavior, expected):\n    @dataclasses.dataclass\n    class Foo:\n        a: str\n        b: str\n        def __post_init__(self):\n            self.b = self.a.upper()\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.str_schema(), init=False),\n            ],\n            extra_behavior=extra_behavior,\n        ),\n        ['a', 'b'],\n        post_init=True,\n    )\n    v = SchemaValidator(schema)\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)) as exc_info:\n            v.validate_python(input_value)\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        assert dataclasses.asdict(v.validate_python(input_value)) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004527", "source": "def _shasum(value: str) -> str:\n    import hashlib\n    m = hashlib.sha256()\n    m.update(value.encode(\"utf-8\"))\n    return m.hexdigest()", "target": "def test_fails(self) -> None:\n        @retries_decorator(rc=0)\n        def foo(x: int, y: int) -> int:\n            return x + y\n        self.assertEqual(foo(\"a\", 4), 0)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004528", "source": "def _check_classvar(v: Optional[Type[Any]]) -> bool:\n    if v is None:\n        return False\n    return v.__class__ == ClassVar.__class__ and getattr(v, '_name', None) == 'ClassVar'", "target": "def test_ser_function_plain():\n    def f(input: Any, info: core_schema.SerializationInfo, /) -> str:\n        return str(info)\n    s = SchemaSerializer(\n        core_schema.any_schema(\n            serialization=core_schema.plain_serializer_function_ser_schema(\n                f, info_arg=True, return_schema=core_schema.str_schema()\n            )\n        )\n    )\n    assert s.to_python(123) == (\n        \"SerializationInfo(include=None, exclude=None, context=None, mode='python', by_alias=False, exclude_unset=False, \"\n        'exclude_defaults=False, exclude_none=False, exclude_computed_fields=False, round_trip=False, serialize_as_any=False)'\n    )"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004529", "source": "def tail(self) -> ConfigWrapper:\n        return self._config_wrapper_stack[-1]", "target": "def test_cache_strings():\n    v = SchemaValidator(cs.str_schema())\n    assert 'cache_strings=True' in plain_repr(v)\n    v = SchemaValidator(cs.str_schema(), config=CoreConfig(cache_strings=True))\n    assert 'cache_strings=True' in plain_repr(v)\n    v = SchemaValidator(cs.str_schema(), config=CoreConfig(cache_strings=False))\n    assert 'cache_strings=False' in plain_repr(v)\n    v = SchemaValidator(cs.str_schema(), config=CoreConfig(cache_strings='keys'))\n    assert \"cache_strings='keys'\" in plain_repr(v)"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004530", "source": "def retries_decorator(\n    rc: Any = None, num_retries: int = 3\n) -> Callable[[Callable[..., T]], Callable[..., T]]:\n    def decorator(f: Callable[..., T]) -> Callable[..., T]:\n        @wraps(f)\n        def wrapper(*args: list[Any], **kwargs: dict[str, Any]) -> T:\n            for idx in range(num_retries):\n                try:\n                    return f(*args, **kwargs)\n                except Exception as e:\n                    print(\n                        f'Attempt {idx} of {num_retries} to call {f.__name__} failed with \"{e}\"'\n                    )\n            return cast(T, rc)\n        return wrapper\n    return decorator", "target": "def test_simple(self) -> None:\n        @retries_decorator()\n        def foo(x: int, y: int) -> int:\n            return x + y\n        self.assertEqual(foo(3, 4), 7)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004531", "source": "def make_dataclass_validator(dc_cls: Type['Dataclass'], config: Type[BaseConfig]) -> 'CallableGenerator':\n    yield from _get_validators(dataclass(dc_cls, config=config, use_proxy=True))", "target": "def test_dataclass_slots_field_after_validator():\n    @dataclasses.dataclass(slots=True)\n    class Foo:\n        a: int\n        b: str\n        @classmethod\n        def validate_b(cls, v: str, info: core_schema.ValidationInfo) -> str:\n            assert v == 'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return 'hello world!'\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(\n                    name='b',\n                    schema=core_schema.with_info_after_validator_function(Foo.validate_b, core_schema.str_schema()),\n                ),\n            ],\n        ),\n        ['a', 'b'],\n        slots=True,\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 1, 'b': b'hello'})\n    assert dataclasses.asdict(foo) == {'a': 1, 'b': 'hello world!'}"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004532", "source": "def _check_output(items: list[str], encoding: str = \"utf-8\") -> str:\n    from subprocess import CalledProcessError, check_output, STDOUT\n    try:\n        return check_output(items, stderr=STDOUT).decode(encoding)\n    except CalledProcessError as e:\n        msg = f\"Command `{' '.join(e.cmd)}` returned non-zero exit code {e.returncode}\"\n        stdout = e.stdout.decode(encoding) if e.stdout is not None else \"\"\n        stderr = e.stderr.decode(encoding) if e.stderr is not None else \"\"\n        print(f\"stdout: \\n{stdout}\")\n        print(f\"stderr: \\n{stderr}\")\n        if len(stderr) == 0:\n            msg += f\"\\n```\\n{stdout}```\"\n        else:\n            msg += f\"\\nstdout:\\n```\\n{stdout}```\\nstderr:\\n```\\n{stderr}```\"\n        raise RuntimeError(msg) from e", "target": "def test_compute_diff(self) -> None:\n        diff = self.repo.diff(\"HEAD\")\n        sha = _shasum(diff)\n        self.assertEqual(len(sha), 64)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004533", "source": "def create_dataclass(cls: type[Any]) -> type[PydanticDataclass]:\n        from ._internal._utils import is_model_class\n        if is_model_class(cls):\n            raise PydanticUserError(\n                f'Cannot create a Pydantic dataclass from {cls.__name__} as it is already a Pydantic model',\n                code='dataclass-on-model',\n            )\n        original_cls = cls\n        has_dataclass_base = any(dataclasses.is_dataclass(base) for base in cls.__bases__)\n        if not has_dataclass_base and config is not None and hasattr(cls, '__pydantic_config__'):\n            warn(\n                f'`config` is set via both the `dataclass` decorator and `__pydantic_config__` for dataclass {cls.__name__}. '\n                f'The `config` specification from `dataclass` decorator will take priority.',\n                category=UserWarning,\n                stacklevel=2,\n            )\n        config_dict = config if config is not None else getattr(cls, '__pydantic_config__', None)\n        config_wrapper = _config.ConfigWrapper(config_dict)\n        decorators = _decorators.DecoratorInfos.build(cls, replace_wrapped_methods=True)\n        decorators.update_from_config(config_wrapper)\n        original_doc = cls.__doc__\n        if _pydantic_dataclasses.is_stdlib_dataclass(cls):\n            original_doc = None\n            bases = (cls,)\n            if issubclass(cls, Generic):\n                generic_base = Generic[cls.__parameters__]\n                bases = bases + (generic_base,)\n            cls = types.new_class(cls.__name__, bases)\n        if frozen is not None:\n            frozen_ = frozen\n            if config_wrapper.frozen:\n                warn(\n                    f'`frozen` is set via both the `dataclass` decorator and `config` for dataclass {cls.__name__!r}.'\n                    'This is not recommended. The `frozen` specification on `dataclass` will take priority.',\n                    category=UserWarning,\n                    stacklevel=2,\n                )\n        else:\n            frozen_ = config_wrapper.frozen or False\n        cls_anns = _typing_extra.safe_get_annotations(cls)\n        for field_name in cls_anns:\n            field_value = getattr(cls, field_name, None)\n            if isinstance(field_value, FieldInfo):\n                setattr(cls, field_name, _pydantic_dataclasses.as_dataclass_field(field_value))\n        with _pydantic_dataclasses.patch_base_fields(cls):\n            cls = dataclasses.dataclass(\n                cls,\n                init=True,\n                repr=repr,\n                eq=eq,\n                order=order,\n                unsafe_hash=unsafe_hash,\n                frozen=frozen_,\n                **kwargs,\n            )\n        if config_wrapper.validate_assignment:\n            original_setattr = cls.__setattr__\n            @functools.wraps(cls.__setattr__)\n            def validated_setattr(instance: PydanticDataclass, name: str, value: Any, /) -> None:\n                if frozen_:\n                    return original_setattr(instance, name, value)\n                inst_cls = type(instance)\n                attr = getattr(inst_cls, name, None)\n                if isinstance(attr, property):\n                    attr.__set__(instance, value)\n                elif isinstance(attr, functools.cached_property):\n                    instance.__dict__.__setitem__(name, value)\n                else:\n                    inst_cls.__pydantic_validator__.validate_assignment(instance, name, value)\n            cls.__setattr__ = validated_setattr.__get__(None, cls)\n            if slots and not hasattr(cls, '__setstate__'):\n                def _dataclass_getstate(self: Any) -> list[Any]:\n                    return [getattr(self, f.name) for f in dataclasses.fields(self)]\n                def _dataclass_setstate(self: Any, state: list[Any]) -> None:\n                    for field, value in zip(dataclasses.fields(self), state):\n                        object.__setattr__(self, field.name, value)\n                cls.__getstate__ = _dataclass_getstate\n                cls.__setstate__ = _dataclass_setstate\n        cls.__is_pydantic_dataclass__ = True\n        cls.__pydantic_decorators__ = decorators\n        cls.__doc__ = original_doc\n        firstlineno = getattr(original_cls, '__firstlineno__', None)\n        cls.__module__ = original_cls.__module__\n        if sys.version_info >= (3, 13) and firstlineno is not None:\n            original_cls.__firstlineno__ = firstlineno\n            cls.__firstlineno__ = firstlineno\n        cls.__qualname__ = original_cls.__qualname__\n        cls.__pydantic_fields_complete__ = classmethod(_pydantic_fields_complete)\n        cls.__pydantic_complete__ = False\n        _pydantic_dataclasses.complete_dataclass(cls, config_wrapper, raise_errors=False)\n        return cls", "target": "def test_dataclass_args(py_and_json: PyAndJson, input_value, expected):\n    schema = core_schema.dataclass_args_schema(\n        'MyDataclass',\n        [\n            core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), kw_only=False),\n            core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), kw_only=False),\n        ],\n    )\n    v = py_and_json(schema)\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)) as exc_info:\n            v.validate_test(input_value)\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        assert v.validate_test(input_value) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004534", "source": "def secs_to_us(time_s):\n    return time_s * 1e6", "target": "def test_get_last_page_num_from_header(self) -> None:\n        for (\n            expected_page_num,\n            mock_header,\n        ) in self.MOCK_HEADER_LINKS_TO_PAGE_NUMS.items():\n            self.assertEqual(\n                get_last_page_num_from_header(mock_header), expected_page_num\n            )"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004535", "source": "def _dataclass_validate_assignment_setattr(self: 'Dataclass', name: str, value: Any) -> None:\n    if self.__pydantic_initialised__:\n        d = dict(self.__dict__)\n        d.pop(name, None)\n        known_field = self.__pydantic_model__.__fields__.get(name, None)\n        if known_field:\n            value, error_ = known_field.validate(value, d, loc=name, cls=self.__class__)\n            if error_:\n                raise ValidationError([error_], self.__class__)\n    object.__setattr__(self, name, value)", "target": "def test_only_validate_by_name(py_and_json) -> None:\n    schema = core_schema.dataclass_schema(\n        BasicDataclass,\n        core_schema.dataclass_args_schema(\n            'BasicDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), validation_alias='FieldA'),\n            ],\n        ),\n        ['a'],\n        config=core_schema.CoreConfig(validate_by_name=True, validate_by_alias=False),\n    )\n    v = py_and_json(schema)\n    assert v.validate_test({'a': 'hello'}) == BasicDataclass(a='hello')\n    with pytest.raises(ValidationError, match=r'a\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'FieldA': 'hello'})"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004536", "source": "def create_dataclass(cls: type[Any]) -> type[PydanticDataclass]:\n        from ._internal._utils import is_model_class\n        if is_model_class(cls):\n            raise PydanticUserError(\n                f'Cannot create a Pydantic dataclass from {cls.__name__} as it is already a Pydantic model',\n                code='dataclass-on-model',\n            )\n        original_cls = cls\n        has_dataclass_base = any(dataclasses.is_dataclass(base) for base in cls.__bases__)\n        if not has_dataclass_base and config is not None and hasattr(cls, '__pydantic_config__'):\n            warn(\n                f'`config` is set via both the `dataclass` decorator and `__pydantic_config__` for dataclass {cls.__name__}. '\n                f'The `config` specification from `dataclass` decorator will take priority.',\n                category=UserWarning,\n                stacklevel=2,\n            )\n        config_dict = config if config is not None else getattr(cls, '__pydantic_config__', None)\n        config_wrapper = _config.ConfigWrapper(config_dict)\n        decorators = _decorators.DecoratorInfos.build(cls, replace_wrapped_methods=True)\n        decorators.update_from_config(config_wrapper)\n        original_doc = cls.__doc__\n        if _pydantic_dataclasses.is_stdlib_dataclass(cls):\n            original_doc = None\n            bases = (cls,)\n            if issubclass(cls, Generic):\n                generic_base = Generic[cls.__parameters__]\n                bases = bases + (generic_base,)\n            cls = types.new_class(cls.__name__, bases)\n        if frozen is not None:\n            frozen_ = frozen\n            if config_wrapper.frozen:\n                warn(\n                    f'`frozen` is set via both the `dataclass` decorator and `config` for dataclass {cls.__name__!r}.'\n                    'This is not recommended. The `frozen` specification on `dataclass` will take priority.',\n                    category=UserWarning,\n                    stacklevel=2,\n                )\n        else:\n            frozen_ = config_wrapper.frozen or False\n        cls_anns = _typing_extra.safe_get_annotations(cls)\n        for field_name in cls_anns:\n            field_value = getattr(cls, field_name, None)\n            if isinstance(field_value, FieldInfo):\n                setattr(cls, field_name, _pydantic_dataclasses.as_dataclass_field(field_value))\n        with _pydantic_dataclasses.patch_base_fields(cls):\n            cls = dataclasses.dataclass(\n                cls,\n                init=True,\n                repr=repr,\n                eq=eq,\n                order=order,\n                unsafe_hash=unsafe_hash,\n                frozen=frozen_,\n                **kwargs,\n            )\n        if config_wrapper.validate_assignment:\n            original_setattr = cls.__setattr__\n            @functools.wraps(cls.__setattr__)\n            def validated_setattr(instance: PydanticDataclass, name: str, value: Any, /) -> None:\n                if frozen_:\n                    return original_setattr(instance, name, value)\n                inst_cls = type(instance)\n                attr = getattr(inst_cls, name, None)\n                if isinstance(attr, property):\n                    attr.__set__(instance, value)\n                elif isinstance(attr, functools.cached_property):\n                    instance.__dict__.__setitem__(name, value)\n                else:\n                    inst_cls.__pydantic_validator__.validate_assignment(instance, name, value)\n            cls.__setattr__ = validated_setattr.__get__(None, cls)\n            if slots and not hasattr(cls, '__setstate__'):\n                def _dataclass_getstate(self: Any) -> list[Any]:\n                    return [getattr(self, f.name) for f in dataclasses.fields(self)]\n                def _dataclass_setstate(self: Any, state: list[Any]) -> None:\n                    for field, value in zip(dataclasses.fields(self), state):\n                        object.__setattr__(self, field.name, value)\n                cls.__getstate__ = _dataclass_getstate\n                cls.__setstate__ = _dataclass_setstate\n        cls.__is_pydantic_dataclass__ = True\n        cls.__pydantic_decorators__ = decorators\n        cls.__doc__ = original_doc\n        firstlineno = getattr(original_cls, '__firstlineno__', None)\n        cls.__module__ = original_cls.__module__\n        if sys.version_info >= (3, 13) and firstlineno is not None:\n            original_cls.__firstlineno__ = firstlineno\n            cls.__firstlineno__ = firstlineno\n        cls.__qualname__ = original_cls.__qualname__\n        cls.__pydantic_fields_complete__ = classmethod(_pydantic_fields_complete)\n        cls.__pydantic_complete__ = False\n        _pydantic_dataclasses.complete_dataclass(cls, config_wrapper, raise_errors=False)\n        return cls", "target": "def test_dataclass_slots_field_after_validator():\n    @dataclasses.dataclass(slots=True)\n    class Foo:\n        a: int\n        b: str\n        @classmethod\n        def validate_b(cls, v: str, info: core_schema.ValidationInfo) -> str:\n            assert v == 'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return 'hello world!'\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(\n                    name='b',\n                    schema=core_schema.with_info_after_validator_function(Foo.validate_b, core_schema.str_schema()),\n                ),\n            ],\n        ),\n        ['a', 'b'],\n        slots=True,\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 1, 'b': b'hello'})\n    assert dataclasses.asdict(foo) == {'a': 1, 'b': 'hello world!'}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004537", "source": "def rebuild_model_fields(\n    cls: type[BaseModel],\n    *,\n    config_wrapper: ConfigWrapper,\n    ns_resolver: NsResolver,\n    typevars_map: Mapping[TypeVar, Any],\n) -> dict[str, FieldInfo]:\n    rebuilt_fields: dict[str, FieldInfo] = {}\n    with ns_resolver.push(cls):\n        for f_name, field_info in cls.__pydantic_fields__.items():\n            if field_info._complete:\n                rebuilt_fields[f_name] = field_info\n            else:\n                new_field = _recreate_field_info(\n                    field_info, ns_resolver=ns_resolver, typevars_map=typevars_map, lenient=False\n                )\n                update_field_from_config(config_wrapper, f_name, new_field)\n                rebuilt_fields[f_name] = new_field\n    return rebuilt_fields", "target": "def test_from_attributes_error():\n    class Foobar:\n        def __init__(self):\n            self.a = 1\n        @property\n        def b(self):\n            raise RuntimeError('intentional error')\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'a': core_schema.model_field(schema=core_schema.int_schema()),\n                'b': core_schema.model_field(schema=core_schema.int_schema()),\n            },\n            from_attributes=True,\n        )\n    )\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python(Foobar())\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'get_attribute_error',\n            'loc': ('b',),\n            'msg': 'Error extracting attribute: RuntimeError: intentional error',\n            'input': HasRepr(IsStr(regex='.+Foobar object at.+')),\n            'ctx': {'error': 'RuntimeError: intentional error'},\n        }\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004538", "source": "def new_init(self: 'Dataclass', *args: Any, **kwargs: Any) -> None:\n            handle_extra_init(self, *args, **kwargs)\n            if self.__class__.__pydantic_run_validation__:\n                self.__pydantic_validate_values__()\n            if hasattr(self, '__post_init_post_parse__'):\n                initvars_and_values: Dict[str, Any] = {}\n                for i, f in enumerate(self.__class__.__dataclass_fields__.values()):\n                    if f._field_type is dataclasses._FIELD_INITVAR:\n                        try:\n                            initvars_and_values[f.name] = args[i]\n                        except IndexError:\n                            initvars_and_values[f.name] = kwargs.get(f.name, f.default)\n                self.__post_init_post_parse__(**initvars_and_values)", "target": "def test_dataclass():\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bytes_schema()),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    s = SchemaSerializer(schema)\n    assert s.to_python(Foo(a='hello', b=b'more')) == IsStrictDict(a='hello', b=b'more')\n    assert s.to_python(Foo(a='hello', b=b'more'), mode='json') == IsStrictDict(a='hello', b='more')\n    j = s.to_json(Foo(a='hello', b=b'more'))\n    if on_pypy:\n        assert json.loads(j) == {'a': 'hello', 'b': 'more'}\n    else:\n        assert j == b'{\"a\":\"hello\",\"b\":\"more\"}'\n    assert s.to_python(Foo(a='hello', b=b'more'), exclude={'b'}) == IsStrictDict(a='hello')\n    assert s.to_json(Foo(a='hello', b=b'more'), include={'a'}) == b'{\"a\":\"hello\"}'"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004539", "source": "def _dataclass_validate_assignment_setattr(self: 'Dataclass', name: str, value: Any) -> None:\n    if self.__pydantic_initialised__:\n        d = dict(self.__dict__)\n        d.pop(name, None)\n        known_field = self.__pydantic_model__.__fields__.get(name, None)\n        if known_field:\n            value, error_ = known_field.validate(value, d, loc=name, cls=self.__class__)\n            if error_:\n                raise ValidationError([error_], self.__class__)\n    object.__setattr__(self, name, value)", "target": "def test_extra_custom_serializer():\n    @dataclasses.dataclass\n    class Model:\n        pass\n    schema = core_schema.dataclass_schema(\n        Model,\n        core_schema.dataclass_args_schema(\n            'Model',\n            [],\n            extra_behavior='allow',\n        ),\n        [],\n    )\n    s = SchemaSerializer(schema)\n    v = SchemaValidator(schema)\n    m = v.validate_python({'extra': 'extra'})\n    assert s.to_python(m) == {'extra': 'extra bam!'}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004540", "source": "def takes_validated_data_argument(\n    default_factory: Callable[[], Any] | Callable[[dict[str, Any]], Any],\n) -> TypeIs[Callable[[dict[str, Any]], Any]]:\n    try:\n        sig = signature(default_factory)\n    except (ValueError, TypeError):\n        return False\n    parameters = list(sig.parameters.values())\n    return len(parameters) == 1 and can_be_positional(parameters[0]) and parameters[0].default is Parameter.empty", "target": "def test_alias_extra_by_name(py_and_json: PyAndJson):\n    v = py_and_json(\n        {\n            'type': 'model-fields',\n            'extra_behavior': 'allow',\n            'from_attributes': True,\n            'fields': {'field_a': {'validation_alias': 'FieldA', 'type': 'model-field', 'schema': {'type': 'int'}}},\n        },\n        config=CoreConfig(validate_by_name=True),\n    )\n    assert v.validate_test({'FieldA': 1}) == ({'field_a': 1}, {}, {'field_a'})\n    assert v.validate_test({'field_a': 1}) == ({'field_a': 1}, {}, {'field_a'})\n    assert v.validate_python(Cls(FieldA=1)) == ({'field_a': 1}, {}, {'field_a'})\n    assert v.validate_python(Cls(field_a=1)) == ({'field_a': 1}, {}, {'field_a'})"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004541", "source": "def env_bool_field(\n    name: str,\n    default: bool = False,\n):\n    return field(default_factory=lambda: env_bool(name, default))", "target": "def test_env_path_optional_unset_returns_none_when_env_var_is_empty(self):\n        with patch.dict(os.environ, {\"P\": \"\"}, clear=True):\n            self.assertIsNone(m.env_path_optional(\"P\"))"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004542", "source": "def cherry_pick(self, ref: str) -> None:\n        self._run_git(\"cherry-pick\", \"-x\", ref)", "target": "def test_simple(self) -> None:\n        @retries_decorator()\n        def foo(x: int, y: int) -> int:\n            return x + y\n        self.assertEqual(foo(3, 4), 7)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004543", "source": "def _apply_alias_generator_to_field_info(\n    alias_generator: Callable[[str], str] | AliasGenerator, field_name: str, field_info: FieldInfo\n):\n    if (\n        field_info.alias_priority is None\n        or field_info.alias_priority <= 1\n        or field_info.alias is None\n        or field_info.validation_alias is None\n        or field_info.serialization_alias is None\n    ):\n        alias, validation_alias, serialization_alias = None, None, None\n        if isinstance(alias_generator, AliasGenerator):\n            alias, validation_alias, serialization_alias = alias_generator.generate_aliases(field_name)\n        elif callable(alias_generator):\n            alias = alias_generator(field_name)\n            if not isinstance(alias, str):\n                raise TypeError(f'alias_generator {alias_generator} must return str, not {alias.__class__}')\n        if field_info.alias_priority is None or field_info.alias_priority <= 1:\n            field_info.alias_priority = 1\n        if field_info.alias_priority == 1:\n            field_info.serialization_alias = get_first_not_none(serialization_alias, alias)\n            field_info.validation_alias = get_first_not_none(validation_alias, alias)\n            field_info.alias = alias\n        if field_info.alias is None:\n            field_info.alias = alias\n        if field_info.serialization_alias is None:\n            field_info.serialization_alias = get_first_not_none(serialization_alias, alias)\n        if field_info.validation_alias is None:\n            field_info.validation_alias = get_first_not_none(validation_alias, alias)", "target": "def test_from_attributes_type_error():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'a': core_schema.model_field(schema=core_schema.int_schema()),\n                'b': core_schema.model_field(schema=core_schema.int_schema()),\n                'c': core_schema.model_field(schema=core_schema.str_schema()),\n            },\n            from_attributes=True,\n            model_name='MyModel',\n        )\n    )\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python('123')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'model_attributes_type',\n            'loc': (),\n            'msg': 'Input should be a valid dictionary or object to extract fields from',\n            'input': '123',\n        }\n    ]\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_json('123')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'model_type',\n            'loc': (),\n            'msg': 'Input should be an object',\n            'input': 123,\n            'ctx': {'class_name': 'MyModel'},\n        }\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004544", "source": "def branches_containing_ref(\n        self, ref: str, *, include_remote: bool = True\n    ) -> list[str]:\n        rc = (\n            self._run_git(\"branch\", \"--remote\", \"--contains\", ref)\n            if include_remote\n            else self._run_git(\"branch\", \"--contains\", ref)\n        )\n        return [x.strip() for x in rc.split(\"\\n\") if x.strip()] if len(rc) > 0 else []", "target": "def test_double_asterisks(self) -> None:\n        allowed_patterns = [\n            \"aten/src/ATen/native/**LinearAlgebra*\",\n        ]\n        patterns_re = patterns_to_regex(allowed_patterns)\n        fnames = [\n            \"aten/src/ATen/native/LinearAlgebra.cpp\",\n            \"aten/src/ATen/native/cpu/LinearAlgebraKernel.cpp\",\n        ]\n        for filename in fnames:\n            self.assertTrue(patterns_re.match(filename))"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004545", "source": "def pydantic_encoder(obj: Any) -> Any:\n    warnings.warn(\n        '`pydantic_encoder` is deprecated, use `pydantic_core.to_jsonable_python` instead.',\n        category=PydanticDeprecatedSince20,\n        stacklevel=2,\n    )\n    from dataclasses import asdict, is_dataclass\n    BaseModel = import_cached_base_model()\n    if isinstance(obj, BaseModel):\n        return obj.model_dump()\n    elif is_dataclass(obj):\n        return asdict(obj)\n    for base in obj.__class__.__mro__[:-1]:\n        try:\n            encoder = ENCODERS_BY_TYPE[base]\n        except KeyError:\n            continue\n        return encoder(obj)\n    else:\n        raise TypeError(f\"Object of type '{obj.__class__.__name__}' is not JSON serializable\")", "target": "def test_cycle_change():\n    def fallback_func_change_id(obj):\n        return Foobar()\n    f = Foobar()\n    with pytest.raises(ValueError, match=r'Circular reference detected \\(depth exceeded\\)'):\n        to_jsonable_python(f, fallback=fallback_func_change_id)\n    with pytest.raises(ValueError, match=r'Circular reference detected \\(depth exceeded\\)'):\n        to_json(f, fallback=fallback_func_change_id)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004546", "source": "def dataclass(\n        *,\n        init: Literal[False] = False,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool = False,\n        config: ConfigDict | type[object] | None = None,\n        validate_on_init: bool | None = None,\n        kw_only: bool = ...,\n        slots: bool = ...,\n    ) -> Callable[[type[_T]], type[PydanticDataclass]]:\n        ...", "target": "def test_dataclass():\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bytes_schema()),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    s = SchemaSerializer(schema)\n    assert s.to_python(Foo(a='hello', b=b'more')) == IsStrictDict(a='hello', b=b'more')\n    assert s.to_python(Foo(a='hello', b=b'more'), mode='json') == IsStrictDict(a='hello', b='more')\n    j = s.to_json(Foo(a='hello', b=b'more'))\n    if on_pypy:\n        assert json.loads(j) == {'a': 'hello', 'b': 'more'}\n    else:\n        assert j == b'{\"a\":\"hello\",\"b\":\"more\"}'\n    assert s.to_python(Foo(a='hello', b=b'more'), exclude={'b'}) == IsStrictDict(a='hello')\n    assert s.to_json(Foo(a='hello', b=b'more'), include={'a'}) == b'{\"a\":\"hello\"}'"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004547", "source": "def update_field_from_config(config_wrapper: ConfigWrapper, field_name: str, field_info: FieldInfo) -> None:\n    field_title_generator = field_info.field_title_generator or config_wrapper.field_title_generator\n    if field_title_generator is not None:\n        _apply_field_title_generator_to_field_info(field_title_generator, field_name, field_info)\n    if config_wrapper.alias_generator is not None:\n        _apply_alias_generator_to_field_info(config_wrapper.alias_generator, field_name, field_info)", "target": "def test_only_allow_alias(py_and_json) -> None:\n    v = py_and_json(\n        {\n            'type': 'model-fields',\n            'fields': {'field_a': {'validation_alias': 'FieldA', 'type': 'model-field', 'schema': {'type': 'int'}}},\n        },\n        config=CoreConfig(validate_by_name=False, validate_by_alias=True),\n    )\n    assert v.validate_test({'FieldA': '123'}) == ({'field_a': 123}, None, {'field_a'})\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'field_a': '123'})"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004548", "source": "def get_all_type_hints(obj: Any, globalns: Any = None, localns: Any = None) -> Any:\n        return get_type_hints(obj, globalns, localns, include_extras=True)", "target": "def test_error_details() -> None:\n    def act_on_error_details(_: ErrorDetails) -> None:\n        pass\n    v = SchemaValidator({'type': 'int'})\n    try:\n        v.validate_python('not an int')\n    except ValidationError as err:\n        for details in err.errors(include_url=False):\n            act_on_error_details(details)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004549", "source": "def _apply_alias_generator_to_field_info(\n    alias_generator: Callable[[str], str] | AliasGenerator, field_name: str, field_info: FieldInfo\n):\n    if (\n        field_info.alias_priority is None\n        or field_info.alias_priority <= 1\n        or field_info.alias is None\n        or field_info.validation_alias is None\n        or field_info.serialization_alias is None\n    ):\n        alias, validation_alias, serialization_alias = None, None, None\n        if isinstance(alias_generator, AliasGenerator):\n            alias, validation_alias, serialization_alias = alias_generator.generate_aliases(field_name)\n        elif callable(alias_generator):\n            alias = alias_generator(field_name)\n            if not isinstance(alias, str):\n                raise TypeError(f'alias_generator {alias_generator} must return str, not {alias.__class__}')\n        if field_info.alias_priority is None or field_info.alias_priority <= 1:\n            field_info.alias_priority = 1\n        if field_info.alias_priority == 1:\n            field_info.serialization_alias = get_first_not_none(serialization_alias, alias)\n            field_info.validation_alias = get_first_not_none(validation_alias, alias)\n            field_info.alias = alias\n        if field_info.alias is None:\n            field_info.alias = alias\n        if field_info.serialization_alias is None:\n            field_info.serialization_alias = get_first_not_none(serialization_alias, alias)\n        if field_info.validation_alias is None:\n            field_info.validation_alias = get_first_not_none(validation_alias, alias)", "target": "def test_strict():\n    v = SchemaValidator(\n        {\n            'type': 'model-fields',\n            'fields': {\n                'field_a': {'type': 'model-field', 'schema': {'type': 'str'}},\n                'field_b': {'type': 'model-field', 'schema': {'type': 'int'}},\n            },\n        },\n        CoreConfig(strict=True),\n    )\n    assert v.validate_python({'field_a': 'hello', 'field_b': 12}) == (\n        {'field_a': 'hello', 'field_b': 12},\n        None,\n        {'field_a', 'field_b'},\n    )\n    with pytest.raises(ValidationError) as exc_info:\n        assert v.validate_python({'field_a': 123, 'field_b': '123'})\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'string_type', 'loc': ('field_a',), 'msg': 'Input should be a valid string', 'input': 123},\n        {'type': 'int_type', 'loc': ('field_b',), 'msg': 'Input should be a valid integer', 'input': '123'},\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004550", "source": "def _general_metadata_cls() -> type[BaseMetadata]:\n    from annotated_types import BaseMetadata\n    class _PydanticGeneralMetadata(PydanticMetadata, BaseMetadata):\n        def __init__(self, metadata: Any):\n            self.__dict__ = metadata\n    return _PydanticGeneralMetadata", "target": "def test_alias(py_and_json: PyAndJson):\n    v = py_and_json(\n        {\n            'type': 'model-fields',\n            'fields': {'field_a': {'validation_alias': 'FieldA', 'type': 'model-field', 'schema': {'type': 'int'}}},\n        }\n    )\n    assert v.validate_test({'FieldA': '123'}) == ({'field_a': 123}, None, {'field_a'})\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'foobar': '123'})\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'field_a': '123'})"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004551", "source": "def create_branch_and_checkout(self, branch: str) -> None:\n        self._run_git(\"checkout\", \"-b\", branch)", "target": "def test_peek(self, input_: str = \"abcdef\") -> None:\n        iter_ = PeekableIterator(input_)\n        for idx, c in enumerate(iter_):\n            if idx + 1 < len(input_):\n                self.assertEqual(iter_.peek(), input_[idx + 1])\n            else:\n                self.assertTrue(iter_.peek() is None)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004552", "source": "def dataclass(\n        *,\n        init: Literal[False] = False,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool | None = None,\n        config: ConfigDict | type[object] | None = None,\n        validate_on_init: bool | None = None,\n    ) -> Callable[[type[_T]], type[PydanticDataclass]]:\n        ...", "target": "def test_dataclass_post_init_args():\n    c_value = None\n    @dataclasses.dataclass\n    class Foo:\n        a: str\n        b: bool\n        c: dataclasses.InitVar[int]\n        def __post_init__(self, c: int):\n            nonlocal c_value\n            c_value = c\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n                core_schema.dataclass_field(name='c', schema=core_schema.int_schema(), init_only=True),\n            ],\n            collect_init_only=True,\n        ),\n        ['a', 'b'],\n        post_init=True,\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': b'hello', 'b': 'true', 'c': '42'})\n    assert foo.a == 'hello'\n    assert foo.b is True\n    assert not hasattr(foo, 'c')\n    assert c_value == 42"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004553", "source": "def remote_url(self) -> str:\n        return self._run_git(\"remote\", \"get-url\", self.remote)", "target": "def test_iterator(self, input_: str = \"abcdef\") -> None:\n        iter_ = PeekableIterator(input_)\n        for idx, c in enumerate(iter_):\n            self.assertEqual(c, input_[idx])"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004554", "source": "def update_model_forward_refs(\n    model: Type[Any],\n    fields: Iterable['ModelField'],\n    json_encoders: Dict[Union[Type[Any], str, ForwardRef], AnyCallable],\n    localns: 'DictStrAny',\n    exc_to_suppress: Tuple[Type[BaseException], ...] = (),\n) -> None:\n    if model.__module__ in sys.modules:\n        globalns = sys.modules[model.__module__].__dict__.copy()\n    else:\n        globalns = {}\n    globalns.setdefault(model.__name__, model)\n    for f in fields:\n        try:\n            update_field_forward_refs(f, globalns=globalns, localns=localns)\n        except exc_to_suppress:\n            pass\n    for key in set(json_encoders.keys()):\n        if isinstance(key, str):\n            fr: ForwardRef = ForwardRef(key)\n        elif isinstance(key, ForwardRef):\n            fr = key\n        else:\n            continue\n        try:\n            new_key = evaluate_forwardref(fr, globalns, localns or None)\n        except exc_to_suppress:\n            continue\n        json_encoders[new_key] = json_encoders.pop(key)", "target": "def test_type_error():\n    try:\n        PydanticKnownError('foobar')\n    except KeyError as exc:\n        assert str(exc) == '\"Invalid error type: \\'foobar\\'\"'\n    else:\n        raise AssertionError(\"PydanticKnownError('foobar') did not raise KeyError\")\n    e = PydanticKnownError('recursion_loop')\n    assert isinstance(e, PydanticKnownError)"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004555", "source": "def generate_dataclass_help(cls) -> str:\n    if not is_dataclass(cls):\n        raise TypeError(f\"{cls} is not a dataclass\")\n    def get_value(f):\n        if f.default is not MISSING:\n            return f.default\n        if f.default_factory is not MISSING:\n            try:\n                return f.default_factory()\n            except Exception as e:\n                return f\"<error: {e}>\"\n        return \"<required>\"\n    lines = [f\"{f.name:<22} = {repr(get_value(f))}\" for f in fields(cls)]\n    return indent(\"\\n\".join(lines), \"    \")", "target": "def test_env_path_raises_when_missing_and_default_none(self):\n        with patch.dict(os.environ, {}, clear=True):\n            with self.assertRaises(ValueError):\n                m.env_path(\"P\", None, resolve=True)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004556", "source": "def is_stdlib_dataclass(cls: type[Any], /) -> TypeIs[type[StandardDataclass]]:\n    return '__dataclass_fields__' in cls.__dict__ and not hasattr(cls, '__pydantic_validator__')", "target": "def test_function_validator_wrapping_args_schema_before() -> None:\n    calls: list[Any] = []\n    def func(*args: Any) -> Any:\n        calls.append(args)\n        return args[0]\n    @dataclasses.dataclass\n    class Model:\n        number: int = 1\n    cs = core_schema.dataclass_schema(\n        Model,\n        core_schema.no_info_before_validator_function(\n            func,\n            core_schema.dataclass_args_schema(\n                'Model', [core_schema.dataclass_field('number', core_schema.int_schema())]\n            ),\n        ),\n        ['number'],\n    )\n    v = SchemaValidator(cs)\n    instance: Model = v.validate_python({'number': 1})\n    assert instance.number == 1\n    assert calls == [({'number': 1},)]\n    v.validate_assignment(instance, 'number', 2)\n    assert instance.number == 2\n    assert calls == [({'number': 1},), ({'number': 2},)]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004557", "source": "def is_none_type(type_: Any) -> bool:\n        return type_ in NONE_TYPES", "target": "def test_ser_function_plain():\n    def f(input: Any, info: core_schema.SerializationInfo, /) -> str:\n        return str(info)\n    s = SchemaSerializer(\n        core_schema.any_schema(\n            serialization=core_schema.plain_serializer_function_ser_schema(\n                f, info_arg=True, return_schema=core_schema.str_schema()\n            )\n        )\n    )\n    assert s.to_python(123) == (\n        \"SerializationInfo(include=None, exclude=None, context=None, mode='python', by_alias=False, exclude_unset=False, \"\n        'exclude_defaults=False, exclude_none=False, exclude_computed_fields=False, round_trip=False, serialize_as_any=False)'\n    )"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004558", "source": "def main():\n    parser = argparse.ArgumentParser(description=\"Lumos CLI\")\n    subparsers = parser.add_subparsers(dest=\"command\", required=True)\n    parser.add_argument(\n        \"--log-level\", default=\"INFO\", help=\"Log level (DEBUG, INFO, WARNING, ERROR)\"\n    )\n    register_build_commands(subparsers)\n    register_test_commands(subparsers)\n    args = parser.parse_args()\n    setup_logging(getattr(logging, args.log_level.upper(), logging.INFO))\n    logger.debug(\"Parsed args: %s\", args)\n    if hasattr(args, \"func\"):\n        args.func(args)\n    else:\n        parser.print_help()", "target": "def test_custom_working_directory_used(patch_module):\n    run_test_plan = patch_module.module.run_test_plan\n    tests_map = {\n        \"customwd\": {\n            \"title\": \"Custom wd\",\n            \"working_directory\": \"examples/ci\",\n            \"steps\": [\"pytest -q\"],\n        }\n    }\n    patch_module.run_command.return_value = 0\n    run_test_plan(\"customwd\", \"cpu\", tests_map)\n    assert patch_module.workdir_calls == [\"examples/ci\"]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004559", "source": "def literal_values(type_: Type[Any]) -> Tuple[Any, ...]:\n    return get_args(type_)", "target": "def test_schema_validator_wrong() -> None:\n    try:\n        SchemaValidator({'type': 'bad'})\n    except SchemaError:\n        pass\n    else:\n        raise AssertionError('SchemaValidator did not raise SchemaError')"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004560", "source": "def _get_validators(cls: 'DataclassClassOrWrapper') -> 'CallableGenerator':\n    yield cls.__validate__", "target": "def test_serialization_alias():\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bytes_schema(), serialization_alias='BAR'),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    s = SchemaSerializer(schema)\n    assert s.to_python(Foo(a='hello', b=b'more'), by_alias=True) == IsStrictDict(a='hello', BAR=b'more')\n    assert s.to_python(Foo(a='hello', b=b'more'), mode='json', by_alias=True) == IsStrictDict(a='hello', BAR='more')\n    j = s.to_json(Foo(a='hello', b=b'more'), by_alias=True)\n    if on_pypy:\n        assert json.loads(j) == {'a': 'hello', 'BAR': 'more'}\n    else:\n        assert j == b'{\"a\":\"hello\",\"BAR\":\"more\"}'"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004561", "source": "def custom_pydantic_encoder(type_encoders: dict[Any, Callable[[type[Any]], Any]], obj: Any) -> Any:\n    warnings.warn(\n        '`custom_pydantic_encoder` is deprecated, use `BaseModel.model_dump` instead.',\n        category=PydanticDeprecatedSince20,\n        stacklevel=2,\n    )\n    for base in obj.__class__.__mro__[:-1]:\n        try:\n            encoder = type_encoders[base]\n        except KeyError:\n            continue\n        return encoder(obj)\n    else:\n        return pydantic_encoder(obj)", "target": "def test_partial_parse():\n    with pytest.raises(ValueError, match='EOF while parsing a string at line 1 column 15'):\n        from_json('[\"aa\", \"bb\", \"c')\n    assert from_json('[\"aa\", \"bb\", \"c', allow_partial=True) == ['aa', 'bb']\n    with pytest.raises(ValueError, match='EOF while parsing a string at line 1 column 15'):\n        from_json(b'[\"aa\", \"bb\", \"c')\n    assert from_json(b'[\"aa\", \"bb\", \"c', allow_partial=True) == ['aa', 'bb']"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004562", "source": "def is_pydantic_dataclass(class_: type[Any], /) -> TypeGuard[type[PydanticDataclass]]:\n    try:\n        return '__is_pydantic_dataclass__' in class_.__dict__ and dataclasses.is_dataclass(class_)\n    except AttributeError:\n        return False", "target": "def test_dataclass_self_init():\n    @dataclasses.dataclass(init=False)\n    class Foo:\n        a: str\n        b: bool\n        def __init__(self, *args, **kwargs):\n            v.validate_python(ArgsKwargs(args, kwargs), self_instance=self)\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), kw_only=False),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), kw_only=False),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    foo = Foo(b'hello', 'True')\n    assert dataclasses.is_dataclass(foo)\n    assert dataclasses.asdict(foo) == {'a': 'hello', 'b': True}"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004563", "source": "def amend_commit_message(self, msg: str) -> None:\n        self._run_git(\"commit\", \"--amend\", \"-m\", msg)", "target": "def test_simple(self) -> None:\n        @retries_decorator()\n        def foo(x: int, y: int) -> int:\n            return x + y\n        self.assertEqual(foo(3, 4), 7)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004564", "source": "def is_valid_privateattr_name(name: str) -> bool:\n    return name.startswith('_') and not name.startswith('__')", "target": "def test_fields_required_by_default_with_default():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'x': core_schema.model_field(schema=core_schema.str_schema()),\n                'y': core_schema.model_field(\n                    schema=core_schema.with_default_schema(schema=core_schema.str_schema(), default='bulbi')\n                ),\n            }\n        )\n    )\n    assert v.validate_python({'x': 'pika', 'y': 'chu'}) == ({'x': 'pika', 'y': 'chu'}, None, {'x', 'y'})\n    assert v.validate_python({'x': 'pika'}) == ({'x': 'pika', 'y': 'bulbi'}, None, {'x'})"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004565", "source": "def new_post_init(self: 'Dataclass', *args: Any, **kwargs: Any) -> None:\n            if config.post_init_call == 'before_validation':\n                post_init(self, *args, **kwargs)\n            if self.__class__.__pydantic_run_validation__:\n                self.__pydantic_validate_values__()\n                if hasattr(self, '__post_init_post_parse__'):\n                    self.__post_init_post_parse__(*args, **kwargs)\n            if config.post_init_call == 'after_validation':\n                post_init(self, *args, **kwargs)", "target": "def test_dataclass_args_init_only_no_fields(py_and_json: PyAndJson, input_value, expected):\n    schema = core_schema.dataclass_args_schema(\n        'MyDataclass', [core_schema.dataclass_field(name='a', schema=core_schema.str_schema())], collect_init_only=True\n    )\n    v = py_and_json(schema)\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=expected.message) as exc_info:\n            v.validate_test(input_value)\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        assert v.validate_test(input_value) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004566", "source": "def _dataclass_validate_assignment_setattr(self: 'Dataclass', name: str, value: Any) -> None:\n    if self.__pydantic_initialised__:\n        d = dict(self.__dict__)\n        d.pop(name, None)\n        known_field = self.__pydantic_model__.__fields__.get(name, None)\n        if known_field:\n            value, error_ = known_field.validate(value, d, loc=name, cls=self.__class__)\n            if error_:\n                raise ValidationError([error_], self.__class__)\n    object.__setattr__(self, name, value)", "target": "def test_dataclass_args_init(input_value, extra_behavior, expected):\n    @dataclasses.dataclass\n    class Foo:\n        a: str\n        b: str\n        def __post_init__(self):\n            self.b = self.a.upper()\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.str_schema(), init=False),\n            ],\n            extra_behavior=extra_behavior,\n        ),\n        ['a', 'b'],\n        post_init=True,\n    )\n    v = SchemaValidator(schema)\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)) as exc_info:\n            v.validate_python(input_value)\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        assert dataclasses.asdict(v.validate_python(input_value)) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004567", "source": "def push(self, config_wrapper: ConfigWrapper | ConfigDict | None):\n        if config_wrapper is None:\n            yield\n            return\n        if not isinstance(config_wrapper, ConfigWrapper):\n            config_wrapper = ConfigWrapper(config_wrapper, check=False)\n        self._config_wrapper_stack.append(config_wrapper)\n        try:\n            yield\n        finally:\n            self._config_wrapper_stack.pop()", "target": "def test_cache_strings():\n    v = SchemaValidator(cs.str_schema())\n    assert 'cache_strings=True' in plain_repr(v)\n    v = SchemaValidator(cs.str_schema(), config=CoreConfig(cache_strings=True))\n    assert 'cache_strings=True' in plain_repr(v)\n    v = SchemaValidator(cs.str_schema(), config=CoreConfig(cache_strings=False))\n    assert 'cache_strings=False' in plain_repr(v)\n    v = SchemaValidator(cs.str_schema(), config=CoreConfig(cache_strings='keys'))\n    assert \"cache_strings='keys'\" in plain_repr(v)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004568", "source": "def wrap(cls: Type[Any]) -> 'DataclassClassOrWrapper':\n        should_use_proxy = (\n            use_proxy\n            if use_proxy is not None\n            else (\n                is_builtin_dataclass(cls)\n                and (cls.__bases__[0] is object or set(dir(cls)) == set(dir(cls.__bases__[0])))\n            )\n        )\n        if should_use_proxy:\n            dc_cls_doc = ''\n            dc_cls = DataclassProxy(cls)\n            default_validate_on_init = False\n        else:\n            dc_cls_doc = cls.__doc__ or ''\n            if sys.version_info >= (3, 10):\n                dc_cls = dataclasses.dataclass(\n                    cls,\n                    init=init,\n                    repr=repr,\n                    eq=eq,\n                    order=order,\n                    unsafe_hash=unsafe_hash,\n                    frozen=frozen,\n                    kw_only=kw_only,\n                )\n            else:\n                dc_cls = dataclasses.dataclass(\n                    cls, init=init, repr=repr, eq=eq, order=order, unsafe_hash=unsafe_hash, frozen=frozen\n                )\n            default_validate_on_init = True\n        should_validate_on_init = default_validate_on_init if validate_on_init is None else validate_on_init\n        _add_pydantic_validation_attributes(cls, the_config, should_validate_on_init, dc_cls_doc)\n        dc_cls.__pydantic_model__.__try_update_forward_refs__(**{cls.__name__: cls})\n        return dc_cls", "target": "def test_dataclass():\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bytes_schema()),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    s = SchemaSerializer(schema)\n    assert s.to_python(Foo(a='hello', b=b'more')) == IsStrictDict(a='hello', b=b'more')\n    assert s.to_python(Foo(a='hello', b=b'more'), mode='json') == IsStrictDict(a='hello', b='more')\n    j = s.to_json(Foo(a='hello', b=b'more'))\n    if on_pypy:\n        assert json.loads(j) == {'a': 'hello', 'b': 'more'}\n    else:\n        assert j == b'{\"a\":\"hello\",\"b\":\"more\"}'\n    assert s.to_python(Foo(a='hello', b=b'more'), exclude={'b'}) == IsStrictDict(a='hello')\n    assert s.to_json(Foo(a='hello', b=b'more'), include={'a'}) == b'{\"a\":\"hello\"}'"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004569", "source": "def flatten(schema: Definition) -> FlatIntermediateDefinition:\n    result: FlatIntermediateDefinition = {}\n    _flatten(key_prefix=(), sub_schema=schema, result=result)\n    for k, v in result.items():\n        assert isinstance(k, tuple)\n        assert all(isinstance(ki, str) for ki in k)\n        assert isinstance(v, (TimerArgs, GroupedBenchmark))\n    return result", "target": "def test_compute_diff(self) -> None:\n        diff = self.repo.diff(\"HEAD\")\n        sha = _shasum(diff)\n        self.assertEqual(len(sha), 64)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004570", "source": "def update_field_from_config(config_wrapper: ConfigWrapper, field_name: str, field_info: FieldInfo) -> None:\n    field_title_generator = field_info.field_title_generator or config_wrapper.field_title_generator\n    if field_title_generator is not None:\n        _apply_field_title_generator_to_field_info(field_title_generator, field_name, field_info)\n    if config_wrapper.alias_generator is not None:\n        _apply_alias_generator_to_field_info(config_wrapper.alias_generator, field_name, field_info)", "target": "def test_from_attributes(input_value, expected, from_attributes_mode):\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'a': core_schema.model_field(schema=core_schema.int_schema()),\n                'b': core_schema.model_field(schema=core_schema.int_schema()),\n                'c': core_schema.model_field(schema=core_schema.str_schema()),\n            },\n            from_attributes=from_attributes_mode == 'schema',\n        )\n    )\n    kwargs = {}\n    if from_attributes_mode == 'validation':\n        kwargs['from_attributes'] = True\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            val = v.validate_python(input_value, **kwargs)\n            print(f'UNEXPECTED OUTPUT: {val!r}')\n    else:\n        output = v.validate_python(input_value, **kwargs)\n        assert output == expected"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004571", "source": "def get_git_remote_name() -> str:\n    return os.getenv(\"GIT_REMOTE_NAME\", \"origin\")", "target": "def test_peek(self, input_: str = \"abcdef\") -> None:\n        iter_ = PeekableIterator(input_)\n        for idx, c in enumerate(iter_):\n            if idx + 1 < len(input_):\n                self.assertEqual(iter_.peek(), input_[idx + 1])\n            else:\n                self.assertTrue(iter_.peek() is None)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004572", "source": "def dataclass(\n        *,\n        init: bool = True,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool = False,\n        config: Union[ConfigDict, Type[object], None] = None,\n        validate_on_init: Optional[bool] = None,\n        use_proxy: Optional[bool] = None,\n    ) -> Callable[[Type[_T]], 'DataclassClassOrWrapper']:\n        ...", "target": "def test_dataclass():\n    schema = core_schema.dataclass_schema(\n        FooDataclass,\n        core_schema.dataclass_args_schema(\n            'FooDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 'hello', 'b': True})\n    assert dataclasses.is_dataclass(foo)\n    assert foo.a == 'hello'\n    assert foo.b is True\n    assert dataclasses.asdict(v.validate_python(FooDataclass(a='hello', b=True))) == {'a': 'hello', 'b': True}\n    with pytest.raises(ValidationError, match='Input should be an instance of FooDataclass') as exc_info:\n        v.validate_python({'a': 'hello', 'b': True}, strict=True)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'dataclass_exact_type',\n            'loc': (),\n            'msg': 'Input should be an instance of FooDataclass',\n            'input': {'a': 'hello', 'b': True},\n            'ctx': {'class_name': 'FooDataclass'},\n        }\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004573", "source": "def _general_metadata_cls() -> type[BaseMetadata]:\n    from annotated_types import BaseMetadata\n    class _PydanticGeneralMetadata(PydanticMetadata, BaseMetadata):\n        def __init__(self, metadata: Any):\n            self.__dict__ = metadata\n    return _PydanticGeneralMetadata", "target": "def test_forbid_extra():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={'field_a': core_schema.model_field(schema=core_schema.str_schema())}, extra_behavior='forbid'\n        )\n    )\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python({'field_a': 'abc', 'field_b': 1})\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'extra_forbidden', 'loc': ('field_b',), 'msg': 'Extra inputs are not permitted', 'input': 1}\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004574", "source": "def extract_weights(mod: nn.Module) -> tuple[tuple[Tensor, ...], list[str]]:\n    orig_params = tuple(mod.parameters())\n    names = []\n    for name, p in list(mod.named_parameters()):\n        _del_nested_attr(mod, name.split(\".\"))\n        names.append(name)\n    params = tuple(p.detach().requires_grad_() for p in orig_params)\n    return params, names", "target": "def test_pr_with_not_user_facing_label(\n        self, mocked_rn_labels: Any, mocked_gql: Any\n    ) -> None:\n        \"Test PR with 'topic: not user facing' label\"\n        pr = GitHubPR(\"pytorch\", \"pytorch\", 75095)\n        self.assertTrue(has_required_labels(pr))"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004575", "source": "def is_stdlib_dataclass(cls: type[Any], /) -> TypeIs[type[StandardDataclass]]:\n    return '__dataclass_fields__' in cls.__dict__ and not hasattr(cls, '__pydantic_validator__')", "target": "def test_slots_mixed():\n    @dataclasses.dataclass(slots=True)\n    class Model:\n        x: int\n        y: dataclasses.InitVar[str]\n        z: ClassVar[str] = 'z-classvar'\n    @dataclasses.dataclass\n    class SubModel(Model):\n        x2: int\n        y2: dataclasses.InitVar[str]\n        z2: ClassVar[str] = 'z2-classvar'\n    schema = core_schema.dataclass_schema(\n        SubModel,\n        core_schema.dataclass_args_schema(\n            'SubModel',\n            [\n                core_schema.dataclass_field(name='x', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(name='y', init_only=True, schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='x2', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(name='y2', init_only=True, schema=core_schema.str_schema()),\n            ],\n        ),\n        ['x'],\n        slots=True,\n    )\n    v = SchemaValidator(schema)\n    dc = v.validate_python({'x': 1, 'y': 'a', 'x2': 2, 'y2': 'b'})\n    assert dc.x == 1\n    assert dc.x2 == 2\n    assert dataclasses.asdict(dc) == {'x': 1, 'x2': 2}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004576", "source": "def update_field_from_config(config_wrapper: ConfigWrapper, field_name: str, field_info: FieldInfo) -> None:\n    field_title_generator = field_info.field_title_generator or config_wrapper.field_title_generator\n    if field_title_generator is not None:\n        _apply_field_title_generator_to_field_info(field_title_generator, field_name, field_info)\n    if config_wrapper.alias_generator is not None:\n        _apply_alias_generator_to_field_info(config_wrapper.alias_generator, field_name, field_info)", "target": "def test_with_default_factory():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'x': core_schema.model_field(\n                    schema=core_schema.with_default_schema(\n                        schema=core_schema.str_schema(), default_factory=lambda: 'pikachu'\n                    )\n                )\n            }\n        )\n    )\n    assert v.validate_python({}) == ({'x': 'pikachu'}, None, set())\n    assert v.validate_python({'x': 'bulbi'}) == ({'x': 'bulbi'}, None, {'x'})"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004577", "source": "def flatten(schema: Definition) -> FlatIntermediateDefinition:\n    result: FlatIntermediateDefinition = {}\n    _flatten(key_prefix=(), sub_schema=schema, result=result)\n    for k, v in result.items():\n        assert isinstance(k, tuple)\n        assert all(isinstance(ki, str) for ki in k)\n        assert isinstance(v, (TimerArgs, GroupedBenchmark))\n    return result", "target": "def test_iterator(self, input_: str = \"abcdef\") -> None:\n        iter_ = PeekableIterator(input_)\n        for idx, c in enumerate(iter_):\n            self.assertEqual(c, input_[idx])"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004578", "source": "def dataclass(\n        _cls: type[_T],\n        *,\n        init: Literal[False] = False,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool | None = None,\n        config: ConfigDict | type[object] | None = None,\n        validate_on_init: bool | None = None,\n        kw_only: bool = ...,\n        slots: bool = ...,\n    ) -> type[PydanticDataclass]: ...", "target": "def test_dataclass_self_init_post_init():\n    calls = []\n    @dataclasses.dataclass(init=False)\n    class Foo:\n        a: str\n        b: bool\n        c: dataclasses.InitVar[int]\n        def __init__(self, *args, **kwargs):\n            v.validate_python(ArgsKwargs(args, kwargs), self_instance=self)\n        def __post_init__(self, c):\n            calls.append(c)\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), kw_only=False),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), kw_only=False),\n                core_schema.dataclass_field(name='c', schema=core_schema.int_schema(), init_only=True),\n            ],\n            collect_init_only=True,\n        ),\n        ['a', 'b', 'c'],\n        post_init=True,\n    )\n    v = SchemaValidator(schema)\n    foo = Foo(b'hello', 'True', c='123')\n    assert dataclasses.is_dataclass(foo)\n    assert dataclasses.asdict(foo) == {'a': 'hello', 'b': True}\n    assert calls == [123]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004579", "source": "def pydantic_general_metadata(**metadata: Any) -> BaseMetadata:\n    return _general_metadata_cls()(metadata)", "target": "def test_simple():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'field_a': core_schema.model_field(schema=core_schema.str_schema()),\n                'field_b': core_schema.model_field(schema=core_schema.int_schema()),\n            }\n        )\n    )\n    assert v.validate_python({'field_a': b'abc', 'field_b': 1}) == (\n        {'field_a': 'abc', 'field_b': 1},\n        None,\n        {'field_a', 'field_b'},\n    )"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004580", "source": "def _validate_dataclass(cls: Type['DataclassT'], v: Any) -> 'DataclassT':\n    with set_validation(cls, True):\n        if isinstance(v, cls):\n            v.__pydantic_validate_values__()\n            return v\n        elif isinstance(v, (list, tuple)):\n            return cls(*v)\n        elif isinstance(v, dict):\n            return cls(**v)\n        else:\n            raise DataclassTypeError(class_name=cls.__name__)", "target": "def test_by_alias_and_name_config_interaction(config, runtime, expected) -> None:\n    @dataclasses.dataclass\n    class Foo:\n        my_field: str\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(\n                    name='my_field', schema=core_schema.str_schema(), serialization_alias='my_alias'\n                ),\n            ],\n        ),\n        ['my_field'],\n        config=core_schema.CoreConfig(serialize_by_alias=config or False),\n    )\n    s = SchemaSerializer(schema)\n    assert s.to_python(Foo(my_field='hello'), by_alias=runtime) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004581", "source": "def dataclass(\n    _cls: Optional[Type[_T]] = None,\n    *,\n    init: bool = True,\n    repr: bool = True,\n    eq: bool = True,\n    order: bool = False,\n    unsafe_hash: bool = False,\n    frozen: bool = False,\n    config: Union[ConfigDict, Type[object], None] = None,\n    validate_on_init: Optional[bool] = None,\n    use_proxy: Optional[bool] = None,\n    kw_only: bool = False,\n) -> Union[Callable[[Type[_T]], 'DataclassClassOrWrapper'], 'DataclassClassOrWrapper']:\n    the_config = get_config(config)\n    def wrap(cls: Type[Any]) -> 'DataclassClassOrWrapper':\n        should_use_proxy = (\n            use_proxy\n            if use_proxy is not None\n            else (\n                is_builtin_dataclass(cls)\n                and (cls.__bases__[0] is object or set(dir(cls)) == set(dir(cls.__bases__[0])))\n            )\n        )\n        if should_use_proxy:\n            dc_cls_doc = ''\n            dc_cls = DataclassProxy(cls)\n            default_validate_on_init = False\n        else:\n            dc_cls_doc = cls.__doc__ or ''\n            if sys.version_info >= (3, 10):\n                dc_cls = dataclasses.dataclass(\n                    cls,\n                    init=init,\n                    repr=repr,\n                    eq=eq,\n                    order=order,\n                    unsafe_hash=unsafe_hash,\n                    frozen=frozen,\n                    kw_only=kw_only,\n                )\n            else:\n                dc_cls = dataclasses.dataclass(\n                    cls, init=init, repr=repr, eq=eq, order=order, unsafe_hash=unsafe_hash, frozen=frozen\n                )\n            default_validate_on_init = True\n        should_validate_on_init = default_validate_on_init if validate_on_init is None else validate_on_init\n        _add_pydantic_validation_attributes(cls, the_config, should_validate_on_init, dc_cls_doc)\n        dc_cls.__pydantic_model__.__try_update_forward_refs__(**{cls.__name__: cls})\n        return dc_cls\n    if _cls is None:\n        return wrap\n    return wrap(_cls)", "target": "def test_extra_behavior_allow(\n    config: Union[core_schema.CoreConfig, None],\n    schema_extra_behavior_kw: dict[str, Any],\n    validate_fn_extra_kw: Union[ExtraBehavior, None],\n):\n    @dataclasses.dataclass\n    class MyModel:\n        f: str\n    v = SchemaValidator(\n        core_schema.dataclass_schema(\n            MyModel,\n            core_schema.dataclass_args_schema(\n                'MyModel', [core_schema.dataclass_field('f', core_schema.str_schema())], **schema_extra_behavior_kw\n            ),\n            ['f'],\n            config=config,\n        )\n    )\n    m: MyModel = v.validate_python({'f': 'x', 'extra_field': '123'}, extra=validate_fn_extra_kw)\n    assert m.f == 'x'\n    assert getattr(m, 'extra_field') == '123'\n    v.validate_assignment(m, 'f', 'y', extra=validate_fn_extra_kw)\n    assert m.f == 'y'\n    v.validate_assignment(m, 'not_f', '123', extra=validate_fn_extra_kw)\n    assert getattr(m, 'not_f') == '123'"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004582", "source": "def takes_validated_data_argument(\n    default_factory: Callable[[], Any] | Callable[[dict[str, Any]], Any],\n) -> TypeIs[Callable[[dict[str, Any]], Any]]:\n    try:\n        sig = signature(default_factory)\n    except (ValueError, TypeError):\n        return False\n    parameters = list(sig.parameters.values())\n    return len(parameters) == 1 and can_be_positional(parameters[0]) and parameters[0].default is Parameter.empty", "target": "def test_from_attributes_type_error():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'a': core_schema.model_field(schema=core_schema.int_schema()),\n                'b': core_schema.model_field(schema=core_schema.int_schema()),\n                'c': core_schema.model_field(schema=core_schema.str_schema()),\n            },\n            from_attributes=True,\n            model_name='MyModel',\n        )\n    )\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python('123')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'model_attributes_type',\n            'loc': (),\n            'msg': 'Input should be a valid dictionary or object to extract fields from',\n            'input': '123',\n        }\n    ]\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_json('123')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'model_type',\n            'loc': (),\n            'msg': 'Input should be an object',\n            'input': 123,\n            'ctx': {'class_name': 'MyModel'},\n        }\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004583", "source": "def timedelta_isoformat(td: datetime.timedelta) -> str:\n    minutes, seconds = divmod(td.seconds, 60)\n    hours, minutes = divmod(minutes, 60)\n    return f'{\"-\" if td.days < 0 else \"\"}P{abs(td.days)}DT{hours:d}H{minutes:d}M{seconds:d}.{td.microseconds:06d}S'", "target": "def test_inf_nan_allow():\n    v = SchemaValidator(core_schema.float_schema(allow_inf_nan=True))\n    assert v.validate_json('Infinity') == float('inf')\n    assert v.validate_json('-Infinity') == float('-inf')\n    assert v.validate_json('NaN') == IsFloatNan()"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004584", "source": "def _dataclass_validate_values(self: 'Dataclass') -> None:\n    if getattr(self, '__pydantic_initialised__'):\n        return\n    if getattr(self, '__pydantic_has_field_info_default__', False):\n        input_data = {\n            k: v\n            for k, v in self.__dict__.items()\n            if not (isinstance(v, FieldInfo) or _is_field_cached_property(self, k))\n        }\n    else:\n        input_data = {k: v for k, v in self.__dict__.items() if not _is_field_cached_property(self, k)}\n    d, _, validation_error = validate_model(self.__pydantic_model__, input_data, cls=self.__class__)\n    if validation_error:\n        raise validation_error\n    self.__dict__.update(d)\n    object.__setattr__(self, '__pydantic_initialised__', True)", "target": "def test_by_alias_and_name_config_interaction(config, runtime, expected) -> None:\n    @dataclasses.dataclass\n    class Foo:\n        my_field: str\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(\n                    name='my_field', schema=core_schema.str_schema(), serialization_alias='my_alias'\n                ),\n            ],\n        ),\n        ['my_field'],\n        config=core_schema.CoreConfig(serialize_by_alias=config or False),\n    )\n    s = SchemaSerializer(schema)\n    assert s.to_python(Foo(my_field='hello'), by_alias=runtime) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004585", "source": "def _update_fields_from_docstrings(cls: type[Any], fields: dict[str, FieldInfo], use_inspect: bool = False) -> None:\n    fields_docs = extract_docstrings_from_cls(cls, use_inspect=use_inspect)\n    for ann_name, field_info in fields.items():\n        if field_info.description is None and ann_name in fields_docs:\n            field_info.description = fields_docs[ann_name]", "target": "def test_alias_allow_pop(py_and_json: PyAndJson):\n    v = py_and_json(\n        {\n            'type': 'model-fields',\n            'fields': {'field_a': {'validation_alias': 'FieldA', 'type': 'model-field', 'schema': {'type': 'int'}}},\n        },\n        config=CoreConfig(validate_by_name=True),\n    )\n    assert v.validate_test({'FieldA': '123'}) == ({'field_a': 123}, None, {'field_a'})\n    assert v.validate_test({'field_a': '123'}) == ({'field_a': 123}, None, {'field_a'})\n    assert v.validate_test({'FieldA': '1', 'field_a': '2'}) == ({'field_a': 1}, None, {'field_a'})\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'foobar': '123'})"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004586", "source": "def _check_typeddict_special(type_: Any) -> bool:\n    return type_ is TypedDictRequired or type_ is TypedDictNotRequired", "target": "def test_schema_validator_wrong() -> None:\n    try:\n        SchemaValidator({'type': 'bad'})\n    except SchemaError:\n        pass\n    else:\n        raise AssertionError('SchemaValidator did not raise SchemaError')"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004587", "source": "def run_command(\n    cmd: str,\n    use_shell: bool = False,\n    log_cmd: bool = True,\n    cwd: Optional[str] = None,\n    env: Optional[dict] = None,\n    check: bool = True,\n) -> int:\n    if use_shell:\n        args = cmd\n        log_prefix = \"[shell]\"\n        executable = \"/bin/bash\"\n    else:\n        args = shlex.split(cmd)\n        log_prefix = \"[cmd]\"\n        executable = None\n    if log_cmd:\n        display_cmd = cmd if use_shell else \" \".join(args)\n        logger.info(\"%s %s\", log_prefix, display_cmd)\n    run_env = {**os.environ, **(env or {})}\n    proc = subprocess.run(\n        args,\n        shell=use_shell,\n        executable=executable,\n        stdout=sys.stdout,\n        stderr=sys.stderr,\n        cwd=cwd,\n        env=run_env,\n        check=False,\n    )\n    if check and proc.returncode != 0:\n        logger.error(\n            \"%s Command failed (exit %s): %s\", log_prefix, proc.returncode, cmd\n        )\n        raise subprocess.CalledProcessError(\n            proc.returncode, args if not use_shell else cmd\n        )\n    return proc.returncode", "target": "def test_merged_lastfailed_content_with_overlap(self) -> None:\n        last_failed_source = {\n            \"tools/tests/test_foo.py::test_num1\": True,\n            \"tools/tests/test_foo.py::test_num2\": True,\n            \"tools/tests/test_bar.py::test_num1\": True,\n        }\n        last_failed_dest = {\n            \"tools/tests/test_foo.py::test_num1\": True,\n            \"tools/tests/test_car.py::test_num1\": True,\n            \"tools/tests/test_car.py::test_num2\": True,\n        }\n        last_failed_merged = {\n            \"tools/tests/test_foo.py::test_num1\": True,\n            \"tools/tests/test_foo.py::test_num2\": True,\n            \"tools/tests/test_bar.py::test_num1\": True,\n            \"tools/tests/test_car.py::test_num1\": True,\n            \"tools/tests/test_car.py::test_num2\": True,\n        }\n        merged = _merged_lastfailed_content(last_failed_source, last_failed_dest)\n        self.assertEqual(merged, last_failed_merged)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004588", "source": "def is_builtin_dataclass(_cls: Type[Any]) -> bool:\n    return (\n        dataclasses.is_dataclass(_cls)\n        and not hasattr(_cls, '__pydantic_model__')\n        and set(_cls.__dataclass_fields__).issuperset(set(getattr(_cls, '__annotations__', {})))\n    )", "target": "def test_extra_behavior_ignore(\n    config: Union[core_schema.CoreConfig, None],\n    schema_extra_behavior_kw: dict[str, Any],\n    validate_fn_extra_kw: Union[ExtraBehavior, None],\n):\n    @dataclasses.dataclass\n    class MyModel:\n        f: str\n    v = SchemaValidator(\n        core_schema.dataclass_schema(\n            MyModel,\n            core_schema.dataclass_args_schema(\n                'MyModel', [core_schema.dataclass_field('f', core_schema.str_schema())], **schema_extra_behavior_kw\n            ),\n            ['f'],\n        ),\n        config=config,\n    )\n    m: MyModel = v.validate_python({'f': 'x', 'extra_field': 123}, extra=validate_fn_extra_kw)\n    assert m.f == 'x'\n    assert not hasattr(m, 'extra_field')\n    v.validate_assignment(m, 'f', 'y', extra=validate_fn_extra_kw)\n    assert m.f == 'y'\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_assignment(m, 'not_f', 'xyz', extra=validate_fn_extra_kw)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'no_such_attribute',\n            'loc': ('not_f',),\n            'msg': \"Object has no attribute 'not_f'\",\n            'input': 'xyz',\n            'ctx': {'attribute': 'not_f'},\n        }\n    ]\n    assert not hasattr(m, 'not_f')"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004589", "source": "def _call_initvar(*args: Any, **kwargs: Any) -> NoReturn:\n        raise TypeError(\"'InitVar' object is not callable\")", "target": "def test_dataclass_wrap_json():\n    schema = core_schema.no_info_wrap_validator_function(\n        lambda v, handler: handler(v),\n        core_schema.dataclass_schema(\n            FooDataclass,\n            core_schema.dataclass_args_schema(\n                'FooDataclass',\n                [\n                    core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                    core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n                ],\n            ),\n            ['a', 'b'],\n        ),\n    )\n    v = SchemaValidator(schema)\n    assert v.validate_json('{\"a\": \"hello\", \"b\": true}') == FooDataclass(a='hello', b=True)\n    assert v.validate_json('{\"a\": \"hello\", \"b\": true}', strict=True) == FooDataclass(a='hello', b=True)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004590", "source": "def decimal_encoder(dec_value: Decimal) -> Union[int, float]:\n    exponent = dec_value.as_tuple().exponent\n    if isinstance(exponent, int) and exponent >= 0:\n        return int(dec_value)\n    else:\n        return float(dec_value)", "target": "def test_to_jsonable_python_schema_serializer():\n    class Foobar:\n        def __init__(self, my_foo: int, my_inners: list['Foobar']):\n            self.my_foo = my_foo\n            self.my_inners = my_inners\n    c = core_schema.definitions_schema(\n        core_schema.definition_reference_schema(schema_ref='foobar'),\n        [\n            core_schema.model_schema(\n                Foobar,\n                core_schema.typed_dict_schema(\n                    {\n                        'my_foo': core_schema.typed_dict_field(core_schema.int_schema(), serialization_alias='myFoo'),\n                        'my_inners': core_schema.typed_dict_field(\n                            core_schema.list_schema(core_schema.definition_reference_schema('foobar')),\n                            serialization_alias='myInners',\n                        ),\n                    }\n                ),\n                ref='foobar',\n            )\n        ],\n    )\n    v = SchemaValidator(c)\n    s = SchemaSerializer(c)\n    Foobar.__pydantic_validator__ = v\n    Foobar.__pydantic_serializer__ = s\n    instance = Foobar(my_foo=1, my_inners=[Foobar(my_foo=2, my_inners=[])])\n    assert to_jsonable_python(instance, by_alias=True) == {'myFoo': 1, 'myInners': [{'myFoo': 2, 'myInners': []}]}\n    assert to_jsonable_python(instance, by_alias=False) == {'my_foo': 1, 'my_inners': [{'my_foo': 2, 'my_inners': []}]}\n    assert to_json(instance, by_alias=True) == b'{\"myFoo\":1,\"myInners\":[{\"myFoo\":2,\"myInners\":[]}]}'\n    assert to_json(instance, by_alias=False) == b'{\"my_foo\":1,\"my_inners\":[{\"my_foo\":2,\"my_inners\":[]}]}'"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004591", "source": "def secs_to_us(time_s):\n    return time_s * 1e6", "target": "def test_raises_for_missing_dir(self):\n        start = Path.cwd()\n        with tempfile.TemporaryDirectory() as td:\n            missing = Path(td) / \"does_not_exist\"\n            with self.assertRaises(FileNotFoundError):\n                with working_directory(str(missing)):\n                    pass\n        self.assertEqual(Path.cwd(), start)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004592", "source": "def dataclass(\n        _cls: Type[_T],\n        *,\n        init: bool = True,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool = False,\n        config: Union[ConfigDict, Type[object], None] = None,\n        validate_on_init: Optional[bool] = None,\n        use_proxy: Optional[bool] = None,\n        kw_only: bool = ...,\n    ) -> 'DataclassClassOrWrapper':\n        ...", "target": "def test_dataclass_self_init_alias():\n    @dataclasses.dataclass(init=False)\n    class Foo:\n        a: str\n        b: bool\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), validation_alias='aAlias'),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), validation_alias=['bAlias', 0]),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    def __init__(self, *args, **kwargs):\n        v.validate_python(ArgsKwargs(args, kwargs), self_instance=self)\n    Foo.__init__ = __init__\n    foo = Foo(aAlias=b'hello', bAlias=['True'])\n    assert dataclasses.is_dataclass(foo)\n    assert dataclasses.asdict(foo) == {'a': 'hello', 'b': True}\n    with pytest.raises(ValidationError) as exc_info:\n        Foo(aAlias=b'hello', bAlias=['wrong'])\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'bool_parsing',\n            'loc': ('bAlias', 0),\n            'msg': 'Input should be a valid boolean, unable to interpret input',\n            'input': 'wrong',\n        }\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004593", "source": "def _recreate_field_info(\n    field_info: FieldInfo,\n    ns_resolver: NsResolver,\n    typevars_map: Mapping[TypeVar, Any],\n    *,\n    lenient: bool,\n) -> FieldInfo:\n    FieldInfo_ = import_cached_field_info()\n    existing_desc = field_info.description\n    if lenient:\n        ann = _generics.replace_types(field_info._original_annotation, typevars_map)\n        ann, evaluated = _typing_extra.try_eval_type(\n            ann,\n            *ns_resolver.types_namespace,\n        )\n    else:\n        ann = _typing_extra.eval_type(\n            field_info._original_annotation,\n            *ns_resolver.types_namespace,\n        )\n        ann = _generics.replace_types(ann, typevars_map)\n        ann = _typing_extra.eval_type(\n            ann,\n            *ns_resolver.types_namespace,\n        )\n        evaluated = True\n    if (assign := field_info._original_assignment) is PydanticUndefined:\n        new_field = FieldInfo_.from_annotation(ann, _source=AnnotationSource.CLASS)\n    else:\n        new_field = FieldInfo_.from_annotated_attribute(ann, assign, _source=AnnotationSource.CLASS)\n        new_field._original_assignment = assign\n    new_field._original_annotation = ann\n    new_field.description = new_field.description if new_field.description is not None else existing_desc\n    if not evaluated:\n        new_field._complete = False\n    return new_field", "target": "def test_missing_error(pydantic_version):\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'field_a': core_schema.model_field(schema=core_schema.str_schema()),\n                'field_b': core_schema.model_field(schema=core_schema.int_schema()),\n            }\n        )\n    )\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python({'field_a': b'abc'})\n    assert (\n        str(exc_info.value)\n        ==\n        + (\n            f'\\n    For further information visit https://errors.pydantic.dev/{pydantic_version}/v/missing'\n            if os.environ.get('PYDANTIC_ERRORS_INCLUDE_URL', '1') != 'false'\n            else ''\n        )\n    )"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004594", "source": "def dataclass(\n        _cls: type[_T],\n        *,\n        init: Literal[False] = False,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool | None = None,\n        config: ConfigDict | type[object] | None = None,\n        validate_on_init: bool | None = None,\n        kw_only: bool = ...,\n        slots: bool = ...,\n    ) -> type[PydanticDataclass]: ...", "target": "def test_function_validator_wrapping_args_schema_after() -> None:\n    calls: list[Any] = []\n    def func(*args: Any) -> Any:\n        calls.append(args)\n        return args[0]\n    @dataclasses.dataclass\n    class Model:\n        number: int = 1\n    cs = core_schema.dataclass_schema(\n        Model,\n        core_schema.no_info_after_validator_function(\n            func,\n            core_schema.dataclass_args_schema(\n                'Model', [core_schema.dataclass_field('number', core_schema.int_schema())]\n            ),\n        ),\n        ['number'],\n    )\n    v = SchemaValidator(cs)\n    instance: Model = v.validate_python({'number': 1})\n    assert instance.number == 1\n    assert calls == [(({'number': 1}, None),)]\n    v.validate_assignment(instance, 'number', 2)\n    assert instance.number == 2\n    assert calls == [(({'number': 1}, None),), (({'number': 2}, None),)]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004595", "source": "def update_field_forward_refs(field: 'ModelField', globalns: Any, localns: Any) -> None:\n    prepare = False\n    if field.type_.__class__ == ForwardRef:\n        prepare = True\n        field.type_ = evaluate_forwardref(field.type_, globalns, localns or None)\n    if field.outer_type_.__class__ == ForwardRef:\n        prepare = True\n        field.outer_type_ = evaluate_forwardref(field.outer_type_, globalns, localns or None)\n    if prepare:\n        field.prepare()\n    if field.sub_fields:\n        for sub_f in field.sub_fields:\n            update_field_forward_refs(sub_f, globalns=globalns, localns=localns)\n    if field.discriminator_key is not None:\n        field.prepare_discriminated_union_sub_fields()", "target": "def test_ser_function_wrap():\n    def f(\n        input: Any, serialize: core_schema.SerializerFunctionWrapHandler, info: core_schema.SerializationInfo, /\n    ) -> str:\n        return f'{serialize} {info}'\n    s = SchemaSerializer(\n        core_schema.any_schema(\n            serialization=core_schema.wrap_serializer_function_ser_schema(\n                f, info_arg=True, schema=core_schema.str_schema(), when_used='json'\n            )\n        )\n    )\n    assert s.to_python(123, mode='json') == (\n        'SerializationCallable(serializer=str) '\n        \"SerializationInfo(include=None, exclude=None, context=None, mode='json', by_alias=False, exclude_unset=False, \"\n        'exclude_defaults=False, exclude_none=False, exclude_computed_fields=False, round_trip=False, serialize_as_any=False)'\n    )"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004596", "source": "def dataclass(\n    _cls: Optional[Type[_T]] = None,\n    *,\n    init: bool = True,\n    repr: bool = True,\n    eq: bool = True,\n    order: bool = False,\n    unsafe_hash: bool = False,\n    frozen: bool = False,\n    config: Union[ConfigDict, Type[object], None] = None,\n    validate_on_init: Optional[bool] = None,\n    use_proxy: Optional[bool] = None,\n    kw_only: bool = False,\n) -> Union[Callable[[Type[_T]], 'DataclassClassOrWrapper'], 'DataclassClassOrWrapper']:\n    the_config = get_config(config)\n    def wrap(cls: Type[Any]) -> 'DataclassClassOrWrapper':\n        should_use_proxy = (\n            use_proxy\n            if use_proxy is not None\n            else (\n                is_builtin_dataclass(cls)\n                and (cls.__bases__[0] is object or set(dir(cls)) == set(dir(cls.__bases__[0])))\n            )\n        )\n        if should_use_proxy:\n            dc_cls_doc = ''\n            dc_cls = DataclassProxy(cls)\n            default_validate_on_init = False\n        else:\n            dc_cls_doc = cls.__doc__ or ''\n            if sys.version_info >= (3, 10):\n                dc_cls = dataclasses.dataclass(\n                    cls,\n                    init=init,\n                    repr=repr,\n                    eq=eq,\n                    order=order,\n                    unsafe_hash=unsafe_hash,\n                    frozen=frozen,\n                    kw_only=kw_only,\n                )\n            else:\n                dc_cls = dataclasses.dataclass(\n                    cls, init=init, repr=repr, eq=eq, order=order, unsafe_hash=unsafe_hash, frozen=frozen\n                )\n            default_validate_on_init = True\n        should_validate_on_init = default_validate_on_init if validate_on_init is None else validate_on_init\n        _add_pydantic_validation_attributes(cls, the_config, should_validate_on_init, dc_cls_doc)\n        dc_cls.__pydantic_model__.__try_update_forward_refs__(**{cls.__name__: cls})\n        return dc_cls\n    if _cls is None:\n        return wrap\n    return wrap(_cls)", "target": "def test_dataclass_field_after_validator():\n    @dataclasses.dataclass\n    class Foo:\n        a: int\n        b: str\n        @classmethod\n        def validate_b(cls, v: str, info: core_schema.ValidationInfo) -> str:\n            assert v == 'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return 'hello world!'\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(\n                    name='b',\n                    schema=core_schema.with_info_after_validator_function(Foo.validate_b, core_schema.str_schema()),\n                ),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 1, 'b': b'hello'})\n    assert dataclasses.asdict(foo) == {'a': 1, 'b': 'hello world!'}"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004597", "source": "def are_ghstack_branches_in_sync(\n    repo: GitRepo, head_ref: str, base_ref: Optional[str] = None\n) -> bool:\n    orig_ref = re.sub(r\"/head$\", \"/orig\", head_ref)\n    if base_ref is None:\n        base_ref = re.sub(r\"/head$\", \"/base\", head_ref)\n    orig_diff_sha = _shasum(repo.diff(f\"{repo.remote}/{orig_ref}\"))\n    head_diff_sha = _shasum(\n        repo.diff(\n            base_ref if is_commit_hash(base_ref) else f\"{repo.remote}/{base_ref}\",\n            f\"{repo.remote}/{head_ref}\",\n        )\n    )\n    return orig_diff_sha == head_diff_sha", "target": "def test_fails(self) -> None:\n        @retries_decorator(rc=0)\n        def foo(x: int, y: int) -> int:\n            return x + y\n        self.assertEqual(foo(\"a\", 4), 0)"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004598", "source": "def _del_nested_attr(obj: nn.Module, names: list[str]) -> None:\n    if len(names) == 1:\n        delattr(obj, names[0])\n    else:\n        _del_nested_attr(getattr(obj, names[0]), names[1:])", "target": "def test_gh_get_labels(\n        self,\n        mock_request_for_labels: Any,\n        mock_get_last_page_num_from_header: Any,\n    ) -> None:\n        res = gh_get_labels(\"mock_org\", \"mock_repo\")\n        mock_get_last_page_num_from_header.assert_called_once()\n        self.assertEqual(res, [\"foo\"] * 3)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004599", "source": "def _apply_alias_generator_to_field_info(\n    alias_generator: Callable[[str], str] | AliasGenerator, field_name: str, field_info: FieldInfo\n):\n    if (\n        field_info.alias_priority is None\n        or field_info.alias_priority <= 1\n        or field_info.alias is None\n        or field_info.validation_alias is None\n        or field_info.serialization_alias is None\n    ):\n        alias, validation_alias, serialization_alias = None, None, None\n        if isinstance(alias_generator, AliasGenerator):\n            alias, validation_alias, serialization_alias = alias_generator.generate_aliases(field_name)\n        elif callable(alias_generator):\n            alias = alias_generator(field_name)\n            if not isinstance(alias, str):\n                raise TypeError(f'alias_generator {alias_generator} must return str, not {alias.__class__}')\n        if field_info.alias_priority is None or field_info.alias_priority <= 1:\n            field_info.alias_priority = 1\n        if field_info.alias_priority == 1:\n            field_info.serialization_alias = get_first_not_none(serialization_alias, alias)\n            field_info.validation_alias = get_first_not_none(validation_alias, alias)\n            field_info.alias = alias\n        if field_info.alias is None:\n            field_info.alias = alias\n        if field_info.serialization_alias is None:\n            field_info.serialization_alias = get_first_not_none(serialization_alias, alias)\n        if field_info.validation_alias is None:\n            field_info.validation_alias = get_first_not_none(validation_alias, alias)", "target": "def test_alias_extra_from_attributes():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            extra_behavior='allow',\n            from_attributes=True,\n            fields={\n                'field_a': core_schema.model_field(\n                    validation_alias=[['FieldA'], ['foo', 2]], schema=core_schema.int_schema()\n                )\n            },\n        )\n    )\n    assert v.validate_python({'FieldA': 1}) == ({'field_a': 1}, {}, {'field_a'})\n    assert v.validate_python(Cls(FieldA=1)) == ({'field_a': 1}, {}, {'field_a'})\n    assert v.validate_python(Cls(foo=[1, 2, 3])) == ({'field_a': 3}, {}, {'field_a'})\n    assert v.validate_python({'foo': [1, 2, 3]}) == ({'field_a': 3}, {}, {'field_a'})"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004600", "source": "def patterns_to_regex(allowed_patterns: list[str]) -> Any:\n    rc = \"(\"\n    for idx, pattern in enumerate(allowed_patterns):\n        if idx > 0:\n            rc += \"|\"\n        pattern_ = PeekableIterator(pattern)\n        assert not any(c in pattern for c in \"{}()[]\\\\\")\n        for c in pattern_:\n            if c == \".\":\n                rc += \"\\\\.\"\n            elif c == \"+\":\n                rc += \"\\\\+\"\n            elif c == \"*\":\n                if pattern_.peek() == \"*\":\n                    next(pattern_)\n                    rc += \".*\"\n                else:\n                    rc += \"[^/]*\"\n            else:\n                rc += c\n    rc += \")\"\n    return re.compile(rc)", "target": "def test_fails(self) -> None:\n        @retries_decorator(rc=0)\n        def foo(x: int, y: int) -> int:\n            return x + y\n        self.assertEqual(foo(\"a\", 4), 0)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004601", "source": "def pydantic_encoder(obj: Any) -> Any:\n    warnings.warn(\n        '`pydantic_encoder` is deprecated, use `pydantic_core.to_jsonable_python` instead.',\n        category=PydanticDeprecatedSince20,\n        stacklevel=2,\n    )\n    from dataclasses import asdict, is_dataclass\n    BaseModel = import_cached_base_model()\n    if isinstance(obj, BaseModel):\n        return obj.model_dump()\n    elif is_dataclass(obj):\n        return asdict(obj)\n    for base in obj.__class__.__mro__[:-1]:\n        try:\n            encoder = ENCODERS_BY_TYPE[base]\n        except KeyError:\n            continue\n        return encoder(obj)\n    else:\n        raise TypeError(f\"Object of type '{obj.__class__.__name__}' is not JSON serializable\")", "target": "def test_json_invalid():\n    v = SchemaValidator(core_schema.bool_schema())\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_json('\"foobar')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'json_invalid',\n            'loc': (),\n            'msg': 'Invalid JSON: EOF while parsing a string at line 1 column 7',\n            'input': '\"foobar',\n            'ctx': {'error': 'EOF while parsing a string at line 1 column 7'},\n        }\n    ]\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_json('[1,\\n2,\\n3,]')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'json_invalid',\n            'loc': (),\n            'msg': 'Invalid JSON: trailing comma at line 3 column 3',\n            'input': '[1,\\n2,\\n3,]',\n            'ctx': {'error': 'trailing comma at line 3 column 3'},\n        }\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004602", "source": "def _check_typeddict_special(type_: Any) -> bool:\n    return type_ is TypedDictRequired or type_ is TypedDictNotRequired", "target": "def test_error_details() -> None:\n    def act_on_error_details(_: ErrorDetails) -> None:\n        pass\n    v = SchemaValidator({'type': 'int'})\n    try:\n        v.validate_python('not an int')\n    except ValidationError as err:\n        for details in err.errors(include_url=False):\n            act_on_error_details(details)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004603", "source": "def pydantic_general_metadata(**metadata: Any) -> BaseMetadata:\n    return _general_metadata_cls()(metadata)", "target": "def test_fields_required_by_default():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'x': core_schema.model_field(schema=core_schema.str_schema()),\n                'y': core_schema.model_field(schema=core_schema.str_schema()),\n            }\n        )\n    )\n    assert v.validate_python({'x': 'pika', 'y': 'chu'}) == ({'x': 'pika', 'y': 'chu'}, None, {'x', 'y'})\n    with pytest.raises(ValidationError) as exc_info:\n        assert v.validate_python({'x': 'pika'})\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'missing', 'loc': ('y',), 'msg': 'Field required', 'input': {'x': 'pika'}}\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004604", "source": "def env_path_field(\n    name: str,\n    default: Union[str, Path] = \"\",\n    *,\n    resolve: bool = True,\n) -> Path:\n    return field(default_factory=lambda: env_path(name, default, resolve=resolve))", "target": "def test_env_path_optional_respects_resolve_true(self):\n        with patch.dict(os.environ, {\"P\": \"a/b\"}, clear=True):\n            p = m.env_path_optional(\"P\", resolve=True)\n            self.assertIsInstance(p, Path)\n            if p:\n                self.assertTrue(p.is_absolute())"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004605", "source": "def env_str_field(\n    name: str,\n    default: str = \"\",\n) -> str:\n    return field(default_factory=lambda: get_env(name, default))", "target": "def test_env_path_optional_respects_resolve_true(self):\n        with patch.dict(os.environ, {\"P\": \"a/b\"}, clear=True):\n            p = m.env_path_optional(\"P\", resolve=True)\n            self.assertIsInstance(p, Path)\n            if p:\n                self.assertTrue(p.is_absolute())"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004606", "source": "def run_command(\n    cmd: str,\n    use_shell: bool = False,\n    log_cmd: bool = True,\n    cwd: Optional[str] = None,\n    env: Optional[dict] = None,\n    check: bool = True,\n) -> int:\n    if use_shell:\n        args = cmd\n        log_prefix = \"[shell]\"\n        executable = \"/bin/bash\"\n    else:\n        args = shlex.split(cmd)\n        log_prefix = \"[cmd]\"\n        executable = None\n    if log_cmd:\n        display_cmd = cmd if use_shell else \" \".join(args)\n        logger.info(\"%s %s\", log_prefix, display_cmd)\n    run_env = {**os.environ, **(env or {})}\n    proc = subprocess.run(\n        args,\n        shell=use_shell,\n        executable=executable,\n        stdout=sys.stdout,\n        stderr=sys.stderr,\n        cwd=cwd,\n        env=run_env,\n        check=False,\n    )\n    if check and proc.returncode != 0:\n        logger.error(\n            \"%s Command failed (exit %s): %s\", log_prefix, proc.returncode, cmd\n        )\n        raise subprocess.CalledProcessError(\n            proc.returncode, args if not use_shell else cmd\n        )\n    return proc.returncode", "target": "def test_pr_with_missing_labels(\n        self, mocked_rn_labels: Any, mocked_gql: Any\n    ) -> None:\n        \"Test PR with no 'release notes:' label or 'topic: not user facing' label\"\n        pr = GitHubPR(\"pytorch\", \"pytorch\", 82169)\n        self.assertFalse(has_required_labels(pr))"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004607", "source": "def run_command(\n    cmd: str,\n    use_shell: bool = False,\n    log_cmd: bool = True,\n    cwd: Optional[str] = None,\n    env: Optional[dict] = None,\n    check: bool = True,\n) -> int:\n    if use_shell:\n        args = cmd\n        log_prefix = \"[shell]\"\n        executable = \"/bin/bash\"\n    else:\n        args = shlex.split(cmd)\n        log_prefix = \"[cmd]\"\n        executable = None\n    if log_cmd:\n        display_cmd = cmd if use_shell else \" \".join(args)\n        logger.info(\"%s %s\", log_prefix, display_cmd)\n    run_env = {**os.environ, **(env or {})}\n    proc = subprocess.run(\n        args,\n        shell=use_shell,\n        executable=executable,\n        stdout=sys.stdout,\n        stderr=sys.stderr,\n        cwd=cwd,\n        env=run_env,\n        check=False,\n    )\n    if check and proc.returncode != 0:\n        logger.error(\n            \"%s Command failed (exit %s): %s\", log_prefix, proc.returncode, cmd\n        )\n        raise subprocess.CalledProcessError(\n            proc.returncode, args if not use_shell else cmd\n        )\n    return proc.returncode", "target": "def test_fails(self) -> None:\n        @retries_decorator(rc=0)\n        def foo(x: int, y: int) -> int:\n            return x + y\n        self.assertEqual(foo(\"a\", 4), 0)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004608", "source": "def _pydantic_fields_complete(cls: type[PydanticDataclass]) -> bool:\n    return all(field_info._complete for field_info in cls.__pydantic_fields__.values())", "target": "def test_dataclass_wrap_json():\n    schema = core_schema.no_info_wrap_validator_function(\n        lambda v, handler: handler(v),\n        core_schema.dataclass_schema(\n            FooDataclass,\n            core_schema.dataclass_args_schema(\n                'FooDataclass',\n                [\n                    core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                    core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n                ],\n            ),\n            ['a', 'b'],\n        ),\n    )\n    v = SchemaValidator(schema)\n    assert v.validate_json('{\"a\": \"hello\", \"b\": true}') == FooDataclass(a='hello', b=True)\n    assert v.validate_json('{\"a\": \"hello\", \"b\": true}', strict=True) == FooDataclass(a='hello', b=True)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004609", "source": "def is_valid_field_name(name: str) -> bool:\n    return not name.startswith('_')", "target": "def test_alias_extra(py_and_json: PyAndJson):\n    v = py_and_json(\n        {\n            'type': 'model-fields',\n            'extra_behavior': 'allow',\n            'fields': {\n                'field_a': {\n                    'validation_alias': [['FieldA'], ['foo', 2]],\n                    'type': 'model-field',\n                    'schema': {'type': 'int'},\n                }\n            },\n        },\n        {'loc_by_alias': False},\n    )\n    assert v.validate_test({'FieldA': 1}) == ({'field_a': 1}, {}, {'field_a'})\n    assert v.validate_test({'foo': [1, 2, 3]}) == ({'field_a': 3}, {}, {'field_a'})\n    with pytest.raises(ValidationError) as exc_info:\n        assert v.validate_test({'FieldA': '...'}) == ({'field_a': 1}, {}, {'field_a'})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('field_a',),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': '...',\n        }\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004610", "source": "def _check_protected_namespaces(\n    protected_namespaces: tuple[str | Pattern[str], ...],\n    ann_name: str,\n    bases: tuple[type[Any], ...],\n    cls_name: str,\n) -> None:\n    BaseModel = import_cached_base_model()\n    for protected_namespace in protected_namespaces:\n        ns_violation = False\n        if isinstance(protected_namespace, Pattern):\n            ns_violation = protected_namespace.match(ann_name) is not None\n        elif isinstance(protected_namespace, str):\n            ns_violation = ann_name.startswith(protected_namespace)\n        if ns_violation:\n            for b in bases:\n                if hasattr(b, ann_name):\n                    if not (issubclass(b, BaseModel) and ann_name in getattr(b, '__pydantic_fields__', {})):\n                        raise ValueError(\n                            f'Field {ann_name!r} conflicts with member {getattr(b, ann_name)}'\n                            f' of protected namespace {protected_namespace!r}.'\n                        )\n            else:\n                valid_namespaces: list[str] = []\n                for pn in protected_namespaces:\n                    if isinstance(pn, Pattern):\n                        if not pn.match(ann_name):\n                            valid_namespaces.append(f're.compile({pn.pattern!r})')\n                    else:\n                        if not ann_name.startswith(pn):\n                            valid_namespaces.append(f\"'{pn}'\")\n                valid_namespaces_str = f'({\", \".join(valid_namespaces)}{\",)\" if len(valid_namespaces) == 1 else \")\"}'\n                warnings.warn(\n                    f'Field {ann_name!r} in {cls_name!r} conflicts with protected namespace {protected_namespace!r}.\\n\\n'\n                    f\"You may be able to solve this by setting the 'protected_namespaces' configuration to {valid_namespaces_str}.\",\n                    UserWarning,\n                    stacklevel=5,\n                )", "target": "def test_aliases_path_multiple(py_and_json: PyAndJson, input_value, expected):\n    v = py_and_json(\n        {\n            'type': 'model-fields',\n            'fields': {\n                'field_a': {\n                    'validation_alias': [['foo', 'bar', 'bat'], ['foo', 3], ['spam']],\n                    'type': 'model-field',\n                    'schema': {'type': 'int'},\n                }\n            },\n        },\n        {'loc_by_alias': False},\n    )\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=expected.message):\n            val = v.validate_test(input_value)\n            print(f'UNEXPECTED OUTPUT: {val!r}')\n    else:\n        output = v.validate_test(input_value)\n        assert output == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004611", "source": "def rebuild_dataclass_fields(\n    cls: type[PydanticDataclass],\n    *,\n    config_wrapper: ConfigWrapper,\n    ns_resolver: NsResolver,\n    typevars_map: Mapping[TypeVar, Any],\n) -> dict[str, FieldInfo]:\n    FieldInfo_ = import_cached_field_info()\n    rebuilt_fields: dict[str, FieldInfo] = {}\n    with ns_resolver.push(cls):\n        for f_name, field_info in cls.__pydantic_fields__.items():\n            if field_info._complete:\n                rebuilt_fields[f_name] = field_info\n            else:\n                existing_desc = field_info.description\n                ann = _typing_extra.eval_type(\n                    field_info._original_annotation,\n                    *ns_resolver.types_namespace,\n                )\n                ann = _generics.replace_types(ann, typevars_map)\n                new_field = FieldInfo_.from_annotated_attribute(\n                    ann,\n                    field_info._original_assignment,\n                    _source=AnnotationSource.DATACLASS,\n                )\n                new_field.description = new_field.description if new_field.description is not None else existing_desc\n                update_field_from_config(config_wrapper, f_name, new_field)\n                rebuilt_fields[f_name] = new_field\n    return rebuilt_fields", "target": "def test_json_error():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'field_a': core_schema.model_field(\n                    schema=core_schema.list_schema(items_schema=core_schema.int_schema())\n                )\n            }\n        )\n    )\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_json('{\"field_a\": [123, \"wrong\"]}')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('field_a', 1),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'wrong',\n        }\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004612", "source": "def from_markdown_table(data: str) -> TimingResultType:\n    out = data.strip().split(\"\\n\")\n    out = out[2:]\n    res: TimingResultType\n    res = defaultdict(defaultdict)\n    for line in out:\n        model, task, mean, var = (f.strip() for f in line.strip().split(\"|\") if f)\n        res[model][task] = (float(mean), float(var))\n    return res", "target": "def test_merged_lastfailed_content_with_empty_dest(self) -> None:\n        last_failed_source = {\n            \"tools/tests/test_car.py::test_num1\": True,\n            \"tools/tests/test_car.py::test_num2\": True,\n        }\n        last_failed_dest = {\n            \"\": True,\n        }\n        last_failed_merged = {\n            \"tools/tests/test_car.py::test_num1\": True,\n            \"tools/tests/test_car.py::test_num2\": True,\n        }\n        merged = _merged_lastfailed_content(last_failed_source, last_failed_dest)\n        self.assertEqual(merged, last_failed_merged)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004613", "source": "def dataclass(\n        _cls: Type[_T],\n        *,\n        init: bool = True,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool = False,\n        config: Union[ConfigDict, Type[object], None] = None,\n        validate_on_init: Optional[bool] = None,\n        use_proxy: Optional[bool] = None,\n    ) -> 'DataclassClassOrWrapper':\n        ...", "target": "def test_dataclass_field_after_validator():\n    @dataclasses.dataclass\n    class Foo:\n        a: int\n        b: str\n        @classmethod\n        def validate_b(cls, v: str, info: core_schema.ValidationInfo) -> str:\n            assert v == 'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return 'hello world!'\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(\n                    name='b',\n                    schema=core_schema.with_info_after_validator_function(Foo.validate_b, core_schema.str_schema()),\n                ),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 1, 'b': b'hello'})\n    assert dataclasses.asdict(foo) == {'a': 1, 'b': 'hello world!'}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004614", "source": "def dataclass(\n        _cls: type[_T],\n        *,\n        init: Literal[False] = False,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool | None = None,\n        config: ConfigDict | type[object] | None = None,\n        validate_on_init: bool | None = None,\n    ) -> type[PydanticDataclass]: ...", "target": "def test_dataclass_field_plain_validator():\n    @dataclasses.dataclass\n    class Foo:\n        a: int\n        b: str\n        @classmethod\n        def validate_b(cls, v: bytes, info: core_schema.ValidationInfo) -> str:\n            assert v == b'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return 'hello world!'\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(\n                    name='b', schema=core_schema.with_info_plain_validator_function(Foo.validate_b)\n                ),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 1, 'b': b'hello'})\n    assert dataclasses.asdict(foo) == {'a': 1, 'b': 'hello world!'}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004615", "source": "def dataclass(\n        *,\n        init: Literal[False] = False,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool | None = None,\n        config: ConfigDict | type[object] | None = None,\n        validate_on_init: bool | None = None,\n    ) -> Callable[[type[_T]], type[PydanticDataclass]]:\n        ...", "target": "def test_dataclass_slots_field_after_validator():\n    @dataclasses.dataclass(slots=True)\n    class Foo:\n        a: int\n        b: str\n        @classmethod\n        def validate_b(cls, v: str, info: core_schema.ValidationInfo) -> str:\n            assert v == 'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return 'hello world!'\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(\n                    name='b',\n                    schema=core_schema.with_info_after_validator_function(Foo.validate_b, core_schema.str_schema()),\n                ),\n            ],\n        ),\n        ['a', 'b'],\n        slots=True,\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 1, 'b': b'hello'})\n    assert dataclasses.asdict(foo) == {'a': 1, 'b': 'hello world!'}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004616", "source": "def inherit_config(self_config: 'ConfigType', parent_config: 'ConfigType', **namespace: Any) -> 'ConfigType':\n    if not self_config:\n        base_classes: Tuple['ConfigType', ...] = (parent_config,)\n    elif self_config == parent_config:\n        base_classes = (self_config,)\n    else:\n        base_classes = self_config, parent_config\n    namespace['json_encoders'] = {\n        **getattr(parent_config, 'json_encoders', {}),\n        **getattr(self_config, 'json_encoders', {}),\n        **namespace.get('json_encoders', {}),\n    }\n    return type('Config', base_classes, namespace)", "target": "def test_on_model_class():\n    v = SchemaValidator(\n        cs.model_schema(\n            cls=MyModel,\n            config=CoreConfig(str_max_length=5),\n            schema=cs.model_fields_schema(fields={'f': cs.model_field(schema=cs.str_schema())}),\n        )\n    )\n    assert 'max_length:Some(5)' in plain_repr(v)\n    assert v.isinstance_python({'f': 'test'}) is True\n    assert v.isinstance_python({'f': 'test long'}) is False"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004617", "source": "def pydantic_encoder(obj: Any) -> Any:\n    from dataclasses import asdict, is_dataclass\n    from pydantic.v1.main import BaseModel\n    if isinstance(obj, BaseModel):\n        return obj.dict()\n    elif is_dataclass(obj):\n        return asdict(obj)\n    for base in obj.__class__.__mro__[:-1]:\n        try:\n            encoder = ENCODERS_BY_TYPE[base]\n        except KeyError:\n            continue\n        return encoder(obj)\n    else:\n        raise TypeError(f\"Object of type '{obj.__class__.__name__}' is not JSON serializable\")", "target": "def test_null():\n    assert SchemaValidator(core_schema.none_schema()).validate_json('null') is None"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004618", "source": "def get_commit(self, ref: str) -> GitCommit:\n        return parse_fuller_format(\n            self._run_git(\"show\", \"--format=fuller\", \"--date=unix\", \"--shortstat\", ref)\n        )", "target": "def test_simple(self) -> None:\n        @retries_decorator()\n        def foo(x: int, y: int) -> int:\n            return x + y\n        self.assertEqual(foo(3, 4), 7)"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004619", "source": "def to_markdown_table(\n    res: TimingResultType, header: Optional[tuple[str, ...]] = None\n) -> str:\n    if header is None:\n        header = (\"model\", \"task\", \"mean\", \"var\")\n    out = \"\"\n    def write_line(*args):\n        nonlocal out\n        out += f\"| {' | '.join(str(a) for a in args)} |\\n\"\n    write_line(*header)\n    write_line(*[\"--\"] * len(header))\n    for model, tasks in res.items():\n        for task, line in tasks.items():\n            write_line(*(model, task) + line)\n    return out", "target": "def test_iterator(self, input_: str = \"abcdef\") -> None:\n        iter_ = PeekableIterator(input_)\n        for idx, c in enumerate(iter_):\n            self.assertEqual(c, input_[idx])"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004620", "source": "def get_temp_dir() -> str:\n    global _TEMPDIR\n    if _TEMPDIR is None:\n        _TEMPDIR = _make_temp_dir(\n            prefix=\"instruction_count_microbenchmarks\", gc_dev_shm=True\n        )\n        atexit.register(shutil.rmtree, path=_TEMPDIR)\n    return _TEMPDIR", "target": "def test_pr_with_missing_labels(\n        self, mocked_rn_labels: Any, mocked_gql: Any\n    ) -> None:\n        \"Test PR with no 'release notes:' label or 'topic: not user facing' label\"\n        pr = GitHubPR(\"pytorch\", \"pytorch\", 82169)\n        self.assertFalse(has_required_labels(pr))"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004621", "source": "def ip_v6_address_validator(v: Any) -> IPv6Address:\n    if isinstance(v, IPv6Address):\n        return v\n    try:\n        return IPv6Address(v)\n    except ValueError:\n        raise errors.IPv6AddressError()", "target": "def test_validators_build(benchmark) -> None:\n    class Base1(BaseModel):\n        a: int\n        @field_validator('a', mode='after')\n        @classmethod\n        def val_a(cls, value: int) -> int: ...\n        @computed_field\n        def prop(self) -> int: ...\n    class Bare:\n        @computed_field\n        def prop_bare(self) -> int: ...\n    class Sub1(Base1):\n        @computed_field\n        def prop_2(self) -> int: ...\n        @computed_field\n        def prop_3(self) -> int: ...\n        @computed_field\n        def prop_4(self) -> int: ...\n    @benchmark\n    def bench() -> None:\n        class SubS(Sub1, Bare, defer_build=True):\n            @computed_field\n            def prop_5(self) -> int: ...\n            @computed_field\n            def prop_6(self) -> int: ...\n            @computed_field\n            def prop_7(self) -> int: ..."}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004622", "source": "def ensure_dir_exists(path: Union[str, Path]) -> Path:\n    path_obj = get_path(path)\n    path_obj.mkdir(parents=True, exist_ok=True)\n    return path_obj", "target": "def test_copy_dir_to_new_dir(self):\n        src = self.tmp_path / \"srcdir\"\n        (src / \"a\").mkdir(parents=True)\n        (src / \"a\" / \"f.txt\").write_text(\"content\")\n        dst = self.tmp_path / \"destdir\"\n        copy(src, dst)\n        self.assertEqual((dst / \"a\" / \"f.txt\").read_text(), \"content\")"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004623", "source": "def with_params_help(params_cls: type, title: str = \"Parameter defaults\"):\n    if not is_dataclass(params_cls):\n        raise TypeError(f\"{params_cls} must be a dataclass\")\n    def _decorator(cls: type) -> type:\n        block = generate_dataclass_help(params_cls)\n        cls.__doc__ = (cls.__doc__ or \"\") + f\"\\n\\n{title}:\\n{block}\"\n        return cls\n    return _decorator", "target": "def test_get_env_not_exist_returns_default(self):\n        with patch.dict(os.environ, {\"FOO\": \"bar\"}, clear=True):\n            self.assertEqual(m.get_env(\"TEST_NOT_EXIST\", \"default\"), \"default\")"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004624", "source": "def _recreate_field_info(\n    field_info: FieldInfo,\n    ns_resolver: NsResolver,\n    typevars_map: Mapping[TypeVar, Any],\n    *,\n    lenient: bool,\n) -> FieldInfo:\n    FieldInfo_ = import_cached_field_info()\n    existing_desc = field_info.description\n    if lenient:\n        ann = _generics.replace_types(field_info._original_annotation, typevars_map)\n        ann, evaluated = _typing_extra.try_eval_type(\n            ann,\n            *ns_resolver.types_namespace,\n        )\n    else:\n        ann = _typing_extra.eval_type(\n            field_info._original_annotation,\n            *ns_resolver.types_namespace,\n        )\n        ann = _generics.replace_types(ann, typevars_map)\n        ann = _typing_extra.eval_type(\n            ann,\n            *ns_resolver.types_namespace,\n        )\n        evaluated = True\n    if (assign := field_info._original_assignment) is PydanticUndefined:\n        new_field = FieldInfo_.from_annotation(ann, _source=AnnotationSource.CLASS)\n    else:\n        new_field = FieldInfo_.from_annotated_attribute(ann, assign, _source=AnnotationSource.CLASS)\n        new_field._original_assignment = assign\n    new_field._original_annotation = ann\n    new_field.description = new_field.description if new_field.description is not None else existing_desc\n    if not evaluated:\n        new_field._complete = False\n    return new_field", "target": "def test_empty_model():\n    v = SchemaValidator(core_schema.model_fields_schema(fields={}))\n    assert v.validate_python({}) == ({}, None, set())\n    with pytest.raises(\n        ValidationError, match=re.escape('Input should be a valid dictionary or instance of Model [type=model_type,')\n    ):\n        v.validate_python('x')"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004625", "source": "def dataclass(\n    _cls: type[_T] | None = None,\n    *,\n    init: Literal[False] = False,\n    repr: bool = True,\n    eq: bool = True,\n    order: bool = False,\n    unsafe_hash: bool = False,\n    frozen: bool | None = None,\n    config: ConfigDict | type[object] | None = None,\n    validate_on_init: bool | None = None,\n    kw_only: bool = False,\n    slots: bool = False,\n) -> Callable[[type[_T]], type[PydanticDataclass]] | type[PydanticDataclass]:\n    assert init is False, 'pydantic.dataclasses.dataclass only supports init=False'\n    assert validate_on_init is not False, 'validate_on_init=False is no longer supported'\n    if sys.version_info >= (3, 10):\n        kwargs = {'kw_only': kw_only, 'slots': slots}\n    else:\n        kwargs = {}\n    def create_dataclass(cls: type[Any]) -> type[PydanticDataclass]:\n        from ._internal._utils import is_model_class\n        if is_model_class(cls):\n            raise PydanticUserError(\n                f'Cannot create a Pydantic dataclass from {cls.__name__} as it is already a Pydantic model',\n                code='dataclass-on-model',\n            )\n        original_cls = cls\n        has_dataclass_base = any(dataclasses.is_dataclass(base) for base in cls.__bases__)\n        if not has_dataclass_base and config is not None and hasattr(cls, '__pydantic_config__'):\n            warn(\n                f'`config` is set via both the `dataclass` decorator and `__pydantic_config__` for dataclass {cls.__name__}. '\n                f'The `config` specification from `dataclass` decorator will take priority.',\n                category=UserWarning,\n                stacklevel=2,\n            )\n        config_dict = config if config is not None else getattr(cls, '__pydantic_config__', None)\n        config_wrapper = _config.ConfigWrapper(config_dict)\n        decorators = _decorators.DecoratorInfos.build(cls, replace_wrapped_methods=True)\n        decorators.update_from_config(config_wrapper)\n        original_doc = cls.__doc__\n        if _pydantic_dataclasses.is_stdlib_dataclass(cls):\n            original_doc = None\n            bases = (cls,)\n            if issubclass(cls, Generic):\n                generic_base = Generic[cls.__parameters__]\n                bases = bases + (generic_base,)\n            cls = types.new_class(cls.__name__, bases)\n        if frozen is not None:\n            frozen_ = frozen\n            if config_wrapper.frozen:\n                warn(\n                    f'`frozen` is set via both the `dataclass` decorator and `config` for dataclass {cls.__name__!r}.'\n                    'This is not recommended. The `frozen` specification on `dataclass` will take priority.',\n                    category=UserWarning,\n                    stacklevel=2,\n                )\n        else:\n            frozen_ = config_wrapper.frozen or False\n        cls_anns = _typing_extra.safe_get_annotations(cls)\n        for field_name in cls_anns:\n            field_value = getattr(cls, field_name, None)\n            if isinstance(field_value, FieldInfo):\n                setattr(cls, field_name, _pydantic_dataclasses.as_dataclass_field(field_value))\n        with _pydantic_dataclasses.patch_base_fields(cls):\n            cls = dataclasses.dataclass(\n                cls,\n                init=True,\n                repr=repr,\n                eq=eq,\n                order=order,\n                unsafe_hash=unsafe_hash,\n                frozen=frozen_,\n                **kwargs,\n            )\n        if config_wrapper.validate_assignment:\n            original_setattr = cls.__setattr__\n            @functools.wraps(cls.__setattr__)\n            def validated_setattr(instance: PydanticDataclass, name: str, value: Any, /) -> None:\n                if frozen_:\n                    return original_setattr(instance, name, value)\n                inst_cls = type(instance)\n                attr = getattr(inst_cls, name, None)\n                if isinstance(attr, property):\n                    attr.__set__(instance, value)\n                elif isinstance(attr, functools.cached_property):\n                    instance.__dict__.__setitem__(name, value)\n                else:\n                    inst_cls.__pydantic_validator__.validate_assignment(instance, name, value)\n            cls.__setattr__ = validated_setattr.__get__(None, cls)\n            if slots and not hasattr(cls, '__setstate__'):\n                def _dataclass_getstate(self: Any) -> list[Any]:\n                    return [getattr(self, f.name) for f in dataclasses.fields(self)]\n                def _dataclass_setstate(self: Any, state: list[Any]) -> None:\n                    for field, value in zip(dataclasses.fields(self), state):\n                        object.__setattr__(self, field.name, value)\n                cls.__getstate__ = _dataclass_getstate\n                cls.__setstate__ = _dataclass_setstate\n        cls.__is_pydantic_dataclass__ = True\n        cls.__pydantic_decorators__ = decorators\n        cls.__doc__ = original_doc\n        firstlineno = getattr(original_cls, '__firstlineno__', None)\n        cls.__module__ = original_cls.__module__\n        if sys.version_info >= (3, 13) and firstlineno is not None:\n            original_cls.__firstlineno__ = firstlineno\n            cls.__firstlineno__ = firstlineno\n        cls.__qualname__ = original_cls.__qualname__\n        cls.__pydantic_fields_complete__ = classmethod(_pydantic_fields_complete)\n        cls.__pydantic_complete__ = False\n        _pydantic_dataclasses.complete_dataclass(cls, config_wrapper, raise_errors=False)\n        return cls\n    return create_dataclass if _cls is None else create_dataclass(_cls)", "target": "def test_dataclass():\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bytes_schema()),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    s = SchemaSerializer(schema)\n    assert s.to_python(Foo(a='hello', b=b'more')) == IsStrictDict(a='hello', b=b'more')\n    assert s.to_python(Foo(a='hello', b=b'more'), mode='json') == IsStrictDict(a='hello', b='more')\n    j = s.to_json(Foo(a='hello', b=b'more'))\n    if on_pypy:\n        assert json.loads(j) == {'a': 'hello', 'b': 'more'}\n    else:\n        assert j == b'{\"a\":\"hello\",\"b\":\"more\"}'\n    assert s.to_python(Foo(a='hello', b=b'more'), exclude={'b'}) == IsStrictDict(a='hello')\n    assert s.to_json(Foo(a='hello', b=b'more'), include={'a'}) == b'{\"a\":\"hello\"}'"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004626", "source": "def new_post_init(self: 'Dataclass', *args: Any, **kwargs: Any) -> None:\n            if config.post_init_call == 'before_validation':\n                post_init(self, *args, **kwargs)\n            if self.__class__.__pydantic_run_validation__:\n                self.__pydantic_validate_values__()\n                if hasattr(self, '__post_init_post_parse__'):\n                    self.__post_init_post_parse__(*args, **kwargs)\n            if config.post_init_call == 'after_validation':\n                post_init(self, *args, **kwargs)", "target": "def test_function_validator_wrapping_args_schema_wrap() -> None:\n    calls: list[Any] = []\n    def func(*args: Any) -> Any:\n        assert len(args) == 2\n        input, handler = args\n        output = handler(input)\n        calls.append((input, output))\n        return output\n    @dataclasses.dataclass\n    class Model:\n        number: int = 1\n    cs = core_schema.dataclass_schema(\n        Model,\n        core_schema.no_info_wrap_validator_function(\n            func,\n            core_schema.dataclass_args_schema(\n                'Model', [core_schema.dataclass_field('number', core_schema.int_schema())]\n            ),\n        ),\n        ['number'],\n    )\n    v = SchemaValidator(cs)\n    instance: Model = v.validate_python({'number': 1})\n    assert instance.number == 1\n    assert calls == [({'number': 1}, ({'number': 1}, None))]\n    v.validate_assignment(instance, 'number', 2)\n    assert instance.number == 2\n    assert calls == [({'number': 1}, ({'number': 1}, None)), ({'number': 2}, ({'number': 2}, None))]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004627", "source": "def dataclass(\n    _cls: Optional[Type[_T]] = None,\n    *,\n    init: bool = True,\n    repr: bool = True,\n    eq: bool = True,\n    order: bool = False,\n    unsafe_hash: bool = False,\n    frozen: bool = False,\n    config: Union[ConfigDict, Type[object], None] = None,\n    validate_on_init: Optional[bool] = None,\n    use_proxy: Optional[bool] = None,\n    kw_only: bool = False,\n) -> Union[Callable[[Type[_T]], 'DataclassClassOrWrapper'], 'DataclassClassOrWrapper']:\n    the_config = get_config(config)\n    def wrap(cls: Type[Any]) -> 'DataclassClassOrWrapper':\n        should_use_proxy = (\n            use_proxy\n            if use_proxy is not None\n            else (\n                is_builtin_dataclass(cls)\n                and (cls.__bases__[0] is object or set(dir(cls)) == set(dir(cls.__bases__[0])))\n            )\n        )\n        if should_use_proxy:\n            dc_cls_doc = ''\n            dc_cls = DataclassProxy(cls)\n            default_validate_on_init = False\n        else:\n            dc_cls_doc = cls.__doc__ or ''\n            if sys.version_info >= (3, 10):\n                dc_cls = dataclasses.dataclass(\n                    cls,\n                    init=init,\n                    repr=repr,\n                    eq=eq,\n                    order=order,\n                    unsafe_hash=unsafe_hash,\n                    frozen=frozen,\n                    kw_only=kw_only,\n                )\n            else:\n                dc_cls = dataclasses.dataclass(\n                    cls, init=init, repr=repr, eq=eq, order=order, unsafe_hash=unsafe_hash, frozen=frozen\n                )\n            default_validate_on_init = True\n        should_validate_on_init = default_validate_on_init if validate_on_init is None else validate_on_init\n        _add_pydantic_validation_attributes(cls, the_config, should_validate_on_init, dc_cls_doc)\n        dc_cls.__pydantic_model__.__try_update_forward_refs__(**{cls.__name__: cls})\n        return dc_cls\n    if _cls is None:\n        return wrap\n    return wrap(_cls)", "target": "def test_custom_dataclass_names():\n    schema = core_schema.dataclass_schema(\n        FooParentDataclass,\n        core_schema.dataclass_args_schema(\n            'FooParentDataclass',\n            [\n                core_schema.dataclass_field(\n                    name='foo',\n                    schema=core_schema.union_schema(\n                        [\n                            core_schema.dataclass_schema(\n                                FooDataclass,\n                                core_schema.dataclass_args_schema(\n                                    'FooDataclass[dataclass_args_schema]',\n                                    [\n                                        core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                                        core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n                                    ],\n                                ),\n                                ['a', 'b'],\n                                cls_name='FooDataclass[cls_name]',\n                            ),\n                            core_schema.none_schema(),\n                        ]\n                    ),\n                )\n            ],\n        ),\n        ['foo'],\n    )\n    v = SchemaValidator(schema)\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python({'foo': 123})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'class_name': 'FooDataclass[dataclass_args_schema]'},\n            'input': 123,\n            'loc': ('foo', 'FooDataclass[cls_name]'),\n            'msg': 'Input should be a dictionary or an instance of FooDataclass[dataclass_args_schema]',\n            'type': 'dataclass_type',\n        },\n        {'input': 123, 'loc': ('foo', 'none'), 'msg': 'Input should be None', 'type': 'none_required'},\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004628", "source": "def _infer_discriminator_values_for_choice(\n        self, choice: core_schema.CoreSchema, source_name: str | None\n    ) -> list[str | int]:\n        if choice['type'] == 'definitions':\n            return self._infer_discriminator_values_for_choice(choice['schema'], source_name=source_name)\n        elif _core_utils.is_function_with_inner_schema(choice):\n            return self._infer_discriminator_values_for_choice(choice['schema'], source_name=source_name)\n        elif choice['type'] == 'lax-or-strict':\n            return sorted(\n                set(\n                    self._infer_discriminator_values_for_choice(choice['lax_schema'], source_name=None)\n                    + self._infer_discriminator_values_for_choice(choice['strict_schema'], source_name=None)\n                )\n            )\n        elif choice['type'] == 'tagged-union':\n            values: list[str | int] = []\n            subchoices = [x for x in choice['choices'].values() if not isinstance(x, (str, int))]\n            for subchoice in subchoices:\n                subchoice_values = self._infer_discriminator_values_for_choice(subchoice, source_name=None)\n                values.extend(subchoice_values)\n            return values\n        elif choice['type'] == 'union':\n            values = []\n            for subchoice in choice['choices']:\n                subchoice_schema = subchoice[0] if isinstance(subchoice, tuple) else subchoice\n                subchoice_values = self._infer_discriminator_values_for_choice(subchoice_schema, source_name=None)\n                values.extend(subchoice_values)\n            return values\n        elif choice['type'] == 'nullable':\n            self._should_be_nullable = True\n            return self._infer_discriminator_values_for_choice(choice['schema'], source_name=None)\n        elif choice['type'] == 'model':\n            return self._infer_discriminator_values_for_choice(choice['schema'], source_name=choice['cls'].__name__)\n        elif choice['type'] == 'dataclass':\n            return self._infer_discriminator_values_for_choice(choice['schema'], source_name=choice['cls'].__name__)\n        elif choice['type'] == 'model-fields':\n            return self._infer_discriminator_values_for_model_choice(choice, source_name=source_name)\n        elif choice['type'] == 'dataclass-args':\n            return self._infer_discriminator_values_for_dataclass_choice(choice, source_name=source_name)\n        elif choice['type'] == 'typed-dict':\n            return self._infer_discriminator_values_for_typed_dict_choice(choice, source_name=source_name)\n        elif choice['type'] == 'definition-ref':\n            schema_ref = choice['schema_ref']\n            if schema_ref not in self.definitions:\n                raise MissingDefinitionForUnionRef(schema_ref)\n            return self._infer_discriminator_values_for_choice(self.definitions[schema_ref], source_name=source_name)\n        else:\n            err_str = f'The core schema type {choice[\"type\"]!r} is not a valid discriminated union variant.'\n            if choice['type'] == 'list':\n                err_str += (\n                    ' If you are making use of a list of union types, make sure the discriminator is applied to the '\n                    'union type and not the list (e.g. `list[Annotated[<T> | <U>, Field(discriminator=...)]]`).'\n                )\n            raise TypeError(err_str)", "target": "def test_efficiency_with_highly_nested_examples(benchmark) -> None:\n    @benchmark\n    def run():\n        for i in range(1, 12):\n            very_nested_input = build_nested_state(i)\n            any_state_adapter.validate_python(very_nested_input)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004629", "source": "def push(self, config_wrapper: ConfigWrapper | ConfigDict | None):\n        if config_wrapper is None:\n            yield\n            return\n        if not isinstance(config_wrapper, ConfigWrapper):\n            config_wrapper = ConfigWrapper(config_wrapper, check=False)\n        self._config_wrapper_stack.append(config_wrapper)\n        try:\n            yield\n        finally:\n            self._config_wrapper_stack.pop()", "target": "def test_allow_inf_nan(config: CoreConfig, float_field_schema, input_value, expected):\n    v = SchemaValidator(\n        cs.model_schema(\n            cls=MyModel,\n            schema=cs.model_fields_schema(fields={'x': cs.model_field(schema=float_field_schema)}),\n            config=config,\n        )\n    )\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            v.validate_python(input_value)\n    else:\n        output_dict = v.validate_python(input_value)\n        assert output_dict == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004630", "source": "def custom_pydantic_encoder(type_encoders: dict[Any, Callable[[type[Any]], Any]], obj: Any) -> Any:\n    warnings.warn(\n        '`custom_pydantic_encoder` is deprecated, use `BaseModel.model_dump` instead.',\n        category=PydanticDeprecatedSince20,\n        stacklevel=2,\n    )\n    for base in obj.__class__.__mro__[:-1]:\n        try:\n            encoder = type_encoders[base]\n        except KeyError:\n            continue\n        return encoder(obj)\n    else:\n        return pydantic_encoder(obj)", "target": "def test_bytes():\n    s = SchemaValidator(core_schema.bytes_schema())\n    assert s.validate_json('\"foobar\"') == b'foobar'\n    with pytest.raises(ValidationError, match=r'Input should be a valid bytes \\[type=bytes_type,'):\n        s.validate_json('false')\n    with pytest.raises(ValidationError, match=r'Input should be a valid bytes \\[type=bytes_type,'):\n        s.validate_json('123')"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004631", "source": "def _general_metadata_cls() -> type[BaseMetadata]:\n    from annotated_types import BaseMetadata\n    class _PydanticGeneralMetadata(PydanticMetadata, BaseMetadata):\n        def __init__(self, metadata: Any):\n            self.__dict__ = metadata\n    return _PydanticGeneralMetadata", "target": "def test_fields_required_by_default():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'x': core_schema.model_field(schema=core_schema.str_schema()),\n                'y': core_schema.model_field(schema=core_schema.str_schema()),\n            }\n        )\n    )\n    assert v.validate_python({'x': 'pika', 'y': 'chu'}) == ({'x': 'pika', 'y': 'chu'}, None, {'x', 'y'})\n    with pytest.raises(ValidationError) as exc_info:\n        assert v.validate_python({'x': 'pika'})\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'missing', 'loc': ('y',), 'msg': 'Field required', 'input': {'x': 'pika'}}\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004632", "source": "def dataclass(\n    _cls: type[_T] | None = None,\n    *,\n    init: Literal[False] = False,\n    repr: bool = True,\n    eq: bool = True,\n    order: bool = False,\n    unsafe_hash: bool = False,\n    frozen: bool | None = None,\n    config: ConfigDict | type[object] | None = None,\n    validate_on_init: bool | None = None,\n    kw_only: bool = False,\n    slots: bool = False,\n) -> Callable[[type[_T]], type[PydanticDataclass]] | type[PydanticDataclass]:\n    assert init is False, 'pydantic.dataclasses.dataclass only supports init=False'\n    assert validate_on_init is not False, 'validate_on_init=False is no longer supported'\n    if sys.version_info >= (3, 10):\n        kwargs = {'kw_only': kw_only, 'slots': slots}\n    else:\n        kwargs = {}\n    def create_dataclass(cls: type[Any]) -> type[PydanticDataclass]:\n        from ._internal._utils import is_model_class\n        if is_model_class(cls):\n            raise PydanticUserError(\n                f'Cannot create a Pydantic dataclass from {cls.__name__} as it is already a Pydantic model',\n                code='dataclass-on-model',\n            )\n        original_cls = cls\n        has_dataclass_base = any(dataclasses.is_dataclass(base) for base in cls.__bases__)\n        if not has_dataclass_base and config is not None and hasattr(cls, '__pydantic_config__'):\n            warn(\n                f'`config` is set via both the `dataclass` decorator and `__pydantic_config__` for dataclass {cls.__name__}. '\n                f'The `config` specification from `dataclass` decorator will take priority.',\n                category=UserWarning,\n                stacklevel=2,\n            )\n        config_dict = config if config is not None else getattr(cls, '__pydantic_config__', None)\n        config_wrapper = _config.ConfigWrapper(config_dict)\n        decorators = _decorators.DecoratorInfos.build(cls, replace_wrapped_methods=True)\n        decorators.update_from_config(config_wrapper)\n        original_doc = cls.__doc__\n        if _pydantic_dataclasses.is_stdlib_dataclass(cls):\n            original_doc = None\n            bases = (cls,)\n            if issubclass(cls, Generic):\n                generic_base = Generic[cls.__parameters__]\n                bases = bases + (generic_base,)\n            cls = types.new_class(cls.__name__, bases)\n        if frozen is not None:\n            frozen_ = frozen\n            if config_wrapper.frozen:\n                warn(\n                    f'`frozen` is set via both the `dataclass` decorator and `config` for dataclass {cls.__name__!r}.'\n                    'This is not recommended. The `frozen` specification on `dataclass` will take priority.',\n                    category=UserWarning,\n                    stacklevel=2,\n                )\n        else:\n            frozen_ = config_wrapper.frozen or False\n        cls_anns = _typing_extra.safe_get_annotations(cls)\n        for field_name in cls_anns:\n            field_value = getattr(cls, field_name, None)\n            if isinstance(field_value, FieldInfo):\n                setattr(cls, field_name, _pydantic_dataclasses.as_dataclass_field(field_value))\n        with _pydantic_dataclasses.patch_base_fields(cls):\n            cls = dataclasses.dataclass(\n                cls,\n                init=True,\n                repr=repr,\n                eq=eq,\n                order=order,\n                unsafe_hash=unsafe_hash,\n                frozen=frozen_,\n                **kwargs,\n            )\n        if config_wrapper.validate_assignment:\n            original_setattr = cls.__setattr__\n            @functools.wraps(cls.__setattr__)\n            def validated_setattr(instance: PydanticDataclass, name: str, value: Any, /) -> None:\n                if frozen_:\n                    return original_setattr(instance, name, value)\n                inst_cls = type(instance)\n                attr = getattr(inst_cls, name, None)\n                if isinstance(attr, property):\n                    attr.__set__(instance, value)\n                elif isinstance(attr, functools.cached_property):\n                    instance.__dict__.__setitem__(name, value)\n                else:\n                    inst_cls.__pydantic_validator__.validate_assignment(instance, name, value)\n            cls.__setattr__ = validated_setattr.__get__(None, cls)\n            if slots and not hasattr(cls, '__setstate__'):\n                def _dataclass_getstate(self: Any) -> list[Any]:\n                    return [getattr(self, f.name) for f in dataclasses.fields(self)]\n                def _dataclass_setstate(self: Any, state: list[Any]) -> None:\n                    for field, value in zip(dataclasses.fields(self), state):\n                        object.__setattr__(self, field.name, value)\n                cls.__getstate__ = _dataclass_getstate\n                cls.__setstate__ = _dataclass_setstate\n        cls.__is_pydantic_dataclass__ = True\n        cls.__pydantic_decorators__ = decorators\n        cls.__doc__ = original_doc\n        firstlineno = getattr(original_cls, '__firstlineno__', None)\n        cls.__module__ = original_cls.__module__\n        if sys.version_info >= (3, 13) and firstlineno is not None:\n            original_cls.__firstlineno__ = firstlineno\n            cls.__firstlineno__ = firstlineno\n        cls.__qualname__ = original_cls.__qualname__\n        cls.__pydantic_fields_complete__ = classmethod(_pydantic_fields_complete)\n        cls.__pydantic_complete__ = False\n        _pydantic_dataclasses.complete_dataclass(cls, config_wrapper, raise_errors=False)\n        return cls\n    return create_dataclass if _cls is None else create_dataclass(_cls)", "target": "def test_dataclass_field_wrap_validator2():\n    @dataclasses.dataclass\n    class Foo:\n        a: int\n        b: str\n        @classmethod\n        def validate_b(\n            cls, v: bytes, nxt: core_schema.ValidatorFunctionWrapHandler, info: core_schema.ValidationInfo\n        ) -> bytes:\n            assert v == b'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return nxt(b'hello world!')\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(\n                    name='b',\n                    schema=core_schema.with_info_wrap_validator_function(Foo.validate_b, core_schema.str_schema()),\n                ),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 1, 'b': b'hello'})\n    assert dataclasses.asdict(foo) == {'a': 1, 'b': 'hello world!'}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004633", "source": "def is_union(tp: Optional[Type[Any]]) -> bool:\n        return tp is Union or tp is types.UnionType", "target": "def test_ser_function_plain():\n    def f(input: Any, info: core_schema.SerializationInfo, /) -> str:\n        return str(info)\n    s = SchemaSerializer(\n        core_schema.any_schema(\n            serialization=core_schema.plain_serializer_function_ser_schema(\n                f, info_arg=True, return_schema=core_schema.str_schema()\n            )\n        )\n    )\n    assert s.to_python(123) == (\n        \"SerializationInfo(include=None, exclude=None, context=None, mode='python', by_alias=False, exclude_unset=False, \"\n        'exclude_defaults=False, exclude_none=False, exclude_computed_fields=False, round_trip=False, serialize_as_any=False)'\n    )"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004634", "source": "def decimal_encoder(dec_value: Decimal) -> Union[int, float]:\n    exponent = dec_value.as_tuple().exponent\n    if isinstance(exponent, int) and exponent >= 0:\n        return int(dec_value)\n    else:\n        return float(dec_value)", "target": "def test_to_json():\n    assert to_json([1, 2]) == b'[1,2]'\n    assert to_json([1, 2], indent=2) == b'[\\n  1,\\n  2\\n]'\n    assert to_json([1, b'x']) == b'[1,\"x\"]'\n    assert to_json(['', '']).decode('utf-8') == '[\"\",\"\"]'\n    assert to_json(['', ''], indent=2).decode('utf-8') == '[\\n  \"\",\\n  \"\"\\n]'\n    assert to_json(['', ''], indent=2, ensure_ascii=True).decode('utf-8') == '[\\n  \"\\\\u00e0\",\\n  \"\\\\u00e9\"\\n]'\n    with pytest.raises(TypeError, match=r'to_json\\(\\) takes 1 positional arguments but 2 were given'):\n        to_json([1, 2], 2)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004635", "source": "def rebuild_model_fields(\n    cls: type[BaseModel],\n    *,\n    config_wrapper: ConfigWrapper,\n    ns_resolver: NsResolver,\n    typevars_map: Mapping[TypeVar, Any],\n) -> dict[str, FieldInfo]:\n    rebuilt_fields: dict[str, FieldInfo] = {}\n    with ns_resolver.push(cls):\n        for f_name, field_info in cls.__pydantic_fields__.items():\n            if field_info._complete:\n                rebuilt_fields[f_name] = field_info\n            else:\n                new_field = _recreate_field_info(\n                    field_info, ns_resolver=ns_resolver, typevars_map=typevars_map, lenient=False\n                )\n                update_field_from_config(config_wrapper, f_name, new_field)\n                rebuilt_fields[f_name] = new_field\n    return rebuilt_fields", "target": "def test_validate_assignment_functions():\n    calls: list[Any] = []\n    def func_a(input_value, info):\n        calls.append(('func_a', input_value))\n        return input_value * 2\n    def func_b(input_value, info):\n        calls.append(('func_b', input_value))\n        return input_value / 2\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'field_a': core_schema.model_field(\n                    schema={\n                        'type': 'function-after',\n                        'function': {'type': 'with-info', 'function': func_a},\n                        'schema': core_schema.str_schema(),\n                    }\n                ),\n                'field_b': core_schema.model_field(\n                    schema={\n                        'type': 'function-after',\n                        'function': {'type': 'with-info', 'function': func_b},\n                        'schema': core_schema.int_schema(),\n                    }\n                ),\n            }\n        )\n    )\n    assert v.validate_python({'field_a': 'test', 'field_b': 12.0}) == (\n        {'field_a': 'testtest', 'field_b': 6},\n        None,\n        {'field_a', 'field_b'},\n    )\n    assert calls == [('func_a', 'test'), ('func_b', 12)]\n    calls.clear()\n    assert v.validate_assignment({'field_a': 'testtest', 'field_b': 6}, 'field_a', 'new-val') == (\n        {'field_a': 'new-valnew-val', 'field_b': 6},\n        None,\n        {'field_a'},\n    )\n    assert calls == [('func_a', 'new-val')]"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004636", "source": "def benchmark_module(config, module, use_throughput_benchmark=False):\n    if use_throughput_benchmark:\n        return benchmark_using_throughput_benchmark(config, module)\n    module.forward(config.num_warmup_iters)\n    print(f\"Running module for {config.num_iters} iterations\")\n    start = time.time()\n    module.forward(config.num_iters)\n    end = time.time()\n    time_elapsed_s = end - start\n    return secs_to_ms(time_elapsed_s) / config.num_iters / NUM_LOOP_ITERS", "target": "def test_pr_with_not_user_facing_label(\n        self, mocked_rn_labels: Any, mocked_gql: Any\n    ) -> None:\n        \"Test PR with 'topic: not user facing' label\"\n        pr = GitHubPR(\"pytorch\", \"pytorch\", 75095)\n        self.assertTrue(has_required_labels(pr))"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004637", "source": "def handle_extra_init(self: 'Dataclass', *args: Any, **kwargs: Any) -> None:\n        if config.extra == Extra.ignore:\n            init(self, *args, **{k: v for k, v in kwargs.items() if k in self.__dataclass_fields__})\n        elif config.extra == Extra.allow:\n            for k, v in kwargs.items():\n                self.__dict__.setdefault(k, v)\n            init(self, *args, **{k: v for k, v in kwargs.items() if k in self.__dataclass_fields__})\n        else:\n            init(self, *args, **kwargs)", "target": "def test_slots_dataclass_subclass(revalidate_instances, input_value, expected):\n    schema = core_schema.dataclass_schema(\n        FooDataclassSlots,\n        core_schema.dataclass_args_schema(\n            'FooDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n            ],\n            extra_behavior='forbid',\n        ),\n        ['a', 'b'],\n        revalidate_instances=revalidate_instances,\n        slots=True,\n    )\n    v = SchemaValidator(schema)\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=expected.message) as exc_info:\n            print(v.validate_python(input_value))\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        dc = v.validate_python(input_value)\n        assert dataclasses.is_dataclass(dc)\n        assert dataclasses.asdict(dc) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004638", "source": "def set_dataclass_fields(\n    cls: type[StandardDataclass],\n    config_wrapper: _config.ConfigWrapper,\n    ns_resolver: NsResolver | None = None,\n) -> None:\n    typevars_map = get_standard_typevars_map(cls)\n    fields = collect_dataclass_fields(\n        cls, ns_resolver=ns_resolver, typevars_map=typevars_map, config_wrapper=config_wrapper\n    )\n    cls.__pydantic_fields__ = fields", "target": "def test_by_alias_and_name_config_interaction(config, runtime, expected) -> None:\n    @dataclasses.dataclass\n    class Foo:\n        my_field: str\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(\n                    name='my_field', schema=core_schema.str_schema(), serialization_alias='my_alias'\n                ),\n            ],\n        ),\n        ['my_field'],\n        config=core_schema.CoreConfig(serialize_by_alias=config or False),\n    )\n    s = SchemaSerializer(schema)\n    assert s.to_python(Foo(my_field='hello'), by_alias=runtime) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004639", "source": "def extract_weights(mod: nn.Module) -> tuple[tuple[Tensor, ...], list[str]]:\n    orig_params = tuple(mod.parameters())\n    names = []\n    for name, p in list(mod.named_parameters()):\n        _del_nested_attr(mod, name.split(\".\"))\n        names.append(name)\n    params = tuple(p.detach().requires_grad_() for p in orig_params)\n    return params, names", "target": "def test_pr_with_missing_labels(\n        self, mocked_rn_labels: Any, mocked_gql: Any\n    ) -> None:\n        \"Test PR with no 'release notes:' label or 'topic: not user facing' label\"\n        pr = GitHubPR(\"pytorch\", \"pytorch\", 82169)\n        self.assertFalse(has_required_labels(pr))"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004640", "source": "def patch_base_fields(cls: type[Any]) -> Generator[None]:\n    original_fields_list: list[tuple[DcFields, DcFields]] = []\n    for base in cls.__mro__[1:]:\n        dc_fields: dict[str, dataclasses.Field[Any]] = base.__dict__.get('__dataclass_fields__', {})\n        dc_fields_with_pydantic_field_defaults = {\n            field_name: field\n            for field_name, field in dc_fields.items()\n            if isinstance(field.default, FieldInfo)\n            and (field.default.description is not None or field.default.kw_only or field.default.repr is not True)\n        }\n        if dc_fields_with_pydantic_field_defaults:\n            original_fields_list.append((dc_fields, dc_fields_with_pydantic_field_defaults))\n            for field_name, field in dc_fields_with_pydantic_field_defaults.items():\n                default = cast(FieldInfo, field.default)\n                new_dc_field = copy.copy(field)\n                if sys.version_info >= (3, 10) and default.kw_only:\n                    new_dc_field.kw_only = True\n                if default.repr is not True:\n                    new_dc_field.repr = default.repr\n                dc_fields[field_name] = new_dc_field\n    try:\n        yield\n    finally:\n        for fields, original_fields in original_fields_list:\n            for field_name, original_field in original_fields.items():\n                fields[field_name] = original_field", "target": "def test_slots_dataclass_subclass(revalidate_instances, input_value, expected):\n    schema = core_schema.dataclass_schema(\n        FooDataclassSlots,\n        core_schema.dataclass_args_schema(\n            'FooDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n            ],\n            extra_behavior='forbid',\n        ),\n        ['a', 'b'],\n        revalidate_instances=revalidate_instances,\n        slots=True,\n    )\n    v = SchemaValidator(schema)\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=expected.message) as exc_info:\n            print(v.validate_python(input_value))\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        dc = v.validate_python(input_value)\n        assert dataclasses.is_dataclass(dc)\n        assert dataclasses.asdict(dc) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004641", "source": "def all_literal_values(type_: Type[Any]) -> Tuple[Any, ...]:\n    if not is_literal_type(type_):\n        return (type_,)\n    values = literal_values(type_)\n    return tuple(x for value in values for x in all_literal_values(value))", "target": "def test_schema_typing() -> None:\n    schema: CoreSchema = {\n        'type': 'union',\n        'choices': [{'type': 'int'}, {'type': 'int', 'ge': 1}, {'type': 'float', 'lt': 1.0}],\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = {\n        'type': 'tagged-union',\n        'discriminator': 'type',\n        'choices': {\n            'apple': {\n                'type': 'typed-dict',\n                'fields': {'pips': {'type': 'typed-dict-field', 'schema': {'type': 'int'}}},\n            },\n            'banana': {\n                'type': 'typed-dict',\n                'fields': {'curvature': {'type': 'typed-dict-field', 'schema': {'type': 'float'}}},\n            },\n        },\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'int', 'ge': 1}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'float', 'lt': 1.0}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'str', 'pattern': r'http://.*'}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'bool', 'strict': False}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'literal', 'expected': [1, '1']}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'any'}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'none'}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'bytes'}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'list', 'items_schema': {'type': 'str'}, 'min_length': 3}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'set', 'items_schema': {'type': 'str'}, 'max_length': 3}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'tuple', 'items_schema': [{'type': 'str'}], 'variadic_item_index': 0, 'max_length': 3}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'tuple', 'items_schema': [{'type': 'str'}, {'type': 'int'}]}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'frozenset', 'items_schema': {'type': 'str'}, 'max_length': 3}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'dict', 'keys_schema': {'type': 'str'}, 'values_schema': {'type': 'any'}}\n    SchemaValidator(schema)\n    schema: CoreSchema = {\n        'type': 'typed-dict',\n        'fields': {'bar': {'type': 'typed-dict-field', 'schema': {'type': 'str'}}},\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = {\n        'type': 'model',\n        'cls': Foo,\n        'schema': {'type': 'model-fields', 'fields': {'bar': {'type': 'model-field', 'schema': {'type': 'str'}}}},\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = {\n        'type': 'typed-dict',\n        'fields': {\n            'a': {'type': 'typed-dict-field', 'schema': {'type': 'str'}},\n            'b': {'type': 'typed-dict-field', 'schema': {'type': 'str'}, 'validation_alias': 'foobar'},\n            'c': {\n                'type': 'typed-dict-field',\n                'schema': {'type': 'str'},\n                'validation_alias': [['foobar', 0, 'bar'], ['foo']],\n            },\n            'd': {\n                'type': 'typed-dict-field',\n                'schema': {'type': 'default', 'schema': {'type': 'str'}, 'default': 'spam'},\n            },\n        },\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = {\n        'type': 'function-wrap',\n        'function': {'type': 'with-info', 'function': wrap_validator, 'field_name': 'foobar'},\n        'schema': {'type': 'str'},\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = core_schema.with_info_plain_validator_function(validator)\n    SchemaValidator(schema)\n    schema: CoreSchema = {\n        'type': 'definitions',\n        'schema': {'type': 'definition-ref', 'schema_ref': 'Branch'},\n        'definitions': [\n            {\n                'type': 'typed-dict',\n                'fields': {\n                    'name': {'type': 'typed-dict-field', 'schema': {'type': 'str'}},\n                    'sub_branch': {\n                        'type': 'typed-dict-field',\n                        'schema': {\n                            'type': 'default',\n                            'schema': {\n                                'type': 'nullable',\n                                'schema': {'type': 'definition-ref', 'schema_ref': 'Branch'},\n                            },\n                            'default': None,\n                        },\n                    },\n                },\n                'ref': 'Branch',\n            }\n        ],\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'date', 'le': date.today()}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'time', 'lt': time(12, 13, 14)}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'datetime', 'ge': datetime.now()}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'is-instance', 'cls': Foo}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'callable'}\n    SchemaValidator(schema)\n    schema: CoreSchema = {\n        'type': 'arguments',\n        'arguments_schema': [\n            {'name': 'a', 'mode': 'positional_only', 'schema': {'type': 'int'}},\n            {'name': 'b', 'schema': {'type': 'str'}},\n            {'name': 'c', 'mode': 'keyword_only', 'schema': {'type': 'bool'}},\n        ],\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'call', 'arguments_schema': {'type': 'any'}, 'function': foo}\n    SchemaValidator(schema)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004642", "source": "def dataclass(\n        *,\n        init: bool = True,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool = False,\n        config: Union[ConfigDict, Type[object], None] = None,\n        validate_on_init: Optional[bool] = None,\n        use_proxy: Optional[bool] = None,\n    ) -> Callable[[Type[_T]], 'DataclassClassOrWrapper']:\n        ...", "target": "def test_dataclass_exact_validation(revalidate_instances, input_value, expected):\n    schema = core_schema.dataclass_schema(\n        FooDataclass,\n        core_schema.dataclass_args_schema(\n            'FooDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n            ],\n        ),\n        ['a', 'b'],\n        revalidate_instances=revalidate_instances,\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python(input_value)\n    assert dataclasses.asdict(foo) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004643", "source": "def all_literal_values(type_: Type[Any]) -> Tuple[Any, ...]:\n    if not is_literal_type(type_):\n        return (type_,)\n    values = literal_values(type_)\n    return tuple(x for value in values for x in all_literal_values(value))", "target": "def test_wrong_function_signature() -> None:\n    def wrong_validator(value: Any) -> Any:\n        return value\n    v = SchemaValidator(core_schema.with_info_plain_validator_function(wrong_validator))\n    try:\n        v.validate_python(1)\n    except TypeError as exc:\n        assert 'takes 1 positional argument but 2 were given' in str(exc)\n    else:\n        raise AssertionError('v.validate_python(1) did not raise TypeError')"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004644", "source": "def rebuild_dataclass(\n    cls: type[PydanticDataclass],\n    *,\n    force: bool = False,\n    raise_errors: bool = True,\n    _parent_namespace_depth: int = 2,\n    _types_namespace: MappingNamespace | None = None,\n) -> bool | None:\n    if not force and cls.__pydantic_complete__:\n        return None\n    for attr in ('__pydantic_core_schema__', '__pydantic_validator__', '__pydantic_serializer__'):\n        if attr in cls.__dict__ and not isinstance(getattr(cls, attr), _mock_val_ser.MockValSer):\n            delattr(cls, attr)\n    cls.__pydantic_complete__ = False\n    if _types_namespace is not None:\n        rebuild_ns = _types_namespace\n    elif _parent_namespace_depth > 0:\n        rebuild_ns = _typing_extra.parent_frame_namespace(parent_depth=_parent_namespace_depth, force=True) or {}\n    else:\n        rebuild_ns = {}\n    ns_resolver = _namespace_utils.NsResolver(\n        parent_namespace=rebuild_ns,\n    )\n    return _pydantic_dataclasses.complete_dataclass(\n        cls,\n        _config.ConfigWrapper(cls.__pydantic_config__, check=False),\n        raise_errors=raise_errors,\n        ns_resolver=ns_resolver,\n        _force_build=True,\n    )", "target": "def test_dataclass_json():\n    schema = core_schema.dataclass_schema(\n        FooDataclass,\n        core_schema.dataclass_args_schema(\n            'FooDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    assert v.validate_json('{\"a\": \"hello\", \"b\": true}') == FooDataclass(a='hello', b=True)\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_json('[\"a\", \"b\"]')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'class_name': 'FooDataclass'},\n            'input': ['a', 'b'],\n            'loc': (),\n            'msg': 'Input should be an object',\n            'type': 'dataclass_type',\n        }\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004645", "source": "def to_markdown_table(\n    res: TimingResultType, header: Optional[tuple[str, ...]] = None\n) -> str:\n    if header is None:\n        header = (\"model\", \"task\", \"mean\", \"var\")\n    out = \"\"\n    def write_line(*args):\n        nonlocal out\n        out += f\"| {' | '.join(str(a) for a in args)} |\\n\"\n    write_line(*header)\n    write_line(*[\"--\"] * len(header))\n    for model, tasks in res.items():\n        for task, line in tasks.items():\n            write_line(*(model, task) + line)\n    return out", "target": "def test_pr_with_missing_labels(\n        self, mocked_rn_labels: Any, mocked_gql: Any\n    ) -> None:\n        \"Test PR with no 'release notes:' label or 'topic: not user facing' label\"\n        pr = GitHubPR(\"pytorch\", \"pytorch\", 82169)\n        self.assertFalse(has_required_labels(pr))"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004646", "source": "def pydantic_general_metadata(**metadata: Any) -> BaseMetadata:\n    return _general_metadata_cls()(metadata)", "target": "def test_on_error_bad_default(self):\n        with pytest.raises(SchemaError, match=\"'on_error = default' requires a `default` or `default_factory`\"):\n            SchemaValidator(\n                schema=core_schema.model_fields_schema(\n                    fields={\n                        'x': core_schema.model_field(\n                            schema=core_schema.with_default_schema(schema=core_schema.str_schema(), on_error='default')\n                        )\n                    }\n                )\n            )"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004647", "source": "def timedelta_isoformat(td: datetime.timedelta) -> str:\n    warnings.warn('`timedelta_isoformat` is deprecated.', category=PydanticDeprecatedSince20, stacklevel=2)\n    minutes, seconds = divmod(td.seconds, 60)\n    hours, minutes = divmod(minutes, 60)\n    return f'{\"-\" if td.days < 0 else \"\"}P{abs(td.days)}DT{hours:d}H{minutes:d}M{seconds:d}.{td.microseconds:06d}S'", "target": "def test_to_jsonable_python_fallback():\n    with pytest.raises(PydanticSerializationError, match=r'Unable to serialize unknown type: <.+\\.Foobar'):\n        to_jsonable_python(Foobar())\n    assert to_jsonable_python(Foobar(), serialize_unknown=True) == 'Foobar.__str__'\n    assert to_jsonable_python(Foobar(), serialize_unknown=True, fallback=fallback_func) == 'fallback:Foobar'\n    assert to_jsonable_python(Foobar(), fallback=fallback_func) == 'fallback:Foobar'"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004648", "source": "def get_class(type_: Type[Any]) -> Union[None, bool, Type[Any]]:\n    if type_ is type:\n        return True\n    if get_origin(type_) is None:\n        return None\n    args = get_args(type_)\n    if not args or not isinstance(args[0], type):\n        return True\n    else:\n        return args[0]", "target": "def test_schema_typing() -> None:\n    schema: CoreSchema = {\n        'type': 'union',\n        'choices': [{'type': 'int'}, {'type': 'int', 'ge': 1}, {'type': 'float', 'lt': 1.0}],\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = {\n        'type': 'tagged-union',\n        'discriminator': 'type',\n        'choices': {\n            'apple': {\n                'type': 'typed-dict',\n                'fields': {'pips': {'type': 'typed-dict-field', 'schema': {'type': 'int'}}},\n            },\n            'banana': {\n                'type': 'typed-dict',\n                'fields': {'curvature': {'type': 'typed-dict-field', 'schema': {'type': 'float'}}},\n            },\n        },\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'int', 'ge': 1}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'float', 'lt': 1.0}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'str', 'pattern': r'http://.*'}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'bool', 'strict': False}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'literal', 'expected': [1, '1']}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'any'}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'none'}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'bytes'}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'list', 'items_schema': {'type': 'str'}, 'min_length': 3}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'set', 'items_schema': {'type': 'str'}, 'max_length': 3}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'tuple', 'items_schema': [{'type': 'str'}], 'variadic_item_index': 0, 'max_length': 3}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'tuple', 'items_schema': [{'type': 'str'}, {'type': 'int'}]}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'frozenset', 'items_schema': {'type': 'str'}, 'max_length': 3}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'dict', 'keys_schema': {'type': 'str'}, 'values_schema': {'type': 'any'}}\n    SchemaValidator(schema)\n    schema: CoreSchema = {\n        'type': 'typed-dict',\n        'fields': {'bar': {'type': 'typed-dict-field', 'schema': {'type': 'str'}}},\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = {\n        'type': 'model',\n        'cls': Foo,\n        'schema': {'type': 'model-fields', 'fields': {'bar': {'type': 'model-field', 'schema': {'type': 'str'}}}},\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = {\n        'type': 'typed-dict',\n        'fields': {\n            'a': {'type': 'typed-dict-field', 'schema': {'type': 'str'}},\n            'b': {'type': 'typed-dict-field', 'schema': {'type': 'str'}, 'validation_alias': 'foobar'},\n            'c': {\n                'type': 'typed-dict-field',\n                'schema': {'type': 'str'},\n                'validation_alias': [['foobar', 0, 'bar'], ['foo']],\n            },\n            'd': {\n                'type': 'typed-dict-field',\n                'schema': {'type': 'default', 'schema': {'type': 'str'}, 'default': 'spam'},\n            },\n        },\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = {\n        'type': 'function-wrap',\n        'function': {'type': 'with-info', 'function': wrap_validator, 'field_name': 'foobar'},\n        'schema': {'type': 'str'},\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = core_schema.with_info_plain_validator_function(validator)\n    SchemaValidator(schema)\n    schema: CoreSchema = {\n        'type': 'definitions',\n        'schema': {'type': 'definition-ref', 'schema_ref': 'Branch'},\n        'definitions': [\n            {\n                'type': 'typed-dict',\n                'fields': {\n                    'name': {'type': 'typed-dict-field', 'schema': {'type': 'str'}},\n                    'sub_branch': {\n                        'type': 'typed-dict-field',\n                        'schema': {\n                            'type': 'default',\n                            'schema': {\n                                'type': 'nullable',\n                                'schema': {'type': 'definition-ref', 'schema_ref': 'Branch'},\n                            },\n                            'default': None,\n                        },\n                    },\n                },\n                'ref': 'Branch',\n            }\n        ],\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'date', 'le': date.today()}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'time', 'lt': time(12, 13, 14)}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'datetime', 'ge': datetime.now()}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'is-instance', 'cls': Foo}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'callable'}\n    SchemaValidator(schema)\n    schema: CoreSchema = {\n        'type': 'arguments',\n        'arguments_schema': [\n            {'name': 'a', 'mode': 'positional_only', 'schema': {'type': 'int'}},\n            {'name': 'b', 'schema': {'type': 'str'}},\n            {'name': 'c', 'mode': 'keyword_only', 'schema': {'type': 'bool'}},\n        ],\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'call', 'arguments_schema': {'type': 'any'}, 'function': foo}\n    SchemaValidator(schema)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004649", "source": "def new_post_init(self: 'Dataclass', *args: Any, **kwargs: Any) -> None:\n            if config.post_init_call == 'before_validation':\n                post_init(self, *args, **kwargs)\n            if self.__class__.__pydantic_run_validation__:\n                self.__pydantic_validate_values__()\n                if hasattr(self, '__post_init_post_parse__'):\n                    self.__post_init_post_parse__(*args, **kwargs)\n            if config.post_init_call == 'after_validation':\n                post_init(self, *args, **kwargs)", "target": "def test_function_validator_wrapping_args_schema_after() -> None:\n    calls: list[Any] = []\n    def func(*args: Any) -> Any:\n        calls.append(args)\n        return args[0]\n    @dataclasses.dataclass\n    class Model:\n        number: int = 1\n    cs = core_schema.dataclass_schema(\n        Model,\n        core_schema.no_info_after_validator_function(\n            func,\n            core_schema.dataclass_args_schema(\n                'Model', [core_schema.dataclass_field('number', core_schema.int_schema())]\n            ),\n        ),\n        ['number'],\n    )\n    v = SchemaValidator(cs)\n    instance: Model = v.validate_python({'number': 1})\n    assert instance.number == 1\n    assert calls == [(({'number': 1}, None),)]\n    v.validate_assignment(instance, 'number', 2)\n    assert instance.number == 2\n    assert calls == [(({'number': 1}, None),), (({'number': 2}, None),)]"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004650", "source": "def flatten(schema: Definition) -> FlatIntermediateDefinition:\n    result: FlatIntermediateDefinition = {}\n    _flatten(key_prefix=(), sub_schema=schema, result=result)\n    for k, v in result.items():\n        assert isinstance(k, tuple)\n        assert all(isinstance(ki, str) for ki in k)\n        assert isinstance(v, (TimerArgs, GroupedBenchmark))\n    return result", "target": "def test_gh_get_labels(\n        self,\n        mock_request_for_labels: Any,\n        mock_get_last_page_num_from_header: Any,\n    ) -> None:\n        res = gh_get_labels(\"mock_org\", \"mock_repo\")\n        mock_get_last_page_num_from_header.assert_called_once()\n        self.assertEqual(res, [\"foo\"] * 3)"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004651", "source": "def write_line(*args):\n        nonlocal out\n        out += f\"| {' | '.join(str(a) for a in args)} |\\n\"", "target": "def test_noop_when_empty_path(self):\n        start = Path.cwd()\n        with working_directory(\"\"):\n            self.assertEqual(Path.cwd(), start)\n        self.assertEqual(Path.cwd(), start)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004652", "source": "def dataclass(\n        _cls: Type[_T],\n        *,\n        init: bool = True,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool = False,\n        config: Union[ConfigDict, Type[object], None] = None,\n        validate_on_init: Optional[bool] = None,\n        use_proxy: Optional[bool] = None,\n    ) -> 'DataclassClassOrWrapper':\n        ...", "target": "def test_dataclass_post_init():\n    @dataclasses.dataclass\n    class Foo:\n        a: str\n        b: bool\n        def __post_init__(self):\n            self.a = self.a.upper()\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n            ],\n        ),\n        ['a', 'b'],\n        post_init=True,\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 'hello', 'b': True})\n    assert foo.a == 'HELLO'\n    assert foo.b is True"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004653", "source": "def get_git_remote_name() -> str:\n    return os.getenv(\"GIT_REMOTE_NAME\", \"origin\")", "target": "def test_simple(self) -> None:\n        @retries_decorator()\n        def foo(x: int, y: int) -> int:\n            return x + y\n        self.assertEqual(foo(3, 4), 7)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004654", "source": "def is_builtin_dataclass(_cls: Type[Any]) -> bool:\n    return (\n        dataclasses.is_dataclass(_cls)\n        and not hasattr(_cls, '__pydantic_model__')\n        and set(_cls.__dataclass_fields__).issuperset(set(getattr(_cls, '__annotations__', {})))\n    )", "target": "def test_function_validator_wrapping_args_schema_wrap() -> None:\n    calls: list[Any] = []\n    def func(*args: Any) -> Any:\n        assert len(args) == 2\n        input, handler = args\n        output = handler(input)\n        calls.append((input, output))\n        return output\n    @dataclasses.dataclass\n    class Model:\n        number: int = 1\n    cs = core_schema.dataclass_schema(\n        Model,\n        core_schema.no_info_wrap_validator_function(\n            func,\n            core_schema.dataclass_args_schema(\n                'Model', [core_schema.dataclass_field('number', core_schema.int_schema())]\n            ),\n        ),\n        ['number'],\n    )\n    v = SchemaValidator(cs)\n    instance: Model = v.validate_python({'number': 1})\n    assert instance.number == 1\n    assert calls == [({'number': 1}, ({'number': 1}, None))]\n    v.validate_assignment(instance, 'number', 2)\n    assert instance.number == 2\n    assert calls == [({'number': 1}, ({'number': 1}, None)), ({'number': 2}, ({'number': 2}, None))]"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004655", "source": "def powspace(start, stop, pow, step):\n    start = math.log(start, pow)\n    stop = math.log(stop, pow)\n    steps = int((stop - start + 1) // step)\n    ret = torch.pow(pow, torch.linspace(start, stop, steps))\n    ret = torch.unique(ret)\n    return list(map(int, ret))", "target": "def test_iterator(self, input_: str = \"abcdef\") -> None:\n        iter_ = PeekableIterator(input_)\n        for idx, c in enumerate(iter_):\n            self.assertEqual(c, input_[idx])"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004656", "source": "def _apply_alias_generator_to_field_info(\n    alias_generator: Callable[[str], str] | AliasGenerator, field_name: str, field_info: FieldInfo\n):\n    if (\n        field_info.alias_priority is None\n        or field_info.alias_priority <= 1\n        or field_info.alias is None\n        or field_info.validation_alias is None\n        or field_info.serialization_alias is None\n    ):\n        alias, validation_alias, serialization_alias = None, None, None\n        if isinstance(alias_generator, AliasGenerator):\n            alias, validation_alias, serialization_alias = alias_generator.generate_aliases(field_name)\n        elif callable(alias_generator):\n            alias = alias_generator(field_name)\n            if not isinstance(alias, str):\n                raise TypeError(f'alias_generator {alias_generator} must return str, not {alias.__class__}')\n        if field_info.alias_priority is None or field_info.alias_priority <= 1:\n            field_info.alias_priority = 1\n        if field_info.alias_priority == 1:\n            field_info.serialization_alias = get_first_not_none(serialization_alias, alias)\n            field_info.validation_alias = get_first_not_none(validation_alias, alias)\n            field_info.alias = alias\n        if field_info.alias is None:\n            field_info.alias = alias\n        if field_info.serialization_alias is None:\n            field_info.serialization_alias = get_first_not_none(serialization_alias, alias)\n        if field_info.validation_alias is None:\n            field_info.validation_alias = get_first_not_none(validation_alias, alias)", "target": "def test_alias_build_error(alias_schema, error):\n    with pytest.raises(SchemaError, match=error):\n        SchemaValidator(\n            schema={\n                'type': 'model-fields',\n                'fields': {'field_a': {'type': 'model-field', 'schema': {'type': 'int'}, **alias_schema}},\n            }\n        )"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004657", "source": "def validated_setattr(instance: PydanticDataclass, name: str, value: Any, /) -> None:\n                if frozen_:\n                    return original_setattr(instance, name, value)\n                inst_cls = type(instance)\n                attr = getattr(inst_cls, name, None)\n                if isinstance(attr, property):\n                    attr.__set__(instance, value)\n                elif isinstance(attr, functools.cached_property):\n                    instance.__dict__.__setitem__(name, value)\n                else:\n                    inst_cls.__pydantic_validator__.validate_assignment(instance, name, value)", "target": "def test_slots_mixed():\n    @dataclasses.dataclass(slots=True)\n    class Model:\n        x: int\n        y: dataclasses.InitVar[str]\n        z: ClassVar[str] = 'z-classvar'\n    @dataclasses.dataclass\n    class SubModel(Model):\n        x2: int\n        y2: dataclasses.InitVar[str]\n        z2: ClassVar[str] = 'z2-classvar'\n    schema = core_schema.dataclass_schema(\n        SubModel,\n        core_schema.dataclass_args_schema(\n            'SubModel',\n            [\n                core_schema.dataclass_field(name='x', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(name='y', init_only=True, schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='x2', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(name='y2', init_only=True, schema=core_schema.str_schema()),\n            ],\n        ),\n        ['x'],\n        slots=True,\n    )\n    v = SchemaValidator(schema)\n    dc = v.validate_python({'x': 1, 'y': 'a', 'x2': 2, 'y2': 'b'})\n    assert dc.x == 1\n    assert dc.x2 == 2\n    assert dataclasses.asdict(dc) == {'x': 1, 'x2': 2}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004658", "source": "def dataclass(\n        *,\n        init: Literal[False] = False,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool | None = None,\n        config: ConfigDict | type[object] | None = None,\n        validate_on_init: bool | None = None,\n    ) -> Callable[[type[_T]], type[PydanticDataclass]]:\n        ...", "target": "def test_dataclass_subclass(revalidate_instances, input_value, expected):\n    schema = core_schema.dataclass_schema(\n        FooDataclass,\n        core_schema.dataclass_args_schema(\n            'FooDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n            ],\n            extra_behavior='forbid',\n        ),\n        ['a', 'b'],\n        revalidate_instances=revalidate_instances,\n    )\n    v = SchemaValidator(schema)\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=expected.message) as exc_info:\n            print(v.validate_python(input_value))\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        dc = v.validate_python(input_value)\n        assert dataclasses.is_dataclass(dc)\n        assert dataclasses.asdict(dc) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004659", "source": "def collect_dataclass_fields(\n    cls: type[StandardDataclass],\n    *,\n    config_wrapper: ConfigWrapper,\n    ns_resolver: NsResolver | None = None,\n    typevars_map: dict[Any, Any] | None = None,\n) -> dict[str, FieldInfo]:\n    FieldInfo_ = import_cached_field_info()\n    fields: dict[str, FieldInfo] = {}\n    ns_resolver = ns_resolver or NsResolver()\n    dataclass_fields = cls.__dataclass_fields__\n    for base in reversed(cls.__mro__):\n        if not dataclasses.is_dataclass(base):\n            continue\n        with ns_resolver.push(base):\n            for ann_name, dataclass_field in dataclass_fields.items():\n                base_anns = _typing_extra.safe_get_annotations(base)\n                if ann_name not in base_anns:\n                    continue\n                globalns, localns = ns_resolver.types_namespace\n                ann_type, evaluated = _typing_extra.try_eval_type(dataclass_field.type, globalns, localns)\n                if _typing_extra.is_classvar_annotation(ann_type):\n                    continue\n                if (\n                    not dataclass_field.init\n                    and dataclass_field.default is dataclasses.MISSING\n                    and dataclass_field.default_factory is dataclasses.MISSING\n                ):\n                    continue\n                if isinstance(dataclass_field.default, FieldInfo_):\n                    if dataclass_field.default.init_var:\n                        if dataclass_field.default.init is False:\n                            raise PydanticUserError(\n                                f'Dataclass field {ann_name} has init=False and init_var=True, but these are mutually exclusive.',\n                                code='clashing-init-and-init-var',\n                            )\n                        continue\n                    field_info = FieldInfo_.from_annotated_attribute(\n                        ann_type, dataclass_field.default, _source=AnnotationSource.DATACLASS\n                    )\n                    field_info._original_assignment = dataclass_field.default\n                else:\n                    field_info = FieldInfo_.from_annotated_attribute(\n                        ann_type, dataclass_field, _source=AnnotationSource.DATACLASS\n                    )\n                    field_info._original_assignment = dataclass_field\n                if not evaluated:\n                    field_info._complete = False\n                    field_info._original_annotation = ann_type\n                fields[ann_name] = field_info\n                update_field_from_config(config_wrapper, ann_name, field_info)\n                if field_info.default is not PydanticUndefined and isinstance(\n                    getattr(cls, ann_name, field_info), FieldInfo_\n                ):\n                    setattr(cls, ann_name, field_info.default)\n    if typevars_map:\n        for field in fields.values():\n            field.apply_typevars_map(typevars_map)\n    if config_wrapper.use_attribute_docstrings:\n        _update_fields_from_docstrings(\n            cls,\n            fields,\n            use_inspect=not hasattr(cls, '__is_pydantic_dataclass__'),\n        )\n    return fields", "target": "def test_wrap_on_error(self, py_and_json: PyAndJson):\n        def wrap_function(input_value, validator, info):\n            try:\n                return validator(input_value)\n            except ValidationError:\n                if isinstance(input_value, list):\n                    return str(len(input_value))\n                else:\n                    return repr(input_value)\n        v = py_and_json(\n            {\n                'type': 'model-fields',\n                'fields': {\n                    'x': {\n                        'type': 'model-field',\n                        'schema': {\n                            'type': 'default',\n                            'on_error': 'raise',\n                            'schema': {\n                                'type': 'function-wrap',\n                                'function': {'type': 'with-info', 'function': wrap_function},\n                                'schema': {'type': 'str'},\n                            },\n                        },\n                    }\n                },\n            }\n        )\n        assert v.validate_test({'x': 'foo'}) == ({'x': 'foo'}, None, {'x'})\n        assert v.validate_test({'x': ['foo']}) == ({'x': '1'}, None, {'x'})\n        assert v.validate_test({'x': ['foo', 'bar']}) == ({'x': '2'}, None, {'x'})\n        assert v.validate_test({'x': {'a': 'b'}}) == ({'x': \"{'a': 'b'}\"}, None, {'x'})"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004660", "source": "def _pydantic_fields_complete(cls: type[PydanticDataclass]) -> bool:\n    return all(field_info._complete for field_info in cls.__pydantic_fields__.values())", "target": "def test_serialization_exclude():\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(\n                    name='a', schema=core_schema.str_schema(), serialization_exclude_if=lambda x: x == 'bye'\n                ),\n                core_schema.dataclass_field(name='b', schema=core_schema.bytes_schema(), serialization_exclude=True),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    s = SchemaSerializer(schema)\n    assert s.to_python(Foo(a='hello', b=b'more')) == {'a': 'hello'}\n    assert s.to_python(Foo(a='hello', b=b'more'), mode='json') == {'a': 'hello'}\n    assert s.to_python(Foo(a='bye', b=b'more'), mode='json') == {}\n    j = s.to_json(Foo(a='hello', b=b'more'))\n    if on_pypy:\n        assert json.loads(j) == {'a': 'hello'}\n    else:\n        assert j == b'{\"a\":\"hello\"}'\n    j = s.to_json(Foo(a='bye', b=b'more'))\n    if on_pypy:\n        assert json.loads(j) == {}\n    else:\n        assert j == b'{}'"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004661", "source": "def cls_kwargs(cls: Type['PydanticErrorMixin'], ctx: 'DictStrAny') -> 'PydanticErrorMixin':\n    return cls(**ctx)", "target": "def test_build_non_default_follows_default() -> None:\n    with pytest.raises(SchemaError, match=\"Required parameter 'b' follows parameter with default\"):\n        SchemaValidator(\n            schema=cs.arguments_v3_schema(\n                [\n                    cs.arguments_v3_parameter(\n                        name='a',\n                        schema=cs.with_default_schema(schema=cs.int_schema(), default_factory=lambda: 42),\n                        mode='positional_or_keyword',\n                    ),\n                    cs.arguments_v3_parameter(name='b', schema=cs.int_schema(), mode='positional_or_keyword'),\n                ]\n            )\n        )"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004662", "source": "def update_field_from_config(config_wrapper: ConfigWrapper, field_name: str, field_info: FieldInfo) -> None:\n    field_title_generator = field_info.field_title_generator or config_wrapper.field_title_generator\n    if field_title_generator is not None:\n        _apply_field_title_generator_to_field_info(field_title_generator, field_name, field_info)\n    if config_wrapper.alias_generator is not None:\n        _apply_alias_generator_to_field_info(config_wrapper.alias_generator, field_name, field_info)", "target": "def test_on_error_default(self, py_and_json: PyAndJson):\n        v = py_and_json(\n            {\n                'type': 'model-fields',\n                'fields': {\n                    'x': {\n                        'type': 'model-field',\n                        'schema': {\n                            'type': 'default',\n                            'schema': {'type': 'str'},\n                            'on_error': 'default',\n                            'default': 'pika',\n                        },\n                    }\n                },\n            }\n        )\n        assert v.validate_test({'x': 'foo'}) == ({'x': 'foo'}, None, {'x'})\n        assert v.validate_test({'x': ['foo']}) == ({'x': 'pika'}, None, {'x'})"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004663", "source": "def create_pydantic_model_from_dataclass(\n    dc_cls: Type['Dataclass'],\n    config: Type[Any] = BaseConfig,\n    dc_cls_doc: Optional[str] = None,\n) -> Type['BaseModel']:\n    field_definitions: Dict[str, Any] = {}\n    for field in dataclasses.fields(dc_cls):\n        default: Any = Undefined\n        default_factory: Optional['NoArgAnyCallable'] = None\n        field_info: FieldInfo\n        if field.default is not dataclasses.MISSING:\n            default = field.default\n        elif field.default_factory is not dataclasses.MISSING:\n            default_factory = field.default_factory\n        else:\n            default = Required\n        if isinstance(default, FieldInfo):\n            field_info = default\n            dc_cls.__pydantic_has_field_info_default__ = True\n        else:\n            field_info = Field(default=default, default_factory=default_factory, **field.metadata)\n        field_definitions[field.name] = (field.type, field_info)\n    validators = gather_all_validators(dc_cls)\n    model: Type['BaseModel'] = create_model(\n        dc_cls.__name__,\n        __config__=config,\n        __module__=dc_cls.__module__,\n        __validators__=validators,\n        __cls_kwargs__={'__resolve_forward_refs__': False},\n        **field_definitions,\n    )\n    model.__doc__ = dc_cls_doc if dc_cls_doc is not None else dc_cls.__doc__ or ''\n    return model", "target": "def test_dataclass_args_init(input_value, extra_behavior, expected):\n    @dataclasses.dataclass\n    class Foo:\n        a: str\n        b: str\n        def __post_init__(self):\n            self.b = self.a.upper()\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.str_schema(), init=False),\n            ],\n            extra_behavior=extra_behavior,\n        ),\n        ['a', 'b'],\n        post_init=True,\n    )\n    v = SchemaValidator(schema)\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)) as exc_info:\n            v.validate_python(input_value)\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        assert dataclasses.asdict(v.validate_python(input_value)) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004664", "source": "def powspace(start, stop, pow, step):\n    start = math.log(start, pow)\n    stop = math.log(stop, pow)\n    steps = int((stop - start + 1) // step)\n    ret = torch.pow(pow, torch.linspace(start, stop, steps))\n    ret = torch.unique(ret)\n    return list(map(int, ret))", "target": "def test_merged_lastfailed_content_without_overlap(self) -> None:\n        last_failed_source = {\n            \"tools/tests/test_foo.py::test_num1\": True,\n            \"tools/tests/test_foo.py::test_num2\": True,\n            \"tools/tests/test_bar.py::test_num1\": True,\n        }\n        last_failed_dest = {\n            \"tools/tests/test_car.py::test_num1\": True,\n            \"tools/tests/test_car.py::test_num2\": True,\n        }\n        last_failed_merged = {\n            \"tools/tests/test_foo.py::test_num1\": True,\n            \"tools/tests/test_foo.py::test_num2\": True,\n            \"tools/tests/test_bar.py::test_num1\": True,\n            \"tools/tests/test_car.py::test_num1\": True,\n            \"tools/tests/test_car.py::test_num2\": True,\n        }\n        merged = _merged_lastfailed_content(last_failed_source, last_failed_dest)\n        self.assertEqual(merged, last_failed_merged)"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004665", "source": "def parse_stmts(stmts: str) -> tuple[str, str]:\n    stmts = textwrap.dedent(stmts).strip()\n    lines: list[str] = stmts.splitlines(keepends=False)\n    assert len(lines) >= 3, f\"Invalid string:\\n{stmts}\"\n    column_header_pattern = r\"^Python\\s{35}\\| C\\+\\+(\\s*)$\"\n    signature_pattern = r\"^: f\\((.*)\\)( -> (.+))?\\s*$\"\n    separation_pattern = r\"^[-]{40} | [-]{40}$\"\n    code_pattern = r\"^(.{40}) \\|($| (.*)$)\"\n    column_match = re.search(column_header_pattern, lines[0])\n    if column_match is None:\n        raise ValueError(\n            f\"Column header `{lines[0]}` \"\n            f\"does not match pattern `{column_header_pattern}`\"\n        )\n    assert re.search(separation_pattern, lines[1])\n    py_lines: list[str] = []\n    cpp_lines: list[str] = []\n    for l in lines[2:]:\n        l_match = re.search(code_pattern, l)\n        if l_match is None:\n            raise ValueError(f\"Invalid line `{l}`\")\n        py_lines.append(l_match.groups()[0])\n        cpp_lines.append(l_match.groups()[2] or \"\")\n        l_from_stmts = f\"{py_lines[-1]:<40} | {cpp_lines[-1]:<40}\".rstrip()\n        assert l_from_stmts == l.rstrip(), f\"Failed to round trip `{l}`\"\n    return \"\\n\".join(py_lines), \"\\n\".join(cpp_lines)", "target": "def test_is_iterable(self) -> None:\n        from collections.abc import Iterator\n        iter_ = PeekableIterator(\"\")\n        self.assertTrue(isinstance(iter_, Iterator))"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004666", "source": "def pydantic_encoder(obj: Any) -> Any:\n    warnings.warn(\n        '`pydantic_encoder` is deprecated, use `pydantic_core.to_jsonable_python` instead.',\n        category=PydanticDeprecatedSince20,\n        stacklevel=2,\n    )\n    from dataclasses import asdict, is_dataclass\n    BaseModel = import_cached_base_model()\n    if isinstance(obj, BaseModel):\n        return obj.model_dump()\n    elif is_dataclass(obj):\n        return asdict(obj)\n    for base in obj.__class__.__mro__[:-1]:\n        try:\n            encoder = ENCODERS_BY_TYPE[base]\n        except KeyError:\n            continue\n        return encoder(obj)\n    else:\n        raise TypeError(f\"Object of type '{obj.__class__.__name__}' is not JSON serializable\")", "target": "def test_dict():\n    v = SchemaValidator(\n        core_schema.dict_schema(keys_schema=core_schema.int_schema(), values_schema=core_schema.int_schema())\n    )\n    assert v.validate_json('{\"1\": 2, \"3\": 4}') == {1: 2, 3: 4}\n    assert json.loads('{\"1\": 1, \"1\": 2}') == {'1': 2}\n    assert v.validate_json('{\"1\": 1, \"1\": 2}') == {1: 2}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004667", "source": "def is_builtin_dataclass(_cls: Type[Any]) -> bool:\n    return (\n        dataclasses.is_dataclass(_cls)\n        and not hasattr(_cls, '__pydantic_model__')\n        and set(_cls.__dataclass_fields__).issuperset(set(getattr(_cls, '__annotations__', {})))\n    )", "target": "def test_dataclass_slots_field_before_validator():\n    @dataclasses.dataclass(slots=True)\n    class Foo:\n        a: int\n        b: str\n        @classmethod\n        def validate_b(cls, v: bytes, info: core_schema.ValidationInfo) -> bytes:\n            assert v == b'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return b'hello world!'\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(\n                    name='b',\n                    schema=core_schema.with_info_before_validator_function(Foo.validate_b, core_schema.str_schema()),\n                ),\n            ],\n        ),\n        ['a', 'b'],\n        slots=True,\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 1, 'b': b'hello'})\n    assert dataclasses.asdict(foo) == {'a': 1, 'b': 'hello world!'}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004668", "source": "def get_field_info(cls, name: str) -> Dict[str, Any]:\n        fields_value = cls.fields.get(name)\n        if isinstance(fields_value, str):\n            field_info: Dict[str, Any] = {'alias': fields_value}\n        elif isinstance(fields_value, dict):\n            field_info = fields_value\n        else:\n            field_info = {}\n        if 'alias' in field_info:\n            field_info.setdefault('alias_priority', 2)\n        if field_info.get('alias_priority', 0) <= 1 and cls.alias_generator:\n            alias = cls.alias_generator(name)\n            if not isinstance(alias, str):\n                raise TypeError(f'Config.alias_generator must return str, not {alias.__class__}')\n            field_info.update(alias=alias, alias_priority=1)\n        return field_info", "target": "def test_field_priority_model():\n    v = SchemaValidator(\n        cs.model_schema(\n            cls=MyModel,\n            config=CoreConfig(str_max_length=10),\n            schema=cs.model_fields_schema(fields={'f': cs.model_field(schema=cs.str_schema(max_length=5))}),\n        )\n    )\n    assert 'max_length:Some(5)' in plain_repr(v)\n    assert v.isinstance_python({'f': 'test'}) is True\n    assert v.isinstance_python({'f': 'test long'}) is False"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004669", "source": "def is_valid_privateattr_name(name: str) -> bool:\n    return name.startswith('_') and not name.startswith('__')", "target": "def test_bad_default_factory(default_factory, error_message):\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'x': core_schema.model_field(\n                    schema=core_schema.with_default_schema(\n                        schema=core_schema.str_schema(), default_factory=default_factory\n                    )\n                )\n            }\n        )\n    )\n    with pytest.raises(TypeError, match=re.escape(error_message)):\n        v.validate_python({})"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004670", "source": "def wrap(cls: Type[Any]) -> 'DataclassClassOrWrapper':\n        should_use_proxy = (\n            use_proxy\n            if use_proxy is not None\n            else (\n                is_builtin_dataclass(cls)\n                and (cls.__bases__[0] is object or set(dir(cls)) == set(dir(cls.__bases__[0])))\n            )\n        )\n        if should_use_proxy:\n            dc_cls_doc = ''\n            dc_cls = DataclassProxy(cls)\n            default_validate_on_init = False\n        else:\n            dc_cls_doc = cls.__doc__ or ''\n            if sys.version_info >= (3, 10):\n                dc_cls = dataclasses.dataclass(\n                    cls,\n                    init=init,\n                    repr=repr,\n                    eq=eq,\n                    order=order,\n                    unsafe_hash=unsafe_hash,\n                    frozen=frozen,\n                    kw_only=kw_only,\n                )\n            else:\n                dc_cls = dataclasses.dataclass(\n                    cls, init=init, repr=repr, eq=eq, order=order, unsafe_hash=unsafe_hash, frozen=frozen\n                )\n            default_validate_on_init = True\n        should_validate_on_init = default_validate_on_init if validate_on_init is None else validate_on_init\n        _add_pydantic_validation_attributes(cls, the_config, should_validate_on_init, dc_cls_doc)\n        dc_cls.__pydantic_model__.__try_update_forward_refs__(**{cls.__name__: cls})\n        return dc_cls", "target": "def test_only_validate_by_name(py_and_json) -> None:\n    schema = core_schema.dataclass_schema(\n        BasicDataclass,\n        core_schema.dataclass_args_schema(\n            'BasicDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), validation_alias='FieldA'),\n            ],\n        ),\n        ['a'],\n        config=core_schema.CoreConfig(validate_by_name=True, validate_by_alias=False),\n    )\n    v = py_and_json(schema)\n    assert v.validate_test({'a': 'hello'}) == BasicDataclass(a='hello')\n    with pytest.raises(ValidationError, match=r'a\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'FieldA': 'hello'})"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004671", "source": "def with_config(**config: Unpack[ConfigDict]) -> Callable[[_TypeT], _TypeT]: ...", "target": "def test_on_model_class():\n    v = SchemaValidator(\n        cs.model_schema(\n            cls=MyModel,\n            config=CoreConfig(str_max_length=5),\n            schema=cs.model_fields_schema(fields={'f': cs.model_field(schema=cs.str_schema())}),\n        )\n    )\n    assert 'max_length:Some(5)' in plain_repr(v)\n    assert v.isinstance_python({'f': 'test'}) is True\n    assert v.isinstance_python({'f': 'test long'}) is False"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004672", "source": "def rebuild_dataclass_fields(\n    cls: type[PydanticDataclass],\n    *,\n    config_wrapper: ConfigWrapper,\n    ns_resolver: NsResolver,\n    typevars_map: Mapping[TypeVar, Any],\n) -> dict[str, FieldInfo]:\n    FieldInfo_ = import_cached_field_info()\n    rebuilt_fields: dict[str, FieldInfo] = {}\n    with ns_resolver.push(cls):\n        for f_name, field_info in cls.__pydantic_fields__.items():\n            if field_info._complete:\n                rebuilt_fields[f_name] = field_info\n            else:\n                existing_desc = field_info.description\n                ann = _typing_extra.eval_type(\n                    field_info._original_annotation,\n                    *ns_resolver.types_namespace,\n                )\n                ann = _generics.replace_types(ann, typevars_map)\n                new_field = FieldInfo_.from_annotated_attribute(\n                    ann,\n                    field_info._original_assignment,\n                    _source=AnnotationSource.DATACLASS,\n                )\n                new_field.description = new_field.description if new_field.description is not None else existing_desc\n                update_field_from_config(config_wrapper, f_name, new_field)\n                rebuilt_fields[f_name] = new_field\n    return rebuilt_fields", "target": "def test_from_attributes_function(input_value, expected):\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={'a': core_schema.model_field(schema=core_schema.any_schema())}, from_attributes=True\n        )\n    )\n    model_dict, model_extra, fields_set = v.validate_python(input_value)\n    assert model_dict == expected\n    assert model_extra is None\n    assert fields_set == {'a'}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004673", "source": "def handle_extra_init(self: 'Dataclass', *args: Any, **kwargs: Any) -> None:\n        if config.extra == Extra.ignore:\n            init(self, *args, **{k: v for k, v in kwargs.items() if k in self.__dataclass_fields__})\n        elif config.extra == Extra.allow:\n            for k, v in kwargs.items():\n                self.__dict__.setdefault(k, v)\n            init(self, *args, **{k: v for k, v in kwargs.items() if k in self.__dataclass_fields__})\n        else:\n            init(self, *args, **kwargs)", "target": "def test_frozen():\n    @dataclasses.dataclass\n    class MyModel:\n        f: str\n    v = SchemaValidator(\n        core_schema.dataclass_schema(\n            MyModel,\n            core_schema.dataclass_args_schema('MyModel', [core_schema.dataclass_field('f', core_schema.str_schema())]),\n            ['f'],\n            frozen=True,\n        )\n    )\n    m = v.validate_python({'f': 'x'})\n    assert m.f == 'x'\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_assignment(m, 'f', 'y')\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'frozen_instance', 'loc': (), 'msg': 'Instance is frozen', 'input': 'y'}\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004674", "source": "def decimal_encoder(dec_value: Decimal) -> Union[int, float]:\n    exponent = dec_value.as_tuple().exponent\n    if isinstance(exponent, int) and exponent >= 0:\n        return int(dec_value)\n    else:\n        return float(dec_value)", "target": "def test_str():\n    s = SchemaValidator(core_schema.str_schema())\n    assert s.validate_json('\"foobar\"') == 'foobar'\n    with pytest.raises(ValidationError, match=r'Input should be a valid string \\[type=string_type,'):\n        s.validate_json('false')\n    with pytest.raises(ValidationError, match=r'Input should be a valid string \\[type=string_type,'):\n        s.validate_json('123')"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004675", "source": "def timedelta_isoformat(td: datetime.timedelta) -> str:\n    warnings.warn('`timedelta_isoformat` is deprecated.', category=PydanticDeprecatedSince20, stacklevel=2)\n    minutes, seconds = divmod(td.seconds, 60)\n    hours, minutes = divmod(minutes, 60)\n    return f'{\"-\" if td.days < 0 else \"\"}P{abs(td.days)}DT{hours:d}H{minutes:d}M{seconds:d}.{td.microseconds:06d}S'", "target": "def test_dict_key_json():\n    s = SchemaSerializer(core_schema.dict_schema(core_schema.json_schema(), core_schema.any_schema()))\n    v = {(1, 2): 3, (4, 5): 9}\n    assert s.to_python(v) == v\n    assert s.to_python(v, round_trip=True) == {'[1,2]': 3, '[4,5]': 9}\n    assert s.to_python(v, mode='json') == {'1,2': 3, '4,5': 9}\n    assert s.to_python(v, mode='json', round_trip=True) == {'[1,2]': 3, '[4,5]': 9}\n    assert s.to_json(v) == b'{\"1,2\":3,\"4,5\":9}'\n    assert s.to_json(v, round_trip=True) == b'{\"[1,2]\":3,\"[4,5]\":9}'"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004676", "source": "def is_path_exist(path: Union[str, Path, None]) -> bool:\n    return bool(path and get_path(path).exists())", "target": "def test_force_create_dir_clears_existing(self):\n        d = self.tmp_path / \"fresh\"\n        (d / \"inner\").mkdir(parents=True)\n        (d / \"inner\" / \"f.txt\").write_text(\"x\")\n        force_create_dir(d)\n        self.assertTrue(d.exists())\n        self.assertEqual(list(d.iterdir()), [])"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004677", "source": "def _generic_get_args(tp: Type[Any]) -> Tuple[Any, ...]:\n        if hasattr(tp, '_nparams'):\n            return (Any,) * tp._nparams\n        try:\n            if tp == Tuple[()] or sys.version_info >= (3, 9) and tp == tuple[()]:\n                return ((),)\n        except TypeError:\n            pass\n        return ()", "target": "def test_schema_validator_wrong() -> None:\n    try:\n        SchemaValidator({'type': 'bad'})\n    except SchemaError:\n        pass\n    else:\n        raise AssertionError('SchemaValidator did not raise SchemaError')"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004678", "source": "def _add_pydantic_validation_attributes(\n    dc_cls: Type['Dataclass'],\n    config: Type[BaseConfig],\n    validate_on_init: bool,\n    dc_cls_doc: str,\n) -> None:\n    init = dc_cls.__init__\n    @wraps(init)\n    def handle_extra_init(self: 'Dataclass', *args: Any, **kwargs: Any) -> None:\n        if config.extra == Extra.ignore:\n            init(self, *args, **{k: v for k, v in kwargs.items() if k in self.__dataclass_fields__})\n        elif config.extra == Extra.allow:\n            for k, v in kwargs.items():\n                self.__dict__.setdefault(k, v)\n            init(self, *args, **{k: v for k, v in kwargs.items() if k in self.__dataclass_fields__})\n        else:\n            init(self, *args, **kwargs)\n    if hasattr(dc_cls, '__post_init__'):\n        try:\n            post_init = dc_cls.__post_init__.__wrapped__\n        except AttributeError:\n            post_init = dc_cls.__post_init__\n        @wraps(post_init)\n        def new_post_init(self: 'Dataclass', *args: Any, **kwargs: Any) -> None:\n            if config.post_init_call == 'before_validation':\n                post_init(self, *args, **kwargs)\n            if self.__class__.__pydantic_run_validation__:\n                self.__pydantic_validate_values__()\n                if hasattr(self, '__post_init_post_parse__'):\n                    self.__post_init_post_parse__(*args, **kwargs)\n            if config.post_init_call == 'after_validation':\n                post_init(self, *args, **kwargs)\n        setattr(dc_cls, '__init__', handle_extra_init)\n        setattr(dc_cls, '__post_init__', new_post_init)\n    else:\n        @wraps(init)\n        def new_init(self: 'Dataclass', *args: Any, **kwargs: Any) -> None:\n            handle_extra_init(self, *args, **kwargs)\n            if self.__class__.__pydantic_run_validation__:\n                self.__pydantic_validate_values__()\n            if hasattr(self, '__post_init_post_parse__'):\n                initvars_and_values: Dict[str, Any] = {}\n                for i, f in enumerate(self.__class__.__dataclass_fields__.values()):\n                    if f._field_type is dataclasses._FIELD_INITVAR:\n                        try:\n                            initvars_and_values[f.name] = args[i]\n                        except IndexError:\n                            initvars_and_values[f.name] = kwargs.get(f.name, f.default)\n                self.__post_init_post_parse__(**initvars_and_values)\n        setattr(dc_cls, '__init__', new_init)\n    setattr(dc_cls, '__pydantic_run_validation__', ClassAttribute('__pydantic_run_validation__', validate_on_init))\n    setattr(dc_cls, '__pydantic_initialised__', False)\n    setattr(dc_cls, '__pydantic_model__', create_pydantic_model_from_dataclass(dc_cls, config, dc_cls_doc))\n    setattr(dc_cls, '__pydantic_validate_values__', _dataclass_validate_values)\n    setattr(dc_cls, '__validate__', classmethod(_validate_dataclass))\n    setattr(dc_cls, '__get_validators__', classmethod(_get_validators))\n    if dc_cls.__pydantic_model__.__config__.validate_assignment and not dc_cls.__dataclass_params__.frozen:\n        setattr(dc_cls, '__setattr__', _dataclass_validate_assignment_setattr)", "target": "def test_dataclass_validate_assignment():\n    schema = core_schema.dataclass_schema(\n        FooDataclass,\n        core_schema.dataclass_args_schema(\n            'FooDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), kw_only=False),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), kw_only=False),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 'hello', 'b': 'True'})\n    assert dataclasses.asdict(foo) == {'a': 'hello', 'b': True}\n    v.validate_assignment(foo, 'a', b'world')\n    assert dataclasses.asdict(foo) == {'a': 'world', 'b': True}\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_assignment(foo, 'a', 123)\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'string_type', 'loc': ('a',), 'msg': 'Input should be a valid string', 'input': 123}\n    ]\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_assignment(foo, 'c', '123')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'no_such_attribute',\n            'loc': ('c',),\n            'msg': \"Object has no attribute 'c'\",\n            'input': '123',\n            'ctx': {'attribute': 'c'},\n        }\n    ]\n    assert not hasattr(foo, 'c')\n    with pytest.raises(AttributeError, match=\"'str' object has no attribute 'a'\"):\n        v.validate_assignment('field_a', 'c', 123)"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004679", "source": "def str2bool(value: Optional[str]) -> bool:\n    if not value:\n        return False\n    if not isinstance(value, str):\n        raise ValueError(\n            f\"Expected a string value for boolean conversion, got {type(value)}\"\n        )\n    value = value.strip().lower()\n    true_value_set = {\"1\", \"true\", \"t\", \"yes\", \"y\", \"on\", \"enable\", \"enabled\", \"found\"}\n    false_value_set = {\"0\", \"false\", \"f\", \"no\", \"n\", \"off\", \"disable\"}\n    if value in true_value_set:\n        return True\n    if value in false_value_set:\n        return False\n    raise ValueError(f\"Invalid string value for boolean conversion: {value}\")", "target": "def test_merged_lastfailed_content_with_empty_source(self) -> None:\n        last_failed_source = {\n            \"\": True,\n        }\n        last_failed_dest = {\n            \"tools/tests/test_car.py::test_num1\": True,\n            \"tools/tests/test_car.py::test_num2\": True,\n        }\n        last_failed_merged = {\n            \"tools/tests/test_car.py::test_num1\": True,\n            \"tools/tests/test_car.py::test_num2\": True,\n        }\n        merged = _merged_lastfailed_content(last_failed_source, last_failed_dest)\n        self.assertEqual(merged, last_failed_merged)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004680", "source": "def _check_protected_namespaces(\n    protected_namespaces: tuple[str | Pattern[str], ...],\n    ann_name: str,\n    bases: tuple[type[Any], ...],\n    cls_name: str,\n) -> None:\n    BaseModel = import_cached_base_model()\n    for protected_namespace in protected_namespaces:\n        ns_violation = False\n        if isinstance(protected_namespace, Pattern):\n            ns_violation = protected_namespace.match(ann_name) is not None\n        elif isinstance(protected_namespace, str):\n            ns_violation = ann_name.startswith(protected_namespace)\n        if ns_violation:\n            for b in bases:\n                if hasattr(b, ann_name):\n                    if not (issubclass(b, BaseModel) and ann_name in getattr(b, '__pydantic_fields__', {})):\n                        raise ValueError(\n                            f'Field {ann_name!r} conflicts with member {getattr(b, ann_name)}'\n                            f' of protected namespace {protected_namespace!r}.'\n                        )\n            else:\n                valid_namespaces: list[str] = []\n                for pn in protected_namespaces:\n                    if isinstance(pn, Pattern):\n                        if not pn.match(ann_name):\n                            valid_namespaces.append(f're.compile({pn.pattern!r})')\n                    else:\n                        if not ann_name.startswith(pn):\n                            valid_namespaces.append(f\"'{pn}'\")\n                valid_namespaces_str = f'({\", \".join(valid_namespaces)}{\",)\" if len(valid_namespaces) == 1 else \")\"}'\n                warnings.warn(\n                    f'Field {ann_name!r} in {cls_name!r} conflicts with protected namespace {protected_namespace!r}.\\n\\n'\n                    f\"You may be able to solve this by setting the 'protected_namespaces' configuration to {valid_namespaces_str}.\",\n                    UserWarning,\n                    stacklevel=5,\n                )", "target": "def test_empty_string_aliases(py_and_json: PyAndJson):\n    v = py_and_json(\n        {\n            'type': 'model-fields',\n            'fields': {'field_a': {'validation_alias': '', 'type': 'model-field', 'schema': {'type': 'int'}}},\n        }\n    )\n    assert v.validate_test({'': 123}) == ({'field_a': 123}, None, {'field_a'})\n    v = py_and_json(\n        {\n            'type': 'model-fields',\n            'fields': {'field_a': {'validation_alias': ['', ''], 'type': 'model-field', 'schema': {'type': 'int'}}},\n        }\n    )\n    assert v.validate_test({'': {'': 123}}) == ({'field_a': 123}, None, {'field_a'})"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004681", "source": "def make_dataclass_validator(dc_cls: Type['Dataclass'], config: Type[BaseConfig]) -> 'CallableGenerator':\n    yield from _get_validators(dataclass(dc_cls, config=config, use_proxy=True))", "target": "def test_frozen():\n    @dataclasses.dataclass\n    class MyModel:\n        f: str\n    v = SchemaValidator(\n        core_schema.dataclass_schema(\n            MyModel,\n            core_schema.dataclass_args_schema('MyModel', [core_schema.dataclass_field('f', core_schema.str_schema())]),\n            ['f'],\n            frozen=True,\n        )\n    )\n    m = v.validate_python({'f': 'x'})\n    assert m.f == 'x'\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_assignment(m, 'f', 'y')\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'frozen_instance', 'loc': (), 'msg': 'Instance is frozen', 'input': 'y'}\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004682", "source": "def _add_pydantic_validation_attributes(\n    dc_cls: Type['Dataclass'],\n    config: Type[BaseConfig],\n    validate_on_init: bool,\n    dc_cls_doc: str,\n) -> None:\n    init = dc_cls.__init__\n    @wraps(init)\n    def handle_extra_init(self: 'Dataclass', *args: Any, **kwargs: Any) -> None:\n        if config.extra == Extra.ignore:\n            init(self, *args, **{k: v for k, v in kwargs.items() if k in self.__dataclass_fields__})\n        elif config.extra == Extra.allow:\n            for k, v in kwargs.items():\n                self.__dict__.setdefault(k, v)\n            init(self, *args, **{k: v for k, v in kwargs.items() if k in self.__dataclass_fields__})\n        else:\n            init(self, *args, **kwargs)\n    if hasattr(dc_cls, '__post_init__'):\n        try:\n            post_init = dc_cls.__post_init__.__wrapped__\n        except AttributeError:\n            post_init = dc_cls.__post_init__\n        @wraps(post_init)\n        def new_post_init(self: 'Dataclass', *args: Any, **kwargs: Any) -> None:\n            if config.post_init_call == 'before_validation':\n                post_init(self, *args, **kwargs)\n            if self.__class__.__pydantic_run_validation__:\n                self.__pydantic_validate_values__()\n                if hasattr(self, '__post_init_post_parse__'):\n                    self.__post_init_post_parse__(*args, **kwargs)\n            if config.post_init_call == 'after_validation':\n                post_init(self, *args, **kwargs)\n        setattr(dc_cls, '__init__', handle_extra_init)\n        setattr(dc_cls, '__post_init__', new_post_init)\n    else:\n        @wraps(init)\n        def new_init(self: 'Dataclass', *args: Any, **kwargs: Any) -> None:\n            handle_extra_init(self, *args, **kwargs)\n            if self.__class__.__pydantic_run_validation__:\n                self.__pydantic_validate_values__()\n            if hasattr(self, '__post_init_post_parse__'):\n                initvars_and_values: Dict[str, Any] = {}\n                for i, f in enumerate(self.__class__.__dataclass_fields__.values()):\n                    if f._field_type is dataclasses._FIELD_INITVAR:\n                        try:\n                            initvars_and_values[f.name] = args[i]\n                        except IndexError:\n                            initvars_and_values[f.name] = kwargs.get(f.name, f.default)\n                self.__post_init_post_parse__(**initvars_and_values)\n        setattr(dc_cls, '__init__', new_init)\n    setattr(dc_cls, '__pydantic_run_validation__', ClassAttribute('__pydantic_run_validation__', validate_on_init))\n    setattr(dc_cls, '__pydantic_initialised__', False)\n    setattr(dc_cls, '__pydantic_model__', create_pydantic_model_from_dataclass(dc_cls, config, dc_cls_doc))\n    setattr(dc_cls, '__pydantic_validate_values__', _dataclass_validate_values)\n    setattr(dc_cls, '__validate__', classmethod(_validate_dataclass))\n    setattr(dc_cls, '__get_validators__', classmethod(_get_validators))\n    if dc_cls.__pydantic_model__.__config__.validate_assignment and not dc_cls.__dataclass_params__.frozen:\n        setattr(dc_cls, '__setattr__', _dataclass_validate_assignment_setattr)", "target": "def test_dataclass():\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bytes_schema()),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    s = SchemaSerializer(schema)\n    assert s.to_python(Foo(a='hello', b=b'more')) == IsStrictDict(a='hello', b=b'more')\n    assert s.to_python(Foo(a='hello', b=b'more'), mode='json') == IsStrictDict(a='hello', b='more')\n    j = s.to_json(Foo(a='hello', b=b'more'))\n    if on_pypy:\n        assert json.loads(j) == {'a': 'hello', 'b': 'more'}\n    else:\n        assert j == b'{\"a\":\"hello\",\"b\":\"more\"}'\n    assert s.to_python(Foo(a='hello', b=b'more'), exclude={'b'}) == IsStrictDict(a='hello')\n    assert s.to_json(Foo(a='hello', b=b'more'), include={'a'}) == b'{\"a\":\"hello\"}'"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004683", "source": "def resolve_annotations(raw_annotations: Dict[str, Type[Any]], module_name: Optional[str]) -> Dict[str, Type[Any]]:\n    base_globals: Optional[Dict[str, Any]] = None\n    if module_name:\n        try:\n            module = sys.modules[module_name]\n        except KeyError:\n            pass\n        else:\n            base_globals = module.__dict__\n    annotations = {}\n    for name, value in raw_annotations.items():\n        if isinstance(value, str):\n            if (3, 10) > sys.version_info >= (3, 9, 8) or sys.version_info >= (3, 10, 1):\n                value = ForwardRef(value, is_argument=False, is_class=True)\n            else:\n                value = ForwardRef(value, is_argument=False)\n        try:\n            if sys.version_info >= (3, 13):\n                value = _eval_type(value, base_globals, None, type_params=())\n            else:\n                value = _eval_type(value, base_globals, None)\n        except NameError:\n            pass\n        annotations[name] = value\n    return annotations", "target": "def test_schema_typing_error() -> None:\n    _: CoreSchema = {'type': 'wrong'}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004684", "source": "def _add_pydantic_validation_attributes(\n    dc_cls: Type['Dataclass'],\n    config: Type[BaseConfig],\n    validate_on_init: bool,\n    dc_cls_doc: str,\n) -> None:\n    init = dc_cls.__init__\n    @wraps(init)\n    def handle_extra_init(self: 'Dataclass', *args: Any, **kwargs: Any) -> None:\n        if config.extra == Extra.ignore:\n            init(self, *args, **{k: v for k, v in kwargs.items() if k in self.__dataclass_fields__})\n        elif config.extra == Extra.allow:\n            for k, v in kwargs.items():\n                self.__dict__.setdefault(k, v)\n            init(self, *args, **{k: v for k, v in kwargs.items() if k in self.__dataclass_fields__})\n        else:\n            init(self, *args, **kwargs)\n    if hasattr(dc_cls, '__post_init__'):\n        try:\n            post_init = dc_cls.__post_init__.__wrapped__\n        except AttributeError:\n            post_init = dc_cls.__post_init__\n        @wraps(post_init)\n        def new_post_init(self: 'Dataclass', *args: Any, **kwargs: Any) -> None:\n            if config.post_init_call == 'before_validation':\n                post_init(self, *args, **kwargs)\n            if self.__class__.__pydantic_run_validation__:\n                self.__pydantic_validate_values__()\n                if hasattr(self, '__post_init_post_parse__'):\n                    self.__post_init_post_parse__(*args, **kwargs)\n            if config.post_init_call == 'after_validation':\n                post_init(self, *args, **kwargs)\n        setattr(dc_cls, '__init__', handle_extra_init)\n        setattr(dc_cls, '__post_init__', new_post_init)\n    else:\n        @wraps(init)\n        def new_init(self: 'Dataclass', *args: Any, **kwargs: Any) -> None:\n            handle_extra_init(self, *args, **kwargs)\n            if self.__class__.__pydantic_run_validation__:\n                self.__pydantic_validate_values__()\n            if hasattr(self, '__post_init_post_parse__'):\n                initvars_and_values: Dict[str, Any] = {}\n                for i, f in enumerate(self.__class__.__dataclass_fields__.values()):\n                    if f._field_type is dataclasses._FIELD_INITVAR:\n                        try:\n                            initvars_and_values[f.name] = args[i]\n                        except IndexError:\n                            initvars_and_values[f.name] = kwargs.get(f.name, f.default)\n                self.__post_init_post_parse__(**initvars_and_values)\n        setattr(dc_cls, '__init__', new_init)\n    setattr(dc_cls, '__pydantic_run_validation__', ClassAttribute('__pydantic_run_validation__', validate_on_init))\n    setattr(dc_cls, '__pydantic_initialised__', False)\n    setattr(dc_cls, '__pydantic_model__', create_pydantic_model_from_dataclass(dc_cls, config, dc_cls_doc))\n    setattr(dc_cls, '__pydantic_validate_values__', _dataclass_validate_values)\n    setattr(dc_cls, '__validate__', classmethod(_validate_dataclass))\n    setattr(dc_cls, '__get_validators__', classmethod(_get_validators))\n    if dc_cls.__pydantic_model__.__config__.validate_assignment and not dc_cls.__dataclass_params__.frozen:\n        setattr(dc_cls, '__setattr__', _dataclass_validate_assignment_setattr)", "target": "def test_dataclass_json():\n    schema = core_schema.dataclass_schema(\n        FooDataclass,\n        core_schema.dataclass_args_schema(\n            'FooDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    assert v.validate_json('{\"a\": \"hello\", \"b\": true}') == FooDataclass(a='hello', b=True)\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_json('[\"a\", \"b\"]')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'class_name': 'FooDataclass'},\n            'input': ['a', 'b'],\n            'loc': (),\n            'msg': 'Input should be an object',\n            'type': 'dataclass_type',\n        }\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004685", "source": "def update_field_from_config(config_wrapper: ConfigWrapper, field_name: str, field_info: FieldInfo) -> None:\n    field_title_generator = field_info.field_title_generator or config_wrapper.field_title_generator\n    if field_title_generator is not None:\n        _apply_field_title_generator_to_field_info(field_title_generator, field_name, field_info)\n    if config_wrapper.alias_generator is not None:\n        _apply_alias_generator_to_field_info(config_wrapper.alias_generator, field_name, field_info)", "target": "def test_allow_extra_fn_override_wrong():\n    v = SchemaValidator(schema=core_schema.model_fields_schema(fields={}))\n    with pytest.raises(ValueError, match='Invalid extra_behavior: `wrong`'):\n        v.validate_python({}, extra='wrong')"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004686", "source": "def gh_owner_and_name(self) -> tuple[str, str]:\n        url = os.getenv(\"GIT_REMOTE_URL\", None)\n        if url is None:\n            url = self.remote_url()\n        rc = RE_GITHUB_URL_MATCH.match(url)\n        if rc is None:\n            raise RuntimeError(f\"Unexpected url format {url}\")\n        return cast(tuple[str, str], rc.groups())", "target": "def test_ghstack_branches_not_in_sync(self) -> None:\n        head_ref = \"gh/clee2000/1/head\"\n        self._skip_if_ref_does_not_exist(head_ref)\n        self.assertFalse(are_ghstack_branches_in_sync(self.repo, head_ref))"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004687", "source": "def dataclass(\n        *,\n        init: bool = True,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool = False,\n        config: Union[ConfigDict, Type[object], None] = None,\n        validate_on_init: Optional[bool] = None,\n        use_proxy: Optional[bool] = None,\n    ) -> Callable[[Type[_T]], 'DataclassClassOrWrapper']:\n        ...", "target": "def test_serialization_alias():\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bytes_schema(), serialization_alias='BAR'),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    s = SchemaSerializer(schema)\n    assert s.to_python(Foo(a='hello', b=b'more'), by_alias=True) == IsStrictDict(a='hello', BAR=b'more')\n    assert s.to_python(Foo(a='hello', b=b'more'), mode='json', by_alias=True) == IsStrictDict(a='hello', BAR='more')\n    j = s.to_json(Foo(a='hello', b=b'more'), by_alias=True)\n    if on_pypy:\n        assert json.loads(j) == {'a': 'hello', 'BAR': 'more'}\n    else:\n        assert j == b'{\"a\":\"hello\",\"BAR\":\"more\"}'"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004688", "source": "def load_weights(mod: nn.Module, names: list[str], params: tuple[Tensor, ...]) -> None:\n    for name, p in zip(names, params):\n        _set_nested_attr(mod, name.split(\".\"), p)", "target": "def test_is_iterable(self) -> None:\n        from collections.abc import Iterator\n        iter_ = PeekableIterator(\"\")\n        self.assertTrue(isinstance(iter_, Iterator))"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004689", "source": "def _update_fields_from_docstrings(cls: type[Any], fields: dict[str, FieldInfo], use_inspect: bool = False) -> None:\n    fields_docs = extract_docstrings_from_cls(cls, use_inspect=use_inspect)\n    for ann_name, field_info in fields.items():\n        if field_info.description is None and ann_name in fields_docs:\n            field_info.description = fields_docs[ann_name]", "target": "def test_extra_behavior_allow_with_validate_fn_override(\n    config: Union[core_schema.CoreConfig, None],\n    schema_extra_behavior_kw: dict[str, Any],\n):\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            {'f': core_schema.model_field(core_schema.str_schema())}, **schema_extra_behavior_kw\n        ),\n        config=config,\n    )\n    m, model_extra, fields_set = v.validate_python({'f': 'x', 'extra_field': '123'}, extra='allow')\n    assert m == {'f': 'x'}\n    assert model_extra == {'extra_field': '123'}\n    assert fields_set == {'f', 'extra_field'}\n    v.validate_assignment(m, 'f', 'y', extra='allow')\n    assert m == {'f': 'y'}\n    new_m, new_model_extra, new_fields_set = v.validate_assignment({**m, **model_extra}, 'not_f', '123', extra='allow')\n    assert new_m == {'f': 'y'}\n    assert new_model_extra == {'extra_field': '123', 'not_f': '123'}\n    assert new_fields_set == {'not_f'}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004690", "source": "def rebuild_model_fields(\n    cls: type[BaseModel],\n    *,\n    config_wrapper: ConfigWrapper,\n    ns_resolver: NsResolver,\n    typevars_map: Mapping[TypeVar, Any],\n) -> dict[str, FieldInfo]:\n    rebuilt_fields: dict[str, FieldInfo] = {}\n    with ns_resolver.push(cls):\n        for f_name, field_info in cls.__pydantic_fields__.items():\n            if field_info._complete:\n                rebuilt_fields[f_name] = field_info\n            else:\n                new_field = _recreate_field_info(\n                    field_info, ns_resolver=ns_resolver, typevars_map=typevars_map, lenient=False\n                )\n                update_field_from_config(config_wrapper, f_name, new_field)\n                rebuilt_fields[f_name] = new_field\n    return rebuilt_fields", "target": "def test_json_error():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'field_a': core_schema.model_field(\n                    schema=core_schema.list_schema(items_schema=core_schema.int_schema())\n                )\n            }\n        )\n    )\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_json('{\"field_a\": [123, \"wrong\"]}')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('field_a', 1),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'wrong',\n        }\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004691", "source": "def rebuild_dataclass_fields(\n    cls: type[PydanticDataclass],\n    *,\n    config_wrapper: ConfigWrapper,\n    ns_resolver: NsResolver,\n    typevars_map: Mapping[TypeVar, Any],\n) -> dict[str, FieldInfo]:\n    FieldInfo_ = import_cached_field_info()\n    rebuilt_fields: dict[str, FieldInfo] = {}\n    with ns_resolver.push(cls):\n        for f_name, field_info in cls.__pydantic_fields__.items():\n            if field_info._complete:\n                rebuilt_fields[f_name] = field_info\n            else:\n                existing_desc = field_info.description\n                ann = _typing_extra.eval_type(\n                    field_info._original_annotation,\n                    *ns_resolver.types_namespace,\n                )\n                ann = _generics.replace_types(ann, typevars_map)\n                new_field = FieldInfo_.from_annotated_attribute(\n                    ann,\n                    field_info._original_assignment,\n                    _source=AnnotationSource.DATACLASS,\n                )\n                new_field.description = new_field.description if new_field.description is not None else existing_desc\n                update_field_from_config(config_wrapper, f_name, new_field)\n                rebuilt_fields[f_name] = new_field\n    return rebuilt_fields", "target": "def test_allow_extra_invalid():\n    with pytest.raises(SchemaError, match='extras_schema can only be used if extra_behavior=allow'):\n        SchemaValidator(\n            schema=core_schema.model_fields_schema(\n                fields={}, extras_schema=core_schema.int_schema(), extra_behavior='ignore'\n            )\n        )\n    with pytest.raises(SchemaError, match='extras_keys_schema can only be used if extra_behavior=allow'):\n        SchemaValidator(\n            schema=core_schema.model_fields_schema(\n                fields={}, extras_keys_schema=core_schema.int_schema(), extra_behavior='ignore'\n            )\n        )"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004692", "source": "def list_validator(v: Any) -> List[Any]:\n    if isinstance(v, list):\n        return v\n    elif sequence_like(v):\n        return list(v)\n    else:\n        raise errors.ListError()", "target": "def test_validators_build(benchmark) -> None:\n    class Base1(BaseModel):\n        a: int\n        @field_validator('a', mode='after')\n        @classmethod\n        def val_a(cls, value: int) -> int: ...\n        @computed_field\n        def prop(self) -> int: ...\n    class Bare:\n        @computed_field\n        def prop_bare(self) -> int: ...\n    class Sub1(Base1):\n        @computed_field\n        def prop_2(self) -> int: ...\n        @computed_field\n        def prop_3(self) -> int: ...\n        @computed_field\n        def prop_4(self) -> int: ...\n    @benchmark\n    def bench() -> None:\n        class SubS(Sub1, Bare, defer_build=True):\n            @computed_field\n            def prop_5(self) -> int: ...\n            @computed_field\n            def prop_6(self) -> int: ...\n            @computed_field\n            def prop_7(self) -> int: ..."}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004693", "source": "def none_validator(v: Any) -> 'Literal[None]':\n    if v is None:\n        return v\n    raise errors.NotNoneError()", "target": "def test_validators_build(benchmark) -> None:\n    class Base1(BaseModel):\n        a: int\n        @field_validator('a', mode='after')\n        @classmethod\n        def val_a(cls, value: int) -> int: ...\n        @computed_field\n        def prop(self) -> int: ...\n    class Bare:\n        @computed_field\n        def prop_bare(self) -> int: ...\n    class Sub1(Base1):\n        @computed_field\n        def prop_2(self) -> int: ...\n        @computed_field\n        def prop_3(self) -> int: ...\n        @computed_field\n        def prop_4(self) -> int: ...\n    @benchmark\n    def bench() -> None:\n        class SubS(Sub1, Bare, defer_build=True):\n            @computed_field\n            def prop_5(self) -> int: ...\n            @computed_field\n            def prop_6(self) -> int: ...\n            @computed_field\n            def prop_7(self) -> int: ..."}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004694", "source": "def is_typeddict_special(type_: Any) -> bool:\n    return _check_typeddict_special(type_) or _check_typeddict_special(get_origin(type_))", "target": "def test_type_error():\n    try:\n        PydanticKnownError('foobar')\n    except KeyError as exc:\n        assert str(exc) == '\"Invalid error type: \\'foobar\\'\"'\n    else:\n        raise AssertionError(\"PydanticKnownError('foobar') did not raise KeyError\")\n    e = PydanticKnownError('recursion_loop')\n    assert isinstance(e, PydanticKnownError)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004695", "source": "def create_dataclass(cls: type[Any]) -> type[PydanticDataclass]:\n        from ._internal._utils import is_model_class\n        if is_model_class(cls):\n            raise PydanticUserError(\n                f'Cannot create a Pydantic dataclass from {cls.__name__} as it is already a Pydantic model',\n                code='dataclass-on-model',\n            )\n        original_cls = cls\n        has_dataclass_base = any(dataclasses.is_dataclass(base) for base in cls.__bases__)\n        if not has_dataclass_base and config is not None and hasattr(cls, '__pydantic_config__'):\n            warn(\n                f'`config` is set via both the `dataclass` decorator and `__pydantic_config__` for dataclass {cls.__name__}. '\n                f'The `config` specification from `dataclass` decorator will take priority.',\n                category=UserWarning,\n                stacklevel=2,\n            )\n        config_dict = config if config is not None else getattr(cls, '__pydantic_config__', None)\n        config_wrapper = _config.ConfigWrapper(config_dict)\n        decorators = _decorators.DecoratorInfos.build(cls, replace_wrapped_methods=True)\n        decorators.update_from_config(config_wrapper)\n        original_doc = cls.__doc__\n        if _pydantic_dataclasses.is_stdlib_dataclass(cls):\n            original_doc = None\n            bases = (cls,)\n            if issubclass(cls, Generic):\n                generic_base = Generic[cls.__parameters__]\n                bases = bases + (generic_base,)\n            cls = types.new_class(cls.__name__, bases)\n        if frozen is not None:\n            frozen_ = frozen\n            if config_wrapper.frozen:\n                warn(\n                    f'`frozen` is set via both the `dataclass` decorator and `config` for dataclass {cls.__name__!r}.'\n                    'This is not recommended. The `frozen` specification on `dataclass` will take priority.',\n                    category=UserWarning,\n                    stacklevel=2,\n                )\n        else:\n            frozen_ = config_wrapper.frozen or False\n        cls_anns = _typing_extra.safe_get_annotations(cls)\n        for field_name in cls_anns:\n            field_value = getattr(cls, field_name, None)\n            if isinstance(field_value, FieldInfo):\n                setattr(cls, field_name, _pydantic_dataclasses.as_dataclass_field(field_value))\n        with _pydantic_dataclasses.patch_base_fields(cls):\n            cls = dataclasses.dataclass(\n                cls,\n                init=True,\n                repr=repr,\n                eq=eq,\n                order=order,\n                unsafe_hash=unsafe_hash,\n                frozen=frozen_,\n                **kwargs,\n            )\n        if config_wrapper.validate_assignment:\n            original_setattr = cls.__setattr__\n            @functools.wraps(cls.__setattr__)\n            def validated_setattr(instance: PydanticDataclass, name: str, value: Any, /) -> None:\n                if frozen_:\n                    return original_setattr(instance, name, value)\n                inst_cls = type(instance)\n                attr = getattr(inst_cls, name, None)\n                if isinstance(attr, property):\n                    attr.__set__(instance, value)\n                elif isinstance(attr, functools.cached_property):\n                    instance.__dict__.__setitem__(name, value)\n                else:\n                    inst_cls.__pydantic_validator__.validate_assignment(instance, name, value)\n            cls.__setattr__ = validated_setattr.__get__(None, cls)\n            if slots and not hasattr(cls, '__setstate__'):\n                def _dataclass_getstate(self: Any) -> list[Any]:\n                    return [getattr(self, f.name) for f in dataclasses.fields(self)]\n                def _dataclass_setstate(self: Any, state: list[Any]) -> None:\n                    for field, value in zip(dataclasses.fields(self), state):\n                        object.__setattr__(self, field.name, value)\n                cls.__getstate__ = _dataclass_getstate\n                cls.__setstate__ = _dataclass_setstate\n        cls.__is_pydantic_dataclass__ = True\n        cls.__pydantic_decorators__ = decorators\n        cls.__doc__ = original_doc\n        firstlineno = getattr(original_cls, '__firstlineno__', None)\n        cls.__module__ = original_cls.__module__\n        if sys.version_info >= (3, 13) and firstlineno is not None:\n            original_cls.__firstlineno__ = firstlineno\n            cls.__firstlineno__ = firstlineno\n        cls.__qualname__ = original_cls.__qualname__\n        cls.__pydantic_fields_complete__ = classmethod(_pydantic_fields_complete)\n        cls.__pydantic_complete__ = False\n        _pydantic_dataclasses.complete_dataclass(cls, config_wrapper, raise_errors=False)\n        return cls", "target": "def test_dataclass_args_init_with_default(input_value, extra_behavior, expected):\n    @dataclasses.dataclass\n    class Foo:\n        a: str\n        b: str\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(\n                    name='b',\n                    schema=core_schema.with_default_schema(schema=core_schema.str_schema(), default='HELLO'),\n                    init=False,\n                ),\n            ],\n            extra_behavior=extra_behavior,\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)) as exc_info:\n            v.validate_python(input_value)\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        assert dataclasses.asdict(v.validate_python(input_value)) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004696", "source": "def _apply_alias_generator_to_field_info(\n    alias_generator: Callable[[str], str] | AliasGenerator, field_name: str, field_info: FieldInfo\n):\n    if (\n        field_info.alias_priority is None\n        or field_info.alias_priority <= 1\n        or field_info.alias is None\n        or field_info.validation_alias is None\n        or field_info.serialization_alias is None\n    ):\n        alias, validation_alias, serialization_alias = None, None, None\n        if isinstance(alias_generator, AliasGenerator):\n            alias, validation_alias, serialization_alias = alias_generator.generate_aliases(field_name)\n        elif callable(alias_generator):\n            alias = alias_generator(field_name)\n            if not isinstance(alias, str):\n                raise TypeError(f'alias_generator {alias_generator} must return str, not {alias.__class__}')\n        if field_info.alias_priority is None or field_info.alias_priority <= 1:\n            field_info.alias_priority = 1\n        if field_info.alias_priority == 1:\n            field_info.serialization_alias = get_first_not_none(serialization_alias, alias)\n            field_info.validation_alias = get_first_not_none(validation_alias, alias)\n            field_info.alias = alias\n        if field_info.alias is None:\n            field_info.alias = alias\n        if field_info.serialization_alias is None:\n            field_info.serialization_alias = get_first_not_none(serialization_alias, alias)\n        if field_info.validation_alias is None:\n            field_info.validation_alias = get_first_not_none(validation_alias, alias)", "target": "def test_from_attributes_extra():\n    def another_function(x):\n        return x\n    class Foobar:\n        def __init__(self):\n            self.a = 1\n            self.b = 2\n            self._private_attribute = 4\n        @property\n        def c(self):\n            return 'ham'\n        @property\n        def _private_property(self):\n            return 'wrong'\n        @property\n        def property_error(self):\n            raise RuntimeError('xxx')\n        def bound_method(self):\n            return f'wrong {self.a}'\n        @staticmethod\n        def static_method():\n            return 'wrong'\n        function_attribute = another_function\n        @classmethod\n        def class_method(cls):\n            return 'wrong'\n    @dataclass\n    class MyDataclass:\n        a: int = 1\n        b: int = 2\n        c: str = 'ham'\n        _d: int = 4\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={'a': core_schema.model_field(schema=core_schema.int_schema())},\n            from_attributes=True,\n            extra_behavior='allow',\n        )\n    )\n    assert v.validate_python(Foobar()) == ({'a': 1}, {}, {'a'})\n    assert v.validate_python(MyDataclass()) == ({'a': 1}, {}, {'a'})\n    assert v.validate_python(Cls(a=1, b=2, c='ham')) == ({'a': 1}, {}, {'a'})\n    assert v.validate_python(Cls(a=1, b=datetime(2000, 1, 1))) == ({'a': 1}, {}, {'a'})\n    assert v.validate_python(Cls(a=1, b=datetime.now, c=lambda: 42)) == ({'a': 1}, {}, {'a'})"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004697", "source": "def dataclass(\n        *,\n        init: Literal[False] = False,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool | None = None,\n        config: ConfigDict | type[object] | None = None,\n        validate_on_init: bool | None = None,\n    ) -> Callable[[type[_T]], type[PydanticDataclass]]:\n        ...", "target": "def test_frozen():\n    @dataclasses.dataclass\n    class MyModel:\n        f: str\n    v = SchemaValidator(\n        core_schema.dataclass_schema(\n            MyModel,\n            core_schema.dataclass_args_schema('MyModel', [core_schema.dataclass_field('f', core_schema.str_schema())]),\n            ['f'],\n            frozen=True,\n        )\n    )\n    m = v.validate_python({'f': 'x'})\n    assert m.f == 'x'\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_assignment(m, 'f', 'y')\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'frozen_instance', 'loc': (), 'msg': 'Instance is frozen', 'input': 'y'}\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004698", "source": "def patch_base_fields(cls: type[Any]) -> Generator[None]:\n    original_fields_list: list[tuple[DcFields, DcFields]] = []\n    for base in cls.__mro__[1:]:\n        dc_fields: dict[str, dataclasses.Field[Any]] = base.__dict__.get('__dataclass_fields__', {})\n        dc_fields_with_pydantic_field_defaults = {\n            field_name: field\n            for field_name, field in dc_fields.items()\n            if isinstance(field.default, FieldInfo)\n            and (field.default.description is not None or field.default.kw_only or field.default.repr is not True)\n        }\n        if dc_fields_with_pydantic_field_defaults:\n            original_fields_list.append((dc_fields, dc_fields_with_pydantic_field_defaults))\n            for field_name, field in dc_fields_with_pydantic_field_defaults.items():\n                default = cast(FieldInfo, field.default)\n                new_dc_field = copy.copy(field)\n                if sys.version_info >= (3, 10) and default.kw_only:\n                    new_dc_field.kw_only = True\n                if default.repr is not True:\n                    new_dc_field.repr = default.repr\n                dc_fields[field_name] = new_dc_field\n    try:\n        yield\n    finally:\n        for fields, original_fields in original_fields_list:\n            for field_name, original_field in original_fields.items():\n                fields[field_name] = original_field", "target": "def test_function_validator_wrapping_args_schema_after() -> None:\n    calls: list[Any] = []\n    def func(*args: Any) -> Any:\n        calls.append(args)\n        return args[0]\n    @dataclasses.dataclass\n    class Model:\n        number: int = 1\n    cs = core_schema.dataclass_schema(\n        Model,\n        core_schema.no_info_after_validator_function(\n            func,\n            core_schema.dataclass_args_schema(\n                'Model', [core_schema.dataclass_field('number', core_schema.int_schema())]\n            ),\n        ),\n        ['number'],\n    )\n    v = SchemaValidator(cs)\n    instance: Model = v.validate_python({'number': 1})\n    assert instance.number == 1\n    assert calls == [(({'number': 1}, None),)]\n    v.validate_assignment(instance, 'number', 2)\n    assert instance.number == 2\n    assert calls == [(({'number': 1}, None),), (({'number': 2}, None),)]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004699", "source": "def is_builtin_dataclass(_cls: Type[Any]) -> bool:\n    return (\n        dataclasses.is_dataclass(_cls)\n        and not hasattr(_cls, '__pydantic_model__')\n        and set(_cls.__dataclass_fields__).issuperset(set(getattr(_cls, '__annotations__', {})))\n    )", "target": "def test_frozen_field():\n    @dataclasses.dataclass\n    class MyModel:\n        f: str\n    v = SchemaValidator(\n        core_schema.dataclass_schema(\n            MyModel,\n            core_schema.dataclass_args_schema(\n                'MyModel', [core_schema.dataclass_field('f', core_schema.str_schema(), frozen=True)]\n            ),\n            ['f'],\n        )\n    )\n    m = v.validate_python({'f': 'x'})\n    assert m.f == 'x'\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_assignment(m, 'f', 'y')\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'frozen_field', 'loc': ('f',), 'msg': 'Field is frozen', 'input': 'y'}\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004700", "source": "def benchmark_using_throughput_benchmark(config, module):\n    print(\"Benchmarking via ThroughputBenchmark\")\n    bench = ThroughputBenchmark(module.module)\n    bench.add_input(*module.tensor_inputs)\n    stats = bench.benchmark(1, config.num_warmup_iters, config.num_iters)\n    return stats.latency_avg_ms / NUM_LOOP_ITERS", "target": "def test_gh_get_labels_raises_with_no_pages(\n        self,\n        mock_request_for_labels: Any,\n        get_last_page_num_from_header: Any,\n    ) -> None:\n        with self.assertRaises(AssertionError) as err:\n            gh_get_labels(\"foo\", \"bar\")\n        self.assertIn(\"number of pages of labels\", str(err.exception))"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004701", "source": "def _flatten(\n    key_prefix: Label, sub_schema: Definition, result: FlatIntermediateDefinition\n) -> None:\n    for k, value in sub_schema.items():\n        if isinstance(k, tuple):\n            assert all(isinstance(ki, str) for ki in k)\n            key_suffix: Label = k\n        elif k is None:\n            key_suffix = ()\n        else:\n            assert isinstance(k, str)\n            key_suffix = (k,)\n        key: Label = key_prefix + key_suffix\n        if isinstance(value, (TimerArgs, GroupedBenchmark)):\n            assert key not in result, f\"duplicate key: {key}\"\n            result[key] = value\n        else:\n            assert isinstance(value, dict)\n            _flatten(key_prefix=key, sub_schema=value, result=result)", "target": "def test_restores_even_on_exception(self):\n        var = \"TEST_TMP_ENV_EXCEPTION\"\n        self.assertNotIn(var, os.environ)\n        with self.assertRaises(RuntimeError):\n            with temp_environ({var: \"x\"}):\n                self.assertEqual(os.environ[var], \"x\")\n                raise RuntimeError(\"boom\")\n        self.assertNotIn(var, os.environ)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004702", "source": "def dataclass(\n        _cls: Type[_T],\n        *,\n        init: bool = True,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool = False,\n        config: Union[ConfigDict, Type[object], None] = None,\n        validate_on_init: Optional[bool] = None,\n        use_proxy: Optional[bool] = None,\n    ) -> 'DataclassClassOrWrapper':\n        ...", "target": "def test_dataclass_self_init():\n    @dataclasses.dataclass(init=False)\n    class Foo:\n        a: str\n        b: bool\n        def __init__(self, *args, **kwargs):\n            v.validate_python(ArgsKwargs(args, kwargs), self_instance=self)\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), kw_only=False),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), kw_only=False),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    foo = Foo(b'hello', 'True')\n    assert dataclasses.is_dataclass(foo)\n    assert dataclasses.asdict(foo) == {'a': 'hello', 'b': True}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004703", "source": "def dataclass(\n        *,\n        init: bool = True,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool = False,\n        config: Union[ConfigDict, Type[object], None] = None,\n        validate_on_init: Optional[bool] = None,\n        use_proxy: Optional[bool] = None,\n    ) -> Callable[[Type[_T]], 'DataclassClassOrWrapper']:\n        ...", "target": "def test_dataclass_post_init_args_multiple():\n    dc_args = None\n    @dataclasses.dataclass\n    class Foo:\n        a: str\n        b: dataclasses.InitVar[bool]\n        c: dataclasses.InitVar[int]\n        def __post_init__(self, *args):\n            nonlocal dc_args\n            dc_args = args\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), init_only=True),\n                core_schema.dataclass_field(name='c', schema=core_schema.int_schema(), init_only=True),\n            ],\n            collect_init_only=True,\n        ),\n        ['a', 'b'],\n        post_init=True,\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': b'hello', 'b': 'true', 'c': '42'})\n    assert dataclasses.asdict(foo) == {'a': 'hello'}\n    assert dc_args == (True, 42)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004704", "source": "def timedelta_isoformat(td: datetime.timedelta) -> str:\n    minutes, seconds = divmod(td.seconds, 60)\n    hours, minutes = divmod(minutes, 60)\n    return f'{\"-\" if td.days < 0 else \"\"}P{abs(td.days)}DT{hours:d}H{minutes:d}M{seconds:d}.{td.microseconds:06d}S'", "target": "def test_json_or_python():\n    class Foo(str):\n        def __eq__(self, o: object) -> bool:\n            if isinstance(o, Foo) and super().__eq__(o):\n                return True\n            return False\n    s = cs.json_or_python_schema(\n        json_schema=cs.no_info_after_validator_function(Foo, cs.str_schema()), python_schema=cs.is_instance_schema(Foo)\n    )\n    v = SchemaValidator(s)\n    assert v.validate_python(Foo('abc')) == Foo('abc')\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python('abc')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'is_instance_of',\n            'loc': (),\n            'msg': 'Input should be an instance of test_json_or_python.<locals>.Foo',\n            'input': 'abc',\n            'ctx': {'class': 'test_json_or_python.<locals>.Foo'},\n        }\n    ]\n    assert v.validate_json('\"abc\"') == Foo('abc')"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004705", "source": "def cherry_pick(self, ref: str) -> None:\n        self._run_git(\"cherry-pick\", \"-x\", ref)", "target": "def test_ghstack_branches_in_sync(self) -> None:\n        head_ref = \"gh/SS-JIA/206/head\"\n        self._skip_if_ref_does_not_exist(head_ref)\n        self.assertTrue(are_ghstack_branches_in_sync(self.repo, head_ref))"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004706", "source": "def _get_validators(cls: 'DataclassClassOrWrapper') -> 'CallableGenerator':\n    yield cls.__validate__", "target": "def test_dataclass_post_init():\n    @dataclasses.dataclass\n    class Foo:\n        a: str\n        b: bool\n        def __post_init__(self):\n            self.a = self.a.upper()\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n            ],\n        ),\n        ['a', 'b'],\n        post_init=True,\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 'hello', 'b': True})\n    assert foo.a == 'HELLO'\n    assert foo.b is True"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004707", "source": "def dataclass(\n    _cls: type[_T] | None = None,\n    *,\n    init: Literal[False] = False,\n    repr: bool = True,\n    eq: bool = True,\n    order: bool = False,\n    unsafe_hash: bool = False,\n    frozen: bool | None = None,\n    config: ConfigDict | type[object] | None = None,\n    validate_on_init: bool | None = None,\n    kw_only: bool = False,\n    slots: bool = False,\n) -> Callable[[type[_T]], type[PydanticDataclass]] | type[PydanticDataclass]:\n    assert init is False, 'pydantic.dataclasses.dataclass only supports init=False'\n    assert validate_on_init is not False, 'validate_on_init=False is no longer supported'\n    if sys.version_info >= (3, 10):\n        kwargs = {'kw_only': kw_only, 'slots': slots}\n    else:\n        kwargs = {}\n    def create_dataclass(cls: type[Any]) -> type[PydanticDataclass]:\n        from ._internal._utils import is_model_class\n        if is_model_class(cls):\n            raise PydanticUserError(\n                f'Cannot create a Pydantic dataclass from {cls.__name__} as it is already a Pydantic model',\n                code='dataclass-on-model',\n            )\n        original_cls = cls\n        has_dataclass_base = any(dataclasses.is_dataclass(base) for base in cls.__bases__)\n        if not has_dataclass_base and config is not None and hasattr(cls, '__pydantic_config__'):\n            warn(\n                f'`config` is set via both the `dataclass` decorator and `__pydantic_config__` for dataclass {cls.__name__}. '\n                f'The `config` specification from `dataclass` decorator will take priority.',\n                category=UserWarning,\n                stacklevel=2,\n            )\n        config_dict = config if config is not None else getattr(cls, '__pydantic_config__', None)\n        config_wrapper = _config.ConfigWrapper(config_dict)\n        decorators = _decorators.DecoratorInfos.build(cls, replace_wrapped_methods=True)\n        decorators.update_from_config(config_wrapper)\n        original_doc = cls.__doc__\n        if _pydantic_dataclasses.is_stdlib_dataclass(cls):\n            original_doc = None\n            bases = (cls,)\n            if issubclass(cls, Generic):\n                generic_base = Generic[cls.__parameters__]\n                bases = bases + (generic_base,)\n            cls = types.new_class(cls.__name__, bases)\n        if frozen is not None:\n            frozen_ = frozen\n            if config_wrapper.frozen:\n                warn(\n                    f'`frozen` is set via both the `dataclass` decorator and `config` for dataclass {cls.__name__!r}.'\n                    'This is not recommended. The `frozen` specification on `dataclass` will take priority.',\n                    category=UserWarning,\n                    stacklevel=2,\n                )\n        else:\n            frozen_ = config_wrapper.frozen or False\n        cls_anns = _typing_extra.safe_get_annotations(cls)\n        for field_name in cls_anns:\n            field_value = getattr(cls, field_name, None)\n            if isinstance(field_value, FieldInfo):\n                setattr(cls, field_name, _pydantic_dataclasses.as_dataclass_field(field_value))\n        with _pydantic_dataclasses.patch_base_fields(cls):\n            cls = dataclasses.dataclass(\n                cls,\n                init=True,\n                repr=repr,\n                eq=eq,\n                order=order,\n                unsafe_hash=unsafe_hash,\n                frozen=frozen_,\n                **kwargs,\n            )\n        if config_wrapper.validate_assignment:\n            original_setattr = cls.__setattr__\n            @functools.wraps(cls.__setattr__)\n            def validated_setattr(instance: PydanticDataclass, name: str, value: Any, /) -> None:\n                if frozen_:\n                    return original_setattr(instance, name, value)\n                inst_cls = type(instance)\n                attr = getattr(inst_cls, name, None)\n                if isinstance(attr, property):\n                    attr.__set__(instance, value)\n                elif isinstance(attr, functools.cached_property):\n                    instance.__dict__.__setitem__(name, value)\n                else:\n                    inst_cls.__pydantic_validator__.validate_assignment(instance, name, value)\n            cls.__setattr__ = validated_setattr.__get__(None, cls)\n            if slots and not hasattr(cls, '__setstate__'):\n                def _dataclass_getstate(self: Any) -> list[Any]:\n                    return [getattr(self, f.name) for f in dataclasses.fields(self)]\n                def _dataclass_setstate(self: Any, state: list[Any]) -> None:\n                    for field, value in zip(dataclasses.fields(self), state):\n                        object.__setattr__(self, field.name, value)\n                cls.__getstate__ = _dataclass_getstate\n                cls.__setstate__ = _dataclass_setstate\n        cls.__is_pydantic_dataclass__ = True\n        cls.__pydantic_decorators__ = decorators\n        cls.__doc__ = original_doc\n        firstlineno = getattr(original_cls, '__firstlineno__', None)\n        cls.__module__ = original_cls.__module__\n        if sys.version_info >= (3, 13) and firstlineno is not None:\n            original_cls.__firstlineno__ = firstlineno\n            cls.__firstlineno__ = firstlineno\n        cls.__qualname__ = original_cls.__qualname__\n        cls.__pydantic_fields_complete__ = classmethod(_pydantic_fields_complete)\n        cls.__pydantic_complete__ = False\n        _pydantic_dataclasses.complete_dataclass(cls, config_wrapper, raise_errors=False)\n        return cls\n    return create_dataclass if _cls is None else create_dataclass(_cls)", "target": "def test_aliases(py_and_json: PyAndJson):\n    schema = core_schema.dataclass_args_schema(\n        'MyDataclass',\n        [\n            core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), validation_alias='Apple'),\n            core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), validation_alias=['Banana', 1]),\n            core_schema.dataclass_field(\n                name='c', schema=core_schema.int_schema(), validation_alias=['Carrot', 'v'], init_only=True\n            ),\n        ],\n        collect_init_only=True,\n    )\n    v = py_and_json(schema)\n    assert v.validate_test({'Apple': 'a', 'Banana': ['x', 'false'], 'Carrot': {'v': '42'}}) == (\n        {'a': 'a', 'b': False},\n        (42,),\n    )"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004708", "source": "def rebuild_model_fields(\n    cls: type[BaseModel],\n    *,\n    config_wrapper: ConfigWrapper,\n    ns_resolver: NsResolver,\n    typevars_map: Mapping[TypeVar, Any],\n) -> dict[str, FieldInfo]:\n    rebuilt_fields: dict[str, FieldInfo] = {}\n    with ns_resolver.push(cls):\n        for f_name, field_info in cls.__pydantic_fields__.items():\n            if field_info._complete:\n                rebuilt_fields[f_name] = field_info\n            else:\n                new_field = _recreate_field_info(\n                    field_info, ns_resolver=ns_resolver, typevars_map=typevars_map, lenient=False\n                )\n                update_field_from_config(config_wrapper, f_name, new_field)\n                rebuilt_fields[f_name] = new_field\n    return rebuilt_fields", "target": "def test_validate_assignment_with_strict():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'x': core_schema.model_field(schema=core_schema.str_schema()),\n                'y': core_schema.model_field(schema=core_schema.int_schema()),\n            }\n        )\n    )\n    r, model_extra, fields_set = v.validate_python({'x': 'a', 'y': '123'})\n    assert r == {'x': 'a', 'y': 123}\n    assert model_extra is None\n    assert fields_set == {'x', 'y'}\n    v.validate_assignment(r, 'y', '124')\n    assert r == {'x': 'a', 'y': 124}\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_assignment(r, 'y', '124', strict=True)\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'int_type', 'loc': ('y',), 'msg': 'Input should be a valid integer', 'input': '124'}\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004709", "source": "def get_temp_dir() -> str:\n    global _TEMPDIR\n    if _TEMPDIR is None:\n        _TEMPDIR = _make_temp_dir(\n            prefix=\"instruction_count_microbenchmarks\", gc_dev_shm=True\n        )\n        atexit.register(shutil.rmtree, path=_TEMPDIR)\n    return _TEMPDIR", "target": "def test_iterator(self, input_: str = \"abcdef\") -> None:\n        iter_ = PeekableIterator(input_)\n        for idx, c in enumerate(iter_):\n            self.assertEqual(c, input_[idx])"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004710", "source": "def _dataclass_validate_assignment_setattr(self: 'Dataclass', name: str, value: Any) -> None:\n    if self.__pydantic_initialised__:\n        d = dict(self.__dict__)\n        d.pop(name, None)\n        known_field = self.__pydantic_model__.__fields__.get(name, None)\n        if known_field:\n            value, error_ = known_field.validate(value, d, loc=name, cls=self.__class__)\n            if error_:\n                raise ValidationError([error_], self.__class__)\n    object.__setattr__(self, name, value)", "target": "def test_dataclass_self_init_alias():\n    @dataclasses.dataclass(init=False)\n    class Foo:\n        a: str\n        b: bool\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), validation_alias='aAlias'),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), validation_alias=['bAlias', 0]),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    def __init__(self, *args, **kwargs):\n        v.validate_python(ArgsKwargs(args, kwargs), self_instance=self)\n    Foo.__init__ = __init__\n    foo = Foo(aAlias=b'hello', bAlias=['True'])\n    assert dataclasses.is_dataclass(foo)\n    assert dataclasses.asdict(foo) == {'a': 'hello', 'b': True}\n    with pytest.raises(ValidationError) as exc_info:\n        Foo(aAlias=b'hello', bAlias=['wrong'])\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'bool_parsing',\n            'loc': ('bAlias', 0),\n            'msg': 'Input should be a valid boolean, unable to interpret input',\n            'input': 'wrong',\n        }\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004711", "source": "def collect_model_fields(\n    cls: type[BaseModel],\n    config_wrapper: ConfigWrapper,\n    ns_resolver: NsResolver,\n    *,\n    typevars_map: Mapping[TypeVar, Any] | None = None,\n) -> tuple[dict[str, FieldInfo], set[str]]:\n    FieldInfo_ = import_cached_field_info()\n    BaseModel_ = import_cached_base_model()\n    bases = cls.__bases__\n    parent_fields_lookup: dict[str, FieldInfo] = {}\n    for base in reversed(bases):\n        if model_fields := getattr(base, '__pydantic_fields__', None):\n            parent_fields_lookup.update(model_fields)\n    type_hints = _typing_extra.get_model_type_hints(cls, ns_resolver=ns_resolver)\n    annotations = _typing_extra.safe_get_annotations(cls)\n    fields: dict[str, FieldInfo] = {}\n    class_vars: set[str] = set()\n    for ann_name, (ann_type, evaluated) in type_hints.items():\n        if ann_name == 'model_config':\n            continue\n        _check_protected_namespaces(\n            protected_namespaces=config_wrapper.protected_namespaces,\n            ann_name=ann_name,\n            bases=bases,\n            cls_name=cls.__name__,\n        )\n        if _typing_extra.is_classvar_annotation(ann_type):\n            class_vars.add(ann_name)\n            continue\n        assigned_value = getattr(cls, ann_name, PydanticUndefined)\n        if assigned_value is not PydanticUndefined and (\n            any(getattr(BaseModel_, depr_name, None) is assigned_value for depr_name in _deprecated_method_names)\n            or (\n                hasattr(assigned_value, '__func__')\n                and any(\n                    getattr(getattr(BaseModel_, depr_name, None), '__func__', None) is assigned_value.__func__\n                    for depr_name in _deprecated_classmethod_names\n                )\n            )\n        ):\n            assigned_value = PydanticUndefined\n        if not is_valid_field_name(ann_name):\n            continue\n        if cls.__pydantic_root_model__ and ann_name != 'root':\n            raise NameError(\n                f\"Unexpected field with name {ann_name!r}; only 'root' is allowed as a field of a `RootModel`\"\n            )\n        generic_origin = getattr(cls, '__pydantic_generic_metadata__', {}).get('origin')\n        for base in bases:\n            dataclass_fields = {\n                field.name for field in (dataclasses.fields(base) if dataclasses.is_dataclass(base) else ())\n            }\n            if hasattr(base, ann_name):\n                if base is generic_origin:\n                    continue\n                if ann_name in dataclass_fields:\n                    continue\n                if ann_name not in annotations:\n                    continue\n                warnings.warn(\n                    f'Field name \"{ann_name}\" in \"{cls.__qualname__}\" shadows an attribute in parent '\n                    f'\"{base.__qualname__}\"',\n                    UserWarning,\n                    stacklevel=4,\n                )\n        if assigned_value is PydanticUndefined:\n            if ann_name in annotations or ann_name not in parent_fields_lookup:\n                field_info = FieldInfo_.from_annotation(ann_type, _source=AnnotationSource.CLASS)\n                field_info._original_annotation = ann_type\n                if not evaluated:\n                    field_info._complete = False\n            else:\n                parent_field_info = parent_fields_lookup[ann_name]._copy()\n                if typevars_map:\n                    field_info = _recreate_field_info(\n                        parent_field_info, ns_resolver=ns_resolver, typevars_map=typevars_map, lenient=True\n                    )\n                else:\n                    field_info = parent_field_info\n        else:\n            if isinstance(assigned_value, FieldInfo_) and ismethoddescriptor(assigned_value.default):\n                default = assigned_value.default.__get__(None, cls)\n                assigned_value.default = default\n                assigned_value._attributes_set['default'] = default\n            field_info = FieldInfo_.from_annotated_attribute(ann_type, assigned_value, _source=AnnotationSource.CLASS)\n            field_info._original_assignment = assigned_value\n            field_info._original_annotation = ann_type\n            if not evaluated:\n                field_info._complete = False\n            elif 'final' in field_info._qualifiers and not field_info.is_required():\n                warnings.warn(\n                    f'Annotation {ann_name!r} is marked as final and has a default value. Pydantic treats {ann_name!r} as a '\n                    'class variable, but it will be considered as a normal field in V3 to be aligned with dataclasses. If you '\n                    f'still want {ann_name!r} to be considered as a class variable, annotate it as: `ClassVar[<type>] = <default>.`',\n                    category=PydanticDeprecatedSince211,\n                    stacklevel=4,\n                )\n                class_vars.add(ann_name)\n                continue\n            try:\n                delattr(cls, ann_name)\n            except AttributeError:\n                pass\n        decorators: DecoratorInfos = cls.__dict__['__pydantic_decorators__']\n        if ann_name in decorators.computed_fields:\n            raise TypeError(\n                f'Field {ann_name!r} of class {cls.__name__!r} overrides symbol of same name in a parent class. '\n                'This override with a computed_field is incompatible.'\n            )\n        fields[ann_name] = field_info\n        if field_info._complete:\n            update_field_from_config(config_wrapper, ann_name, field_info)\n    if config_wrapper.use_attribute_docstrings:\n        _update_fields_from_docstrings(cls, fields)\n    return fields, class_vars", "target": "def test_extra_behavior_allow(\n    config: Union[core_schema.CoreConfig, None],\n    schema_extra_behavior_kw: dict[str, Any],\n    extras_schema_kw: dict[str, Any],\n    expected_extra_value: Any,\n):\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            {'f': core_schema.model_field(core_schema.str_schema())}, **schema_extra_behavior_kw, **extras_schema_kw\n        ),\n        config=config,\n    )\n    m, model_extra, fields_set = v.validate_python({'f': 'x', 'extra_field': '123'})\n    assert m == {'f': 'x'}\n    assert model_extra == {'extra_field': expected_extra_value}\n    assert fields_set == {'f', 'extra_field'}\n    v.validate_assignment(m, 'f', 'y')\n    assert m == {'f': 'y'}\n    new_m, new_model_extra, new_fields_set = v.validate_assignment({**m, **model_extra}, 'not_f', '123')\n    assert new_m == {'f': 'y'}\n    assert new_model_extra == {'extra_field': expected_extra_value, 'not_f': expected_extra_value}\n    assert new_fields_set == {'not_f'}"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004712", "source": "def temp_environ(updates: dict[str, str]):\n    missing = object()\n    old: dict[str, str | object] = {k: os.environ.get(k, missing) for k in updates}\n    try:\n        os.environ.update(updates)\n        yield\n    finally:\n        for k, v in old.items():\n            if v is missing:\n                os.environ.pop(k, None)\n            else:\n                os.environ[k] = v", "target": "def test_pr_with_not_user_facing_label(\n        self, mocked_rn_labels: Any, mocked_gql: Any\n    ) -> None:\n        \"Test PR with 'topic: not user facing' label\"\n        pr = GitHubPR(\"pytorch\", \"pytorch\", 75095)\n        self.assertTrue(has_required_labels(pr))"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004713", "source": "def generate_dataclass_help(cls) -> str:\n    if not is_dataclass(cls):\n        raise TypeError(f\"{cls} is not a dataclass\")\n    def get_value(f):\n        if f.default is not MISSING:\n            return f.default\n        if f.default_factory is not MISSING:\n            try:\n                return f.default_factory()\n            except Exception as e:\n                return f\"<error: {e}>\"\n        return \"<required>\"\n    lines = [f\"{f.name:<22} = {repr(get_value(f))}\" for f in fields(cls)]\n    return indent(\"\\n\".join(lines), \"    \")", "target": "def test_env_bool_uses_default_when_unset(self):\n        with patch.dict(os.environ, {}, clear=True):\n            self.assertTrue(m.env_bool(\"FLAG\", default=True))\n            self.assertFalse(m.env_bool(\"FLAG\", default=False))"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004714", "source": "def generate_dataclass_help(cls) -> str:\n    if not is_dataclass(cls):\n        raise TypeError(f\"{cls} is not a dataclass\")\n    def get_value(f):\n        if f.default is not MISSING:\n            return f.default\n        if f.default_factory is not MISSING:\n            try:\n                return f.default_factory()\n            except Exception as e:\n                return f\"<error: {e}>\"\n        return \"<required>\"\n    lines = [f\"{f.name:<22} = {repr(get_value(f))}\" for f in fields(cls)]\n    return indent(\"\\n\".join(lines), \"    \")", "target": "def test_get_env_empty_returns_default(self):\n        with patch.dict(os.environ, {\"FOO\": \"\"}, clear=True):\n            self.assertEqual(m.get_env(\"FOO\", \"default\"), \"default\")"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004715", "source": "def device_sync(device):\n    if \"cuda\" in device:\n        torch.cuda.synchronize(device)\n    elif \"cpu\" in device:\n        pass\n    else:\n        print(f\"device={device} is not yet supported\")", "target": "def testAliasWithOffset(self) -> list[Tensor]:\n        x = torch.tensor([100, 200])\n        a = [x[0], x[1]]\n        return a"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004716", "source": "def literal_values(type_: Type[Any]) -> Tuple[Any, ...]:\n    return get_args(type_)", "target": "def test_correct_function_signature() -> None:\n    def my_validator(value: Any, info: Any) -> str:\n        return str(value)\n    v = SchemaValidator(core_schema.with_info_plain_validator_function(my_validator))\n    assert v.validate_python(1) == '1'"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004717", "source": "def benchmark_module(config, module, use_throughput_benchmark=False):\n    if use_throughput_benchmark:\n        return benchmark_using_throughput_benchmark(config, module)\n    module.forward(config.num_warmup_iters)\n    print(f\"Running module for {config.num_iters} iterations\")\n    start = time.time()\n    module.forward(config.num_iters)\n    end = time.time()\n    time_elapsed_s = end - start\n    return secs_to_ms(time_elapsed_s) / config.num_iters / NUM_LOOP_ITERS", "target": "def test_get_last_page_num_from_header(self) -> None:\n        for (\n            expected_page_num,\n            mock_header,\n        ) in self.MOCK_HEADER_LINKS_TO_PAGE_NUMS.items():\n            self.assertEqual(\n                get_last_page_num_from_header(mock_header), expected_page_num\n            )"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004718", "source": "def dataclass(\n        _cls: type[_T],\n        *,\n        init: Literal[False] = False,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool | None = None,\n        config: ConfigDict | type[object] | None = None,\n        validate_on_init: bool | None = None,\n    ) -> type[PydanticDataclass]: ...", "target": "def test_dataclass_post_init_args_multiple():\n    dc_args = None\n    @dataclasses.dataclass\n    class Foo:\n        a: str\n        b: dataclasses.InitVar[bool]\n        c: dataclasses.InitVar[int]\n        def __post_init__(self, *args):\n            nonlocal dc_args\n            dc_args = args\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), init_only=True),\n                core_schema.dataclass_field(name='c', schema=core_schema.int_schema(), init_only=True),\n            ],\n            collect_init_only=True,\n        ),\n        ['a', 'b'],\n        post_init=True,\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': b'hello', 'b': 'true', 'c': '42'})\n    assert dataclasses.asdict(foo) == {'a': 'hello'}\n    assert dc_args == (True, 42)"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004719", "source": "def amend_commit_message(self, msg: str) -> None:\n        self._run_git(\"commit\", \"--amend\", \"-m\", msg)", "target": "def test_peek(self, input_: str = \"abcdef\") -> None:\n        iter_ = PeekableIterator(input_)\n        for idx, c in enumerate(iter_):\n            if idx + 1 < len(input_):\n                self.assertEqual(iter_.peek(), input_[idx + 1])\n            else:\n                self.assertTrue(iter_.peek() is None)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004720", "source": "def takes_validated_data_argument(\n    default_factory: Callable[[], Any] | Callable[[dict[str, Any]], Any],\n) -> TypeIs[Callable[[dict[str, Any]], Any]]:\n    try:\n        sig = signature(default_factory)\n    except (ValueError, TypeError):\n        return False\n    parameters = list(sig.parameters.values())\n    return len(parameters) == 1 and can_be_positional(parameters[0]) and parameters[0].default is Parameter.empty", "target": "def test_validate_assignment_allow_extra():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={'field_a': core_schema.model_field(schema=core_schema.str_schema())}, extra_behavior='allow'\n        )\n    )\n    assert v.validate_python({'field_a': 'test'}) == ({'field_a': 'test'}, {}, {'field_a'})\n    assert v.validate_assignment({'field_a': 'test'}, 'other_field', 456) == (\n        {'field_a': 'test'},\n        {'other_field': 456},\n        {'other_field'},\n    )"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004721", "source": "def are_ghstack_branches_in_sync(\n    repo: GitRepo, head_ref: str, base_ref: Optional[str] = None\n) -> bool:\n    orig_ref = re.sub(r\"/head$\", \"/orig\", head_ref)\n    if base_ref is None:\n        base_ref = re.sub(r\"/head$\", \"/base\", head_ref)\n    orig_diff_sha = _shasum(repo.diff(f\"{repo.remote}/{orig_ref}\"))\n    head_diff_sha = _shasum(\n        repo.diff(\n            base_ref if is_commit_hash(base_ref) else f\"{repo.remote}/{base_ref}\",\n            f\"{repo.remote}/{head_ref}\",\n        )\n    )\n    return orig_diff_sha == head_diff_sha", "target": "def test_peek(self, input_: str = \"abcdef\") -> None:\n        iter_ = PeekableIterator(input_)\n        for idx, c in enumerate(iter_):\n            if idx + 1 < len(input_):\n                self.assertEqual(iter_.peek(), input_[idx + 1])\n            else:\n                self.assertTrue(iter_.peek() is None)"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004722", "source": "def load_weights(mod: nn.Module, names: list[str], params: tuple[Tensor, ...]) -> None:\n    for name, p in zip(names, params):\n        _set_nested_attr(mod, name.split(\".\"), p)", "target": "def test_pr_with_not_user_facing_label(\n        self, mocked_rn_labels: Any, mocked_gql: Any\n    ) -> None:\n        \"Test PR with 'topic: not user facing' label\"\n        pr = GitHubPR(\"pytorch\", \"pytorch\", 75095)\n        self.assertTrue(has_required_labels(pr))"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004723", "source": "def pattern_bytes_validator(input_value: Any, /) -> re.Pattern[bytes]:\n    if isinstance(input_value, re.Pattern):\n        if isinstance(input_value.pattern, bytes):\n            return input_value\n        else:\n            raise PydanticCustomError('pattern_bytes_type', 'Input should be a bytes pattern')\n    elif isinstance(input_value, bytes):\n        return compile_pattern(input_value)\n    elif isinstance(input_value, str):\n        raise PydanticCustomError('pattern_bytes_type', 'Input should be a bytes pattern')\n    else:\n        raise PydanticCustomError('pattern_type', 'Input should be a valid pattern')", "target": "def test_validators_build(benchmark) -> None:\n    class Base1(BaseModel):\n        a: int\n        @field_validator('a', mode='after')\n        @classmethod\n        def val_a(cls, value: int) -> int: ...\n        @computed_field\n        def prop(self) -> int: ...\n    class Bare:\n        @computed_field\n        def prop_bare(self) -> int: ...\n    class Sub1(Base1):\n        @computed_field\n        def prop_2(self) -> int: ...\n        @computed_field\n        def prop_3(self) -> int: ...\n        @computed_field\n        def prop_4(self) -> int: ...\n    @benchmark\n    def bench() -> None:\n        class SubS(Sub1, Bare, defer_build=True):\n            @computed_field\n            def prop_5(self) -> int: ...\n            @computed_field\n            def prop_6(self) -> int: ...\n            @computed_field\n            def prop_7(self) -> int: ..."}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004724", "source": "def _recreate_field_info(\n    field_info: FieldInfo,\n    ns_resolver: NsResolver,\n    typevars_map: Mapping[TypeVar, Any],\n    *,\n    lenient: bool,\n) -> FieldInfo:\n    FieldInfo_ = import_cached_field_info()\n    existing_desc = field_info.description\n    if lenient:\n        ann = _generics.replace_types(field_info._original_annotation, typevars_map)\n        ann, evaluated = _typing_extra.try_eval_type(\n            ann,\n            *ns_resolver.types_namespace,\n        )\n    else:\n        ann = _typing_extra.eval_type(\n            field_info._original_annotation,\n            *ns_resolver.types_namespace,\n        )\n        ann = _generics.replace_types(ann, typevars_map)\n        ann = _typing_extra.eval_type(\n            ann,\n            *ns_resolver.types_namespace,\n        )\n        evaluated = True\n    if (assign := field_info._original_assignment) is PydanticUndefined:\n        new_field = FieldInfo_.from_annotation(ann, _source=AnnotationSource.CLASS)\n    else:\n        new_field = FieldInfo_.from_annotated_attribute(ann, assign, _source=AnnotationSource.CLASS)\n        new_field._original_assignment = assign\n    new_field._original_annotation = ann\n    new_field.description = new_field.description if new_field.description is not None else existing_desc\n    if not evaluated:\n        new_field._complete = False\n    return new_field", "target": "def test_simple():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'field_a': core_schema.model_field(schema=core_schema.str_schema()),\n                'field_b': core_schema.model_field(schema=core_schema.int_schema()),\n            }\n        )\n    )\n    assert v.validate_python({'field_a': b'abc', 'field_b': 1}) == (\n        {'field_a': 'abc', 'field_b': 1},\n        None,\n        {'field_a', 'field_b'},\n    )"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004725", "source": "def set_validation(cls: Type['DataclassT'], value: bool) -> Generator[Type['DataclassT'], None, None]:\n    original_run_validation = cls.__pydantic_run_validation__\n    try:\n        cls.__pydantic_run_validation__ = value\n        yield cls\n    finally:\n        cls.__pydantic_run_validation__ = original_run_validation", "target": "def test_extra_behavior_allow(\n    config: Union[core_schema.CoreConfig, None],\n    schema_extra_behavior_kw: dict[str, Any],\n    validate_fn_extra_kw: Union[ExtraBehavior, None],\n):\n    @dataclasses.dataclass\n    class MyModel:\n        f: str\n    v = SchemaValidator(\n        core_schema.dataclass_schema(\n            MyModel,\n            core_schema.dataclass_args_schema(\n                'MyModel', [core_schema.dataclass_field('f', core_schema.str_schema())], **schema_extra_behavior_kw\n            ),\n            ['f'],\n            config=config,\n        )\n    )\n    m: MyModel = v.validate_python({'f': 'x', 'extra_field': '123'}, extra=validate_fn_extra_kw)\n    assert m.f == 'x'\n    assert getattr(m, 'extra_field') == '123'\n    v.validate_assignment(m, 'f', 'y', extra=validate_fn_extra_kw)\n    assert m.f == 'y'\n    v.validate_assignment(m, 'not_f', '123', extra=validate_fn_extra_kw)\n    assert getattr(m, 'not_f') == '123'"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004726", "source": "def _general_metadata_cls() -> type[BaseMetadata]:\n    from annotated_types import BaseMetadata\n    class _PydanticGeneralMetadata(PydanticMetadata, BaseMetadata):\n        def __init__(self, metadata: Any):\n            self.__dict__ = metadata\n    return _PydanticGeneralMetadata", "target": "def test_model_fields_deep():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'field_a': core_schema.model_field(schema=core_schema.str_schema()),\n                'field_b': core_schema.model_field(\n                    schema=core_schema.model_fields_schema(\n                        fields={\n                            'field_c': core_schema.model_field(schema=core_schema.str_schema()),\n                            'field_d': core_schema.model_field(\n                                schema=core_schema.model_fields_schema(\n                                    fields={\n                                        'field_e': core_schema.model_field(schema=core_schema.str_schema()),\n                                        'field_f': core_schema.model_field(schema=core_schema.int_schema()),\n                                    }\n                                )\n                            ),\n                        }\n                    )\n                ),\n            }\n        )\n    )\n    model_dict, model_extra, fields_set = v.validate_python(\n        {'field_a': '1', 'field_b': {'field_c': '2', 'field_d': {'field_e': '4', 'field_f': 4}}}\n    )\n    assert model_dict == {\n        'field_a': '1',\n        'field_b': (\n            {'field_c': '2', 'field_d': ({'field_e': '4', 'field_f': 4}, None, {'field_f', 'field_e'})},\n            None,\n            {'field_d', 'field_c'},\n        ),\n    }\n    assert model_extra is None\n    assert fields_set == {'field_a', 'field_b'}\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python({'field_a': '1', 'field_b': {'field_c': '2', 'field_d': {'field_e': '4', 'field_f': 'xx'}}})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('field_b', 'field_d', 'field_f'),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'xx',\n        }\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004727", "source": "def is_none_type(type_: Any) -> bool:\n        return type_ in NONE_TYPES", "target": "def test_schema_typing() -> None:\n    schema: CoreSchema = {\n        'type': 'union',\n        'choices': [{'type': 'int'}, {'type': 'int', 'ge': 1}, {'type': 'float', 'lt': 1.0}],\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = {\n        'type': 'tagged-union',\n        'discriminator': 'type',\n        'choices': {\n            'apple': {\n                'type': 'typed-dict',\n                'fields': {'pips': {'type': 'typed-dict-field', 'schema': {'type': 'int'}}},\n            },\n            'banana': {\n                'type': 'typed-dict',\n                'fields': {'curvature': {'type': 'typed-dict-field', 'schema': {'type': 'float'}}},\n            },\n        },\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'int', 'ge': 1}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'float', 'lt': 1.0}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'str', 'pattern': r'http://.*'}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'bool', 'strict': False}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'literal', 'expected': [1, '1']}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'any'}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'none'}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'bytes'}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'list', 'items_schema': {'type': 'str'}, 'min_length': 3}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'set', 'items_schema': {'type': 'str'}, 'max_length': 3}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'tuple', 'items_schema': [{'type': 'str'}], 'variadic_item_index': 0, 'max_length': 3}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'tuple', 'items_schema': [{'type': 'str'}, {'type': 'int'}]}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'frozenset', 'items_schema': {'type': 'str'}, 'max_length': 3}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'dict', 'keys_schema': {'type': 'str'}, 'values_schema': {'type': 'any'}}\n    SchemaValidator(schema)\n    schema: CoreSchema = {\n        'type': 'typed-dict',\n        'fields': {'bar': {'type': 'typed-dict-field', 'schema': {'type': 'str'}}},\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = {\n        'type': 'model',\n        'cls': Foo,\n        'schema': {'type': 'model-fields', 'fields': {'bar': {'type': 'model-field', 'schema': {'type': 'str'}}}},\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = {\n        'type': 'typed-dict',\n        'fields': {\n            'a': {'type': 'typed-dict-field', 'schema': {'type': 'str'}},\n            'b': {'type': 'typed-dict-field', 'schema': {'type': 'str'}, 'validation_alias': 'foobar'},\n            'c': {\n                'type': 'typed-dict-field',\n                'schema': {'type': 'str'},\n                'validation_alias': [['foobar', 0, 'bar'], ['foo']],\n            },\n            'd': {\n                'type': 'typed-dict-field',\n                'schema': {'type': 'default', 'schema': {'type': 'str'}, 'default': 'spam'},\n            },\n        },\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = {\n        'type': 'function-wrap',\n        'function': {'type': 'with-info', 'function': wrap_validator, 'field_name': 'foobar'},\n        'schema': {'type': 'str'},\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = core_schema.with_info_plain_validator_function(validator)\n    SchemaValidator(schema)\n    schema: CoreSchema = {\n        'type': 'definitions',\n        'schema': {'type': 'definition-ref', 'schema_ref': 'Branch'},\n        'definitions': [\n            {\n                'type': 'typed-dict',\n                'fields': {\n                    'name': {'type': 'typed-dict-field', 'schema': {'type': 'str'}},\n                    'sub_branch': {\n                        'type': 'typed-dict-field',\n                        'schema': {\n                            'type': 'default',\n                            'schema': {\n                                'type': 'nullable',\n                                'schema': {'type': 'definition-ref', 'schema_ref': 'Branch'},\n                            },\n                            'default': None,\n                        },\n                    },\n                },\n                'ref': 'Branch',\n            }\n        ],\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'date', 'le': date.today()}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'time', 'lt': time(12, 13, 14)}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'datetime', 'ge': datetime.now()}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'is-instance', 'cls': Foo}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'callable'}\n    SchemaValidator(schema)\n    schema: CoreSchema = {\n        'type': 'arguments',\n        'arguments_schema': [\n            {'name': 'a', 'mode': 'positional_only', 'schema': {'type': 'int'}},\n            {'name': 'b', 'schema': {'type': 'str'}},\n            {'name': 'c', 'mode': 'keyword_only', 'schema': {'type': 'bool'}},\n        ],\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'call', 'arguments_schema': {'type': 'any'}, 'function': foo}\n    SchemaValidator(schema)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004728", "source": "def custom_pydantic_encoder(type_encoders: dict[Any, Callable[[type[Any]], Any]], obj: Any) -> Any:\n    warnings.warn(\n        '`custom_pydantic_encoder` is deprecated, use `BaseModel.model_dump` instead.',\n        category=PydanticDeprecatedSince20,\n        stacklevel=2,\n    )\n    for base in obj.__class__.__mro__[:-1]:\n        try:\n            encoder = type_encoders[base]\n        except KeyError:\n            continue\n        return encoder(obj)\n    else:\n        return pydantic_encoder(obj)", "target": "def test_bool(input_value, output_value):\n    v = SchemaValidator(core_schema.bool_schema())\n    assert v.validate_json(input_value) == output_value"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004729", "source": "def custom_pydantic_encoder(type_encoders: Dict[Any, Callable[[Type[Any]], Any]], obj: Any) -> Any:\n    for base in obj.__class__.__mro__[:-1]:\n        try:\n            encoder = type_encoders[base]\n        except KeyError:\n            continue\n        return encoder(obj)\n    else:\n        return pydantic_encoder(obj)", "target": "def test_dict_key_json():\n    s = SchemaSerializer(core_schema.dict_schema(core_schema.json_schema(), core_schema.any_schema()))\n    v = {(1, 2): 3, (4, 5): 9}\n    assert s.to_python(v) == v\n    assert s.to_python(v, round_trip=True) == {'[1,2]': 3, '[4,5]': 9}\n    assert s.to_python(v, mode='json') == {'1,2': 3, '4,5': 9}\n    assert s.to_python(v, mode='json', round_trip=True) == {'[1,2]': 3, '[4,5]': 9}\n    assert s.to_json(v) == b'{\"1,2\":3,\"4,5\":9}'\n    assert s.to_json(v, round_trip=True) == b'{\"[1,2]\":3,\"[4,5]\":9}'"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004730", "source": "def to_markdown_table(\n    res: TimingResultType, header: Optional[tuple[str, ...]] = None\n) -> str:\n    if header is None:\n        header = (\"model\", \"task\", \"mean\", \"var\")\n    out = \"\"\n    def write_line(*args):\n        nonlocal out\n        out += f\"| {' | '.join(str(a) for a in args)} |\\n\"\n    write_line(*header)\n    write_line(*[\"--\"] * len(header))\n    for model, tasks in res.items():\n        for task, line in tasks.items():\n            write_line(*(model, task) + line)\n    return out", "target": "def test_restores_on_exception(self):\n        start = Path.cwd()\n        with tempfile.TemporaryDirectory() as td:\n            target = Path(td) / \"wd_exc\"\n            target.mkdir()\n            with self.assertRaises(ValueError):\n                with working_directory(str(target)):\n                    self.assertEqual(Path.cwd().resolve(), target.resolve())\n                    raise ValueError(\"boom\")\n        self.assertEqual(Path.cwd().resolve(), start.resolve())"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004731", "source": "def env_bool(\n    name: str,\n    default: bool = False,\n) -> bool:\n    val = get_env(name)\n    if not val:\n        return default\n    return str2bool(val)", "target": "def test_get_env_not_exist_without_default(self):\n        with patch.dict(os.environ, {\"FOO\": \"bar\"}, clear=True):\n            self.assertEqual(m.get_env(\"TEST_NOT_EXIST\"), \"\")"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004732", "source": "def dataclass(\n        _cls: Type[_T],\n        *,\n        init: bool = True,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool = False,\n        config: Union[ConfigDict, Type[object], None] = None,\n        validate_on_init: Optional[bool] = None,\n        use_proxy: Optional[bool] = None,\n        kw_only: bool = ...,\n    ) -> 'DataclassClassOrWrapper':\n        ...", "target": "def test_dataclass_subclass_strict_never_revalidate():\n    v = SchemaValidator(\n        core_schema.dataclass_schema(\n            FooDataclass,\n            core_schema.dataclass_args_schema(\n                'FooDataclass',\n                [\n                    core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                    core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n                ],\n            ),\n            ['a', 'b'],\n            revalidate_instances='never',\n            strict=True,\n        )\n    )\n    foo = FooDataclass(a='hello', b=True)\n    assert v.validate_python(foo) is foo\n    sub_foo = FooDataclassSame(a='hello', b=True)\n    assert v.validate_python(sub_foo) is sub_foo\n    with pytest.raises(ValidationError, match='Input should be an instance of FooDataclass'):\n        v.validate_python(ArgsKwargs((), {'a': 'hello', 'b': True}))"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004733", "source": "def collect_model_fields(\n    cls: type[BaseModel],\n    config_wrapper: ConfigWrapper,\n    ns_resolver: NsResolver,\n    *,\n    typevars_map: Mapping[TypeVar, Any] | None = None,\n) -> tuple[dict[str, FieldInfo], set[str]]:\n    FieldInfo_ = import_cached_field_info()\n    BaseModel_ = import_cached_base_model()\n    bases = cls.__bases__\n    parent_fields_lookup: dict[str, FieldInfo] = {}\n    for base in reversed(bases):\n        if model_fields := getattr(base, '__pydantic_fields__', None):\n            parent_fields_lookup.update(model_fields)\n    type_hints = _typing_extra.get_model_type_hints(cls, ns_resolver=ns_resolver)\n    annotations = _typing_extra.safe_get_annotations(cls)\n    fields: dict[str, FieldInfo] = {}\n    class_vars: set[str] = set()\n    for ann_name, (ann_type, evaluated) in type_hints.items():\n        if ann_name == 'model_config':\n            continue\n        _check_protected_namespaces(\n            protected_namespaces=config_wrapper.protected_namespaces,\n            ann_name=ann_name,\n            bases=bases,\n            cls_name=cls.__name__,\n        )\n        if _typing_extra.is_classvar_annotation(ann_type):\n            class_vars.add(ann_name)\n            continue\n        assigned_value = getattr(cls, ann_name, PydanticUndefined)\n        if assigned_value is not PydanticUndefined and (\n            any(getattr(BaseModel_, depr_name, None) is assigned_value for depr_name in _deprecated_method_names)\n            or (\n                hasattr(assigned_value, '__func__')\n                and any(\n                    getattr(getattr(BaseModel_, depr_name, None), '__func__', None) is assigned_value.__func__\n                    for depr_name in _deprecated_classmethod_names\n                )\n            )\n        ):\n            assigned_value = PydanticUndefined\n        if not is_valid_field_name(ann_name):\n            continue\n        if cls.__pydantic_root_model__ and ann_name != 'root':\n            raise NameError(\n                f\"Unexpected field with name {ann_name!r}; only 'root' is allowed as a field of a `RootModel`\"\n            )\n        generic_origin = getattr(cls, '__pydantic_generic_metadata__', {}).get('origin')\n        for base in bases:\n            dataclass_fields = {\n                field.name for field in (dataclasses.fields(base) if dataclasses.is_dataclass(base) else ())\n            }\n            if hasattr(base, ann_name):\n                if base is generic_origin:\n                    continue\n                if ann_name in dataclass_fields:\n                    continue\n                if ann_name not in annotations:\n                    continue\n                warnings.warn(\n                    f'Field name \"{ann_name}\" in \"{cls.__qualname__}\" shadows an attribute in parent '\n                    f'\"{base.__qualname__}\"',\n                    UserWarning,\n                    stacklevel=4,\n                )\n        if assigned_value is PydanticUndefined:\n            if ann_name in annotations or ann_name not in parent_fields_lookup:\n                field_info = FieldInfo_.from_annotation(ann_type, _source=AnnotationSource.CLASS)\n                field_info._original_annotation = ann_type\n                if not evaluated:\n                    field_info._complete = False\n            else:\n                parent_field_info = parent_fields_lookup[ann_name]._copy()\n                if typevars_map:\n                    field_info = _recreate_field_info(\n                        parent_field_info, ns_resolver=ns_resolver, typevars_map=typevars_map, lenient=True\n                    )\n                else:\n                    field_info = parent_field_info\n        else:\n            if isinstance(assigned_value, FieldInfo_) and ismethoddescriptor(assigned_value.default):\n                default = assigned_value.default.__get__(None, cls)\n                assigned_value.default = default\n                assigned_value._attributes_set['default'] = default\n            field_info = FieldInfo_.from_annotated_attribute(ann_type, assigned_value, _source=AnnotationSource.CLASS)\n            field_info._original_assignment = assigned_value\n            field_info._original_annotation = ann_type\n            if not evaluated:\n                field_info._complete = False\n            elif 'final' in field_info._qualifiers and not field_info.is_required():\n                warnings.warn(\n                    f'Annotation {ann_name!r} is marked as final and has a default value. Pydantic treats {ann_name!r} as a '\n                    'class variable, but it will be considered as a normal field in V3 to be aligned with dataclasses. If you '\n                    f'still want {ann_name!r} to be considered as a class variable, annotate it as: `ClassVar[<type>] = <default>.`',\n                    category=PydanticDeprecatedSince211,\n                    stacklevel=4,\n                )\n                class_vars.add(ann_name)\n                continue\n            try:\n                delattr(cls, ann_name)\n            except AttributeError:\n                pass\n        decorators: DecoratorInfos = cls.__dict__['__pydantic_decorators__']\n        if ann_name in decorators.computed_fields:\n            raise TypeError(\n                f'Field {ann_name!r} of class {cls.__name__!r} overrides symbol of same name in a parent class. '\n                'This override with a computed_field is incompatible.'\n            )\n        fields[ann_name] = field_info\n        if field_info._complete:\n            update_field_from_config(config_wrapper, ann_name, field_info)\n    if config_wrapper.use_attribute_docstrings:\n        _update_fields_from_docstrings(cls, fields)\n    return fields, class_vars", "target": "def test_from_attributes_override_true():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={'a': core_schema.model_field(schema=core_schema.int_schema())}, from_attributes=False\n        )\n    )\n    with pytest.raises(ValidationError, match='Input should be a valid dictionary'):\n        v.validate_python(Cls(a=1))\n    assert v.validate_python(Cls(a=1), from_attributes=True) == ({'a': 1}, None, {'a'})\n    assert v.isinstance_python(Cls(a=1), from_attributes=True) is True\n    assert v.isinstance_python(Cls(a=1)) is False"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004734", "source": "def get_value(f):\n        if f.default is not MISSING:\n            return f.default\n        if f.default_factory is not MISSING:\n            try:\n                return f.default_factory()\n            except Exception as e:\n                return f\"<error: {e}>\"\n        return \"<required>\"", "target": "def test_env_bool_uses_str2bool_when_set(self):\n        def fake_str2bool(s: str) -> bool:\n            return s.lower() in {\"1\", \"true\", \"yes\", \"on\", \"y\"}\n        with (\n            patch.dict(os.environ, {\"FLAG\": \"yEs\"}, clear=True),\n            patch.object(m, \"str2bool\", fake_str2bool),\n        ):\n            self.assertTrue(m.env_bool(\"FLAG\", default=False))"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004735", "source": "def get_sub_types(tp: Any) -> List[Any]:\n    origin = get_origin(tp)\n    if origin is Annotated:\n        return get_sub_types(get_args(tp)[0])\n    elif is_union(origin):\n        return [x for t in get_args(tp) for x in get_sub_types(t)]\n    else:\n        return [tp]", "target": "def test_correct_function_signature() -> None:\n    def my_validator(value: Any, info: Any) -> str:\n        return str(value)\n    v = SchemaValidator(core_schema.with_info_plain_validator_function(my_validator))\n    assert v.validate_python(1) == '1'"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004736", "source": "def pydantic_encoder(obj: Any) -> Any:\n    from dataclasses import asdict, is_dataclass\n    from pydantic.v1.main import BaseModel\n    if isinstance(obj, BaseModel):\n        return obj.dict()\n    elif is_dataclass(obj):\n        return asdict(obj)\n    for base in obj.__class__.__mro__[:-1]:\n        try:\n            encoder = ENCODERS_BY_TYPE[base]\n        except KeyError:\n            continue\n        return encoder(obj)\n    else:\n        raise TypeError(f\"Object of type '{obj.__class__.__name__}' is not JSON serializable\")", "target": "def test_list_json():\n    s = SchemaSerializer(core_schema.list_schema(core_schema.json_schema()))\n    v = ['a', [1, 2], None]\n    assert s.to_python(v) == v\n    assert s.to_python(v, round_trip=True) == ['\"a\"', '[1,2]', 'null']\n    assert s.to_python(v, mode='json') == v\n    assert s.to_python(v, mode='json', round_trip=True) == ['\"a\"', '[1,2]', 'null']\n    assert s.to_json(v) == b'[\"a\",[1,2],null]'\n    assert s.to_json(v, round_trip=True) == b'[\"\\\\\"a\\\\\"\",\"[1,2]\",\"null\"]'"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004737", "source": "def _apply_field_title_generator_to_field_info(\n    title_generator: Callable[[str, FieldInfo], str],\n    field_name: str,\n    field_info: FieldInfo,\n):\n    if field_info.title is None:\n        title = title_generator(field_name, field_info)\n        if not isinstance(title, str):\n            raise TypeError(f'field_title_generator {title_generator} must return str, not {title.__class__}')\n        field_info.title = title", "target": "def test_model_fields_deep():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'field_a': core_schema.model_field(schema=core_schema.str_schema()),\n                'field_b': core_schema.model_field(\n                    schema=core_schema.model_fields_schema(\n                        fields={\n                            'field_c': core_schema.model_field(schema=core_schema.str_schema()),\n                            'field_d': core_schema.model_field(\n                                schema=core_schema.model_fields_schema(\n                                    fields={\n                                        'field_e': core_schema.model_field(schema=core_schema.str_schema()),\n                                        'field_f': core_schema.model_field(schema=core_schema.int_schema()),\n                                    }\n                                )\n                            ),\n                        }\n                    )\n                ),\n            }\n        )\n    )\n    model_dict, model_extra, fields_set = v.validate_python(\n        {'field_a': '1', 'field_b': {'field_c': '2', 'field_d': {'field_e': '4', 'field_f': 4}}}\n    )\n    assert model_dict == {\n        'field_a': '1',\n        'field_b': (\n            {'field_c': '2', 'field_d': ({'field_e': '4', 'field_f': 4}, None, {'field_f', 'field_e'})},\n            None,\n            {'field_d', 'field_c'},\n        ),\n    }\n    assert model_extra is None\n    assert fields_set == {'field_a', 'field_b'}\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python({'field_a': '1', 'field_b': {'field_c': '2', 'field_d': {'field_e': '4', 'field_f': 'xx'}}})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('field_b', 'field_d', 'field_f'),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'xx',\n        }\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004738", "source": "def create_pydantic_model_from_dataclass(\n    dc_cls: Type['Dataclass'],\n    config: Type[Any] = BaseConfig,\n    dc_cls_doc: Optional[str] = None,\n) -> Type['BaseModel']:\n    field_definitions: Dict[str, Any] = {}\n    for field in dataclasses.fields(dc_cls):\n        default: Any = Undefined\n        default_factory: Optional['NoArgAnyCallable'] = None\n        field_info: FieldInfo\n        if field.default is not dataclasses.MISSING:\n            default = field.default\n        elif field.default_factory is not dataclasses.MISSING:\n            default_factory = field.default_factory\n        else:\n            default = Required\n        if isinstance(default, FieldInfo):\n            field_info = default\n            dc_cls.__pydantic_has_field_info_default__ = True\n        else:\n            field_info = Field(default=default, default_factory=default_factory, **field.metadata)\n        field_definitions[field.name] = (field.type, field_info)\n    validators = gather_all_validators(dc_cls)\n    model: Type['BaseModel'] = create_model(\n        dc_cls.__name__,\n        __config__=config,\n        __module__=dc_cls.__module__,\n        __validators__=validators,\n        __cls_kwargs__={'__resolve_forward_refs__': False},\n        **field_definitions,\n    )\n    model.__doc__ = dc_cls_doc if dc_cls_doc is not None else dc_cls.__doc__ or ''\n    return model", "target": "def test_dataclass_field_wrap_validator2():\n    @dataclasses.dataclass\n    class Foo:\n        a: int\n        b: str\n        @classmethod\n        def validate_b(\n            cls, v: bytes, nxt: core_schema.ValidatorFunctionWrapHandler, info: core_schema.ValidationInfo\n        ) -> bytes:\n            assert v == b'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return nxt(b'hello world!')\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(\n                    name='b',\n                    schema=core_schema.with_info_wrap_validator_function(Foo.validate_b, core_schema.str_schema()),\n                ),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 1, 'b': b'hello'})\n    assert dataclasses.asdict(foo) == {'a': 1, 'b': 'hello world!'}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004739", "source": "def dataclass(\n        _cls: Type[_T],\n        *,\n        init: bool = True,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool = False,\n        config: Union[ConfigDict, Type[object], None] = None,\n        validate_on_init: Optional[bool] = None,\n        use_proxy: Optional[bool] = None,\n        kw_only: bool = ...,\n    ) -> 'DataclassClassOrWrapper':\n        ...", "target": "def test_dataclass_json_duplicate_keys():\n    @dataclasses.dataclass\n    class MyDataclass:\n        name: str\n        age: int\n    schema = core_schema.dataclass_schema(\n        MyDataclass,\n        core_schema.dataclass_args_schema(\n            'MyDataclass',\n            [\n                core_schema.dataclass_field(name='name', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='age', schema=core_schema.int_schema()),\n            ],\n        ),\n        ['name', 'age'],\n    )\n    v = SchemaValidator(schema)\n    json_with_duplicates = '{\"name\": \"Alice\", \"age\": 30, \"name\": \"Bob\", \"age\": 25}'\n    result = v.validate_json(json_with_duplicates)\n    assert result.name == 'Bob', \"Last value for 'name' should win\"\n    assert result.age == 25, \"Last value for 'age' should win\"\n    assert dataclasses.asdict(result) == {'name': 'Bob', 'age': 25}\n    json_multiple_duplicates = '{\"name\": \"First\", \"age\": 1, \"name\": \"Second\", \"name\": \"Third\", \"age\": 3}'\n    result2 = v.validate_json(json_multiple_duplicates)\n    assert result2.name == 'Third', 'Last value among multiple duplicates should win'\n    assert result2.age == 3\n    assert dataclasses.asdict(result2) == {'name': 'Third', 'age': 3}"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004740", "source": "def create_branch_and_checkout(self, branch: str) -> None:\n        self._run_git(\"checkout\", \"-b\", branch)", "target": "def test_compute_diff(self) -> None:\n        diff = self.repo.diff(\"HEAD\")\n        sha = _shasum(diff)\n        self.assertEqual(len(sha), 64)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004741", "source": "def is_valid_field_name(name: str) -> bool:\n    return not name.startswith('_')", "target": "def test_from_attributes_type_error():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'a': core_schema.model_field(schema=core_schema.int_schema()),\n                'b': core_schema.model_field(schema=core_schema.int_schema()),\n                'c': core_schema.model_field(schema=core_schema.str_schema()),\n            },\n            from_attributes=True,\n            model_name='MyModel',\n        )\n    )\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python('123')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'model_attributes_type',\n            'loc': (),\n            'msg': 'Input should be a valid dictionary or object to extract fields from',\n            'input': '123',\n        }\n    ]\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_json('123')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'model_type',\n            'loc': (),\n            'msg': 'Input should be an object',\n            'input': 123,\n            'ctx': {'class_name': 'MyModel'},\n        }\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004742", "source": "def gh_owner_and_name(self) -> tuple[str, str]:\n        url = os.getenv(\"GIT_REMOTE_URL\", None)\n        if url is None:\n            url = self.remote_url()\n        rc = RE_GITHUB_URL_MATCH.match(url)\n        if rc is None:\n            raise RuntimeError(f\"Unexpected url format {url}\")\n        return cast(tuple[str, str], rc.groups())", "target": "def test_fails(self) -> None:\n        @retries_decorator(rc=0)\n        def foo(x: int, y: int) -> int:\n            return x + y\n        self.assertEqual(foo(\"a\", 4), 0)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004743", "source": "def _is_field_cached_property(obj: 'Dataclass', k: str) -> bool:\n        return False", "target": "def test_dataclass_field_wrap_validator1():\n    @dataclasses.dataclass\n    class Foo:\n        a: int\n        b: str\n        @classmethod\n        def validate_b(\n            cls, v: bytes, nxt: core_schema.ValidatorFunctionWrapHandler, info: core_schema.ValidationInfo\n        ) -> str:\n            assert v == b'hello'\n            v = nxt(v)\n            assert v == 'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return 'hello world!'\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(\n                    name='b',\n                    schema=core_schema.with_info_wrap_validator_function(Foo.validate_b, core_schema.str_schema()),\n                ),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 1, 'b': b'hello'})\n    assert dataclasses.asdict(foo) == {'a': 1, 'b': 'hello world!'}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004744", "source": "def rebuild_model_fields(\n    cls: type[BaseModel],\n    *,\n    config_wrapper: ConfigWrapper,\n    ns_resolver: NsResolver,\n    typevars_map: Mapping[TypeVar, Any],\n) -> dict[str, FieldInfo]:\n    rebuilt_fields: dict[str, FieldInfo] = {}\n    with ns_resolver.push(cls):\n        for f_name, field_info in cls.__pydantic_fields__.items():\n            if field_info._complete:\n                rebuilt_fields[f_name] = field_info\n            else:\n                new_field = _recreate_field_info(\n                    field_info, ns_resolver=ns_resolver, typevars_map=typevars_map, lenient=False\n                )\n                update_field_from_config(config_wrapper, f_name, new_field)\n                rebuilt_fields[f_name] = new_field\n    return rebuilt_fields", "target": "def test_allow_extra_invalid():\n    with pytest.raises(SchemaError, match='extras_schema can only be used if extra_behavior=allow'):\n        SchemaValidator(\n            schema=core_schema.model_fields_schema(\n                fields={}, extras_schema=core_schema.int_schema(), extra_behavior='ignore'\n            )\n        )\n    with pytest.raises(SchemaError, match='extras_keys_schema can only be used if extra_behavior=allow'):\n        SchemaValidator(\n            schema=core_schema.model_fields_schema(\n                fields={}, extras_keys_schema=core_schema.int_schema(), extra_behavior='ignore'\n            )\n        )"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004745", "source": "def _update_fields_from_docstrings(cls: type[Any], fields: dict[str, FieldInfo], use_inspect: bool = False) -> None:\n    fields_docs = extract_docstrings_from_cls(cls, use_inspect=use_inspect)\n    for ann_name, field_info in fields.items():\n        if field_info.description is None and ann_name in fields_docs:\n            field_info.description = fields_docs[ann_name]", "target": "def test_aliases_path_negative_json(py_and_json: PyAndJson, input_value, expected):\n    v = py_and_json(\n        {\n            'type': 'model-fields',\n            'fields': {'field_a': {'validation_alias': ['foo', -2], 'type': 'model-field', 'schema': {'type': 'int'}}},\n        }\n    )\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=expected.message):\n            val = v.validate_test(input_value)\n            print(f'UNEXPECTED OUTPUT: {val!r}')\n    else:\n        output = v.validate_test(input_value)\n        assert output == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004746", "source": "def timedelta_isoformat(td: datetime.timedelta) -> str:\n    minutes, seconds = divmod(td.seconds, 60)\n    hours, minutes = divmod(minutes, 60)\n    return f'{\"-\" if td.days < 0 else \"\"}P{abs(td.days)}DT{hours:d}H{minutes:d}M{seconds:d}.{td.microseconds:06d}S'", "target": "def test_bool(input_value, output_value):\n    v = SchemaValidator(core_schema.bool_schema())\n    assert v.validate_json(input_value) == output_value"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004747", "source": "def dataclass(\n        *,\n        init: Literal[False] = False,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool | None = None,\n        config: ConfigDict | type[object] | None = None,\n        validate_on_init: bool | None = None,\n    ) -> Callable[[type[_T]], type[PydanticDataclass]]:\n        ...", "target": "def test_dataclass_field_plain_validator():\n    @dataclasses.dataclass\n    class Foo:\n        a: int\n        b: str\n        @classmethod\n        def validate_b(cls, v: bytes, info: core_schema.ValidationInfo) -> str:\n            assert v == b'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return 'hello world!'\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(\n                    name='b', schema=core_schema.with_info_plain_validator_function(Foo.validate_b)\n                ),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 1, 'b': b'hello'})\n    assert dataclasses.asdict(foo) == {'a': 1, 'b': 'hello world!'}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004748", "source": "def is_builtin_dataclass(_cls: Type[Any]) -> bool:\n    return (\n        dataclasses.is_dataclass(_cls)\n        and not hasattr(_cls, '__pydantic_model__')\n        and set(_cls.__dataclass_fields__).issuperset(set(getattr(_cls, '__annotations__', {})))\n    )", "target": "def test_dataclass_field_wrap_validator1():\n    @dataclasses.dataclass\n    class Foo:\n        a: int\n        b: str\n        @classmethod\n        def validate_b(\n            cls, v: bytes, nxt: core_schema.ValidatorFunctionWrapHandler, info: core_schema.ValidationInfo\n        ) -> str:\n            assert v == b'hello'\n            v = nxt(v)\n            assert v == 'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return 'hello world!'\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(\n                    name='b',\n                    schema=core_schema.with_info_wrap_validator_function(Foo.validate_b, core_schema.str_schema()),\n                ),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 1, 'b': b'hello'})\n    assert dataclasses.asdict(foo) == {'a': 1, 'b': 'hello world!'}"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004749", "source": "def temp_environ(updates: dict[str, str]):\n    missing = object()\n    old: dict[str, str | object] = {k: os.environ.get(k, missing) for k in updates}\n    try:\n        os.environ.update(updates)\n        yield\n    finally:\n        for k, v in old.items():\n            if v is missing:\n                os.environ.pop(k, None)\n            else:\n                os.environ[k] = v", "target": "def test_gh_get_labels(\n        self,\n        mock_request_for_labels: Any,\n        mock_get_last_page_num_from_header: Any,\n    ) -> None:\n        res = gh_get_labels(\"mock_org\", \"mock_repo\")\n        mock_get_last_page_num_from_header.assert_called_once()\n        self.assertEqual(res, [\"foo\"] * 3)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004750", "source": "def dataclass(\n        _cls: Type[_T],\n        *,\n        init: bool = True,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool = False,\n        config: Union[ConfigDict, Type[object], None] = None,\n        validate_on_init: Optional[bool] = None,\n        use_proxy: Optional[bool] = None,\n    ) -> 'DataclassClassOrWrapper':\n        ...", "target": "def test_by_alias_and_name_config_interaction(\n    config_by_alias: Union[bool, None],\n    config_by_name: Union[bool, None],\n    runtime_by_alias: Union[bool, None],\n    runtime_by_name: Union[bool, None],\n) -> None:\n    if config_by_alias is False and config_by_name is False and runtime_by_alias is False and runtime_by_name is False:\n        pytest.skip(\"Can't have both by_alias and by_name as effectively False\")\n    core_config = {\n        **({'validate_by_alias': config_by_alias} if config_by_alias is not None else {}),\n        **({'validate_by_name': config_by_name} if config_by_name is not None else {}),\n    }\n    @dataclasses.dataclass\n    class MyDataclass:\n        my_field: int\n    schema = core_schema.dataclass_schema(\n        MyDataclass,\n        core_schema.dataclass_args_schema(\n            'MyDataclass',\n            [\n                core_schema.dataclass_field(\n                    name='my_field', schema=core_schema.int_schema(), validation_alias='my_alias'\n                ),\n            ],\n        ),\n        ['my_field'],\n        config=core_schema.CoreConfig(**core_config),\n    )\n    s = SchemaValidator(schema)\n    alias_allowed = next(x for x in (runtime_by_alias, config_by_alias, True) if x is not None)\n    name_allowed = next(x for x in (runtime_by_name, config_by_name, False) if x is not None)\n    if alias_allowed:\n        assert dataclasses.asdict(\n            s.validate_python({'my_alias': 1}, by_alias=runtime_by_alias, by_name=runtime_by_name)\n        ) == {'my_field': 1}\n    if name_allowed:\n        assert dataclasses.asdict(\n            s.validate_python({'my_field': 1}, by_alias=runtime_by_alias, by_name=runtime_by_name)\n        ) == {'my_field': 1}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004751", "source": "def dataclass(\n    _cls: type[_T] | None = None,\n    *,\n    init: Literal[False] = False,\n    repr: bool = True,\n    eq: bool = True,\n    order: bool = False,\n    unsafe_hash: bool = False,\n    frozen: bool | None = None,\n    config: ConfigDict | type[object] | None = None,\n    validate_on_init: bool | None = None,\n    kw_only: bool = False,\n    slots: bool = False,\n) -> Callable[[type[_T]], type[PydanticDataclass]] | type[PydanticDataclass]:\n    assert init is False, 'pydantic.dataclasses.dataclass only supports init=False'\n    assert validate_on_init is not False, 'validate_on_init=False is no longer supported'\n    if sys.version_info >= (3, 10):\n        kwargs = {'kw_only': kw_only, 'slots': slots}\n    else:\n        kwargs = {}\n    def create_dataclass(cls: type[Any]) -> type[PydanticDataclass]:\n        from ._internal._utils import is_model_class\n        if is_model_class(cls):\n            raise PydanticUserError(\n                f'Cannot create a Pydantic dataclass from {cls.__name__} as it is already a Pydantic model',\n                code='dataclass-on-model',\n            )\n        original_cls = cls\n        has_dataclass_base = any(dataclasses.is_dataclass(base) for base in cls.__bases__)\n        if not has_dataclass_base and config is not None and hasattr(cls, '__pydantic_config__'):\n            warn(\n                f'`config` is set via both the `dataclass` decorator and `__pydantic_config__` for dataclass {cls.__name__}. '\n                f'The `config` specification from `dataclass` decorator will take priority.',\n                category=UserWarning,\n                stacklevel=2,\n            )\n        config_dict = config if config is not None else getattr(cls, '__pydantic_config__', None)\n        config_wrapper = _config.ConfigWrapper(config_dict)\n        decorators = _decorators.DecoratorInfos.build(cls, replace_wrapped_methods=True)\n        decorators.update_from_config(config_wrapper)\n        original_doc = cls.__doc__\n        if _pydantic_dataclasses.is_stdlib_dataclass(cls):\n            original_doc = None\n            bases = (cls,)\n            if issubclass(cls, Generic):\n                generic_base = Generic[cls.__parameters__]\n                bases = bases + (generic_base,)\n            cls = types.new_class(cls.__name__, bases)\n        if frozen is not None:\n            frozen_ = frozen\n            if config_wrapper.frozen:\n                warn(\n                    f'`frozen` is set via both the `dataclass` decorator and `config` for dataclass {cls.__name__!r}.'\n                    'This is not recommended. The `frozen` specification on `dataclass` will take priority.',\n                    category=UserWarning,\n                    stacklevel=2,\n                )\n        else:\n            frozen_ = config_wrapper.frozen or False\n        cls_anns = _typing_extra.safe_get_annotations(cls)\n        for field_name in cls_anns:\n            field_value = getattr(cls, field_name, None)\n            if isinstance(field_value, FieldInfo):\n                setattr(cls, field_name, _pydantic_dataclasses.as_dataclass_field(field_value))\n        with _pydantic_dataclasses.patch_base_fields(cls):\n            cls = dataclasses.dataclass(\n                cls,\n                init=True,\n                repr=repr,\n                eq=eq,\n                order=order,\n                unsafe_hash=unsafe_hash,\n                frozen=frozen_,\n                **kwargs,\n            )\n        if config_wrapper.validate_assignment:\n            original_setattr = cls.__setattr__\n            @functools.wraps(cls.__setattr__)\n            def validated_setattr(instance: PydanticDataclass, name: str, value: Any, /) -> None:\n                if frozen_:\n                    return original_setattr(instance, name, value)\n                inst_cls = type(instance)\n                attr = getattr(inst_cls, name, None)\n                if isinstance(attr, property):\n                    attr.__set__(instance, value)\n                elif isinstance(attr, functools.cached_property):\n                    instance.__dict__.__setitem__(name, value)\n                else:\n                    inst_cls.__pydantic_validator__.validate_assignment(instance, name, value)\n            cls.__setattr__ = validated_setattr.__get__(None, cls)\n            if slots and not hasattr(cls, '__setstate__'):\n                def _dataclass_getstate(self: Any) -> list[Any]:\n                    return [getattr(self, f.name) for f in dataclasses.fields(self)]\n                def _dataclass_setstate(self: Any, state: list[Any]) -> None:\n                    for field, value in zip(dataclasses.fields(self), state):\n                        object.__setattr__(self, field.name, value)\n                cls.__getstate__ = _dataclass_getstate\n                cls.__setstate__ = _dataclass_setstate\n        cls.__is_pydantic_dataclass__ = True\n        cls.__pydantic_decorators__ = decorators\n        cls.__doc__ = original_doc\n        firstlineno = getattr(original_cls, '__firstlineno__', None)\n        cls.__module__ = original_cls.__module__\n        if sys.version_info >= (3, 13) and firstlineno is not None:\n            original_cls.__firstlineno__ = firstlineno\n            cls.__firstlineno__ = firstlineno\n        cls.__qualname__ = original_cls.__qualname__\n        cls.__pydantic_fields_complete__ = classmethod(_pydantic_fields_complete)\n        cls.__pydantic_complete__ = False\n        _pydantic_dataclasses.complete_dataclass(cls, config_wrapper, raise_errors=False)\n        return cls\n    return create_dataclass if _cls is None else create_dataclass(_cls)", "target": "def test_function_validator_wrapping_args_schema_after() -> None:\n    calls: list[Any] = []\n    def func(*args: Any) -> Any:\n        calls.append(args)\n        return args[0]\n    @dataclasses.dataclass\n    class Model:\n        number: int = 1\n    cs = core_schema.dataclass_schema(\n        Model,\n        core_schema.no_info_after_validator_function(\n            func,\n            core_schema.dataclass_args_schema(\n                'Model', [core_schema.dataclass_field('number', core_schema.int_schema())]\n            ),\n        ),\n        ['number'],\n    )\n    v = SchemaValidator(cs)\n    instance: Model = v.validate_python({'number': 1})\n    assert instance.number == 1\n    assert calls == [(({'number': 1}, None),)]\n    v.validate_assignment(instance, 'number', 2)\n    assert instance.number == 2\n    assert calls == [(({'number': 1}, None),), (({'number': 2}, None),)]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004752", "source": "def check_deprecated(config_dict: ConfigDict) -> None:\n    deprecated_removed_keys = V2_REMOVED_KEYS & config_dict.keys()\n    deprecated_renamed_keys = V2_RENAMED_KEYS.keys() & config_dict.keys()\n    if deprecated_removed_keys or deprecated_renamed_keys:\n        renamings = {k: V2_RENAMED_KEYS[k] for k in sorted(deprecated_renamed_keys)}\n        renamed_bullets = [f'* {k!r} has been renamed to {v!r}' for k, v in renamings.items()]\n        removed_bullets = [f'* {k!r} has been removed' for k in sorted(deprecated_removed_keys)]\n        message = '\\n'.join(['Valid config keys have changed in V2:'] + renamed_bullets + removed_bullets)\n        warnings.warn(message, UserWarning)", "target": "def test_allow_inf_nan(config: CoreConfig, float_field_schema, input_value, expected):\n    v = SchemaValidator(\n        cs.model_schema(\n            cls=MyModel,\n            schema=cs.model_fields_schema(fields={'x': cs.model_field(schema=float_field_schema)}),\n            config=config,\n        )\n    )\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            v.validate_python(input_value)\n    else:\n        output_dict = v.validate_python(input_value)\n        assert output_dict == expected"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004753", "source": "def retries_decorator(\n    rc: Any = None, num_retries: int = 3\n) -> Callable[[Callable[..., T]], Callable[..., T]]:\n    def decorator(f: Callable[..., T]) -> Callable[..., T]:\n        @wraps(f)\n        def wrapper(*args: list[Any], **kwargs: dict[str, Any]) -> T:\n            for idx in range(num_retries):\n                try:\n                    return f(*args, **kwargs)\n                except Exception as e:\n                    print(\n                        f'Attempt {idx} of {num_retries} to call {f.__name__} failed with \"{e}\"'\n                    )\n            return cast(T, rc)\n        return wrapper\n    return decorator", "target": "def test_peek(self, input_: str = \"abcdef\") -> None:\n        iter_ = PeekableIterator(input_)\n        for idx, c in enumerate(iter_):\n            if idx + 1 < len(input_):\n                self.assertEqual(iter_.peek(), input_[idx + 1])\n            else:\n                self.assertTrue(iter_.peek() is None)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004754", "source": "def _general_metadata_cls() -> type[BaseMetadata]:\n    from annotated_types import BaseMetadata\n    class _PydanticGeneralMetadata(PydanticMetadata, BaseMetadata):\n        def __init__(self, metadata: Any):\n            self.__dict__ = metadata\n    return _PydanticGeneralMetadata", "target": "def test_alias_error_loc_field_names(py_and_json: PyAndJson):\n    v = py_and_json(\n        {\n            'type': 'model-fields',\n            'fields': {\n                'field_a': {\n                    'type': 'model-field',\n                    'schema': {'type': 'int'},\n                    'validation_alias': [['foo'], ['bar', 1, -1]],\n                }\n            },\n        },\n        {'loc_by_alias': False},\n    )\n    assert v.validate_test({'foo': 42}) == ({'field_a': 42}, None, {'field_a'})\n    assert v.validate_test({'bar': ['x', [1, 2, 42]]}) == ({'field_a': 42}, None, {'field_a'})\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_test({'foo': 'not_int'})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('field_a',),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'not_int',\n        }\n    ]\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_test({'bar': ['x', [1, 2, 'not_int']]})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('field_a',),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'not_int',\n        }\n    ]\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_test({})\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'missing', 'loc': ('field_a',), 'msg': 'Field required', 'input': {}}\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004755", "source": "def dataclass(\n        *,\n        init: bool = True,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool = False,\n        config: Union[ConfigDict, Type[object], None] = None,\n        validate_on_init: Optional[bool] = None,\n        use_proxy: Optional[bool] = None,\n    ) -> Callable[[Type[_T]], 'DataclassClassOrWrapper']:\n        ...", "target": "def test_dataclass_self_init():\n    @dataclasses.dataclass(init=False)\n    class Foo:\n        a: str\n        b: bool\n        def __init__(self, *args, **kwargs):\n            v.validate_python(ArgsKwargs(args, kwargs), self_instance=self)\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), kw_only=False),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), kw_only=False),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    foo = Foo(b'hello', 'True')\n    assert dataclasses.is_dataclass(foo)\n    assert dataclasses.asdict(foo) == {'a': 'hello', 'b': True}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004756", "source": "def rebuild_dataclass_fields(\n    cls: type[PydanticDataclass],\n    *,\n    config_wrapper: ConfigWrapper,\n    ns_resolver: NsResolver,\n    typevars_map: Mapping[TypeVar, Any],\n) -> dict[str, FieldInfo]:\n    FieldInfo_ = import_cached_field_info()\n    rebuilt_fields: dict[str, FieldInfo] = {}\n    with ns_resolver.push(cls):\n        for f_name, field_info in cls.__pydantic_fields__.items():\n            if field_info._complete:\n                rebuilt_fields[f_name] = field_info\n            else:\n                existing_desc = field_info.description\n                ann = _typing_extra.eval_type(\n                    field_info._original_annotation,\n                    *ns_resolver.types_namespace,\n                )\n                ann = _generics.replace_types(ann, typevars_map)\n                new_field = FieldInfo_.from_annotated_attribute(\n                    ann,\n                    field_info._original_assignment,\n                    _source=AnnotationSource.DATACLASS,\n                )\n                new_field.description = new_field.description if new_field.description is not None else existing_desc\n                update_field_from_config(config_wrapper, f_name, new_field)\n                rebuilt_fields[f_name] = new_field\n    return rebuilt_fields", "target": "def test_only_allow_alias(py_and_json) -> None:\n    v = py_and_json(\n        {\n            'type': 'model-fields',\n            'fields': {'field_a': {'validation_alias': 'FieldA', 'type': 'model-field', 'schema': {'type': 'int'}}},\n        },\n        config=CoreConfig(validate_by_name=False, validate_by_alias=True),\n    )\n    assert v.validate_test({'FieldA': '123'}) == ({'field_a': 123}, None, {'field_a'})\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'field_a': '123'})"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004757", "source": "def defaultdict_validator(\n    input_value: Any, handler: core_schema.ValidatorFunctionWrapHandler, default_default_factory: Callable[[], Any]\n) -> collections.defaultdict[Any, Any]:\n    if isinstance(input_value, collections.defaultdict):\n        default_factory = input_value.default_factory\n        return collections.defaultdict(default_factory, handler(input_value))\n    else:\n        return collections.defaultdict(default_default_factory, handler(input_value))", "target": "def test_validators_build(benchmark) -> None:\n    class Base1(BaseModel):\n        a: int\n        @field_validator('a', mode='after')\n        @classmethod\n        def val_a(cls, value: int) -> int: ...\n        @computed_field\n        def prop(self) -> int: ...\n    class Bare:\n        @computed_field\n        def prop_bare(self) -> int: ...\n    class Sub1(Base1):\n        @computed_field\n        def prop_2(self) -> int: ...\n        @computed_field\n        def prop_3(self) -> int: ...\n        @computed_field\n        def prop_4(self) -> int: ...\n    @benchmark\n    def bench() -> None:\n        class SubS(Sub1, Bare, defer_build=True):\n            @computed_field\n            def prop_5(self) -> int: ...\n            @computed_field\n            def prop_6(self) -> int: ...\n            @computed_field\n            def prop_7(self) -> int: ..."}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004758", "source": "def is_valid_field_name(name: str) -> bool:\n    return not name.startswith('_')", "target": "def test_config(config: CoreConfig, input_value, expected):\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'a': core_schema.model_field(schema=core_schema.int_schema()),\n                'b': core_schema.model_field(\n                    schema=core_schema.with_default_schema(schema=core_schema.float_schema(), default=4.2)\n                ),\n            }\n        ),\n        config=config,\n    )\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            val = v.validate_python(input_value)\n            print(f'UNEXPECTED OUTPUT: {val!r}')\n    else:\n        result = v.validate_python(input_value)\n        assert result == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004759", "source": "def set_dataclass_fields(\n    cls: type[StandardDataclass],\n    config_wrapper: _config.ConfigWrapper,\n    ns_resolver: NsResolver | None = None,\n) -> None:\n    typevars_map = get_standard_typevars_map(cls)\n    fields = collect_dataclass_fields(\n        cls, ns_resolver=ns_resolver, typevars_map=typevars_map, config_wrapper=config_wrapper\n    )\n    cls.__pydantic_fields__ = fields", "target": "def test_slots_mixed():\n    @dataclasses.dataclass(slots=True)\n    class Model:\n        x: int\n        y: dataclasses.InitVar[str]\n        z: ClassVar[str] = 'z-classvar'\n    @dataclasses.dataclass\n    class SubModel(Model):\n        x2: int\n        y2: dataclasses.InitVar[str]\n        z2: ClassVar[str] = 'z2-classvar'\n    schema = core_schema.dataclass_schema(\n        SubModel,\n        core_schema.dataclass_args_schema(\n            'SubModel',\n            [\n                core_schema.dataclass_field(name='x', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(name='y', init_only=True, schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='x2', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(name='y2', init_only=True, schema=core_schema.str_schema()),\n            ],\n        ),\n        ['x', 'x2'],\n        slots=True,\n    )\n    dc = SubModel(x=1, y='a', x2=2, y2='b')\n    assert dataclasses.asdict(dc) == {'x': 1, 'x2': 2}\n    s = SchemaSerializer(schema)\n    assert s.to_python(dc) == {'x': 1, 'x2': 2}\n    assert s.to_json(dc) == b'{\"x\":1,\"x2\":2}'"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004760", "source": "def is_builtin_dataclass(_cls: Type[Any]) -> bool:\n    return (\n        dataclasses.is_dataclass(_cls)\n        and not hasattr(_cls, '__pydantic_model__')\n        and set(_cls.__dataclass_fields__).issuperset(set(getattr(_cls, '__annotations__', {})))\n    )", "target": "def test_dataclass_exact_validation(revalidate_instances, input_value, expected):\n    schema = core_schema.dataclass_schema(\n        FooDataclass,\n        core_schema.dataclass_args_schema(\n            'FooDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n            ],\n        ),\n        ['a', 'b'],\n        revalidate_instances=revalidate_instances,\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python(input_value)\n    assert dataclasses.asdict(foo) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004761", "source": "def handle_extra_init(self: 'Dataclass', *args: Any, **kwargs: Any) -> None:\n        if config.extra == Extra.ignore:\n            init(self, *args, **{k: v for k, v in kwargs.items() if k in self.__dataclass_fields__})\n        elif config.extra == Extra.allow:\n            for k, v in kwargs.items():\n                self.__dict__.setdefault(k, v)\n            init(self, *args, **{k: v for k, v in kwargs.items() if k in self.__dataclass_fields__})\n        else:\n            init(self, *args, **kwargs)", "target": "def test_by_alias_and_name_config_interaction(\n    config_by_alias: Union[bool, None],\n    config_by_name: Union[bool, None],\n    runtime_by_alias: Union[bool, None],\n    runtime_by_name: Union[bool, None],\n) -> None:\n    if config_by_alias is False and config_by_name is False and runtime_by_alias is False and runtime_by_name is False:\n        pytest.skip(\"Can't have both by_alias and by_name as effectively False\")\n    core_config = {\n        **({'validate_by_alias': config_by_alias} if config_by_alias is not None else {}),\n        **({'validate_by_name': config_by_name} if config_by_name is not None else {}),\n    }\n    @dataclasses.dataclass\n    class MyDataclass:\n        my_field: int\n    schema = core_schema.dataclass_schema(\n        MyDataclass,\n        core_schema.dataclass_args_schema(\n            'MyDataclass',\n            [\n                core_schema.dataclass_field(\n                    name='my_field', schema=core_schema.int_schema(), validation_alias='my_alias'\n                ),\n            ],\n        ),\n        ['my_field'],\n        config=core_schema.CoreConfig(**core_config),\n    )\n    s = SchemaValidator(schema)\n    alias_allowed = next(x for x in (runtime_by_alias, config_by_alias, True) if x is not None)\n    name_allowed = next(x for x in (runtime_by_name, config_by_name, False) if x is not None)\n    if alias_allowed:\n        assert dataclasses.asdict(\n            s.validate_python({'my_alias': 1}, by_alias=runtime_by_alias, by_name=runtime_by_name)\n        ) == {'my_field': 1}\n    if name_allowed:\n        assert dataclasses.asdict(\n            s.validate_python({'my_field': 1}, by_alias=runtime_by_alias, by_name=runtime_by_name)\n        ) == {'my_field': 1}"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004762", "source": "def env_path_field(\n    name: str,\n    default: Union[str, Path] = \"\",\n    *,\n    resolve: bool = True,\n) -> Path:\n    return field(default_factory=lambda: env_path(name, default, resolve=resolve))", "target": "def test_get_env_unset_returns_default(self):\n        with patch.dict(os.environ, {}, clear=True):\n            self.assertEqual(m.get_env(\"FOO\", \"default\"), \"default\")"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004763", "source": "def dataclass(\n        _cls: Type[_T],\n        *,\n        init: bool = True,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool = False,\n        config: Union[ConfigDict, Type[object], None] = None,\n        validate_on_init: Optional[bool] = None,\n        use_proxy: Optional[bool] = None,\n        kw_only: bool = ...,\n    ) -> 'DataclassClassOrWrapper':\n        ...", "target": "def test_dataclass_subclass_subclass_revalidate():\n    v = SchemaValidator(\n        core_schema.dataclass_schema(\n            FooDataclass,\n            core_schema.dataclass_args_schema(\n                'FooDataclass',\n                [\n                    core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                    core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n                ],\n            ),\n            ['a', 'b'],\n            revalidate_instances='subclass-instances',\n            strict=True,\n        )\n    )\n    foo = FooDataclass(a='hello', b=True)\n    assert v.validate_python(foo) is foo\n    sub_foo = FooDataclassSame(a='hello', b='True')\n    sub_foo2 = v.validate_python(sub_foo)\n    assert sub_foo2 is not sub_foo\n    assert type(sub_foo2) is FooDataclass\n    assert dataclasses.asdict(sub_foo2) == dict(a='hello', b=True)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004764", "source": "def dataclass(\n        _cls: Type[_T],\n        *,\n        init: bool = True,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool = False,\n        config: Union[ConfigDict, Type[object], None] = None,\n        validate_on_init: Optional[bool] = None,\n        use_proxy: Optional[bool] = None,\n        kw_only: bool = ...,\n    ) -> 'DataclassClassOrWrapper':\n        ...", "target": "def test_only_allow_alias(py_and_json) -> None:\n    schema = core_schema.dataclass_schema(\n        BasicDataclass,\n        core_schema.dataclass_args_schema(\n            'BasicDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), validation_alias='FieldA'),\n            ],\n        ),\n        ['a'],\n        config=core_schema.CoreConfig(validate_by_name=False, validate_by_alias=True),\n    )\n    v = py_and_json(schema)\n    assert v.validate_test({'FieldA': 'hello'}) == BasicDataclass(a='hello')\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'a': 'hello'})"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004765", "source": "def get_value(f):\n        if f.default is not MISSING:\n            return f.default\n        if f.default_factory is not MISSING:\n            try:\n                return f.default_factory()\n            except Exception as e:\n                return f\"<error: {e}>\"\n        return \"<required>\"", "target": "def test_env_path_optional_respects_resolve_true(self):\n        with patch.dict(os.environ, {\"P\": \"a/b\"}, clear=True):\n            p = m.env_path_optional(\"P\", resolve=True)\n            self.assertIsInstance(p, Path)\n            if p:\n                self.assertTrue(p.is_absolute())"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004766", "source": "def _recreate_field_info(\n    field_info: FieldInfo,\n    ns_resolver: NsResolver,\n    typevars_map: Mapping[TypeVar, Any],\n    *,\n    lenient: bool,\n) -> FieldInfo:\n    FieldInfo_ = import_cached_field_info()\n    existing_desc = field_info.description\n    if lenient:\n        ann = _generics.replace_types(field_info._original_annotation, typevars_map)\n        ann, evaluated = _typing_extra.try_eval_type(\n            ann,\n            *ns_resolver.types_namespace,\n        )\n    else:\n        ann = _typing_extra.eval_type(\n            field_info._original_annotation,\n            *ns_resolver.types_namespace,\n        )\n        ann = _generics.replace_types(ann, typevars_map)\n        ann = _typing_extra.eval_type(\n            ann,\n            *ns_resolver.types_namespace,\n        )\n        evaluated = True\n    if (assign := field_info._original_assignment) is PydanticUndefined:\n        new_field = FieldInfo_.from_annotation(ann, _source=AnnotationSource.CLASS)\n    else:\n        new_field = FieldInfo_.from_annotated_attribute(ann, assign, _source=AnnotationSource.CLASS)\n        new_field._original_assignment = assign\n    new_field._original_annotation = ann\n    new_field.description = new_field.description if new_field.description is not None else existing_desc\n    if not evaluated:\n        new_field._complete = False\n    return new_field", "target": "def test_extra_behavior_allow_with_validate_fn_override(\n    config: Union[core_schema.CoreConfig, None],\n    schema_extra_behavior_kw: dict[str, Any],\n):\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            {'f': core_schema.model_field(core_schema.str_schema())}, **schema_extra_behavior_kw\n        ),\n        config=config,\n    )\n    m, model_extra, fields_set = v.validate_python({'f': 'x', 'extra_field': '123'}, extra='allow')\n    assert m == {'f': 'x'}\n    assert model_extra == {'extra_field': '123'}\n    assert fields_set == {'f', 'extra_field'}\n    v.validate_assignment(m, 'f', 'y', extra='allow')\n    assert m == {'f': 'y'}\n    new_m, new_model_extra, new_fields_set = v.validate_assignment({**m, **model_extra}, 'not_f', '123', extra='allow')\n    assert new_m == {'f': 'y'}\n    assert new_model_extra == {'extra_field': '123', 'not_f': '123'}\n    assert new_fields_set == {'not_f'}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004767", "source": "def collect_dataclass_fields(\n    cls: type[StandardDataclass],\n    *,\n    config_wrapper: ConfigWrapper,\n    ns_resolver: NsResolver | None = None,\n    typevars_map: dict[Any, Any] | None = None,\n) -> dict[str, FieldInfo]:\n    FieldInfo_ = import_cached_field_info()\n    fields: dict[str, FieldInfo] = {}\n    ns_resolver = ns_resolver or NsResolver()\n    dataclass_fields = cls.__dataclass_fields__\n    for base in reversed(cls.__mro__):\n        if not dataclasses.is_dataclass(base):\n            continue\n        with ns_resolver.push(base):\n            for ann_name, dataclass_field in dataclass_fields.items():\n                base_anns = _typing_extra.safe_get_annotations(base)\n                if ann_name not in base_anns:\n                    continue\n                globalns, localns = ns_resolver.types_namespace\n                ann_type, evaluated = _typing_extra.try_eval_type(dataclass_field.type, globalns, localns)\n                if _typing_extra.is_classvar_annotation(ann_type):\n                    continue\n                if (\n                    not dataclass_field.init\n                    and dataclass_field.default is dataclasses.MISSING\n                    and dataclass_field.default_factory is dataclasses.MISSING\n                ):\n                    continue\n                if isinstance(dataclass_field.default, FieldInfo_):\n                    if dataclass_field.default.init_var:\n                        if dataclass_field.default.init is False:\n                            raise PydanticUserError(\n                                f'Dataclass field {ann_name} has init=False and init_var=True, but these are mutually exclusive.',\n                                code='clashing-init-and-init-var',\n                            )\n                        continue\n                    field_info = FieldInfo_.from_annotated_attribute(\n                        ann_type, dataclass_field.default, _source=AnnotationSource.DATACLASS\n                    )\n                    field_info._original_assignment = dataclass_field.default\n                else:\n                    field_info = FieldInfo_.from_annotated_attribute(\n                        ann_type, dataclass_field, _source=AnnotationSource.DATACLASS\n                    )\n                    field_info._original_assignment = dataclass_field\n                if not evaluated:\n                    field_info._complete = False\n                    field_info._original_annotation = ann_type\n                fields[ann_name] = field_info\n                update_field_from_config(config_wrapper, ann_name, field_info)\n                if field_info.default is not PydanticUndefined and isinstance(\n                    getattr(cls, ann_name, field_info), FieldInfo_\n                ):\n                    setattr(cls, ann_name, field_info.default)\n    if typevars_map:\n        for field in fields.values():\n            field.apply_typevars_map(typevars_map)\n    if config_wrapper.use_attribute_docstrings:\n        _update_fields_from_docstrings(\n            cls,\n            fields,\n            use_inspect=not hasattr(cls, '__is_pydantic_dataclass__'),\n        )\n    return fields", "target": "def test_with_default():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'field_a': core_schema.model_field(schema=core_schema.str_schema()),\n                'field_b': core_schema.model_field(\n                    schema=core_schema.with_default_schema(schema=core_schema.int_schema(), default=666)\n                ),\n            }\n        )\n    )\n    assert v.validate_python({'field_a': b'abc'}) == ({'field_a': 'abc', 'field_b': 666}, None, {'field_a'})\n    assert v.validate_python({'field_a': b'abc', 'field_b': 1}) == (\n        {'field_a': 'abc', 'field_b': 1},\n        None,\n        {'field_b', 'field_a'},\n    )"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004768", "source": "def working_directory(path: str):\n    if not path:\n        yield\n        return\n    prev_cwd = os.getcwd()\n    try:\n        os.chdir(path)\n        yield\n    finally:\n        os.chdir(prev_cwd)", "target": "def test_fails(self) -> None:\n        @retries_decorator(rc=0)\n        def foo(x: int, y: int) -> int:\n            return x + y\n        self.assertEqual(foo(\"a\", 4), 0)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004769", "source": "def dataclass(\n        *,\n        init: Literal[False] = False,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool | None = None,\n        config: ConfigDict | type[object] | None = None,\n        validate_on_init: bool | None = None,\n    ) -> Callable[[type[_T]], type[PydanticDataclass]]:\n        ...", "target": "def test_dataclass_validate_assignment():\n    schema = core_schema.dataclass_schema(\n        FooDataclass,\n        core_schema.dataclass_args_schema(\n            'FooDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), kw_only=False),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), kw_only=False),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 'hello', 'b': 'True'})\n    assert dataclasses.asdict(foo) == {'a': 'hello', 'b': True}\n    v.validate_assignment(foo, 'a', b'world')\n    assert dataclasses.asdict(foo) == {'a': 'world', 'b': True}\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_assignment(foo, 'a', 123)\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'string_type', 'loc': ('a',), 'msg': 'Input should be a valid string', 'input': 123}\n    ]\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_assignment(foo, 'c', '123')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'no_such_attribute',\n            'loc': ('c',),\n            'msg': \"Object has no attribute 'c'\",\n            'input': '123',\n            'ctx': {'attribute': 'c'},\n        }\n    ]\n    assert not hasattr(foo, 'c')\n    with pytest.raises(AttributeError, match=\"'str' object has no attribute 'a'\"):\n        v.validate_assignment('field_a', 'c', 123)"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004770", "source": "def benchmark_using_throughput_benchmark(config, module):\n    print(\"Benchmarking via ThroughputBenchmark\")\n    bench = ThroughputBenchmark(module.module)\n    bench.add_input(*module.tensor_inputs)\n    stats = bench.benchmark(1, config.num_warmup_iters, config.num_iters)\n    return stats.latency_avg_ms / NUM_LOOP_ITERS", "target": "def test_merged_lastfailed_content_with_empty_dest(self) -> None:\n        last_failed_source = {\n            \"tools/tests/test_car.py::test_num1\": True,\n            \"tools/tests/test_car.py::test_num2\": True,\n        }\n        last_failed_dest = {\n            \"\": True,\n        }\n        last_failed_merged = {\n            \"tools/tests/test_car.py::test_num1\": True,\n            \"tools/tests/test_car.py::test_num2\": True,\n        }\n        merged = _merged_lastfailed_content(last_failed_source, last_failed_dest)\n        self.assertEqual(merged, last_failed_merged)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004771", "source": "def rebuild_model_fields(\n    cls: type[BaseModel],\n    *,\n    config_wrapper: ConfigWrapper,\n    ns_resolver: NsResolver,\n    typevars_map: Mapping[TypeVar, Any],\n) -> dict[str, FieldInfo]:\n    rebuilt_fields: dict[str, FieldInfo] = {}\n    with ns_resolver.push(cls):\n        for f_name, field_info in cls.__pydantic_fields__.items():\n            if field_info._complete:\n                rebuilt_fields[f_name] = field_info\n            else:\n                new_field = _recreate_field_info(\n                    field_info, ns_resolver=ns_resolver, typevars_map=typevars_map, lenient=False\n                )\n                update_field_from_config(config_wrapper, f_name, new_field)\n                rebuilt_fields[f_name] = new_field\n    return rebuilt_fields", "target": "def test_from_attributes_extra_ignore_no_attributes_accessed() -> None:\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={'a': core_schema.model_field(schema=core_schema.int_schema())},\n            from_attributes=True,\n            extra_behavior='ignore',\n        )\n    )\n    accessed: list[str] = []\n    class Source:\n        a = 1\n        b = 2\n        def __getattribute__(self, name: str, /) -> Any:\n            accessed.append(name)\n            return super().__getattribute__(name)\n    assert v.validate_python(Source()) == ({'a': 1}, None, {'a'})\n    assert 'a' in accessed and 'b' not in accessed"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004772", "source": "def validated_setattr(instance: PydanticDataclass, name: str, value: Any, /) -> None:\n                if frozen_:\n                    return original_setattr(instance, name, value)\n                inst_cls = type(instance)\n                attr = getattr(inst_cls, name, None)\n                if isinstance(attr, property):\n                    attr.__set__(instance, value)\n                elif isinstance(attr, functools.cached_property):\n                    instance.__dict__.__setitem__(name, value)\n                else:\n                    inst_cls.__pydantic_validator__.validate_assignment(instance, name, value)", "target": "def test_dataclass_self_init():\n    @dataclasses.dataclass(init=False)\n    class Foo:\n        a: str\n        b: bool\n        def __init__(self, *args, **kwargs):\n            v.validate_python(ArgsKwargs(args, kwargs), self_instance=self)\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), kw_only=False),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), kw_only=False),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    foo = Foo(b'hello', 'True')\n    assert dataclasses.is_dataclass(foo)\n    assert dataclasses.asdict(foo) == {'a': 'hello', 'b': True}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004773", "source": "def set_dataclass_fields(\n    cls: type[StandardDataclass],\n    config_wrapper: _config.ConfigWrapper,\n    ns_resolver: NsResolver | None = None,\n) -> None:\n    typevars_map = get_standard_typevars_map(cls)\n    fields = collect_dataclass_fields(\n        cls, ns_resolver=ns_resolver, typevars_map=typevars_map, config_wrapper=config_wrapper\n    )\n    cls.__pydantic_fields__ = fields", "target": "def test_dataclass_subclass(revalidate_instances, input_value, expected):\n    schema = core_schema.dataclass_schema(\n        FooDataclass,\n        core_schema.dataclass_args_schema(\n            'FooDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n            ],\n            extra_behavior='forbid',\n        ),\n        ['a', 'b'],\n        revalidate_instances=revalidate_instances,\n    )\n    v = SchemaValidator(schema)\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=expected.message) as exc_info:\n            print(v.validate_python(input_value))\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        dc = v.validate_python(input_value)\n        assert dataclasses.is_dataclass(dc)\n        assert dataclasses.asdict(dc) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004774", "source": "def rebuild_model_fields(\n    cls: type[BaseModel],\n    *,\n    config_wrapper: ConfigWrapper,\n    ns_resolver: NsResolver,\n    typevars_map: Mapping[TypeVar, Any],\n) -> dict[str, FieldInfo]:\n    rebuilt_fields: dict[str, FieldInfo] = {}\n    with ns_resolver.push(cls):\n        for f_name, field_info in cls.__pydantic_fields__.items():\n            if field_info._complete:\n                rebuilt_fields[f_name] = field_info\n            else:\n                new_field = _recreate_field_info(\n                    field_info, ns_resolver=ns_resolver, typevars_map=typevars_map, lenient=False\n                )\n                update_field_from_config(config_wrapper, f_name, new_field)\n                rebuilt_fields[f_name] = new_field\n    return rebuilt_fields", "target": "def test_from_attributes_extra_forbid() -> None:\n    class Source:\n        a = 1\n        b = 2\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={'a': core_schema.model_field(schema=core_schema.int_schema())},\n            from_attributes=True,\n            extra_behavior='forbid',\n        )\n    )\n    assert v.validate_python(Source()) == ({'a': 1}, None, {'a'})"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004775", "source": "def compute_branch_diffs(\n        self, from_branch: str, to_branch: str\n    ) -> tuple[list[str], list[str]]:\n        from_ref = self.rev_parse(from_branch)\n        to_ref = self.rev_parse(to_branch)\n        merge_base = self.get_merge_base(from_ref, to_ref)\n        from_commits = self.revlist(f\"{merge_base}..{from_ref}\")\n        to_commits = self.revlist(f\"{merge_base}..{to_ref}\")\n        from_ids = fuzzy_list_to_dict(self.patch_id(from_commits))\n        to_ids = fuzzy_list_to_dict(self.patch_id(to_commits))\n        for patch_id in set(from_ids).intersection(set(to_ids)):\n            from_values = from_ids[patch_id]\n            to_values = to_ids[patch_id]\n            if len(from_values) != len(to_values):\n                while len(from_values) > 0 and len(to_values) > 0:\n                    frc = self.get_commit(from_values.pop())\n                    toc = self.get_commit(to_values.pop())\n                    if frc.title != toc.title or frc.author_date != toc.author_date:\n                        if (\n                            \"pytorch/pytorch\" not in self.remote_url()\n                            or frc.commit_hash\n                            not in {\n                                \"0a6a1b27a464ba5be5f587cce2ee12ab8c504dbf\",\n                                \"6d0f4a1d545a8f161df459e8d4ccafd4b9017dbe\",\n                                \"edf909e58f06150f7be41da2f98a3b9de3167bca\",\n                                \"a58c6aea5a0c9f8759a4154e46f544c8b03b8db1\",\n                                \"7106d216c29ca16a3504aa2bedad948ebcf4abc2\",\n                            }\n                        ):\n                            raise RuntimeError(\n                                f\"Unexpected differences between {frc} and {toc}\"\n                            )\n                    from_commits.remove(frc.commit_hash)\n                    to_commits.remove(toc.commit_hash)\n                continue\n            for commit in from_values:\n                from_commits.remove(commit)\n            for commit in to_values:\n                to_commits.remove(commit)\n        if \"pytorch/pytorch\" in self.remote_url():\n            for excluded_commit in {\n                \"8e09e20c1dafcdbdb45c2d1574da68a32e54a3a5\",\n                \"5f37e5c2a39c3acb776756a17730b865f0953432\",\n                \"b5222584e6d6990c6585981a936defd1af14c0ba\",\n                \"84d9a2e42d5ed30ec3b8b4140c38dd83abbce88d\",\n                \"f211ec90a6cdc8a2a5795478b5b5c8d7d7896f7e\",\n            }:\n                if excluded_commit in from_commits:\n                    from_commits.remove(excluded_commit)\n        return (from_commits, to_commits)", "target": "def test_ghstack_branches_not_in_sync(self) -> None:\n        head_ref = \"gh/clee2000/1/head\"\n        self._skip_if_ref_does_not_exist(head_ref)\n        self.assertFalse(are_ghstack_branches_in_sync(self.repo, head_ref))"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004776", "source": "def get_wheels(\n    output_dir: Path,\n    max_depth: Optional[int] = None,\n) -> list[str]:\n    root = Path(output_dir)\n    if not root.exists():\n        return []\n    items = []\n    for dirpath, _, filenames in os.walk(root):\n        depth = Path(dirpath).relative_to(root).parts\n        if max_depth is not None and len(depth) > max_depth:\n            continue\n        for fname in sorted(filenames):\n            if fname.endswith(\".whl\"):\n                pkg = fname.split(\"-\")[0]\n                relpath = str((Path(dirpath) / fname).relative_to(root))\n                items.append({\"pkg\": pkg, \"relpath\": relpath})\n    return items", "target": "def test_simple(self) -> None:\n        @retries_decorator()\n        def foo(x: int, y: int) -> int:\n            return x + y\n        self.assertEqual(foo(3, 4), 7)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004777", "source": "def create_dataclass(cls: type[Any]) -> type[PydanticDataclass]:\n        from ._internal._utils import is_model_class\n        if is_model_class(cls):\n            raise PydanticUserError(\n                f'Cannot create a Pydantic dataclass from {cls.__name__} as it is already a Pydantic model',\n                code='dataclass-on-model',\n            )\n        original_cls = cls\n        has_dataclass_base = any(dataclasses.is_dataclass(base) for base in cls.__bases__)\n        if not has_dataclass_base and config is not None and hasattr(cls, '__pydantic_config__'):\n            warn(\n                f'`config` is set via both the `dataclass` decorator and `__pydantic_config__` for dataclass {cls.__name__}. '\n                f'The `config` specification from `dataclass` decorator will take priority.',\n                category=UserWarning,\n                stacklevel=2,\n            )\n        config_dict = config if config is not None else getattr(cls, '__pydantic_config__', None)\n        config_wrapper = _config.ConfigWrapper(config_dict)\n        decorators = _decorators.DecoratorInfos.build(cls, replace_wrapped_methods=True)\n        decorators.update_from_config(config_wrapper)\n        original_doc = cls.__doc__\n        if _pydantic_dataclasses.is_stdlib_dataclass(cls):\n            original_doc = None\n            bases = (cls,)\n            if issubclass(cls, Generic):\n                generic_base = Generic[cls.__parameters__]\n                bases = bases + (generic_base,)\n            cls = types.new_class(cls.__name__, bases)\n        if frozen is not None:\n            frozen_ = frozen\n            if config_wrapper.frozen:\n                warn(\n                    f'`frozen` is set via both the `dataclass` decorator and `config` for dataclass {cls.__name__!r}.'\n                    'This is not recommended. The `frozen` specification on `dataclass` will take priority.',\n                    category=UserWarning,\n                    stacklevel=2,\n                )\n        else:\n            frozen_ = config_wrapper.frozen or False\n        cls_anns = _typing_extra.safe_get_annotations(cls)\n        for field_name in cls_anns:\n            field_value = getattr(cls, field_name, None)\n            if isinstance(field_value, FieldInfo):\n                setattr(cls, field_name, _pydantic_dataclasses.as_dataclass_field(field_value))\n        with _pydantic_dataclasses.patch_base_fields(cls):\n            cls = dataclasses.dataclass(\n                cls,\n                init=True,\n                repr=repr,\n                eq=eq,\n                order=order,\n                unsafe_hash=unsafe_hash,\n                frozen=frozen_,\n                **kwargs,\n            )\n        if config_wrapper.validate_assignment:\n            original_setattr = cls.__setattr__\n            @functools.wraps(cls.__setattr__)\n            def validated_setattr(instance: PydanticDataclass, name: str, value: Any, /) -> None:\n                if frozen_:\n                    return original_setattr(instance, name, value)\n                inst_cls = type(instance)\n                attr = getattr(inst_cls, name, None)\n                if isinstance(attr, property):\n                    attr.__set__(instance, value)\n                elif isinstance(attr, functools.cached_property):\n                    instance.__dict__.__setitem__(name, value)\n                else:\n                    inst_cls.__pydantic_validator__.validate_assignment(instance, name, value)\n            cls.__setattr__ = validated_setattr.__get__(None, cls)\n            if slots and not hasattr(cls, '__setstate__'):\n                def _dataclass_getstate(self: Any) -> list[Any]:\n                    return [getattr(self, f.name) for f in dataclasses.fields(self)]\n                def _dataclass_setstate(self: Any, state: list[Any]) -> None:\n                    for field, value in zip(dataclasses.fields(self), state):\n                        object.__setattr__(self, field.name, value)\n                cls.__getstate__ = _dataclass_getstate\n                cls.__setstate__ = _dataclass_setstate\n        cls.__is_pydantic_dataclass__ = True\n        cls.__pydantic_decorators__ = decorators\n        cls.__doc__ = original_doc\n        firstlineno = getattr(original_cls, '__firstlineno__', None)\n        cls.__module__ = original_cls.__module__\n        if sys.version_info >= (3, 13) and firstlineno is not None:\n            original_cls.__firstlineno__ = firstlineno\n            cls.__firstlineno__ = firstlineno\n        cls.__qualname__ = original_cls.__qualname__\n        cls.__pydantic_fields_complete__ = classmethod(_pydantic_fields_complete)\n        cls.__pydantic_complete__ = False\n        _pydantic_dataclasses.complete_dataclass(cls, config_wrapper, raise_errors=False)\n        return cls", "target": "def test_dataclass_json():\n    schema = core_schema.dataclass_schema(\n        FooDataclass,\n        core_schema.dataclass_args_schema(\n            'FooDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    assert v.validate_json('{\"a\": \"hello\", \"b\": true}') == FooDataclass(a='hello', b=True)\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_json('[\"a\", \"b\"]')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'class_name': 'FooDataclass'},\n            'input': ['a', 'b'],\n            'loc': (),\n            'msg': 'Input should be an object',\n            'type': 'dataclass_type',\n        }\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004778", "source": "def dataclass(\n        *,\n        init: bool = True,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool = False,\n        config: Union[ConfigDict, Type[object], None] = None,\n        validate_on_init: Optional[bool] = None,\n        use_proxy: Optional[bool] = None,\n        kw_only: bool = ...,\n    ) -> Callable[[Type[_T]], 'DataclassClassOrWrapper']:\n        ...", "target": "def test_dataclass_subclass_subclass_revalidate():\n    v = SchemaValidator(\n        core_schema.dataclass_schema(\n            FooDataclass,\n            core_schema.dataclass_args_schema(\n                'FooDataclass',\n                [\n                    core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                    core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n                ],\n            ),\n            ['a', 'b'],\n            revalidate_instances='subclass-instances',\n            strict=True,\n        )\n    )\n    foo = FooDataclass(a='hello', b=True)\n    assert v.validate_python(foo) is foo\n    sub_foo = FooDataclassSame(a='hello', b='True')\n    sub_foo2 = v.validate_python(sub_foo)\n    assert sub_foo2 is not sub_foo\n    assert type(sub_foo2) is FooDataclass\n    assert dataclasses.asdict(sub_foo2) == dict(a='hello', b=True)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004779", "source": "def set_validation(cls: Type['DataclassT'], value: bool) -> Generator[Type['DataclassT'], None, None]:\n    original_run_validation = cls.__pydantic_run_validation__\n    try:\n        cls.__pydantic_run_validation__ = value\n        yield cls\n    finally:\n        cls.__pydantic_run_validation__ = original_run_validation", "target": "def test_dataclass_exact_validation(revalidate_instances, input_value, expected):\n    schema = core_schema.dataclass_schema(\n        FooDataclass,\n        core_schema.dataclass_args_schema(\n            'FooDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n            ],\n        ),\n        ['a', 'b'],\n        revalidate_instances=revalidate_instances,\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python(input_value)\n    assert dataclasses.asdict(foo) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004780", "source": "def _generic_get_args(tp: Type[Any]) -> Tuple[Any, ...]:\n        if hasattr(tp, '_nparams'):\n            return (Any,) * tp._nparams\n        try:\n            if tp == Tuple[()] or sys.version_info >= (3, 9) and tp == tuple[()]:\n                return ((),)\n        except TypeError:\n            pass\n        return ()", "target": "def test_ser_function_wrap():\n    def f(\n        input: Any, serialize: core_schema.SerializerFunctionWrapHandler, info: core_schema.SerializationInfo, /\n    ) -> str:\n        return f'{serialize} {info}'\n    s = SchemaSerializer(\n        core_schema.any_schema(\n            serialization=core_schema.wrap_serializer_function_ser_schema(\n                f, info_arg=True, schema=core_schema.str_schema(), when_used='json'\n            )\n        )\n    )\n    assert s.to_python(123, mode='json') == (\n        'SerializationCallable(serializer=str) '\n        \"SerializationInfo(include=None, exclude=None, context=None, mode='json', by_alias=False, exclude_unset=False, \"\n        'exclude_defaults=False, exclude_none=False, exclude_computed_fields=False, round_trip=False, serialize_as_any=False)'\n    )"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004781", "source": "def get_git_remote_name() -> str:\n    return os.getenv(\"GIT_REMOTE_NAME\", \"origin\")", "target": "def test_compute_diff(self) -> None:\n        diff = self.repo.diff(\"HEAD\")\n        sha = _shasum(diff)\n        self.assertEqual(len(sha), 64)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004782", "source": "def _validate_dataclass(cls: Type['DataclassT'], v: Any) -> 'DataclassT':\n    with set_validation(cls, True):\n        if isinstance(v, cls):\n            v.__pydantic_validate_values__()\n            return v\n        elif isinstance(v, (list, tuple)):\n            return cls(*v)\n        elif isinstance(v, dict):\n            return cls(**v)\n        else:\n            raise DataclassTypeError(class_name=cls.__name__)", "target": "def test_dataclass_wrap_json():\n    schema = core_schema.no_info_wrap_validator_function(\n        lambda v, handler: handler(v),\n        core_schema.dataclass_schema(\n            FooDataclass,\n            core_schema.dataclass_args_schema(\n                'FooDataclass',\n                [\n                    core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                    core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n                ],\n            ),\n            ['a', 'b'],\n        ),\n    )\n    v = SchemaValidator(schema)\n    assert v.validate_json('{\"a\": \"hello\", \"b\": true}') == FooDataclass(a='hello', b=True)\n    assert v.validate_json('{\"a\": \"hello\", \"b\": true}', strict=True) == FooDataclass(a='hello', b=True)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004783", "source": "def dataclass(\n        *,\n        init: Literal[False] = False,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool | None = None,\n        config: ConfigDict | type[object] | None = None,\n        validate_on_init: bool | None = None,\n    ) -> Callable[[type[_T]], type[PydanticDataclass]]:\n        ...", "target": "def test_dataclass_subclass_subclass_revalidate():\n    v = SchemaValidator(\n        core_schema.dataclass_schema(\n            FooDataclass,\n            core_schema.dataclass_args_schema(\n                'FooDataclass',\n                [\n                    core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                    core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n                ],\n            ),\n            ['a', 'b'],\n            revalidate_instances='subclass-instances',\n            strict=True,\n        )\n    )\n    foo = FooDataclass(a='hello', b=True)\n    assert v.validate_python(foo) is foo\n    sub_foo = FooDataclassSame(a='hello', b='True')\n    sub_foo2 = v.validate_python(sub_foo)\n    assert sub_foo2 is not sub_foo\n    assert type(sub_foo2) is FooDataclass\n    assert dataclasses.asdict(sub_foo2) == dict(a='hello', b=True)"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004784", "source": "def decorator(f: Callable[..., T]) -> Callable[..., T]:\n        @wraps(f)\n        def wrapper(*args: list[Any], **kwargs: dict[str, Any]) -> T:\n            for idx in range(num_retries):\n                try:\n                    return f(*args, **kwargs)\n                except Exception as e:\n                    print(\n                        f'Attempt {idx} of {num_retries} to call {f.__name__} failed with \"{e}\"'\n                    )\n            return cast(T, rc)\n        return wrapper", "target": "def test_fails(self) -> None:\n        @retries_decorator(rc=0)\n        def foo(x: int, y: int) -> int:\n            return x + y\n        self.assertEqual(foo(\"a\", 4), 0)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004785", "source": "def _dataclass_setstate(self: Any, state: list[Any]) -> None:\n                    for field, value in zip(dataclasses.fields(self), state):\n                        object.__setattr__(self, field.name, value)", "target": "def test_dataclass_post_init():\n    @dataclasses.dataclass\n    class Foo:\n        a: str\n        b: bool\n        def __post_init__(self):\n            self.a = self.a.upper()\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n            ],\n        ),\n        ['a', 'b'],\n        post_init=True,\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 'hello', 'b': True})\n    assert foo.a == 'HELLO'\n    assert foo.b is True"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004786", "source": "def _apply_field_title_generator_to_field_info(\n    title_generator: Callable[[str, FieldInfo], str],\n    field_name: str,\n    field_info: FieldInfo,\n):\n    if field_info.title is None:\n        title = title_generator(field_name, field_info)\n        if not isinstance(title, str):\n            raise TypeError(f'field_title_generator {title_generator} must return str, not {title.__class__}')\n        field_info.title = title", "target": "def test_missing_error(pydantic_version):\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'field_a': core_schema.model_field(schema=core_schema.str_schema()),\n                'field_b': core_schema.model_field(schema=core_schema.int_schema()),\n            }\n        )\n    )\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python({'field_a': b'abc'})\n    assert (\n        str(exc_info.value)\n        ==\n        + (\n            f'\\n    For further information visit https://errors.pydantic.dev/{pydantic_version}/v/missing'\n            if os.environ.get('PYDANTIC_ERRORS_INCLUDE_URL', '1') != 'false'\n            else ''\n        )\n    )"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004787", "source": "def _dataclass_getstate(self: Any) -> list[Any]:\n                    return [getattr(self, f.name) for f in dataclasses.fields(self)]", "target": "def test_dataclass_args_init(input_value, extra_behavior, expected):\n    @dataclasses.dataclass\n    class Foo:\n        a: str\n        b: str\n        def __post_init__(self):\n            self.b = self.a.upper()\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.str_schema(), init=False),\n            ],\n            extra_behavior=extra_behavior,\n        ),\n        ['a', 'b'],\n        post_init=True,\n    )\n    v = SchemaValidator(schema)\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)) as exc_info:\n            v.validate_python(input_value)\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        assert dataclasses.asdict(v.validate_python(input_value)) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004788", "source": "def check_for_functorch():\n    try:\n        import functorch\n        return True\n    except ImportError:\n        return False", "target": "def test_get_last_page_num_from_header(self) -> None:\n        for (\n            expected_page_num,\n            mock_header,\n        ) in self.MOCK_HEADER_LINKS_TO_PAGE_NUMS.items():\n            self.assertEqual(\n                get_last_page_num_from_header(mock_header), expected_page_num\n            )"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004789", "source": "def _dataclass_validate_values(self: 'Dataclass') -> None:\n    if getattr(self, '__pydantic_initialised__'):\n        return\n    if getattr(self, '__pydantic_has_field_info_default__', False):\n        input_data = {\n            k: v\n            for k, v in self.__dict__.items()\n            if not (isinstance(v, FieldInfo) or _is_field_cached_property(self, k))\n        }\n    else:\n        input_data = {k: v for k, v in self.__dict__.items() if not _is_field_cached_property(self, k)}\n    d, _, validation_error = validate_model(self.__pydantic_model__, input_data, cls=self.__class__)\n    if validation_error:\n        raise validation_error\n    self.__dict__.update(d)\n    object.__setattr__(self, '__pydantic_initialised__', True)", "target": "def test_dataclass():\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bytes_schema()),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    s = SchemaSerializer(schema)\n    assert s.to_python(Foo(a='hello', b=b'more')) == IsStrictDict(a='hello', b=b'more')\n    assert s.to_python(Foo(a='hello', b=b'more'), mode='json') == IsStrictDict(a='hello', b='more')\n    j = s.to_json(Foo(a='hello', b=b'more'))\n    if on_pypy:\n        assert json.loads(j) == {'a': 'hello', 'b': 'more'}\n    else:\n        assert j == b'{\"a\":\"hello\",\"b\":\"more\"}'\n    assert s.to_python(Foo(a='hello', b=b'more'), exclude={'b'}) == IsStrictDict(a='hello')\n    assert s.to_json(Foo(a='hello', b=b'more'), include={'a'}) == b'{\"a\":\"hello\"}'"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004790", "source": "def new_init(self: 'Dataclass', *args: Any, **kwargs: Any) -> None:\n            handle_extra_init(self, *args, **kwargs)\n            if self.__class__.__pydantic_run_validation__:\n                self.__pydantic_validate_values__()\n            if hasattr(self, '__post_init_post_parse__'):\n                initvars_and_values: Dict[str, Any] = {}\n                for i, f in enumerate(self.__class__.__dataclass_fields__.values()):\n                    if f._field_type is dataclasses._FIELD_INITVAR:\n                        try:\n                            initvars_and_values[f.name] = args[i]\n                        except IndexError:\n                            initvars_and_values[f.name] = kwargs.get(f.name, f.default)\n                self.__post_init_post_parse__(**initvars_and_values)", "target": "def test_dataclass_args_init_with_default(input_value, extra_behavior, expected):\n    @dataclasses.dataclass\n    class Foo:\n        a: str\n        b: str\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(\n                    name='b',\n                    schema=core_schema.with_default_schema(schema=core_schema.str_schema(), default='HELLO'),\n                    init=False,\n                ),\n            ],\n            extra_behavior=extra_behavior,\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)) as exc_info:\n            v.validate_python(input_value)\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        assert dataclasses.asdict(v.validate_python(input_value)) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004791", "source": "def _check_protected_namespaces(\n    protected_namespaces: tuple[str | Pattern[str], ...],\n    ann_name: str,\n    bases: tuple[type[Any], ...],\n    cls_name: str,\n) -> None:\n    BaseModel = import_cached_base_model()\n    for protected_namespace in protected_namespaces:\n        ns_violation = False\n        if isinstance(protected_namespace, Pattern):\n            ns_violation = protected_namespace.match(ann_name) is not None\n        elif isinstance(protected_namespace, str):\n            ns_violation = ann_name.startswith(protected_namespace)\n        if ns_violation:\n            for b in bases:\n                if hasattr(b, ann_name):\n                    if not (issubclass(b, BaseModel) and ann_name in getattr(b, '__pydantic_fields__', {})):\n                        raise ValueError(\n                            f'Field {ann_name!r} conflicts with member {getattr(b, ann_name)}'\n                            f' of protected namespace {protected_namespace!r}.'\n                        )\n            else:\n                valid_namespaces: list[str] = []\n                for pn in protected_namespaces:\n                    if isinstance(pn, Pattern):\n                        if not pn.match(ann_name):\n                            valid_namespaces.append(f're.compile({pn.pattern!r})')\n                    else:\n                        if not ann_name.startswith(pn):\n                            valid_namespaces.append(f\"'{pn}'\")\n                valid_namespaces_str = f'({\", \".join(valid_namespaces)}{\",)\" if len(valid_namespaces) == 1 else \")\"}'\n                warnings.warn(\n                    f'Field {ann_name!r} in {cls_name!r} conflicts with protected namespace {protected_namespace!r}.\\n\\n'\n                    f\"You may be able to solve this by setting the 'protected_namespaces' configuration to {valid_namespaces_str}.\",\n                    UserWarning,\n                    stacklevel=5,\n                )", "target": "def test_validate_assignment():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(fields={'field_a': core_schema.model_field(schema=core_schema.str_schema())})\n    )\n    assert v.validate_python({'field_a': 'test'}) == ({'field_a': 'test'}, None, {'field_a'})\n    data = {'field_a': 'test'}\n    assert v.validate_assignment(data, 'field_a', b'abc') == ({'field_a': 'abc'}, None, {'field_a'})\n    assert data == {'field_a': 'abc'}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004792", "source": "def _apply_field_title_generator_to_field_info(\n    title_generator: Callable[[str, FieldInfo], str],\n    field_name: str,\n    field_info: FieldInfo,\n):\n    if field_info.title is None:\n        title = title_generator(field_name, field_info)\n        if not isinstance(title, str):\n            raise TypeError(f'field_title_generator {title_generator} must return str, not {title.__class__}')\n        field_info.title = title", "target": "def test_allow_extra_invalid():\n    with pytest.raises(SchemaError, match='extras_schema can only be used if extra_behavior=allow'):\n        SchemaValidator(\n            schema=core_schema.model_fields_schema(\n                fields={}, extras_schema=core_schema.int_schema(), extra_behavior='ignore'\n            )\n        )\n    with pytest.raises(SchemaError, match='extras_keys_schema can only be used if extra_behavior=allow'):\n        SchemaValidator(\n            schema=core_schema.model_fields_schema(\n                fields={}, extras_keys_schema=core_schema.int_schema(), extra_behavior='ignore'\n            )\n        )"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004793", "source": "def frozenset_validator(v: Any) -> FrozenSet[Any]:\n    if isinstance(v, frozenset):\n        return v\n    elif sequence_like(v):\n        return frozenset(v)\n    else:\n        raise errors.FrozenSetError()", "target": "def test_validators_build(benchmark) -> None:\n    class Base1(BaseModel):\n        a: int\n        @field_validator('a', mode='after')\n        @classmethod\n        def val_a(cls, value: int) -> int: ...\n        @computed_field\n        def prop(self) -> int: ...\n    class Bare:\n        @computed_field\n        def prop_bare(self) -> int: ...\n    class Sub1(Base1):\n        @computed_field\n        def prop_2(self) -> int: ...\n        @computed_field\n        def prop_3(self) -> int: ...\n        @computed_field\n        def prop_4(self) -> int: ...\n    @benchmark\n    def bench() -> None:\n        class SubS(Sub1, Bare, defer_build=True):\n            @computed_field\n            def prop_5(self) -> int: ...\n            @computed_field\n            def prop_6(self) -> int: ...\n            @computed_field\n            def prop_7(self) -> int: ..."}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004794", "source": "def _is_field_cached_property(obj: 'Dataclass', k: str) -> bool:\n        return False", "target": "def test_function_validator_wrapping_args_schema_wrap() -> None:\n    calls: list[Any] = []\n    def func(*args: Any) -> Any:\n        assert len(args) == 2\n        input, handler = args\n        output = handler(input)\n        calls.append((input, output))\n        return output\n    @dataclasses.dataclass\n    class Model:\n        number: int = 1\n    cs = core_schema.dataclass_schema(\n        Model,\n        core_schema.no_info_wrap_validator_function(\n            func,\n            core_schema.dataclass_args_schema(\n                'Model', [core_schema.dataclass_field('number', core_schema.int_schema())]\n            ),\n        ),\n        ['number'],\n    )\n    v = SchemaValidator(cs)\n    instance: Model = v.validate_python({'number': 1})\n    assert instance.number == 1\n    assert calls == [({'number': 1}, ({'number': 1}, None))]\n    v.validate_assignment(instance, 'number', 2)\n    assert instance.number == 2\n    assert calls == [({'number': 1}, ({'number': 1}, None)), ({'number': 2}, ({'number': 2}, None))]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004795", "source": "def _add_pydantic_validation_attributes(\n    dc_cls: Type['Dataclass'],\n    config: Type[BaseConfig],\n    validate_on_init: bool,\n    dc_cls_doc: str,\n) -> None:\n    init = dc_cls.__init__\n    @wraps(init)\n    def handle_extra_init(self: 'Dataclass', *args: Any, **kwargs: Any) -> None:\n        if config.extra == Extra.ignore:\n            init(self, *args, **{k: v for k, v in kwargs.items() if k in self.__dataclass_fields__})\n        elif config.extra == Extra.allow:\n            for k, v in kwargs.items():\n                self.__dict__.setdefault(k, v)\n            init(self, *args, **{k: v for k, v in kwargs.items() if k in self.__dataclass_fields__})\n        else:\n            init(self, *args, **kwargs)\n    if hasattr(dc_cls, '__post_init__'):\n        try:\n            post_init = dc_cls.__post_init__.__wrapped__\n        except AttributeError:\n            post_init = dc_cls.__post_init__\n        @wraps(post_init)\n        def new_post_init(self: 'Dataclass', *args: Any, **kwargs: Any) -> None:\n            if config.post_init_call == 'before_validation':\n                post_init(self, *args, **kwargs)\n            if self.__class__.__pydantic_run_validation__:\n                self.__pydantic_validate_values__()\n                if hasattr(self, '__post_init_post_parse__'):\n                    self.__post_init_post_parse__(*args, **kwargs)\n            if config.post_init_call == 'after_validation':\n                post_init(self, *args, **kwargs)\n        setattr(dc_cls, '__init__', handle_extra_init)\n        setattr(dc_cls, '__post_init__', new_post_init)\n    else:\n        @wraps(init)\n        def new_init(self: 'Dataclass', *args: Any, **kwargs: Any) -> None:\n            handle_extra_init(self, *args, **kwargs)\n            if self.__class__.__pydantic_run_validation__:\n                self.__pydantic_validate_values__()\n            if hasattr(self, '__post_init_post_parse__'):\n                initvars_and_values: Dict[str, Any] = {}\n                for i, f in enumerate(self.__class__.__dataclass_fields__.values()):\n                    if f._field_type is dataclasses._FIELD_INITVAR:\n                        try:\n                            initvars_and_values[f.name] = args[i]\n                        except IndexError:\n                            initvars_and_values[f.name] = kwargs.get(f.name, f.default)\n                self.__post_init_post_parse__(**initvars_and_values)\n        setattr(dc_cls, '__init__', new_init)\n    setattr(dc_cls, '__pydantic_run_validation__', ClassAttribute('__pydantic_run_validation__', validate_on_init))\n    setattr(dc_cls, '__pydantic_initialised__', False)\n    setattr(dc_cls, '__pydantic_model__', create_pydantic_model_from_dataclass(dc_cls, config, dc_cls_doc))\n    setattr(dc_cls, '__pydantic_validate_values__', _dataclass_validate_values)\n    setattr(dc_cls, '__validate__', classmethod(_validate_dataclass))\n    setattr(dc_cls, '__get_validators__', classmethod(_get_validators))\n    if dc_cls.__pydantic_model__.__config__.validate_assignment and not dc_cls.__dataclass_params__.frozen:\n        setattr(dc_cls, '__setattr__', _dataclass_validate_assignment_setattr)", "target": "def test_function_validator_wrapping_args_schema_after() -> None:\n    calls: list[Any] = []\n    def func(*args: Any) -> Any:\n        calls.append(args)\n        return args[0]\n    @dataclasses.dataclass\n    class Model:\n        number: int = 1\n    cs = core_schema.dataclass_schema(\n        Model,\n        core_schema.no_info_after_validator_function(\n            func,\n            core_schema.dataclass_args_schema(\n                'Model', [core_schema.dataclass_field('number', core_schema.int_schema())]\n            ),\n        ),\n        ['number'],\n    )\n    v = SchemaValidator(cs)\n    instance: Model = v.validate_python({'number': 1})\n    assert instance.number == 1\n    assert calls == [(({'number': 1}, None),)]\n    v.validate_assignment(instance, 'number', 2)\n    assert instance.number == 2\n    assert calls == [(({'number': 1}, None),), (({'number': 2}, None),)]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004796", "source": "def _pydantic_fields_complete(cls: type[PydanticDataclass]) -> bool:\n    return all(field_info._complete for field_info in cls.__pydantic_fields__.values())", "target": "def test_dataclass_args_init_with_default(input_value, extra_behavior, expected):\n    @dataclasses.dataclass\n    class Foo:\n        a: str\n        b: str\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(\n                    name='b',\n                    schema=core_schema.with_default_schema(schema=core_schema.str_schema(), default='HELLO'),\n                    init=False,\n                ),\n            ],\n            extra_behavior=extra_behavior,\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)) as exc_info:\n            v.validate_python(input_value)\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        assert dataclasses.asdict(v.validate_python(input_value)) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004797", "source": "def dataclass(\n        *,\n        init: bool = True,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool = False,\n        config: Union[ConfigDict, Type[object], None] = None,\n        validate_on_init: Optional[bool] = None,\n        use_proxy: Optional[bool] = None,\n    ) -> Callable[[Type[_T]], 'DataclassClassOrWrapper']:\n        ...", "target": "def test_dataclass_args_init_with_default(input_value, extra_behavior, expected):\n    @dataclasses.dataclass\n    class Foo:\n        a: str\n        b: str\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(\n                    name='b',\n                    schema=core_schema.with_default_schema(schema=core_schema.str_schema(), default='HELLO'),\n                    init=False,\n                ),\n            ],\n            extra_behavior=extra_behavior,\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)) as exc_info:\n            v.validate_python(input_value)\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        assert dataclasses.asdict(v.validate_python(input_value)) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004798", "source": "def secs_to_ms(time_s):\n    return time_s * 1e3", "target": "def test_merged_lastfailed_content_without_overlap(self) -> None:\n        last_failed_source = {\n            \"tools/tests/test_foo.py::test_num1\": True,\n            \"tools/tests/test_foo.py::test_num2\": True,\n            \"tools/tests/test_bar.py::test_num1\": True,\n        }\n        last_failed_dest = {\n            \"tools/tests/test_car.py::test_num1\": True,\n            \"tools/tests/test_car.py::test_num2\": True,\n        }\n        last_failed_merged = {\n            \"tools/tests/test_foo.py::test_num1\": True,\n            \"tools/tests/test_foo.py::test_num2\": True,\n            \"tools/tests/test_bar.py::test_num1\": True,\n            \"tools/tests/test_car.py::test_num1\": True,\n            \"tools/tests/test_car.py::test_num2\": True,\n        }\n        merged = _merged_lastfailed_content(last_failed_source, last_failed_dest)\n        self.assertEqual(merged, last_failed_merged)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004799", "source": "def int_validator(v: Any) -> int:\n    if isinstance(v, int) and not (v is True or v is False):\n        return v\n    if isinstance(v, (str, bytes, bytearray)) and len(v) > max_str_int:\n        raise errors.IntegerError()\n    try:\n        return int(v)\n    except (TypeError, ValueError, OverflowError):\n        raise errors.IntegerError()", "target": "def test_validators_build(benchmark) -> None:\n    class Base1(BaseModel):\n        a: int\n        @field_validator('a', mode='after')\n        @classmethod\n        def val_a(cls, value: int) -> int: ...\n        @computed_field\n        def prop(self) -> int: ...\n    class Bare:\n        @computed_field\n        def prop_bare(self) -> int: ...\n    class Sub1(Base1):\n        @computed_field\n        def prop_2(self) -> int: ...\n        @computed_field\n        def prop_3(self) -> int: ...\n        @computed_field\n        def prop_4(self) -> int: ...\n    @benchmark\n    def bench() -> None:\n        class SubS(Sub1, Bare, defer_build=True):\n            @computed_field\n            def prop_5(self) -> int: ...\n            @computed_field\n            def prop_6(self) -> int: ...\n            @computed_field\n            def prop_7(self) -> int: ..."}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004800", "source": "def rounded_linspace(low, high, steps, div):\n    ret = torch.linspace(low, high, steps)\n    ret = (ret.int() + div - 1) // div * div\n    ret = torch.unique(ret)\n    return list(map(int, ret))", "target": "def test_pr_with_missing_labels(\n        self, mocked_rn_labels: Any, mocked_gql: Any\n    ) -> None:\n        \"Test PR with no 'release notes:' label or 'topic: not user facing' label\"\n        pr = GitHubPR(\"pytorch\", \"pytorch\", 82169)\n        self.assertFalse(has_required_labels(pr))"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004801", "source": "def is_typeddict_special(type_: Any) -> bool:\n    return _check_typeddict_special(type_) or _check_typeddict_special(get_origin(type_))", "target": "def test_correct_function_signature() -> None:\n    def my_validator(value: Any, info: Any) -> str:\n        return str(value)\n    v = SchemaValidator(core_schema.with_info_plain_validator_function(my_validator))\n    assert v.validate_python(1) == '1'"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004802", "source": "def isoformat(o: Union[datetime.date, datetime.time]) -> str:\n    return o.isoformat()", "target": "def test_cycle_change():\n    def fallback_func_change_id(obj):\n        return Foobar()\n    f = Foobar()\n    with pytest.raises(ValueError, match=r'Circular reference detected \\(depth exceeded\\)'):\n        to_jsonable_python(f, fallback=fallback_func_change_id)\n    with pytest.raises(ValueError, match=r'Circular reference detected \\(depth exceeded\\)'):\n        to_json(f, fallback=fallback_func_change_id)"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004803", "source": "def fetch(self, ref: Optional[str] = None, branch: Optional[str] = None) -> None:\n        if branch is None and ref is None:\n            self._run_git(\"fetch\", self.remote)\n        elif branch is None:\n            self._run_git(\"fetch\", self.remote, ref)\n        else:\n            self._run_git(\"fetch\", self.remote, f\"{ref}:{branch}\")", "target": "def test_compute_diff(self) -> None:\n        diff = self.repo.diff(\"HEAD\")\n        sha = _shasum(diff)\n        self.assertEqual(len(sha), 64)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004804", "source": "def _is_field_cached_property(obj: 'Dataclass', k: str) -> bool:\n        return False", "target": "def test_dataclass_post_init_args():\n    c_value = None\n    @dataclasses.dataclass\n    class Foo:\n        a: str\n        b: bool\n        c: dataclasses.InitVar[int]\n        def __post_init__(self, c: int):\n            nonlocal c_value\n            c_value = c\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n                core_schema.dataclass_field(name='c', schema=core_schema.int_schema(), init_only=True),\n            ],\n            collect_init_only=True,\n        ),\n        ['a', 'b'],\n        post_init=True,\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': b'hello', 'b': 'true', 'c': '42'})\n    assert foo.a == 'hello'\n    assert foo.b is True\n    assert not hasattr(foo, 'c')\n    assert c_value == 42"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004805", "source": "def _is_field_cached_property(obj: 'Dataclass', k: str) -> bool:\n        return isinstance(getattr(type(obj), k, None), cached_property)", "target": "def test_dataclass_post_init_args():\n    c_value = None\n    @dataclasses.dataclass\n    class Foo:\n        a: str\n        b: bool\n        c: dataclasses.InitVar[int]\n        def __post_init__(self, c: int):\n            nonlocal c_value\n            c_value = c\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n                core_schema.dataclass_field(name='c', schema=core_schema.int_schema(), init_only=True),\n            ],\n            collect_init_only=True,\n        ),\n        ['a', 'b'],\n        post_init=True,\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': b'hello', 'b': 'true', 'c': '42'})\n    assert foo.a == 'hello'\n    assert foo.b is True\n    assert not hasattr(foo, 'c')\n    assert c_value == 42"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004806", "source": "def is_pydantic_dataclass(class_: type[Any], /) -> TypeGuard[type[PydanticDataclass]]:\n    try:\n        return '__is_pydantic_dataclass__' in class_.__dict__ and dataclasses.is_dataclass(class_)\n    except AttributeError:\n        return False", "target": "def test_dataclass():\n    schema = core_schema.dataclass_schema(\n        FooDataclass,\n        core_schema.dataclass_args_schema(\n            'FooDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 'hello', 'b': True})\n    assert dataclasses.is_dataclass(foo)\n    assert foo.a == 'hello'\n    assert foo.b is True\n    assert dataclasses.asdict(v.validate_python(FooDataclass(a='hello', b=True))) == {'a': 'hello', 'b': True}\n    with pytest.raises(ValidationError, match='Input should be an instance of FooDataclass') as exc_info:\n        v.validate_python({'a': 'hello', 'b': True}, strict=True)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'dataclass_exact_type',\n            'loc': (),\n            'msg': 'Input should be an instance of FooDataclass',\n            'input': {'a': 'hello', 'b': True},\n            'ctx': {'class_name': 'FooDataclass'},\n        }\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004807", "source": "def _apply_field_title_generator_to_field_info(\n    title_generator: Callable[[str, FieldInfo], str],\n    field_name: str,\n    field_info: FieldInfo,\n):\n    if field_info.title is None:\n        title = title_generator(field_name, field_info)\n        if not isinstance(title, str):\n            raise TypeError(f'field_title_generator {title_generator} must return str, not {title.__class__}')\n        field_info.title = title", "target": "def test_aliases_debug():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'field_a': core_schema.model_field(\n                    validation_alias=[['foo', 'bar', 'bat'], ['foo', 3]], schema=core_schema.int_schema()\n                )\n            }\n        )\n    )\n    print(repr(v))\n    assert repr(v).startswith('SchemaValidator(title=\"model-fields\", validator=ModelFields(')\n    assert 'PathChoices(' in repr(v)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004808", "source": "def _dataclass_getstate(self: Any) -> list[Any]:\n                    return [getattr(self, f.name) for f in dataclasses.fields(self)]", "target": "def test_dataclass():\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bytes_schema()),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    s = SchemaSerializer(schema)\n    assert s.to_python(Foo(a='hello', b=b'more')) == IsStrictDict(a='hello', b=b'more')\n    assert s.to_python(Foo(a='hello', b=b'more'), mode='json') == IsStrictDict(a='hello', b='more')\n    j = s.to_json(Foo(a='hello', b=b'more'))\n    if on_pypy:\n        assert json.loads(j) == {'a': 'hello', 'b': 'more'}\n    else:\n        assert j == b'{\"a\":\"hello\",\"b\":\"more\"}'\n    assert s.to_python(Foo(a='hello', b=b'more'), exclude={'b'}) == IsStrictDict(a='hello')\n    assert s.to_json(Foo(a='hello', b=b'more'), include={'a'}) == b'{\"a\":\"hello\"}'"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004809", "source": "def timedelta_isoformat(td: datetime.timedelta) -> str:\n    minutes, seconds = divmod(td.seconds, 60)\n    hours, minutes = divmod(minutes, 60)\n    return f'{\"-\" if td.days < 0 else \"\"}P{abs(td.days)}DT{hours:d}H{minutes:d}M{seconds:d}.{td.microseconds:06d}S'", "target": "def test_cycle_change():\n    def fallback_func_change_id(obj):\n        return Foobar()\n    f = Foobar()\n    with pytest.raises(ValueError, match=r'Circular reference detected \\(depth exceeded\\)'):\n        to_jsonable_python(f, fallback=fallback_func_change_id)\n    with pytest.raises(ValueError, match=r'Circular reference detected \\(depth exceeded\\)'):\n        to_json(f, fallback=fallback_func_change_id)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004810", "source": "def _dataclass_validate_assignment_setattr(self: 'Dataclass', name: str, value: Any) -> None:\n    if self.__pydantic_initialised__:\n        d = dict(self.__dict__)\n        d.pop(name, None)\n        known_field = self.__pydantic_model__.__fields__.get(name, None)\n        if known_field:\n            value, error_ = known_field.validate(value, d, loc=name, cls=self.__class__)\n            if error_:\n                raise ValidationError([error_], self.__class__)\n    object.__setattr__(self, name, value)", "target": "def test_dataclass_post_init_args():\n    c_value = None\n    @dataclasses.dataclass\n    class Foo:\n        a: str\n        b: bool\n        c: dataclasses.InitVar[int]\n        def __post_init__(self, c: int):\n            nonlocal c_value\n            c_value = c\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n                core_schema.dataclass_field(name='c', schema=core_schema.int_schema(), init_only=True),\n            ],\n            collect_init_only=True,\n        ),\n        ['a', 'b'],\n        post_init=True,\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': b'hello', 'b': 'true', 'c': '42'})\n    assert foo.a == 'hello'\n    assert foo.b is True\n    assert not hasattr(foo, 'c')\n    assert c_value == 42"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004811", "source": "def set_validation(cls: Type['DataclassT'], value: bool) -> Generator[Type['DataclassT'], None, None]:\n    original_run_validation = cls.__pydantic_run_validation__\n    try:\n        cls.__pydantic_run_validation__ = value\n        yield cls\n    finally:\n        cls.__pydantic_run_validation__ = original_run_validation", "target": "def test_properties():\n    @dataclasses.dataclass\n    class FooProp:\n        a: str\n        b: bytes\n        @property\n        def c(self) -> str:\n            return f'{self.a} {self.b.decode()}'\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'FooProp',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bytes_schema()),\n            ],\n            computed_fields=[core_schema.computed_field('c', core_schema.str_schema())],\n        ),\n        ['a', 'b'],\n    )\n    s = SchemaSerializer(schema)\n    assert s.to_python(FooProp(a='hello', b=b'more')) == IsStrictDict(a='hello', b=b'more', c='hello more')\n    assert s.to_python(FooProp(a='hello', b=b'more'), mode='json') == IsStrictDict(a='hello', b='more', c='hello more')\n    j = s.to_json(FooProp(a='hello', b=b'more'))\n    if on_pypy:\n        assert json.loads(j) == {'a': 'hello', 'b': 'more', 'c': 'hello more'}\n    else:\n        assert j == b'{\"a\":\"hello\",\"b\":\"more\",\"c\":\"hello more\"}'\n    assert s.to_python(FooProp(a='hello', b=b'more'), exclude={'b'}) == IsStrictDict(a='hello', c='hello more')\n    assert s.to_json(FooProp(a='hello', b=b'more'), include={'a'}) == b'{\"a\":\"hello\"}'"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004812", "source": "def _is_field_cached_property(obj: 'Dataclass', k: str) -> bool:\n        return False", "target": "def test_slots_mixed():\n    @dataclasses.dataclass(slots=True)\n    class Model:\n        x: int\n        y: dataclasses.InitVar[str]\n        z: ClassVar[str] = 'z-classvar'\n    @dataclasses.dataclass\n    class SubModel(Model):\n        x2: int\n        y2: dataclasses.InitVar[str]\n        z2: ClassVar[str] = 'z2-classvar'\n    schema = core_schema.dataclass_schema(\n        SubModel,\n        core_schema.dataclass_args_schema(\n            'SubModel',\n            [\n                core_schema.dataclass_field(name='x', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(name='y', init_only=True, schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='x2', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(name='y2', init_only=True, schema=core_schema.str_schema()),\n            ],\n        ),\n        ['x'],\n        slots=True,\n    )\n    v = SchemaValidator(schema)\n    dc = v.validate_python({'x': 1, 'y': 'a', 'x2': 2, 'y2': 'b'})\n    assert dc.x == 1\n    assert dc.x2 == 2\n    assert dataclasses.asdict(dc) == {'x': 1, 'x2': 2}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004813", "source": "def new_post_init(self: 'Dataclass', *args: Any, **kwargs: Any) -> None:\n            if config.post_init_call == 'before_validation':\n                post_init(self, *args, **kwargs)\n            if self.__class__.__pydantic_run_validation__:\n                self.__pydantic_validate_values__()\n                if hasattr(self, '__post_init_post_parse__'):\n                    self.__post_init_post_parse__(*args, **kwargs)\n            if config.post_init_call == 'after_validation':\n                post_init(self, *args, **kwargs)", "target": "def test_only_validate_by_name(py_and_json) -> None:\n    schema = core_schema.dataclass_schema(\n        BasicDataclass,\n        core_schema.dataclass_args_schema(\n            'BasicDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), validation_alias='FieldA'),\n            ],\n        ),\n        ['a'],\n        config=core_schema.CoreConfig(validate_by_name=True, validate_by_alias=False),\n    )\n    v = py_and_json(schema)\n    assert v.validate_test({'a': 'hello'}) == BasicDataclass(a='hello')\n    with pytest.raises(ValidationError, match=r'a\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'FieldA': 'hello'})"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004814", "source": "def new_init(self: 'Dataclass', *args: Any, **kwargs: Any) -> None:\n            handle_extra_init(self, *args, **kwargs)\n            if self.__class__.__pydantic_run_validation__:\n                self.__pydantic_validate_values__()\n            if hasattr(self, '__post_init_post_parse__'):\n                initvars_and_values: Dict[str, Any] = {}\n                for i, f in enumerate(self.__class__.__dataclass_fields__.values()):\n                    if f._field_type is dataclasses._FIELD_INITVAR:\n                        try:\n                            initvars_and_values[f.name] = args[i]\n                        except IndexError:\n                            initvars_and_values[f.name] = kwargs.get(f.name, f.default)\n                self.__post_init_post_parse__(**initvars_and_values)", "target": "def test_function_validator_wrapping_args_schema_after() -> None:\n    calls: list[Any] = []\n    def func(*args: Any) -> Any:\n        calls.append(args)\n        return args[0]\n    @dataclasses.dataclass\n    class Model:\n        number: int = 1\n    cs = core_schema.dataclass_schema(\n        Model,\n        core_schema.no_info_after_validator_function(\n            func,\n            core_schema.dataclass_args_schema(\n                'Model', [core_schema.dataclass_field('number', core_schema.int_schema())]\n            ),\n        ),\n        ['number'],\n    )\n    v = SchemaValidator(cs)\n    instance: Model = v.validate_python({'number': 1})\n    assert instance.number == 1\n    assert calls == [(({'number': 1}, None),)]\n    v.validate_assignment(instance, 'number', 2)\n    assert instance.number == 2\n    assert calls == [(({'number': 1}, None),), (({'number': 2}, None),)]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004815", "source": "def pydantic_general_metadata(**metadata: Any) -> BaseMetadata:\n    return _general_metadata_cls()(metadata)", "target": "def test_from_attributes(input_value, expected, from_attributes_mode):\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'a': core_schema.model_field(schema=core_schema.int_schema()),\n                'b': core_schema.model_field(schema=core_schema.int_schema()),\n                'c': core_schema.model_field(schema=core_schema.str_schema()),\n            },\n            from_attributes=from_attributes_mode == 'schema',\n        )\n    )\n    kwargs = {}\n    if from_attributes_mode == 'validation':\n        kwargs['from_attributes'] = True\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            val = v.validate_python(input_value, **kwargs)\n            print(f'UNEXPECTED OUTPUT: {val!r}')\n    else:\n        output = v.validate_python(input_value, **kwargs)\n        assert output == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004816", "source": "def is_stdlib_dataclass(cls: type[Any], /) -> TypeIs[type[StandardDataclass]]:\n    return '__dataclass_fields__' in cls.__dict__ and not hasattr(cls, '__pydantic_validator__')", "target": "def test_dataclass_self_init_post_init():\n    calls = []\n    @dataclasses.dataclass(init=False)\n    class Foo:\n        a: str\n        b: bool\n        c: dataclasses.InitVar[int]\n        def __init__(self, *args, **kwargs):\n            v.validate_python(ArgsKwargs(args, kwargs), self_instance=self)\n        def __post_init__(self, c):\n            calls.append(c)\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), kw_only=False),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), kw_only=False),\n                core_schema.dataclass_field(name='c', schema=core_schema.int_schema(), init_only=True),\n            ],\n            collect_init_only=True,\n        ),\n        ['a', 'b', 'c'],\n        post_init=True,\n    )\n    v = SchemaValidator(schema)\n    foo = Foo(b'hello', 'True', c='123')\n    assert dataclasses.is_dataclass(foo)\n    assert dataclasses.asdict(foo) == {'a': 'hello', 'b': True}\n    assert calls == [123]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004817", "source": "def pydantic_general_metadata(**metadata: Any) -> BaseMetadata:\n    return _general_metadata_cls()(metadata)", "target": "def test_validate_assignment():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(fields={'field_a': core_schema.model_field(schema=core_schema.str_schema())})\n    )\n    assert v.validate_python({'field_a': 'test'}) == ({'field_a': 'test'}, None, {'field_a'})\n    data = {'field_a': 'test'}\n    assert v.validate_assignment(data, 'field_a', b'abc') == ({'field_a': 'abc'}, None, {'field_a'})\n    assert data == {'field_a': 'abc'}"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004818", "source": "def benchmark_using_throughput_benchmark(config, module):\n    print(\"Benchmarking via ThroughputBenchmark\")\n    bench = ThroughputBenchmark(module.module)\n    bench.add_input(*module.tensor_inputs)\n    stats = bench.benchmark(1, config.num_warmup_iters, config.num_iters)\n    return stats.latency_avg_ms / NUM_LOOP_ITERS", "target": "def test_simple(self) -> None:\n        @retries_decorator()\n        def foo(x: int, y: int) -> int:\n            return x + y\n        self.assertEqual(foo(3, 4), 7)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004819", "source": "def is_builtin_dataclass(_cls: Type[Any]) -> bool:\n    return (\n        dataclasses.is_dataclass(_cls)\n        and not hasattr(_cls, '__pydantic_model__')\n        and set(_cls.__dataclass_fields__).issuperset(set(getattr(_cls, '__annotations__', {})))\n    )", "target": "def test_frozen():\n    @dataclasses.dataclass\n    class MyModel:\n        f: str\n    v = SchemaValidator(\n        core_schema.dataclass_schema(\n            MyModel,\n            core_schema.dataclass_args_schema('MyModel', [core_schema.dataclass_field('f', core_schema.str_schema())]),\n            ['f'],\n            frozen=True,\n        )\n    )\n    m = v.validate_python({'f': 'x'})\n    assert m.f == 'x'\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_assignment(m, 'f', 'y')\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'frozen_instance', 'loc': (), 'msg': 'Instance is frozen', 'input': 'y'}\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004820", "source": "def collect_model_fields(\n    cls: type[BaseModel],\n    config_wrapper: ConfigWrapper,\n    ns_resolver: NsResolver,\n    *,\n    typevars_map: Mapping[TypeVar, Any] | None = None,\n) -> tuple[dict[str, FieldInfo], set[str]]:\n    FieldInfo_ = import_cached_field_info()\n    BaseModel_ = import_cached_base_model()\n    bases = cls.__bases__\n    parent_fields_lookup: dict[str, FieldInfo] = {}\n    for base in reversed(bases):\n        if model_fields := getattr(base, '__pydantic_fields__', None):\n            parent_fields_lookup.update(model_fields)\n    type_hints = _typing_extra.get_model_type_hints(cls, ns_resolver=ns_resolver)\n    annotations = _typing_extra.safe_get_annotations(cls)\n    fields: dict[str, FieldInfo] = {}\n    class_vars: set[str] = set()\n    for ann_name, (ann_type, evaluated) in type_hints.items():\n        if ann_name == 'model_config':\n            continue\n        _check_protected_namespaces(\n            protected_namespaces=config_wrapper.protected_namespaces,\n            ann_name=ann_name,\n            bases=bases,\n            cls_name=cls.__name__,\n        )\n        if _typing_extra.is_classvar_annotation(ann_type):\n            class_vars.add(ann_name)\n            continue\n        assigned_value = getattr(cls, ann_name, PydanticUndefined)\n        if assigned_value is not PydanticUndefined and (\n            any(getattr(BaseModel_, depr_name, None) is assigned_value for depr_name in _deprecated_method_names)\n            or (\n                hasattr(assigned_value, '__func__')\n                and any(\n                    getattr(getattr(BaseModel_, depr_name, None), '__func__', None) is assigned_value.__func__\n                    for depr_name in _deprecated_classmethod_names\n                )\n            )\n        ):\n            assigned_value = PydanticUndefined\n        if not is_valid_field_name(ann_name):\n            continue\n        if cls.__pydantic_root_model__ and ann_name != 'root':\n            raise NameError(\n                f\"Unexpected field with name {ann_name!r}; only 'root' is allowed as a field of a `RootModel`\"\n            )\n        generic_origin = getattr(cls, '__pydantic_generic_metadata__', {}).get('origin')\n        for base in bases:\n            dataclass_fields = {\n                field.name for field in (dataclasses.fields(base) if dataclasses.is_dataclass(base) else ())\n            }\n            if hasattr(base, ann_name):\n                if base is generic_origin:\n                    continue\n                if ann_name in dataclass_fields:\n                    continue\n                if ann_name not in annotations:\n                    continue\n                warnings.warn(\n                    f'Field name \"{ann_name}\" in \"{cls.__qualname__}\" shadows an attribute in parent '\n                    f'\"{base.__qualname__}\"',\n                    UserWarning,\n                    stacklevel=4,\n                )\n        if assigned_value is PydanticUndefined:\n            if ann_name in annotations or ann_name not in parent_fields_lookup:\n                field_info = FieldInfo_.from_annotation(ann_type, _source=AnnotationSource.CLASS)\n                field_info._original_annotation = ann_type\n                if not evaluated:\n                    field_info._complete = False\n            else:\n                parent_field_info = parent_fields_lookup[ann_name]._copy()\n                if typevars_map:\n                    field_info = _recreate_field_info(\n                        parent_field_info, ns_resolver=ns_resolver, typevars_map=typevars_map, lenient=True\n                    )\n                else:\n                    field_info = parent_field_info\n        else:\n            if isinstance(assigned_value, FieldInfo_) and ismethoddescriptor(assigned_value.default):\n                default = assigned_value.default.__get__(None, cls)\n                assigned_value.default = default\n                assigned_value._attributes_set['default'] = default\n            field_info = FieldInfo_.from_annotated_attribute(ann_type, assigned_value, _source=AnnotationSource.CLASS)\n            field_info._original_assignment = assigned_value\n            field_info._original_annotation = ann_type\n            if not evaluated:\n                field_info._complete = False\n            elif 'final' in field_info._qualifiers and not field_info.is_required():\n                warnings.warn(\n                    f'Annotation {ann_name!r} is marked as final and has a default value. Pydantic treats {ann_name!r} as a '\n                    'class variable, but it will be considered as a normal field in V3 to be aligned with dataclasses. If you '\n                    f'still want {ann_name!r} to be considered as a class variable, annotate it as: `ClassVar[<type>] = <default>.`',\n                    category=PydanticDeprecatedSince211,\n                    stacklevel=4,\n                )\n                class_vars.add(ann_name)\n                continue\n            try:\n                delattr(cls, ann_name)\n            except AttributeError:\n                pass\n        decorators: DecoratorInfos = cls.__dict__['__pydantic_decorators__']\n        if ann_name in decorators.computed_fields:\n            raise TypeError(\n                f'Field {ann_name!r} of class {cls.__name__!r} overrides symbol of same name in a parent class. '\n                'This override with a computed_field is incompatible.'\n            )\n        fields[ann_name] = field_info\n        if field_info._complete:\n            update_field_from_config(config_wrapper, ann_name, field_info)\n    if config_wrapper.use_attribute_docstrings:\n        _update_fields_from_docstrings(cls, fields)\n    return fields, class_vars", "target": "def test_from_attributes(input_value, expected, from_attributes_mode):\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'a': core_schema.model_field(schema=core_schema.int_schema()),\n                'b': core_schema.model_field(schema=core_schema.int_schema()),\n                'c': core_schema.model_field(schema=core_schema.str_schema()),\n            },\n            from_attributes=from_attributes_mode == 'schema',\n        )\n    )\n    kwargs = {}\n    if from_attributes_mode == 'validation':\n        kwargs['from_attributes'] = True\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            val = v.validate_python(input_value, **kwargs)\n            print(f'UNEXPECTED OUTPUT: {val!r}')\n    else:\n        output = v.validate_python(input_value, **kwargs)\n        assert output == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004821", "source": "def constr_strip_whitespace(v: 'StrBytes', field: 'ModelField', config: 'BaseConfig') -> 'StrBytes':\n    strip_whitespace = field.type_.strip_whitespace or config.anystr_strip_whitespace\n    if strip_whitespace:\n        v = v.strip()\n    return v", "target": "def test_validators_build(benchmark) -> None:\n    class Base1(BaseModel):\n        a: int\n        @field_validator('a', mode='after')\n        @classmethod\n        def val_a(cls, value: int) -> int: ...\n        @computed_field\n        def prop(self) -> int: ...\n    class Bare:\n        @computed_field\n        def prop_bare(self) -> int: ...\n    class Sub1(Base1):\n        @computed_field\n        def prop_2(self) -> int: ...\n        @computed_field\n        def prop_3(self) -> int: ...\n        @computed_field\n        def prop_4(self) -> int: ...\n    @benchmark\n    def bench() -> None:\n        class SubS(Sub1, Bare, defer_build=True):\n            @computed_field\n            def prop_5(self) -> int: ...\n            @computed_field\n            def prop_6(self) -> int: ...\n            @computed_field\n            def prop_7(self) -> int: ..."}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004822", "source": "def amend_commit_message(self, msg: str) -> None:\n        self._run_git(\"commit\", \"--amend\", \"-m\", msg)", "target": "def test_is_iterable(self) -> None:\n        from collections.abc import Iterator\n        iter_ = PeekableIterator(\"\")\n        self.assertTrue(isinstance(iter_, Iterator))"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004823", "source": "def dataclass(\n        *,\n        init: bool = True,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool = False,\n        config: Union[ConfigDict, Type[object], None] = None,\n        validate_on_init: Optional[bool] = None,\n        use_proxy: Optional[bool] = None,\n        kw_only: bool = ...,\n    ) -> Callable[[Type[_T]], 'DataclassClassOrWrapper']:\n        ...", "target": "def test_slots_mixed():\n    @dataclasses.dataclass(slots=True)\n    class Model:\n        x: int\n        y: dataclasses.InitVar[str]\n        z: ClassVar[str] = 'z-classvar'\n    @dataclasses.dataclass\n    class SubModel(Model):\n        x2: int\n        y2: dataclasses.InitVar[str]\n        z2: ClassVar[str] = 'z2-classvar'\n    schema = core_schema.dataclass_schema(\n        SubModel,\n        core_schema.dataclass_args_schema(\n            'SubModel',\n            [\n                core_schema.dataclass_field(name='x', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(name='y', init_only=True, schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='x2', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(name='y2', init_only=True, schema=core_schema.str_schema()),\n            ],\n        ),\n        ['x'],\n        slots=True,\n    )\n    v = SchemaValidator(schema)\n    dc = v.validate_python({'x': 1, 'y': 'a', 'x2': 2, 'y2': 'b'})\n    assert dc.x == 1\n    assert dc.x2 == 2\n    assert dataclasses.asdict(dc) == {'x': 1, 'x2': 2}"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004824", "source": "def _flatten(\n    key_prefix: Label, sub_schema: Definition, result: FlatIntermediateDefinition\n) -> None:\n    for k, value in sub_schema.items():\n        if isinstance(k, tuple):\n            assert all(isinstance(ki, str) for ki in k)\n            key_suffix: Label = k\n        elif k is None:\n            key_suffix = ()\n        else:\n            assert isinstance(k, str)\n            key_suffix = (k,)\n        key: Label = key_prefix + key_suffix\n        if isinstance(value, (TimerArgs, GroupedBenchmark)):\n            assert key not in result, f\"duplicate key: {key}\"\n            result[key] = value\n        else:\n            assert isinstance(value, dict)\n            _flatten(key_prefix=key, sub_schema=value, result=result)", "target": "def test_merged_lastfailed_content_with_empty_dest(self) -> None:\n        last_failed_source = {\n            \"tools/tests/test_car.py::test_num1\": True,\n            \"tools/tests/test_car.py::test_num2\": True,\n        }\n        last_failed_dest = {\n            \"\": True,\n        }\n        last_failed_merged = {\n            \"tools/tests/test_car.py::test_num1\": True,\n            \"tools/tests/test_car.py::test_num2\": True,\n        }\n        merged = _merged_lastfailed_content(last_failed_source, last_failed_dest)\n        self.assertEqual(merged, last_failed_merged)"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004825", "source": "def benchmark_using_throughput_benchmark(config, module):\n    print(\"Benchmarking via ThroughputBenchmark\")\n    bench = ThroughputBenchmark(module.module)\n    bench.add_input(*module.tensor_inputs)\n    stats = bench.benchmark(1, config.num_warmup_iters, config.num_iters)\n    return stats.latency_avg_ms / NUM_LOOP_ITERS", "target": "def test_overwrites_and_restores_existing_var(self):\n        var = \"TEST_TMP_ENV_OVERWRITE\"\n        os.environ[var] = \"orig\"\n        with temp_environ({var: \"override\"}):\n            self.assertEqual(os.environ[var], \"override\")\n        self.assertEqual(os.environ[var], \"orig\")"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004826", "source": "def make_dataclass_validator(dc_cls: Type['Dataclass'], config: Type[BaseConfig]) -> 'CallableGenerator':\n    yield from _get_validators(dataclass(dc_cls, config=config, use_proxy=True))", "target": "def test_dataclass_field_wrap_validator1():\n    @dataclasses.dataclass\n    class Foo:\n        a: int\n        b: str\n        @classmethod\n        def validate_b(\n            cls, v: bytes, nxt: core_schema.ValidatorFunctionWrapHandler, info: core_schema.ValidationInfo\n        ) -> str:\n            assert v == b'hello'\n            v = nxt(v)\n            assert v == 'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return 'hello world!'\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(\n                    name='b',\n                    schema=core_schema.with_info_wrap_validator_function(Foo.validate_b, core_schema.str_schema()),\n                ),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 1, 'b': b'hello'})\n    assert dataclasses.asdict(foo) == {'a': 1, 'b': 'hello world!'}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004827", "source": "def _general_metadata_cls() -> type[BaseMetadata]:\n    from annotated_types import BaseMetadata\n    class _PydanticGeneralMetadata(PydanticMetadata, BaseMetadata):\n        def __init__(self, metadata: Any):\n            self.__dict__ = metadata\n    return _PydanticGeneralMetadata", "target": "def test_on_error_default(self, py_and_json: PyAndJson):\n        v = py_and_json(\n            {\n                'type': 'model-fields',\n                'fields': {\n                    'x': {\n                        'type': 'model-field',\n                        'schema': {\n                            'type': 'default',\n                            'schema': {'type': 'str'},\n                            'on_error': 'default',\n                            'default': 'pika',\n                        },\n                    }\n                },\n            }\n        )\n        assert v.validate_test({'x': 'foo'}) == ({'x': 'foo'}, None, {'x'})\n        assert v.validate_test({'x': ['foo']}) == ({'x': 'pika'}, None, {'x'})"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004828", "source": "def secs_to_ms(time_s):\n    return time_s * 1e3", "target": "def test_overwrites_and_restores_existing_var(self):\n        var = \"TEST_TMP_ENV_OVERWRITE\"\n        os.environ[var] = \"orig\"\n        with temp_environ({var: \"override\"}):\n            self.assertEqual(os.environ[var], \"override\")\n        self.assertEqual(os.environ[var], \"orig\")"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004829", "source": "def is_union(tp: Optional[Type[Any]]) -> bool:\n        return tp is Union or tp is types.UnionType", "target": "def test_wrong_function_signature() -> None:\n    def wrong_validator(value: Any) -> Any:\n        return value\n    v = SchemaValidator(core_schema.with_info_plain_validator_function(wrong_validator))\n    try:\n        v.validate_python(1)\n    except TypeError as exc:\n        assert 'takes 1 positional argument but 2 were given' in str(exc)\n    else:\n        raise AssertionError('v.validate_python(1) did not raise TypeError')"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004830", "source": "def main():\n    parser = argparse.ArgumentParser(description=\"Lumos CLI\")\n    subparsers = parser.add_subparsers(dest=\"command\", required=True)\n    parser.add_argument(\n        \"--log-level\", default=\"INFO\", help=\"Log level (DEBUG, INFO, WARNING, ERROR)\"\n    )\n    register_build_commands(subparsers)\n    register_test_commands(subparsers)\n    args = parser.parse_args()\n    setup_logging(getattr(logging, args.log_level.upper(), logging.INFO))\n    logger.debug(\"Parsed args: %s\", args)\n    if hasattr(args, \"func\"):\n        args.func(args)\n    else:\n        parser.print_help()", "target": "def test_aggregates_failures_and_raises(monkeypatch, patch_module):\n    run_test_plan = patch_module.module.run_test_plan\n    tests_map = {\n        \"mix\": {\n            \"title\": \"Some pass some fail\",\n            \"steps\": [\n                \"pytest test_a.py\",\n                \"pytest test_b.py\",\n                \"pytest test_c.py\",\n            ],\n        }\n    }\n    patch_module.run_command.side_effect = [0, 1, 2]\n    with pytest.raises(RuntimeError) as ei:\n        run_test_plan(\"mix\", \"cpu\", tests_map)\n    msg = str(ei.value)\n    assert \"2 pytest runs failed\" in msg\n    patch_module.logger.error.assert_called_once()\n    assert patch_module.run_command.call_count == 3"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004831", "source": "def env_bool_field(\n    name: str,\n    default: bool = False,\n):\n    return field(default_factory=lambda: env_bool(name, default))", "target": "def test_env_path_optional_unset_returns_default_str(self):\n        default_str = \"x/y\"\n        with patch.dict(os.environ, {}, clear=True):\n            p = m.env_path_optional(\"P\", default=default_str)\n            self.assertIsInstance(p, Path)\n            self.assertIsNotNone(p)\n            if p:\n                self.assertTrue(p.is_absolute())\n                self.assertEqual(p.parts[-2:], (\"x\", \"y\"))"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004832", "source": "def _general_metadata_cls() -> type[BaseMetadata]:\n    from annotated_types import BaseMetadata\n    class _PydanticGeneralMetadata(PydanticMetadata, BaseMetadata):\n        def __init__(self, metadata: Any):\n            self.__dict__ = metadata\n    return _PydanticGeneralMetadata", "target": "def test_frozen_field():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'name': core_schema.model_field(schema=core_schema.str_schema()),\n                'age': core_schema.model_field(schema=core_schema.int_schema()),\n                'is_developer': core_schema.model_field(\n                    schema=core_schema.with_default_schema(schema=core_schema.bool_schema(), default=True), frozen=True\n                ),\n            }\n        )\n    )\n    r1, model_extra, fields_set = v.validate_python({'name': 'Samuel', 'age': '36'})\n    assert r1 == {'name': 'Samuel', 'age': 36, 'is_developer': True}\n    assert model_extra is None\n    assert fields_set == {'name', 'age'}\n    v.validate_assignment(r1, 'age', '35')\n    assert r1 == {'name': 'Samuel', 'age': 35, 'is_developer': True}\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_assignment(r1, 'is_developer', False)\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'frozen_field', 'loc': ('is_developer',), 'msg': 'Field is frozen', 'input': False}\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004833", "source": "def fuzzy_list_to_dict(items: list[tuple[str, str]]) -> dict[str, list[str]]:\n    rc: dict[str, list[str]] = defaultdict(list)\n    for key, val in items:\n        rc[key].append(val)\n    return dict(rc)", "target": "def test_simple(self) -> None:\n        @retries_decorator()\n        def foo(x: int, y: int) -> int:\n            return x + y\n        self.assertEqual(foo(3, 4), 7)"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004834", "source": "def to_markdown_table(\n    res: TimingResultType, header: Optional[tuple[str, ...]] = None\n) -> str:\n    if header is None:\n        header = (\"model\", \"task\", \"mean\", \"var\")\n    out = \"\"\n    def write_line(*args):\n        nonlocal out\n        out += f\"| {' | '.join(str(a) for a in args)} |\\n\"\n    write_line(*header)\n    write_line(*[\"--\"] * len(header))\n    for model, tasks in res.items():\n        for task, line in tasks.items():\n            write_line(*(model, task) + line)\n    return out", "target": "def test_gh_get_labels(\n        self,\n        mock_request_for_labels: Any,\n        mock_get_last_page_num_from_header: Any,\n    ) -> None:\n        res = gh_get_labels(\"mock_org\", \"mock_repo\")\n        mock_get_last_page_num_from_header.assert_called_once()\n        self.assertEqual(res, [\"foo\"] * 3)"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004835", "source": "def secs_to_us(time_s):\n    return time_s * 1e6", "target": "def test_fails(self) -> None:\n        @retries_decorator(rc=0)\n        def foo(x: int, y: int) -> int:\n            return x + y\n        self.assertEqual(foo(\"a\", 4), 0)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004836", "source": "def _is_field_cached_property(obj: 'Dataclass', k: str) -> bool:\n        return isinstance(getattr(type(obj), k, None), cached_property)", "target": "def test_frozen():\n    @dataclasses.dataclass\n    class MyModel:\n        f: str\n    v = SchemaValidator(\n        core_schema.dataclass_schema(\n            MyModel,\n            core_schema.dataclass_args_schema('MyModel', [core_schema.dataclass_field('f', core_schema.str_schema())]),\n            ['f'],\n            frozen=True,\n        )\n    )\n    m = v.validate_python({'f': 'x'})\n    assert m.f == 'x'\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_assignment(m, 'f', 'y')\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'frozen_instance', 'loc': (), 'msg': 'Instance is frozen', 'input': 'y'}\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004837", "source": "def rebuild_dataclass(\n    cls: type[PydanticDataclass],\n    *,\n    force: bool = False,\n    raise_errors: bool = True,\n    _parent_namespace_depth: int = 2,\n    _types_namespace: MappingNamespace | None = None,\n) -> bool | None:\n    if not force and cls.__pydantic_complete__:\n        return None\n    for attr in ('__pydantic_core_schema__', '__pydantic_validator__', '__pydantic_serializer__'):\n        if attr in cls.__dict__ and not isinstance(getattr(cls, attr), _mock_val_ser.MockValSer):\n            delattr(cls, attr)\n    cls.__pydantic_complete__ = False\n    if _types_namespace is not None:\n        rebuild_ns = _types_namespace\n    elif _parent_namespace_depth > 0:\n        rebuild_ns = _typing_extra.parent_frame_namespace(parent_depth=_parent_namespace_depth, force=True) or {}\n    else:\n        rebuild_ns = {}\n    ns_resolver = _namespace_utils.NsResolver(\n        parent_namespace=rebuild_ns,\n    )\n    return _pydantic_dataclasses.complete_dataclass(\n        cls,\n        _config.ConfigWrapper(cls.__pydantic_config__, check=False),\n        raise_errors=raise_errors,\n        ns_resolver=ns_resolver,\n        _force_build=True,\n    )", "target": "def test_validate_assignment_function():\n    @dataclasses.dataclass\n    class MyDataclass:\n        field_a: str\n        field_b: int\n        field_c: int\n    calls = []\n    def func(x, info):\n        calls.append(str(info))\n        return x * 2\n    v = SchemaValidator(\n        core_schema.dataclass_schema(\n            MyDataclass,\n            core_schema.dataclass_args_schema(\n                'MyDataclass',\n                [\n                    core_schema.dataclass_field('field_a', core_schema.str_schema()),\n                    core_schema.dataclass_field(\n                        'field_b',\n                        core_schema.with_info_after_validator_function(func, core_schema.int_schema()),\n                    ),\n                    core_schema.dataclass_field('field_c', core_schema.int_schema()),\n                ],\n            ),\n            ['field_a', 'field_b', 'field_c'],\n        )\n    )\n    m = v.validate_python({'field_a': 'x', 'field_b': 123, 'field_c': 456})\n    assert m.field_a == 'x'\n    assert m.field_b == 246\n    assert m.field_c == 456\n    assert calls == [\"ValidationInfo(config=None, context=None, data={'field_a': 'x'}, field_name='field_b')\"]\n    v.validate_assignment(m, 'field_b', '111')\n    assert m.field_b == 222\n    assert calls == [\n        \"ValidationInfo(config=None, context=None, data={'field_a': 'x'}, field_name='field_b')\",\n        \"ValidationInfo(config=None, context=None, data={'field_a': 'x', 'field_c': 456}, field_name='field_b')\",\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004838", "source": "def _check_protected_namespaces(\n    protected_namespaces: tuple[str | Pattern[str], ...],\n    ann_name: str,\n    bases: tuple[type[Any], ...],\n    cls_name: str,\n) -> None:\n    BaseModel = import_cached_base_model()\n    for protected_namespace in protected_namespaces:\n        ns_violation = False\n        if isinstance(protected_namespace, Pattern):\n            ns_violation = protected_namespace.match(ann_name) is not None\n        elif isinstance(protected_namespace, str):\n            ns_violation = ann_name.startswith(protected_namespace)\n        if ns_violation:\n            for b in bases:\n                if hasattr(b, ann_name):\n                    if not (issubclass(b, BaseModel) and ann_name in getattr(b, '__pydantic_fields__', {})):\n                        raise ValueError(\n                            f'Field {ann_name!r} conflicts with member {getattr(b, ann_name)}'\n                            f' of protected namespace {protected_namespace!r}.'\n                        )\n            else:\n                valid_namespaces: list[str] = []\n                for pn in protected_namespaces:\n                    if isinstance(pn, Pattern):\n                        if not pn.match(ann_name):\n                            valid_namespaces.append(f're.compile({pn.pattern!r})')\n                    else:\n                        if not ann_name.startswith(pn):\n                            valid_namespaces.append(f\"'{pn}'\")\n                valid_namespaces_str = f'({\", \".join(valid_namespaces)}{\",)\" if len(valid_namespaces) == 1 else \")\"}'\n                warnings.warn(\n                    f'Field {ann_name!r} in {cls_name!r} conflicts with protected namespace {protected_namespace!r}.\\n\\n'\n                    f\"You may be able to solve this by setting the 'protected_namespaces' configuration to {valid_namespaces_str}.\",\n                    UserWarning,\n                    stacklevel=5,\n                )", "target": "def test_paths_allow_by_name(py_and_json: PyAndJson, input_value):\n    v = py_and_json(\n        {\n            'type': 'model-fields',\n            'fields': {\n                'field_a': {\n                    'validation_alias': [['foo', 'bar'], ['foo']],\n                    'type': 'model-field',\n                    'schema': {'type': 'int'},\n                }\n            },\n        },\n        config=CoreConfig(validate_by_name=True),\n    )\n    assert v.validate_test(input_value) == ({'field_a': 42}, None, {'field_a'})"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004839", "source": "def write_line(*args):\n        nonlocal out\n        out += f\"| {' | '.join(str(a) for a in args)} |\\n\"", "target": "def test_peek(self, input_: str = \"abcdef\") -> None:\n        iter_ = PeekableIterator(input_)\n        for idx, c in enumerate(iter_):\n            if idx + 1 < len(input_):\n                self.assertEqual(iter_.peek(), input_[idx + 1])\n            else:\n                self.assertTrue(iter_.peek() is None)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004840", "source": "def _dataclass_validate_assignment_setattr(self: 'Dataclass', name: str, value: Any) -> None:\n    if self.__pydantic_initialised__:\n        d = dict(self.__dict__)\n        d.pop(name, None)\n        known_field = self.__pydantic_model__.__fields__.get(name, None)\n        if known_field:\n            value, error_ = known_field.validate(value, d, loc=name, cls=self.__class__)\n            if error_:\n                raise ValidationError([error_], self.__class__)\n    object.__setattr__(self, name, value)", "target": "def test_only_allow_alias(py_and_json) -> None:\n    schema = core_schema.dataclass_schema(\n        BasicDataclass,\n        core_schema.dataclass_args_schema(\n            'BasicDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), validation_alias='FieldA'),\n            ],\n        ),\n        ['a'],\n        config=core_schema.CoreConfig(validate_by_name=False, validate_by_alias=True),\n    )\n    v = py_and_json(schema)\n    assert v.validate_test({'FieldA': 'hello'}) == BasicDataclass(a='hello')\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'a': 'hello'})"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004841", "source": "def check_for_functorch():\n    try:\n        import functorch\n        return True\n    except ImportError:\n        return False", "target": "def test_sets_and_restores_new_var(self):\n        var = \"TEST_TMP_ENV_NEW\"\n        self.assertNotIn(var, os.environ)\n        with temp_environ({var: \"123\"}):\n            self.assertEqual(os.environ[var], \"123\")\n        self.assertNotIn(var, os.environ)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004842", "source": "def _dataclass_validate_assignment_setattr(self: 'Dataclass', name: str, value: Any) -> None:\n    if self.__pydantic_initialised__:\n        d = dict(self.__dict__)\n        d.pop(name, None)\n        known_field = self.__pydantic_model__.__fields__.get(name, None)\n        if known_field:\n            value, error_ = known_field.validate(value, d, loc=name, cls=self.__class__)\n            if error_:\n                raise ValidationError([error_], self.__class__)\n    object.__setattr__(self, name, value)", "target": "def test_extra_behavior_allow(\n    config: Union[core_schema.CoreConfig, None],\n    schema_extra_behavior_kw: dict[str, Any],\n    validate_fn_extra_kw: Union[ExtraBehavior, None],\n):\n    @dataclasses.dataclass\n    class MyModel:\n        f: str\n    v = SchemaValidator(\n        core_schema.dataclass_schema(\n            MyModel,\n            core_schema.dataclass_args_schema(\n                'MyModel', [core_schema.dataclass_field('f', core_schema.str_schema())], **schema_extra_behavior_kw\n            ),\n            ['f'],\n            config=config,\n        )\n    )\n    m: MyModel = v.validate_python({'f': 'x', 'extra_field': '123'}, extra=validate_fn_extra_kw)\n    assert m.f == 'x'\n    assert getattr(m, 'extra_field') == '123'\n    v.validate_assignment(m, 'f', 'y', extra=validate_fn_extra_kw)\n    assert m.f == 'y'\n    v.validate_assignment(m, 'not_f', '123', extra=validate_fn_extra_kw)\n    assert getattr(m, 'not_f') == '123'"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004843", "source": "def check_for_functorch():\n    try:\n        import functorch\n        return True\n    except ImportError:\n        return False", "target": "def test_is_iterable(self) -> None:\n        from collections.abc import Iterator\n        iter_ = PeekableIterator(\"\")\n        self.assertTrue(isinstance(iter_, Iterator))"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004844", "source": "def _apply_field_title_generator_to_field_info(\n    title_generator: Callable[[str, FieldInfo], str],\n    field_name: str,\n    field_info: FieldInfo,\n):\n    if field_info.title is None:\n        title = title_generator(field_name, field_info)\n        if not isinstance(title, str):\n            raise TypeError(f'field_title_generator {title_generator} must return str, not {title.__class__}')\n        field_info.title = title", "target": "def test_alias_path(py_and_json: PyAndJson, input_value, expected):\n    v = py_and_json(\n        {\n            'type': 'model-fields',\n            'fields': {\n                'field_a': {'validation_alias': ['foo', 'bar'], 'type': 'model-field', 'schema': {'type': 'int'}}\n            },\n        }\n    )\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=expected.message):\n            v.validate_test(input_value)\n    else:\n        output = v.validate_test(input_value)\n        assert output == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004845", "source": "def get_args(tp: Type[Any]) -> Tuple[Any, ...]:\n        if type(tp).__name__ in AnnotatedTypeNames:\n            return tp.__args__ + tp.__metadata__\n        return _typing_get_args(tp) or getattr(tp, '__args__', ()) or _generic_get_args(tp)", "target": "def test_schema_validator() -> None:\n    SchemaValidator({'type': 'int'})"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004846", "source": "def update_field_from_config(config_wrapper: ConfigWrapper, field_name: str, field_info: FieldInfo) -> None:\n    field_title_generator = field_info.field_title_generator or config_wrapper.field_title_generator\n    if field_title_generator is not None:\n        _apply_field_title_generator_to_field_info(field_title_generator, field_name, field_info)\n    if config_wrapper.alias_generator is not None:\n        _apply_alias_generator_to_field_info(config_wrapper.alias_generator, field_name, field_info)", "target": "def test_only_validate_by_name(py_and_json) -> None:\n    v = py_and_json(\n        {\n            'type': 'model-fields',\n            'fields': {'field_a': {'validation_alias': 'FieldA', 'type': 'model-field', 'schema': {'type': 'int'}}},\n        },\n        config=CoreConfig(validate_by_name=True, validate_by_alias=False),\n    )\n    assert v.validate_test({'field_a': '123'}) == ({'field_a': 123}, None, {'field_a'})\n    with pytest.raises(ValidationError, match=r'field_a\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'FieldA': '123'})"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004847", "source": "def benchmark_module(config, module, use_throughput_benchmark=False):\n    if use_throughput_benchmark:\n        return benchmark_using_throughput_benchmark(config, module)\n    module.forward(config.num_warmup_iters)\n    print(f\"Running module for {config.num_iters} iterations\")\n    start = time.time()\n    module.forward(config.num_iters)\n    end = time.time()\n    time_elapsed_s = end - start\n    return secs_to_ms(time_elapsed_s) / config.num_iters / NUM_LOOP_ITERS", "target": "def test_fails(self) -> None:\n        @retries_decorator(rc=0)\n        def foo(x: int, y: int) -> int:\n            return x + y\n        self.assertEqual(foo(\"a\", 4), 0)"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004848", "source": "def clone_repo(username: str, password: str, org: str, project: str) -> GitRepo:\n    path = tempfile.mkdtemp()\n    _check_output(\n        [\n            \"git\",\n            \"clone\",\n            f\"https://{username}:{password}@github.com/{org}/{project}\",\n            path,\n        ]\n    ).strip()\n    return GitRepo(path=path)", "target": "def test_ghstack_branches_not_in_sync(self) -> None:\n        head_ref = \"gh/clee2000/1/head\"\n        self._skip_if_ref_does_not_exist(head_ref)\n        self.assertFalse(are_ghstack_branches_in_sync(self.repo, head_ref))"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004849", "source": "def complete_dataclass(\n    cls: type[Any],\n    config_wrapper: _config.ConfigWrapper,\n    *,\n    raise_errors: bool = True,\n    ns_resolver: NsResolver | None = None,\n    _force_build: bool = False,\n) -> bool:\n    original_init = cls.__init__\n    def __init__(__dataclass_self__: PydanticDataclass, *args: Any, **kwargs: Any) -> None:\n        __tracebackhide__ = True\n        s = __dataclass_self__\n        s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)\n    __init__.__qualname__ = f'{cls.__qualname__}.__init__'\n    cls.__init__ = __init__\n    cls.__pydantic_config__ = config_wrapper.config_dict\n    set_dataclass_fields(cls, config_wrapper=config_wrapper, ns_resolver=ns_resolver)\n    if not _force_build and config_wrapper.defer_build:\n        set_dataclass_mocks(cls)\n        return False\n    if hasattr(cls, '__post_init_post_parse__'):\n        warnings.warn(\n            'Support for `__post_init_post_parse__` has been dropped, the method will not be called',\n            PydanticDeprecatedSince20,\n        )\n    typevars_map = get_standard_typevars_map(cls)\n    gen_schema = GenerateSchema(\n        config_wrapper,\n        ns_resolver=ns_resolver,\n        typevars_map=typevars_map,\n    )\n    cls.__signature__ = LazyClassAttribute(\n        '__signature__',\n        partial(\n            generate_pydantic_signature,\n            init=original_init,\n            fields=cls.__pydantic_fields__,\n            validate_by_name=config_wrapper.validate_by_name,\n            extra=config_wrapper.extra,\n            is_dataclass=True,\n        ),\n    )\n    try:\n        schema = gen_schema.generate_schema(cls)\n    except PydanticUndefinedAnnotation as e:\n        if raise_errors:\n            raise\n        set_dataclass_mocks(cls, f'`{e.name}`')\n        return False\n    core_config = config_wrapper.core_config(title=cls.__name__)\n    try:\n        schema = gen_schema.clean_schema(schema)\n    except InvalidSchemaError:\n        set_dataclass_mocks(cls)\n        return False\n    cls = cast('type[PydanticDataclass]', cls)\n    cls.__pydantic_core_schema__ = schema\n    cls.__pydantic_validator__ = create_schema_validator(\n        schema, cls, cls.__module__, cls.__qualname__, 'dataclass', core_config, config_wrapper.plugin_settings\n    )\n    cls.__pydantic_serializer__ = SchemaSerializer(schema, core_config)\n    cls.__pydantic_complete__ = True\n    return True", "target": "def test_slots_mixed():\n    @dataclasses.dataclass(slots=True)\n    class Model:\n        x: int\n        y: dataclasses.InitVar[str]\n        z: ClassVar[str] = 'z-classvar'\n    @dataclasses.dataclass\n    class SubModel(Model):\n        x2: int\n        y2: dataclasses.InitVar[str]\n        z2: ClassVar[str] = 'z2-classvar'\n    schema = core_schema.dataclass_schema(\n        SubModel,\n        core_schema.dataclass_args_schema(\n            'SubModel',\n            [\n                core_schema.dataclass_field(name='x', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(name='y', init_only=True, schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='x2', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(name='y2', init_only=True, schema=core_schema.str_schema()),\n            ],\n        ),\n        ['x'],\n        slots=True,\n    )\n    v = SchemaValidator(schema)\n    dc = v.validate_python({'x': 1, 'y': 'a', 'x2': 2, 'y2': 'b'})\n    assert dc.x == 1\n    assert dc.x2 == 2\n    assert dataclasses.asdict(dc) == {'x': 1, 'x2': 2}"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004850", "source": "def _check_output(items: list[str], encoding: str = \"utf-8\") -> str:\n    from subprocess import CalledProcessError, check_output, STDOUT\n    try:\n        return check_output(items, stderr=STDOUT).decode(encoding)\n    except CalledProcessError as e:\n        msg = f\"Command `{' '.join(e.cmd)}` returned non-zero exit code {e.returncode}\"\n        stdout = e.stdout.decode(encoding) if e.stdout is not None else \"\"\n        stderr = e.stderr.decode(encoding) if e.stderr is not None else \"\"\n        print(f\"stdout: \\n{stdout}\")\n        print(f\"stderr: \\n{stderr}\")\n        if len(stderr) == 0:\n            msg += f\"\\n```\\n{stdout}```\"\n        else:\n            msg += f\"\\nstdout:\\n```\\n{stdout}```\\nstderr:\\n```\\n{stderr}```\"\n        raise RuntimeError(msg) from e", "target": "def test_simple(self) -> None:\n        @retries_decorator()\n        def foo(x: int, y: int) -> int:\n            return x + y\n        self.assertEqual(foo(3, 4), 7)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004851", "source": "def _recreate_field_info(\n    field_info: FieldInfo,\n    ns_resolver: NsResolver,\n    typevars_map: Mapping[TypeVar, Any],\n    *,\n    lenient: bool,\n) -> FieldInfo:\n    FieldInfo_ = import_cached_field_info()\n    existing_desc = field_info.description\n    if lenient:\n        ann = _generics.replace_types(field_info._original_annotation, typevars_map)\n        ann, evaluated = _typing_extra.try_eval_type(\n            ann,\n            *ns_resolver.types_namespace,\n        )\n    else:\n        ann = _typing_extra.eval_type(\n            field_info._original_annotation,\n            *ns_resolver.types_namespace,\n        )\n        ann = _generics.replace_types(ann, typevars_map)\n        ann = _typing_extra.eval_type(\n            ann,\n            *ns_resolver.types_namespace,\n        )\n        evaluated = True\n    if (assign := field_info._original_assignment) is PydanticUndefined:\n        new_field = FieldInfo_.from_annotation(ann, _source=AnnotationSource.CLASS)\n    else:\n        new_field = FieldInfo_.from_annotated_attribute(ann, assign, _source=AnnotationSource.CLASS)\n        new_field._original_assignment = assign\n    new_field._original_annotation = ann\n    new_field.description = new_field.description if new_field.description is not None else existing_desc\n    if not evaluated:\n        new_field._complete = False\n    return new_field", "target": "def test_allow_extra_invalid():\n    with pytest.raises(SchemaError, match='extras_schema can only be used if extra_behavior=allow'):\n        SchemaValidator(\n            schema=core_schema.model_fields_schema(\n                fields={}, extras_schema=core_schema.int_schema(), extra_behavior='ignore'\n            )\n        )\n    with pytest.raises(SchemaError, match='extras_keys_schema can only be used if extra_behavior=allow'):\n        SchemaValidator(\n            schema=core_schema.model_fields_schema(\n                fields={}, extras_keys_schema=core_schema.int_schema(), extra_behavior='ignore'\n            )\n        )"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004852", "source": "def collect_dataclass_fields(\n    cls: type[StandardDataclass],\n    *,\n    config_wrapper: ConfigWrapper,\n    ns_resolver: NsResolver | None = None,\n    typevars_map: dict[Any, Any] | None = None,\n) -> dict[str, FieldInfo]:\n    FieldInfo_ = import_cached_field_info()\n    fields: dict[str, FieldInfo] = {}\n    ns_resolver = ns_resolver or NsResolver()\n    dataclass_fields = cls.__dataclass_fields__\n    for base in reversed(cls.__mro__):\n        if not dataclasses.is_dataclass(base):\n            continue\n        with ns_resolver.push(base):\n            for ann_name, dataclass_field in dataclass_fields.items():\n                base_anns = _typing_extra.safe_get_annotations(base)\n                if ann_name not in base_anns:\n                    continue\n                globalns, localns = ns_resolver.types_namespace\n                ann_type, evaluated = _typing_extra.try_eval_type(dataclass_field.type, globalns, localns)\n                if _typing_extra.is_classvar_annotation(ann_type):\n                    continue\n                if (\n                    not dataclass_field.init\n                    and dataclass_field.default is dataclasses.MISSING\n                    and dataclass_field.default_factory is dataclasses.MISSING\n                ):\n                    continue\n                if isinstance(dataclass_field.default, FieldInfo_):\n                    if dataclass_field.default.init_var:\n                        if dataclass_field.default.init is False:\n                            raise PydanticUserError(\n                                f'Dataclass field {ann_name} has init=False and init_var=True, but these are mutually exclusive.',\n                                code='clashing-init-and-init-var',\n                            )\n                        continue\n                    field_info = FieldInfo_.from_annotated_attribute(\n                        ann_type, dataclass_field.default, _source=AnnotationSource.DATACLASS\n                    )\n                    field_info._original_assignment = dataclass_field.default\n                else:\n                    field_info = FieldInfo_.from_annotated_attribute(\n                        ann_type, dataclass_field, _source=AnnotationSource.DATACLASS\n                    )\n                    field_info._original_assignment = dataclass_field\n                if not evaluated:\n                    field_info._complete = False\n                    field_info._original_annotation = ann_type\n                fields[ann_name] = field_info\n                update_field_from_config(config_wrapper, ann_name, field_info)\n                if field_info.default is not PydanticUndefined and isinstance(\n                    getattr(cls, ann_name, field_info), FieldInfo_\n                ):\n                    setattr(cls, ann_name, field_info.default)\n    if typevars_map:\n        for field in fields.values():\n            field.apply_typevars_map(typevars_map)\n    if config_wrapper.use_attribute_docstrings:\n        _update_fields_from_docstrings(\n            cls,\n            fields,\n            use_inspect=not hasattr(cls, '__is_pydantic_dataclass__'),\n        )\n    return fields", "target": "def test_only_validate_by_name(py_and_json) -> None:\n    v = py_and_json(\n        {\n            'type': 'model-fields',\n            'fields': {'field_a': {'validation_alias': 'FieldA', 'type': 'model-field', 'schema': {'type': 'int'}}},\n        },\n        config=CoreConfig(validate_by_name=True, validate_by_alias=False),\n    )\n    assert v.validate_test({'field_a': '123'}) == ({'field_a': 123}, None, {'field_a'})\n    with pytest.raises(ValidationError, match=r'field_a\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'FieldA': '123'})"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004853", "source": "def handle_extra_init(self: 'Dataclass', *args: Any, **kwargs: Any) -> None:\n        if config.extra == Extra.ignore:\n            init(self, *args, **{k: v for k, v in kwargs.items() if k in self.__dataclass_fields__})\n        elif config.extra == Extra.allow:\n            for k, v in kwargs.items():\n                self.__dict__.setdefault(k, v)\n            init(self, *args, **{k: v for k, v in kwargs.items() if k in self.__dataclass_fields__})\n        else:\n            init(self, *args, **kwargs)", "target": "def test_dataclass_field_wrap_validator2():\n    @dataclasses.dataclass\n    class Foo:\n        a: int\n        b: str\n        @classmethod\n        def validate_b(\n            cls, v: bytes, nxt: core_schema.ValidatorFunctionWrapHandler, info: core_schema.ValidationInfo\n        ) -> bytes:\n            assert v == b'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return nxt(b'hello world!')\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(\n                    name='b',\n                    schema=core_schema.with_info_wrap_validator_function(Foo.validate_b, core_schema.str_schema()),\n                ),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 1, 'b': b'hello'})\n    assert dataclasses.asdict(foo) == {'a': 1, 'b': 'hello world!'}"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004854", "source": "def ms_to_us(time_ms):\n    return time_ms * 1e3", "target": "def test_get_last_page_num_from_header(self) -> None:\n        for (\n            expected_page_num,\n            mock_header,\n        ) in self.MOCK_HEADER_LINKS_TO_PAGE_NUMS.items():\n            self.assertEqual(\n                get_last_page_num_from_header(mock_header), expected_page_num\n            )"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004855", "source": "def is_valid_privateattr_name(name: str) -> bool:\n    return name.startswith('_') and not name.startswith('__')", "target": "def test_alias(py_and_json: PyAndJson):\n    v = py_and_json(\n        {\n            'type': 'model-fields',\n            'fields': {'field_a': {'validation_alias': 'FieldA', 'type': 'model-field', 'schema': {'type': 'int'}}},\n        }\n    )\n    assert v.validate_test({'FieldA': '123'}) == ({'field_a': 123}, None, {'field_a'})\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'foobar': '123'})\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'field_a': '123'})"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004856", "source": "def write_line(*args):\n        nonlocal out\n        out += f\"| {' | '.join(str(a) for a in args)} |\\n\"", "target": "def test_changes_and_restores(self):\n        start = Path.cwd()\n        with tempfile.TemporaryDirectory() as td:\n            target = Path(td) / \"wd\"\n            target.mkdir()\n            with working_directory(str(target)):\n                self.assertEqual(Path.cwd().resolve(), target.resolve())\n        self.assertEqual(Path.cwd(), start)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004857", "source": "def is_valid_field_name(name: str) -> bool:\n    return not name.startswith('_')", "target": "def test_from_attributes_path_error():\n    class PropertyError:\n        @property\n        def foo(self):\n            raise RuntimeError('intentional error')\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'my_field': core_schema.model_field(\n                    validation_alias=[['foo', 'bar', 'bat'], ['foo', 3], ['spam']], schema=core_schema.int_schema()\n                )\n            },\n            from_attributes=True,\n        )\n    )\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python(PropertyError())\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'get_attribute_error',\n            'loc': ('my_field',),\n            'msg': 'Error extracting attribute: RuntimeError: intentional error',\n            'input': HasRepr(IsStr(regex='.+PropertyError object at.+')),\n            'ctx': {'error': 'RuntimeError: intentional error'},\n        }\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004858", "source": "def collect_dataclass_fields(\n    cls: type[StandardDataclass],\n    *,\n    config_wrapper: ConfigWrapper,\n    ns_resolver: NsResolver | None = None,\n    typevars_map: dict[Any, Any] | None = None,\n) -> dict[str, FieldInfo]:\n    FieldInfo_ = import_cached_field_info()\n    fields: dict[str, FieldInfo] = {}\n    ns_resolver = ns_resolver or NsResolver()\n    dataclass_fields = cls.__dataclass_fields__\n    for base in reversed(cls.__mro__):\n        if not dataclasses.is_dataclass(base):\n            continue\n        with ns_resolver.push(base):\n            for ann_name, dataclass_field in dataclass_fields.items():\n                base_anns = _typing_extra.safe_get_annotations(base)\n                if ann_name not in base_anns:\n                    continue\n                globalns, localns = ns_resolver.types_namespace\n                ann_type, evaluated = _typing_extra.try_eval_type(dataclass_field.type, globalns, localns)\n                if _typing_extra.is_classvar_annotation(ann_type):\n                    continue\n                if (\n                    not dataclass_field.init\n                    and dataclass_field.default is dataclasses.MISSING\n                    and dataclass_field.default_factory is dataclasses.MISSING\n                ):\n                    continue\n                if isinstance(dataclass_field.default, FieldInfo_):\n                    if dataclass_field.default.init_var:\n                        if dataclass_field.default.init is False:\n                            raise PydanticUserError(\n                                f'Dataclass field {ann_name} has init=False and init_var=True, but these are mutually exclusive.',\n                                code='clashing-init-and-init-var',\n                            )\n                        continue\n                    field_info = FieldInfo_.from_annotated_attribute(\n                        ann_type, dataclass_field.default, _source=AnnotationSource.DATACLASS\n                    )\n                    field_info._original_assignment = dataclass_field.default\n                else:\n                    field_info = FieldInfo_.from_annotated_attribute(\n                        ann_type, dataclass_field, _source=AnnotationSource.DATACLASS\n                    )\n                    field_info._original_assignment = dataclass_field\n                if not evaluated:\n                    field_info._complete = False\n                    field_info._original_annotation = ann_type\n                fields[ann_name] = field_info\n                update_field_from_config(config_wrapper, ann_name, field_info)\n                if field_info.default is not PydanticUndefined and isinstance(\n                    getattr(cls, ann_name, field_info), FieldInfo_\n                ):\n                    setattr(cls, ann_name, field_info.default)\n    if typevars_map:\n        for field in fields.values():\n            field.apply_typevars_map(typevars_map)\n    if config_wrapper.use_attribute_docstrings:\n        _update_fields_from_docstrings(\n            cls,\n            fields,\n            use_inspect=not hasattr(cls, '__is_pydantic_dataclass__'),\n        )\n    return fields", "target": "def test_ignore_extra():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'field_a': core_schema.model_field(schema=core_schema.str_schema()),\n                'field_b': core_schema.model_field(schema=core_schema.int_schema()),\n            }\n        )\n    )\n    assert v.validate_python({'field_a': b'123', 'field_b': 1, 'field_c': 123}) == (\n        {'field_a': '123', 'field_b': 1},\n        None,\n        {'field_b', 'field_a'},\n    )"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004859", "source": "def rebuild_dataclass(\n    cls: type[PydanticDataclass],\n    *,\n    force: bool = False,\n    raise_errors: bool = True,\n    _parent_namespace_depth: int = 2,\n    _types_namespace: MappingNamespace | None = None,\n) -> bool | None:\n    if not force and cls.__pydantic_complete__:\n        return None\n    for attr in ('__pydantic_core_schema__', '__pydantic_validator__', '__pydantic_serializer__'):\n        if attr in cls.__dict__ and not isinstance(getattr(cls, attr), _mock_val_ser.MockValSer):\n            delattr(cls, attr)\n    cls.__pydantic_complete__ = False\n    if _types_namespace is not None:\n        rebuild_ns = _types_namespace\n    elif _parent_namespace_depth > 0:\n        rebuild_ns = _typing_extra.parent_frame_namespace(parent_depth=_parent_namespace_depth, force=True) or {}\n    else:\n        rebuild_ns = {}\n    ns_resolver = _namespace_utils.NsResolver(\n        parent_namespace=rebuild_ns,\n    )\n    return _pydantic_dataclasses.complete_dataclass(\n        cls,\n        _config.ConfigWrapper(cls.__pydantic_config__, check=False),\n        raise_errors=raise_errors,\n        ns_resolver=ns_resolver,\n        _force_build=True,\n    )", "target": "def test_dataclass():\n    schema = core_schema.dataclass_schema(\n        FooDataclass,\n        core_schema.dataclass_args_schema(\n            'FooDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 'hello', 'b': True})\n    assert dataclasses.is_dataclass(foo)\n    assert foo.a == 'hello'\n    assert foo.b is True\n    assert dataclasses.asdict(v.validate_python(FooDataclass(a='hello', b=True))) == {'a': 'hello', 'b': True}\n    with pytest.raises(ValidationError, match='Input should be an instance of FooDataclass') as exc_info:\n        v.validate_python({'a': 'hello', 'b': True}, strict=True)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'dataclass_exact_type',\n            'loc': (),\n            'msg': 'Input should be an instance of FooDataclass',\n            'input': {'a': 'hello', 'b': True},\n            'ctx': {'class_name': 'FooDataclass'},\n        }\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004860", "source": "def is_namedtuple(type_: Type[Any]) -> bool:\n    from pydantic.v1.utils import lenient_issubclass\n    return lenient_issubclass(type_, tuple) and hasattr(type_, '_fields')", "target": "def test_wrong_function_signature() -> None:\n    def wrong_validator(value: Any) -> Any:\n        return value\n    v = SchemaValidator(core_schema.with_info_plain_validator_function(wrong_validator))\n    try:\n        v.validate_python(1)\n    except TypeError as exc:\n        assert 'takes 1 positional argument but 2 were given' in str(exc)\n    else:\n        raise AssertionError('v.validate_python(1) did not raise TypeError')"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004861", "source": "def _dataclass_validate_values(self: 'Dataclass') -> None:\n    if getattr(self, '__pydantic_initialised__'):\n        return\n    if getattr(self, '__pydantic_has_field_info_default__', False):\n        input_data = {\n            k: v\n            for k, v in self.__dict__.items()\n            if not (isinstance(v, FieldInfo) or _is_field_cached_property(self, k))\n        }\n    else:\n        input_data = {k: v for k, v in self.__dict__.items() if not _is_field_cached_property(self, k)}\n    d, _, validation_error = validate_model(self.__pydantic_model__, input_data, cls=self.__class__)\n    if validation_error:\n        raise validation_error\n    self.__dict__.update(d)\n    object.__setattr__(self, '__pydantic_initialised__', True)", "target": "def test_dataclass_exact_validation(revalidate_instances, input_value, expected):\n    schema = core_schema.dataclass_schema(\n        FooDataclass,\n        core_schema.dataclass_args_schema(\n            'FooDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n            ],\n        ),\n        ['a', 'b'],\n        revalidate_instances=revalidate_instances,\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python(input_value)\n    assert dataclasses.asdict(foo) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004862", "source": "def get_merge_base(self, from_ref: str, to_ref: str) -> str:\n        return self._run_git(\"merge-base\", from_ref, to_ref).strip()", "target": "def test_compute_diff(self) -> None:\n        diff = self.repo.diff(\"HEAD\")\n        sha = _shasum(diff)\n        self.assertEqual(len(sha), 64)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004863", "source": "def new_post_init(self: 'Dataclass', *args: Any, **kwargs: Any) -> None:\n            if config.post_init_call == 'before_validation':\n                post_init(self, *args, **kwargs)\n            if self.__class__.__pydantic_run_validation__:\n                self.__pydantic_validate_values__()\n                if hasattr(self, '__post_init_post_parse__'):\n                    self.__post_init_post_parse__(*args, **kwargs)\n            if config.post_init_call == 'after_validation':\n                post_init(self, *args, **kwargs)", "target": "def test_dataclass_initvar_not_required_on_union_ser() -> None:\n    @dataclasses.dataclass\n    class Foo:\n        x: int\n        init_var: dataclasses.InitVar[int] = 1\n    @dataclasses.dataclass\n    class Bar:\n        x: int\n    schema = core_schema.union_schema(\n        [\n            core_schema.dataclass_schema(\n                Foo,\n                core_schema.dataclass_args_schema(\n                    'Foo',\n                    [\n                        core_schema.dataclass_field(name='x', schema=core_schema.int_schema()),\n                        core_schema.dataclass_field(\n                            name='init_var',\n                            init_only=True,\n                            schema=core_schema.with_default_schema(core_schema.int_schema(), default=1),\n                        ),\n                    ],\n                ),\n                ['x'],\n                post_init=True,\n            ),\n            core_schema.dataclass_schema(\n                Bar,\n                core_schema.dataclass_args_schema(\n                    'Bar', [core_schema.dataclass_field(name='x', schema=core_schema.int_schema())]\n                ),\n                ['x'],\n            ),\n        ]\n    )\n    s = SchemaSerializer(schema)\n    assert s.to_python(Foo(x=1), warnings='error') == {'x': 1}\n    assert s.to_python(Foo(x=1, init_var=2), warnings='error') == {'x': 1}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004864", "source": "def new_post_init(self: 'Dataclass', *args: Any, **kwargs: Any) -> None:\n            if config.post_init_call == 'before_validation':\n                post_init(self, *args, **kwargs)\n            if self.__class__.__pydantic_run_validation__:\n                self.__pydantic_validate_values__()\n                if hasattr(self, '__post_init_post_parse__'):\n                    self.__post_init_post_parse__(*args, **kwargs)\n            if config.post_init_call == 'after_validation':\n                post_init(self, *args, **kwargs)", "target": "def test_dataclass_subclass(revalidate_instances, input_value, expected):\n    schema = core_schema.dataclass_schema(\n        FooDataclass,\n        core_schema.dataclass_args_schema(\n            'FooDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n            ],\n            extra_behavior='forbid',\n        ),\n        ['a', 'b'],\n        revalidate_instances=revalidate_instances,\n    )\n    v = SchemaValidator(schema)\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=expected.message) as exc_info:\n            print(v.validate_python(input_value))\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        dc = v.validate_python(input_value)\n        assert dataclasses.is_dataclass(dc)\n        assert dataclasses.asdict(dc) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004865", "source": "def decimal_encoder(dec_value: Decimal) -> Union[int, float]:\n    if dec_value.as_tuple().exponent >= 0:\n        return int(dec_value)\n    else:\n        return float(dec_value)", "target": "def test_to_jsonable_python_fallback():\n    with pytest.raises(PydanticSerializationError, match=r'Unable to serialize unknown type: <.+\\.Foobar'):\n        to_jsonable_python(Foobar())\n    assert to_jsonable_python(Foobar(), serialize_unknown=True) == 'Foobar.__str__'\n    assert to_jsonable_python(Foobar(), serialize_unknown=True, fallback=fallback_func) == 'fallback:Foobar'\n    assert to_jsonable_python(Foobar(), fallback=fallback_func) == 'fallback:Foobar'"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004866", "source": "def force_create_dir(path: Union[str, Path]) -> Path:\n    remove_dir(path)\n    return ensure_dir_exists(path)", "target": "def test_get_path_resolves(self):\n        rel_str = \"sub/f.txt\"\n        p = get_path(str(self.tmp_path / rel_str), resolve=True)\n        self.assertTrue(p.is_absolute())\n        self.assertTrue(str(p).endswith(rel_str))"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004867", "source": "def str2bool(value: Optional[str]) -> bool:\n    if not value:\n        return False\n    if not isinstance(value, str):\n        raise ValueError(\n            f\"Expected a string value for boolean conversion, got {type(value)}\"\n        )\n    value = value.strip().lower()\n    true_value_set = {\"1\", \"true\", \"t\", \"yes\", \"y\", \"on\", \"enable\", \"enabled\", \"found\"}\n    false_value_set = {\"0\", \"false\", \"f\", \"no\", \"n\", \"off\", \"disable\"}\n    if value in true_value_set:\n        return True\n    if value in false_value_set:\n        return False\n    raise ValueError(f\"Invalid string value for boolean conversion: {value}\")", "target": "def test_double_asterisks(self) -> None:\n        allowed_patterns = [\n            \"aten/src/ATen/native/**LinearAlgebra*\",\n        ]\n        patterns_re = patterns_to_regex(allowed_patterns)\n        fnames = [\n            \"aten/src/ATen/native/LinearAlgebra.cpp\",\n            \"aten/src/ATen/native/cpu/LinearAlgebraKernel.cpp\",\n        ]\n        for filename in fnames:\n            self.assertTrue(patterns_re.match(filename))"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004868", "source": "def is_stdlib_dataclass(cls: type[Any], /) -> TypeIs[type[StandardDataclass]]:\n    return '__dataclass_fields__' in cls.__dict__ and not hasattr(cls, '__pydantic_validator__')", "target": "def test_dataclass_post_init_args_multiple():\n    dc_args = None\n    @dataclasses.dataclass\n    class Foo:\n        a: str\n        b: dataclasses.InitVar[bool]\n        c: dataclasses.InitVar[int]\n        def __post_init__(self, *args):\n            nonlocal dc_args\n            dc_args = args\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), init_only=True),\n                core_schema.dataclass_field(name='c', schema=core_schema.int_schema(), init_only=True),\n            ],\n            collect_init_only=True,\n        ),\n        ['a', 'b'],\n        post_init=True,\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': b'hello', 'b': 'true', 'c': '42'})\n    assert dataclasses.asdict(foo) == {'a': 'hello'}\n    assert dc_args == (True, 42)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004869", "source": "def collect_dataclass_fields(\n    cls: type[StandardDataclass],\n    *,\n    config_wrapper: ConfigWrapper,\n    ns_resolver: NsResolver | None = None,\n    typevars_map: dict[Any, Any] | None = None,\n) -> dict[str, FieldInfo]:\n    FieldInfo_ = import_cached_field_info()\n    fields: dict[str, FieldInfo] = {}\n    ns_resolver = ns_resolver or NsResolver()\n    dataclass_fields = cls.__dataclass_fields__\n    for base in reversed(cls.__mro__):\n        if not dataclasses.is_dataclass(base):\n            continue\n        with ns_resolver.push(base):\n            for ann_name, dataclass_field in dataclass_fields.items():\n                base_anns = _typing_extra.safe_get_annotations(base)\n                if ann_name not in base_anns:\n                    continue\n                globalns, localns = ns_resolver.types_namespace\n                ann_type, evaluated = _typing_extra.try_eval_type(dataclass_field.type, globalns, localns)\n                if _typing_extra.is_classvar_annotation(ann_type):\n                    continue\n                if (\n                    not dataclass_field.init\n                    and dataclass_field.default is dataclasses.MISSING\n                    and dataclass_field.default_factory is dataclasses.MISSING\n                ):\n                    continue\n                if isinstance(dataclass_field.default, FieldInfo_):\n                    if dataclass_field.default.init_var:\n                        if dataclass_field.default.init is False:\n                            raise PydanticUserError(\n                                f'Dataclass field {ann_name} has init=False and init_var=True, but these are mutually exclusive.',\n                                code='clashing-init-and-init-var',\n                            )\n                        continue\n                    field_info = FieldInfo_.from_annotated_attribute(\n                        ann_type, dataclass_field.default, _source=AnnotationSource.DATACLASS\n                    )\n                    field_info._original_assignment = dataclass_field.default\n                else:\n                    field_info = FieldInfo_.from_annotated_attribute(\n                        ann_type, dataclass_field, _source=AnnotationSource.DATACLASS\n                    )\n                    field_info._original_assignment = dataclass_field\n                if not evaluated:\n                    field_info._complete = False\n                    field_info._original_annotation = ann_type\n                fields[ann_name] = field_info\n                update_field_from_config(config_wrapper, ann_name, field_info)\n                if field_info.default is not PydanticUndefined and isinstance(\n                    getattr(cls, ann_name, field_info), FieldInfo_\n                ):\n                    setattr(cls, ann_name, field_info.default)\n    if typevars_map:\n        for field in fields.values():\n            field.apply_typevars_map(typevars_map)\n    if config_wrapper.use_attribute_docstrings:\n        _update_fields_from_docstrings(\n            cls,\n            fields,\n            use_inspect=not hasattr(cls, '__is_pydantic_dataclass__'),\n        )\n    return fields", "target": "def test_with_default_factory():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'x': core_schema.model_field(\n                    schema=core_schema.with_default_schema(\n                        schema=core_schema.str_schema(), default_factory=lambda: 'pikachu'\n                    )\n                )\n            }\n        )\n    )\n    assert v.validate_python({}) == ({'x': 'pikachu'}, None, set())\n    assert v.validate_python({'x': 'bulbi'}) == ({'x': 'bulbi'}, None, {'x'})"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004870", "source": "def get_temp_dir() -> str:\n    global _TEMPDIR\n    if _TEMPDIR is None:\n        _TEMPDIR = _make_temp_dir(\n            prefix=\"instruction_count_microbenchmarks\", gc_dev_shm=True\n        )\n        atexit.register(shutil.rmtree, path=_TEMPDIR)\n    return _TEMPDIR", "target": "def test_compute_diff(self) -> None:\n        diff = self.repo.diff(\"HEAD\")\n        sha = _shasum(diff)\n        self.assertEqual(len(sha), 64)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004871", "source": "def create_dataclass(cls: type[Any]) -> type[PydanticDataclass]:\n        from ._internal._utils import is_model_class\n        if is_model_class(cls):\n            raise PydanticUserError(\n                f'Cannot create a Pydantic dataclass from {cls.__name__} as it is already a Pydantic model',\n                code='dataclass-on-model',\n            )\n        original_cls = cls\n        has_dataclass_base = any(dataclasses.is_dataclass(base) for base in cls.__bases__)\n        if not has_dataclass_base and config is not None and hasattr(cls, '__pydantic_config__'):\n            warn(\n                f'`config` is set via both the `dataclass` decorator and `__pydantic_config__` for dataclass {cls.__name__}. '\n                f'The `config` specification from `dataclass` decorator will take priority.',\n                category=UserWarning,\n                stacklevel=2,\n            )\n        config_dict = config if config is not None else getattr(cls, '__pydantic_config__', None)\n        config_wrapper = _config.ConfigWrapper(config_dict)\n        decorators = _decorators.DecoratorInfos.build(cls, replace_wrapped_methods=True)\n        decorators.update_from_config(config_wrapper)\n        original_doc = cls.__doc__\n        if _pydantic_dataclasses.is_stdlib_dataclass(cls):\n            original_doc = None\n            bases = (cls,)\n            if issubclass(cls, Generic):\n                generic_base = Generic[cls.__parameters__]\n                bases = bases + (generic_base,)\n            cls = types.new_class(cls.__name__, bases)\n        if frozen is not None:\n            frozen_ = frozen\n            if config_wrapper.frozen:\n                warn(\n                    f'`frozen` is set via both the `dataclass` decorator and `config` for dataclass {cls.__name__!r}.'\n                    'This is not recommended. The `frozen` specification on `dataclass` will take priority.',\n                    category=UserWarning,\n                    stacklevel=2,\n                )\n        else:\n            frozen_ = config_wrapper.frozen or False\n        cls_anns = _typing_extra.safe_get_annotations(cls)\n        for field_name in cls_anns:\n            field_value = getattr(cls, field_name, None)\n            if isinstance(field_value, FieldInfo):\n                setattr(cls, field_name, _pydantic_dataclasses.as_dataclass_field(field_value))\n        with _pydantic_dataclasses.patch_base_fields(cls):\n            cls = dataclasses.dataclass(\n                cls,\n                init=True,\n                repr=repr,\n                eq=eq,\n                order=order,\n                unsafe_hash=unsafe_hash,\n                frozen=frozen_,\n                **kwargs,\n            )\n        if config_wrapper.validate_assignment:\n            original_setattr = cls.__setattr__\n            @functools.wraps(cls.__setattr__)\n            def validated_setattr(instance: PydanticDataclass, name: str, value: Any, /) -> None:\n                if frozen_:\n                    return original_setattr(instance, name, value)\n                inst_cls = type(instance)\n                attr = getattr(inst_cls, name, None)\n                if isinstance(attr, property):\n                    attr.__set__(instance, value)\n                elif isinstance(attr, functools.cached_property):\n                    instance.__dict__.__setitem__(name, value)\n                else:\n                    inst_cls.__pydantic_validator__.validate_assignment(instance, name, value)\n            cls.__setattr__ = validated_setattr.__get__(None, cls)\n            if slots and not hasattr(cls, '__setstate__'):\n                def _dataclass_getstate(self: Any) -> list[Any]:\n                    return [getattr(self, f.name) for f in dataclasses.fields(self)]\n                def _dataclass_setstate(self: Any, state: list[Any]) -> None:\n                    for field, value in zip(dataclasses.fields(self), state):\n                        object.__setattr__(self, field.name, value)\n                cls.__getstate__ = _dataclass_getstate\n                cls.__setstate__ = _dataclass_setstate\n        cls.__is_pydantic_dataclass__ = True\n        cls.__pydantic_decorators__ = decorators\n        cls.__doc__ = original_doc\n        firstlineno = getattr(original_cls, '__firstlineno__', None)\n        cls.__module__ = original_cls.__module__\n        if sys.version_info >= (3, 13) and firstlineno is not None:\n            original_cls.__firstlineno__ = firstlineno\n            cls.__firstlineno__ = firstlineno\n        cls.__qualname__ = original_cls.__qualname__\n        cls.__pydantic_fields_complete__ = classmethod(_pydantic_fields_complete)\n        cls.__pydantic_complete__ = False\n        _pydantic_dataclasses.complete_dataclass(cls, config_wrapper, raise_errors=False)\n        return cls", "target": "def test_dataclass_field_before_validator():\n    @dataclasses.dataclass\n    class Foo:\n        a: int\n        b: str\n        @classmethod\n        def validate_b(cls, v: bytes, info: core_schema.ValidationInfo) -> bytes:\n            assert v == b'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return b'hello world!'\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(\n                    name='b',\n                    schema=core_schema.with_info_before_validator_function(Foo.validate_b, core_schema.str_schema()),\n                ),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 1, 'b': b'hello'})\n    assert dataclasses.asdict(foo) == {'a': 1, 'b': 'hello world!'}"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004872", "source": "def get_merge_base(self, from_ref: str, to_ref: str) -> str:\n        return self._run_git(\"merge-base\", from_ref, to_ref).strip()", "target": "def test_fails(self) -> None:\n        @retries_decorator(rc=0)\n        def foo(x: int, y: int) -> int:\n            return x + y\n        self.assertEqual(foo(\"a\", 4), 0)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004873", "source": "def timedelta_isoformat(td: datetime.timedelta) -> str:\n    warnings.warn('`timedelta_isoformat` is deprecated.', category=PydanticDeprecatedSince20, stacklevel=2)\n    minutes, seconds = divmod(td.seconds, 60)\n    hours, minutes = divmod(minutes, 60)\n    return f'{\"-\" if td.days < 0 else \"\"}P{abs(td.days)}DT{hours:d}H{minutes:d}M{seconds:d}.{td.microseconds:06d}S'", "target": "def test_list_int(py_and_json: PyAndJson, input_value, expected):\n    v = py_and_json(core_schema.json_schema(core_schema.list_schema(core_schema.int_schema())))\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=expected.message) as exc_info:\n            v.validate_test(input_value)\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        assert v.validate_test(input_value) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004874", "source": "def _general_metadata_cls() -> type[BaseMetadata]:\n    from annotated_types import BaseMetadata\n    class _PydanticGeneralMetadata(PydanticMetadata, BaseMetadata):\n        def __init__(self, metadata: Any):\n            self.__dict__ = metadata\n    return _PydanticGeneralMetadata", "target": "def test_extra_behavior_allow_keys_validation() -> None:\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            {}, extra_behavior='allow', extras_keys_schema=core_schema.str_schema(max_length=3)\n        )\n    )\n    m, model_extra, fields_set = v.validate_python({'ext': 123})\n    assert m == {}\n    assert model_extra == {'ext': 123}\n    assert fields_set == {'ext'}\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python({'extra_too_long': 123})\n    assert exc_info.value.errors()[0]['type'] == 'string_too_long'"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004875", "source": "def _dataclass_validate_assignment_setattr(self: 'Dataclass', name: str, value: Any) -> None:\n    if self.__pydantic_initialised__:\n        d = dict(self.__dict__)\n        d.pop(name, None)\n        known_field = self.__pydantic_model__.__fields__.get(name, None)\n        if known_field:\n            value, error_ = known_field.validate(value, d, loc=name, cls=self.__class__)\n            if error_:\n                raise ValidationError([error_], self.__class__)\n    object.__setattr__(self, name, value)", "target": "def test_dataclass_field_wrap_validator1():\n    @dataclasses.dataclass\n    class Foo:\n        a: int\n        b: str\n        @classmethod\n        def validate_b(\n            cls, v: bytes, nxt: core_schema.ValidatorFunctionWrapHandler, info: core_schema.ValidationInfo\n        ) -> str:\n            assert v == b'hello'\n            v = nxt(v)\n            assert v == 'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return 'hello world!'\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(\n                    name='b',\n                    schema=core_schema.with_info_wrap_validator_function(Foo.validate_b, core_schema.str_schema()),\n                ),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 1, 'b': b'hello'})\n    assert dataclasses.asdict(foo) == {'a': 1, 'b': 'hello world!'}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004876", "source": "def is_pydantic_dataclass(class_: type[Any], /) -> TypeGuard[type[PydanticDataclass]]:\n    try:\n        return '__is_pydantic_dataclass__' in class_.__dict__ and dataclasses.is_dataclass(class_)\n    except AttributeError:\n        return False", "target": "def test_frozen_field():\n    @dataclasses.dataclass\n    class MyModel:\n        f: str\n    v = SchemaValidator(\n        core_schema.dataclass_schema(\n            MyModel,\n            core_schema.dataclass_args_schema(\n                'MyModel', [core_schema.dataclass_field('f', core_schema.str_schema(), frozen=True)]\n            ),\n            ['f'],\n        )\n    )\n    m = v.validate_python({'f': 'x'})\n    assert m.f == 'x'\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_assignment(m, 'f', 'y')\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'frozen_field', 'loc': ('f',), 'msg': 'Field is frozen', 'input': 'y'}\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004877", "source": "def current_branch(self) -> Optional[str]:\n        try:\n            return self._run_git(\"symbolic-ref\", \"--short\", \"HEAD\").strip()\n        except RuntimeError:\n            return None", "target": "def test_simple(self) -> None:\n        @retries_decorator()\n        def foo(x: int, y: int) -> int:\n            return x + y\n        self.assertEqual(foo(3, 4), 7)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004878", "source": "def prepare_config(config: ConfigDict | dict[str, Any] | type[Any] | None) -> ConfigDict:\n    if config is None:\n        return ConfigDict()\n    if not isinstance(config, dict):\n        warnings.warn(DEPRECATION_MESSAGE, PydanticDeprecatedSince20, stacklevel=4)\n        config = {k: getattr(config, k) for k in dir(config) if not k.startswith('__')}\n    config_dict = cast(ConfigDict, config)\n    check_deprecated(config_dict)\n    return config_dict", "target": "def test_cache_strings():\n    v = SchemaValidator(cs.str_schema())\n    assert 'cache_strings=True' in plain_repr(v)\n    v = SchemaValidator(cs.str_schema(), config=CoreConfig(cache_strings=True))\n    assert 'cache_strings=True' in plain_repr(v)\n    v = SchemaValidator(cs.str_schema(), config=CoreConfig(cache_strings=False))\n    assert 'cache_strings=False' in plain_repr(v)\n    v = SchemaValidator(cs.str_schema(), config=CoreConfig(cache_strings='keys'))\n    assert \"cache_strings='keys'\" in plain_repr(v)"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004879", "source": "def to_markdown_table(\n    res: TimingResultType, header: Optional[tuple[str, ...]] = None\n) -> str:\n    if header is None:\n        header = (\"model\", \"task\", \"mean\", \"var\")\n    out = \"\"\n    def write_line(*args):\n        nonlocal out\n        out += f\"| {' | '.join(str(a) for a in args)} |\\n\"\n    write_line(*header)\n    write_line(*[\"--\"] * len(header))\n    for model, tasks in res.items():\n        for task, line in tasks.items():\n            write_line(*(model, task) + line)\n    return out", "target": "def test_is_iterable(self) -> None:\n        from collections.abc import Iterator\n        iter_ = PeekableIterator(\"\")\n        self.assertTrue(isinstance(iter_, Iterator))"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004880", "source": "def _apply_field_title_generator_to_field_info(\n    title_generator: Callable[[str, FieldInfo], str],\n    field_name: str,\n    field_info: FieldInfo,\n):\n    if field_info.title is None:\n        title = title_generator(field_name, field_info)\n        if not isinstance(title, str):\n            raise TypeError(f'field_title_generator {title_generator} must return str, not {title.__class__}')\n        field_info.title = title", "target": "def test_alias_error_loc_field_names(py_and_json: PyAndJson):\n    v = py_and_json(\n        {\n            'type': 'model-fields',\n            'fields': {\n                'field_a': {\n                    'type': 'model-field',\n                    'schema': {'type': 'int'},\n                    'validation_alias': [['foo'], ['bar', 1, -1]],\n                }\n            },\n        },\n        {'loc_by_alias': False},\n    )\n    assert v.validate_test({'foo': 42}) == ({'field_a': 42}, None, {'field_a'})\n    assert v.validate_test({'bar': ['x', [1, 2, 42]]}) == ({'field_a': 42}, None, {'field_a'})\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_test({'foo': 'not_int'})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('field_a',),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'not_int',\n        }\n    ]\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_test({'bar': ['x', [1, 2, 'not_int']]})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('field_a',),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'not_int',\n        }\n    ]\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_test({})\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'missing', 'loc': ('field_a',), 'msg': 'Field required', 'input': {}}\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004881", "source": "def check_for_functorch():\n    try:\n        import functorch\n        return True\n    except ImportError:\n        return False", "target": "def test_multiple_vars_and_missing_cleanup(self):\n        v1, v2 = \"TEST_ENV_V1\", \"TEST_ENV_V2\"\n        os.environ.pop(v1, None)\n        os.environ[v2] = \"keep\"\n        with temp_environ({v1: \"a\", v2: \"b\"}):\n            self.assertEqual(os.environ[v1], \"a\")\n            self.assertEqual(os.environ[v2], \"b\")\n        self.assertNotIn(v1, os.environ)\n        self.assertEqual(os.environ[v2], \"keep\")"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004882", "source": "def handle_extra_init(self: 'Dataclass', *args: Any, **kwargs: Any) -> None:\n        if config.extra == Extra.ignore:\n            init(self, *args, **{k: v for k, v in kwargs.items() if k in self.__dataclass_fields__})\n        elif config.extra == Extra.allow:\n            for k, v in kwargs.items():\n                self.__dict__.setdefault(k, v)\n            init(self, *args, **{k: v for k, v in kwargs.items() if k in self.__dataclass_fields__})\n        else:\n            init(self, *args, **kwargs)", "target": "def test_dataclass_post_init_args():\n    c_value = None\n    @dataclasses.dataclass\n    class Foo:\n        a: str\n        b: bool\n        c: dataclasses.InitVar[int]\n        def __post_init__(self, c: int):\n            nonlocal c_value\n            c_value = c\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n                core_schema.dataclass_field(name='c', schema=core_schema.int_schema(), init_only=True),\n            ],\n            collect_init_only=True,\n        ),\n        ['a', 'b'],\n        post_init=True,\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': b'hello', 'b': 'true', 'c': '42'})\n    assert foo.a == 'hello'\n    assert foo.b is True\n    assert not hasattr(foo, 'c')\n    assert c_value == 42"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004883", "source": "def create_dataclass(cls: type[Any]) -> type[PydanticDataclass]:\n        from ._internal._utils import is_model_class\n        if is_model_class(cls):\n            raise PydanticUserError(\n                f'Cannot create a Pydantic dataclass from {cls.__name__} as it is already a Pydantic model',\n                code='dataclass-on-model',\n            )\n        original_cls = cls\n        has_dataclass_base = any(dataclasses.is_dataclass(base) for base in cls.__bases__)\n        if not has_dataclass_base and config is not None and hasattr(cls, '__pydantic_config__'):\n            warn(\n                f'`config` is set via both the `dataclass` decorator and `__pydantic_config__` for dataclass {cls.__name__}. '\n                f'The `config` specification from `dataclass` decorator will take priority.',\n                category=UserWarning,\n                stacklevel=2,\n            )\n        config_dict = config if config is not None else getattr(cls, '__pydantic_config__', None)\n        config_wrapper = _config.ConfigWrapper(config_dict)\n        decorators = _decorators.DecoratorInfos.build(cls, replace_wrapped_methods=True)\n        decorators.update_from_config(config_wrapper)\n        original_doc = cls.__doc__\n        if _pydantic_dataclasses.is_stdlib_dataclass(cls):\n            original_doc = None\n            bases = (cls,)\n            if issubclass(cls, Generic):\n                generic_base = Generic[cls.__parameters__]\n                bases = bases + (generic_base,)\n            cls = types.new_class(cls.__name__, bases)\n        if frozen is not None:\n            frozen_ = frozen\n            if config_wrapper.frozen:\n                warn(\n                    f'`frozen` is set via both the `dataclass` decorator and `config` for dataclass {cls.__name__!r}.'\n                    'This is not recommended. The `frozen` specification on `dataclass` will take priority.',\n                    category=UserWarning,\n                    stacklevel=2,\n                )\n        else:\n            frozen_ = config_wrapper.frozen or False\n        cls_anns = _typing_extra.safe_get_annotations(cls)\n        for field_name in cls_anns:\n            field_value = getattr(cls, field_name, None)\n            if isinstance(field_value, FieldInfo):\n                setattr(cls, field_name, _pydantic_dataclasses.as_dataclass_field(field_value))\n        with _pydantic_dataclasses.patch_base_fields(cls):\n            cls = dataclasses.dataclass(\n                cls,\n                init=True,\n                repr=repr,\n                eq=eq,\n                order=order,\n                unsafe_hash=unsafe_hash,\n                frozen=frozen_,\n                **kwargs,\n            )\n        if config_wrapper.validate_assignment:\n            original_setattr = cls.__setattr__\n            @functools.wraps(cls.__setattr__)\n            def validated_setattr(instance: PydanticDataclass, name: str, value: Any, /) -> None:\n                if frozen_:\n                    return original_setattr(instance, name, value)\n                inst_cls = type(instance)\n                attr = getattr(inst_cls, name, None)\n                if isinstance(attr, property):\n                    attr.__set__(instance, value)\n                elif isinstance(attr, functools.cached_property):\n                    instance.__dict__.__setitem__(name, value)\n                else:\n                    inst_cls.__pydantic_validator__.validate_assignment(instance, name, value)\n            cls.__setattr__ = validated_setattr.__get__(None, cls)\n            if slots and not hasattr(cls, '__setstate__'):\n                def _dataclass_getstate(self: Any) -> list[Any]:\n                    return [getattr(self, f.name) for f in dataclasses.fields(self)]\n                def _dataclass_setstate(self: Any, state: list[Any]) -> None:\n                    for field, value in zip(dataclasses.fields(self), state):\n                        object.__setattr__(self, field.name, value)\n                cls.__getstate__ = _dataclass_getstate\n                cls.__setstate__ = _dataclass_setstate\n        cls.__is_pydantic_dataclass__ = True\n        cls.__pydantic_decorators__ = decorators\n        cls.__doc__ = original_doc\n        firstlineno = getattr(original_cls, '__firstlineno__', None)\n        cls.__module__ = original_cls.__module__\n        if sys.version_info >= (3, 13) and firstlineno is not None:\n            original_cls.__firstlineno__ = firstlineno\n            cls.__firstlineno__ = firstlineno\n        cls.__qualname__ = original_cls.__qualname__\n        cls.__pydantic_fields_complete__ = classmethod(_pydantic_fields_complete)\n        cls.__pydantic_complete__ = False\n        _pydantic_dataclasses.complete_dataclass(cls, config_wrapper, raise_errors=False)\n        return cls", "target": "def test_dataclass_validate_assignment():\n    schema = core_schema.dataclass_schema(\n        FooDataclass,\n        core_schema.dataclass_args_schema(\n            'FooDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), kw_only=False),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), kw_only=False),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 'hello', 'b': 'True'})\n    assert dataclasses.asdict(foo) == {'a': 'hello', 'b': True}\n    v.validate_assignment(foo, 'a', b'world')\n    assert dataclasses.asdict(foo) == {'a': 'world', 'b': True}\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_assignment(foo, 'a', 123)\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'string_type', 'loc': ('a',), 'msg': 'Input should be a valid string', 'input': 123}\n    ]\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_assignment(foo, 'c', '123')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'no_such_attribute',\n            'loc': ('c',),\n            'msg': \"Object has no attribute 'c'\",\n            'input': '123',\n            'ctx': {'attribute': 'c'},\n        }\n    ]\n    assert not hasattr(foo, 'c')\n    with pytest.raises(AttributeError, match=\"'str' object has no attribute 'a'\"):\n        v.validate_assignment('field_a', 'c', 123)"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004884", "source": "def write_line(*args):\n        nonlocal out\n        out += f\"| {' | '.join(str(a) for a in args)} |\\n\"", "target": "def test_restores_on_exception(self):\n        start = Path.cwd()\n        with tempfile.TemporaryDirectory() as td:\n            target = Path(td) / \"wd_exc\"\n            target.mkdir()\n            with self.assertRaises(ValueError):\n                with working_directory(str(target)):\n                    self.assertEqual(Path.cwd().resolve(), target.resolve())\n                    raise ValueError(\"boom\")\n        self.assertEqual(Path.cwd().resolve(), start.resolve())"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004885", "source": "def _is_field_cached_property(obj: 'Dataclass', k: str) -> bool:\n        return False", "target": "def test_aliases(py_and_json: PyAndJson):\n    schema = core_schema.dataclass_args_schema(\n        'MyDataclass',\n        [\n            core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), validation_alias='Apple'),\n            core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), validation_alias=['Banana', 1]),\n            core_schema.dataclass_field(\n                name='c', schema=core_schema.int_schema(), validation_alias=['Carrot', 'v'], init_only=True\n            ),\n        ],\n        collect_init_only=True,\n    )\n    v = py_and_json(schema)\n    assert v.validate_test({'Apple': 'a', 'Banana': ['x', 'false'], 'Carrot': {'v': '42'}}) == (\n        {'a': 'a', 'b': False},\n        (42,),\n    )"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004886", "source": "def patterns_to_regex(allowed_patterns: list[str]) -> Any:\n    rc = \"(\"\n    for idx, pattern in enumerate(allowed_patterns):\n        if idx > 0:\n            rc += \"|\"\n        pattern_ = PeekableIterator(pattern)\n        assert not any(c in pattern for c in \"{}()[]\\\\\")\n        for c in pattern_:\n            if c == \".\":\n                rc += \"\\\\.\"\n            elif c == \"+\":\n                rc += \"\\\\+\"\n            elif c == \"*\":\n                if pattern_.peek() == \"*\":\n                    next(pattern_)\n                    rc += \".*\"\n                else:\n                    rc += \"[^/]*\"\n            else:\n                rc += c\n    rc += \")\"\n    return re.compile(rc)", "target": "def test_peek(self, input_: str = \"abcdef\") -> None:\n        iter_ = PeekableIterator(input_)\n        for idx, c in enumerate(iter_):\n            if idx + 1 < len(input_):\n                self.assertEqual(iter_.peek(), input_[idx + 1])\n            else:\n                self.assertTrue(iter_.peek() is None)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004887", "source": "def _general_metadata_cls() -> type[BaseMetadata]:\n    from annotated_types import BaseMetadata\n    class _PydanticGeneralMetadata(PydanticMetadata, BaseMetadata):\n        def __init__(self, metadata: Any):\n            self.__dict__ = metadata\n    return _PydanticGeneralMetadata", "target": "def test_validate_assignment():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(fields={'field_a': core_schema.model_field(schema=core_schema.str_schema())})\n    )\n    assert v.validate_python({'field_a': 'test'}) == ({'field_a': 'test'}, None, {'field_a'})\n    data = {'field_a': 'test'}\n    assert v.validate_assignment(data, 'field_a', b'abc') == ({'field_a': 'abc'}, None, {'field_a'})\n    assert data == {'field_a': 'abc'}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004888", "source": "def dataclass(\n        *,\n        init: Literal[False] = False,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool = False,\n        config: ConfigDict | type[object] | None = None,\n        validate_on_init: bool | None = None,\n        kw_only: bool = ...,\n        slots: bool = ...,\n    ) -> Callable[[type[_T]], type[PydanticDataclass]]:\n        ...", "target": "def test_dataclass_self_init():\n    @dataclasses.dataclass(init=False)\n    class Foo:\n        a: str\n        b: bool\n        def __init__(self, *args, **kwargs):\n            v.validate_python(ArgsKwargs(args, kwargs), self_instance=self)\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), kw_only=False),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), kw_only=False),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    foo = Foo(b'hello', 'True')\n    assert dataclasses.is_dataclass(foo)\n    assert dataclasses.asdict(foo) == {'a': 'hello', 'b': True}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004889", "source": "def dataclass(\n        _cls: Type[_T],\n        *,\n        init: bool = True,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool = False,\n        config: Union[ConfigDict, Type[object], None] = None,\n        validate_on_init: Optional[bool] = None,\n        use_proxy: Optional[bool] = None,\n    ) -> 'DataclassClassOrWrapper':\n        ...", "target": "def test_dataclass_initvar_not_required_on_union_ser() -> None:\n    @dataclasses.dataclass\n    class Foo:\n        x: int\n        init_var: dataclasses.InitVar[int] = 1\n    @dataclasses.dataclass\n    class Bar:\n        x: int\n    schema = core_schema.union_schema(\n        [\n            core_schema.dataclass_schema(\n                Foo,\n                core_schema.dataclass_args_schema(\n                    'Foo',\n                    [\n                        core_schema.dataclass_field(name='x', schema=core_schema.int_schema()),\n                        core_schema.dataclass_field(\n                            name='init_var',\n                            init_only=True,\n                            schema=core_schema.with_default_schema(core_schema.int_schema(), default=1),\n                        ),\n                    ],\n                ),\n                ['x'],\n                post_init=True,\n            ),\n            core_schema.dataclass_schema(\n                Bar,\n                core_schema.dataclass_args_schema(\n                    'Bar', [core_schema.dataclass_field(name='x', schema=core_schema.int_schema())]\n                ),\n                ['x'],\n            ),\n        ]\n    )\n    s = SchemaSerializer(schema)\n    assert s.to_python(Foo(x=1), warnings='error') == {'x': 1}\n    assert s.to_python(Foo(x=1, init_var=2), warnings='error') == {'x': 1}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004890", "source": "def collect_dataclass_fields(\n    cls: type[StandardDataclass],\n    *,\n    config_wrapper: ConfigWrapper,\n    ns_resolver: NsResolver | None = None,\n    typevars_map: dict[Any, Any] | None = None,\n) -> dict[str, FieldInfo]:\n    FieldInfo_ = import_cached_field_info()\n    fields: dict[str, FieldInfo] = {}\n    ns_resolver = ns_resolver or NsResolver()\n    dataclass_fields = cls.__dataclass_fields__\n    for base in reversed(cls.__mro__):\n        if not dataclasses.is_dataclass(base):\n            continue\n        with ns_resolver.push(base):\n            for ann_name, dataclass_field in dataclass_fields.items():\n                base_anns = _typing_extra.safe_get_annotations(base)\n                if ann_name not in base_anns:\n                    continue\n                globalns, localns = ns_resolver.types_namespace\n                ann_type, evaluated = _typing_extra.try_eval_type(dataclass_field.type, globalns, localns)\n                if _typing_extra.is_classvar_annotation(ann_type):\n                    continue\n                if (\n                    not dataclass_field.init\n                    and dataclass_field.default is dataclasses.MISSING\n                    and dataclass_field.default_factory is dataclasses.MISSING\n                ):\n                    continue\n                if isinstance(dataclass_field.default, FieldInfo_):\n                    if dataclass_field.default.init_var:\n                        if dataclass_field.default.init is False:\n                            raise PydanticUserError(\n                                f'Dataclass field {ann_name} has init=False and init_var=True, but these are mutually exclusive.',\n                                code='clashing-init-and-init-var',\n                            )\n                        continue\n                    field_info = FieldInfo_.from_annotated_attribute(\n                        ann_type, dataclass_field.default, _source=AnnotationSource.DATACLASS\n                    )\n                    field_info._original_assignment = dataclass_field.default\n                else:\n                    field_info = FieldInfo_.from_annotated_attribute(\n                        ann_type, dataclass_field, _source=AnnotationSource.DATACLASS\n                    )\n                    field_info._original_assignment = dataclass_field\n                if not evaluated:\n                    field_info._complete = False\n                    field_info._original_annotation = ann_type\n                fields[ann_name] = field_info\n                update_field_from_config(config_wrapper, ann_name, field_info)\n                if field_info.default is not PydanticUndefined and isinstance(\n                    getattr(cls, ann_name, field_info), FieldInfo_\n                ):\n                    setattr(cls, ann_name, field_info.default)\n    if typevars_map:\n        for field in fields.values():\n            field.apply_typevars_map(typevars_map)\n    if config_wrapper.use_attribute_docstrings:\n        _update_fields_from_docstrings(\n            cls,\n            fields,\n            use_inspect=not hasattr(cls, '__is_pydantic_dataclass__'),\n        )\n    return fields", "target": "def test_empty_model():\n    v = SchemaValidator(core_schema.model_fields_schema(fields={}))\n    assert v.validate_python({}) == ({}, None, set())\n    with pytest.raises(\n        ValidationError, match=re.escape('Input should be a valid dictionary or instance of Model [type=model_type,')\n    ):\n        v.validate_python('x')"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004891", "source": "def env_str_field(\n    name: str,\n    default: str = \"\",\n) -> str:\n    return field(default_factory=lambda: get_env(name, default))", "target": "def test_env_bool_uses_default_when_unset(self):\n        with patch.dict(os.environ, {}, clear=True):\n            self.assertTrue(m.env_bool(\"FLAG\", default=True))\n            self.assertFalse(m.env_bool(\"FLAG\", default=False))"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004892", "source": "def patterns_to_regex(allowed_patterns: list[str]) -> Any:\n    rc = \"(\"\n    for idx, pattern in enumerate(allowed_patterns):\n        if idx > 0:\n            rc += \"|\"\n        pattern_ = PeekableIterator(pattern)\n        assert not any(c in pattern for c in \"{}()[]\\\\\")\n        for c in pattern_:\n            if c == \".\":\n                rc += \"\\\\.\"\n            elif c == \"+\":\n                rc += \"\\\\+\"\n            elif c == \"*\":\n                if pattern_.peek() == \"*\":\n                    next(pattern_)\n                    rc += \".*\"\n                else:\n                    rc += \"[^/]*\"\n            else:\n                rc += c\n    rc += \")\"\n    return re.compile(rc)", "target": "def test_is_iterable(self) -> None:\n        from collections.abc import Iterator\n        iter_ = PeekableIterator(\"\")\n        self.assertTrue(isinstance(iter_, Iterator))"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004893", "source": "def working_directory(path: str):\n    if not path:\n        yield\n        return\n    prev_cwd = os.getcwd()\n    try:\n        os.chdir(path)\n        yield\n    finally:\n        os.chdir(prev_cwd)", "target": "def test_pr_with_missing_labels(\n        self, mocked_rn_labels: Any, mocked_gql: Any\n    ) -> None:\n        \"Test PR with no 'release notes:' label or 'topic: not user facing' label\"\n        pr = GitHubPR(\"pytorch\", \"pytorch\", 82169)\n        self.assertFalse(has_required_labels(pr))"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004894", "source": "def _del_nested_attr(obj: nn.Module, names: list[str]) -> None:\n    if len(names) == 1:\n        delattr(obj, names[0])\n    else:\n        _del_nested_attr(getattr(obj, names[0]), names[1:])", "target": "def test_compute_diff(self) -> None:\n        diff = self.repo.diff(\"HEAD\")\n        sha = _shasum(diff)\n        self.assertEqual(len(sha), 64)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004895", "source": "def _check_classvar(v: Optional[Type[Any]]) -> bool:\n    if v is None:\n        return False\n    return v.__class__ == ClassVar.__class__ and getattr(v, '_name', None) == 'ClassVar'", "target": "def test_schema_validator() -> None:\n    SchemaValidator({'type': 'int'})"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004896", "source": "def to_markdown_table(\n    res: TimingResultType, header: Optional[tuple[str, ...]] = None\n) -> str:\n    if header is None:\n        header = (\"model\", \"task\", \"mean\", \"var\")\n    out = \"\"\n    def write_line(*args):\n        nonlocal out\n        out += f\"| {' | '.join(str(a) for a in args)} |\\n\"\n    write_line(*header)\n    write_line(*[\"--\"] * len(header))\n    for model, tasks in res.items():\n        for task, line in tasks.items():\n            write_line(*(model, task) + line)\n    return out", "target": "def test_overwrites_and_restores_existing_var(self):\n        var = \"TEST_TMP_ENV_OVERWRITE\"\n        os.environ[var] = \"orig\"\n        with temp_environ({var: \"override\"}):\n            self.assertEqual(os.environ[var], \"override\")\n        self.assertEqual(os.environ[var], \"orig\")"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004897", "source": "def working_directory(path: str):\n    if not path:\n        yield\n        return\n    prev_cwd = os.getcwd()\n    try:\n        os.chdir(path)\n        yield\n    finally:\n        os.chdir(prev_cwd)", "target": "def test_get_last_page_num_from_header(self) -> None:\n        for (\n            expected_page_num,\n            mock_header,\n        ) in self.MOCK_HEADER_LINKS_TO_PAGE_NUMS.items():\n            self.assertEqual(\n                get_last_page_num_from_header(mock_header), expected_page_num\n            )"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004898", "source": "def _apply_field_title_generator_to_field_info(\n    title_generator: Callable[[str, FieldInfo], str],\n    field_name: str,\n    field_info: FieldInfo,\n):\n    if field_info.title is None:\n        title = title_generator(field_name, field_info)\n        if not isinstance(title, str):\n            raise TypeError(f'field_title_generator {title_generator} must return str, not {title.__class__}')\n        field_info.title = title", "target": "def test_wrap_on_error(self, py_and_json: PyAndJson):\n        def wrap_function(input_value, validator, info):\n            try:\n                return validator(input_value)\n            except ValidationError:\n                if isinstance(input_value, list):\n                    return str(len(input_value))\n                else:\n                    return repr(input_value)\n        v = py_and_json(\n            {\n                'type': 'model-fields',\n                'fields': {\n                    'x': {\n                        'type': 'model-field',\n                        'schema': {\n                            'type': 'default',\n                            'on_error': 'raise',\n                            'schema': {\n                                'type': 'function-wrap',\n                                'function': {'type': 'with-info', 'function': wrap_function},\n                                'schema': {'type': 'str'},\n                            },\n                        },\n                    }\n                },\n            }\n        )\n        assert v.validate_test({'x': 'foo'}) == ({'x': 'foo'}, None, {'x'})\n        assert v.validate_test({'x': ['foo']}) == ({'x': '1'}, None, {'x'})\n        assert v.validate_test({'x': ['foo', 'bar']}) == ({'x': '2'}, None, {'x'})\n        assert v.validate_test({'x': {'a': 'b'}}) == ({'x': \"{'a': 'b'}\"}, None, {'x'})"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004899", "source": "def is_path_exist(path: Union[str, Path, None]) -> bool:\n    return bool(path and get_path(path).exists())", "target": "def test_remove_dir_none_is_noop(self):\n        remove_dir(None)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004900", "source": "def takes_validated_data_argument(\n    default_factory: Callable[[], Any] | Callable[[dict[str, Any]], Any],\n) -> TypeIs[Callable[[dict[str, Any]], Any]]:\n    try:\n        sig = signature(default_factory)\n    except (ValueError, TypeError):\n        return False\n    parameters = list(sig.parameters.values())\n    return len(parameters) == 1 and can_be_positional(parameters[0]) and parameters[0].default is Parameter.empty", "target": "def test_frozen_field():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'name': core_schema.model_field(schema=core_schema.str_schema()),\n                'age': core_schema.model_field(schema=core_schema.int_schema()),\n                'is_developer': core_schema.model_field(\n                    schema=core_schema.with_default_schema(schema=core_schema.bool_schema(), default=True), frozen=True\n                ),\n            }\n        )\n    )\n    r1, model_extra, fields_set = v.validate_python({'name': 'Samuel', 'age': '36'})\n    assert r1 == {'name': 'Samuel', 'age': 36, 'is_developer': True}\n    assert model_extra is None\n    assert fields_set == {'name', 'age'}\n    v.validate_assignment(r1, 'age', '35')\n    assert r1 == {'name': 'Samuel', 'age': 35, 'is_developer': True}\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_assignment(r1, 'is_developer', False)\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'frozen_field', 'loc': ('is_developer',), 'msg': 'Field is frozen', 'input': False}\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004901", "source": "def is_callable_type(type_: Type[Any]) -> bool:\n    return type_ is Callable or get_origin(type_) is Callable", "target": "def test_type_error():\n    try:\n        PydanticKnownError('foobar')\n    except KeyError as exc:\n        assert str(exc) == '\"Invalid error type: \\'foobar\\'\"'\n    else:\n        raise AssertionError(\"PydanticKnownError('foobar') did not raise KeyError\")\n    e = PydanticKnownError('recursion_loop')\n    assert isinstance(e, PydanticKnownError)"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004902", "source": "def _flatten(\n    key_prefix: Label, sub_schema: Definition, result: FlatIntermediateDefinition\n) -> None:\n    for k, value in sub_schema.items():\n        if isinstance(k, tuple):\n            assert all(isinstance(ki, str) for ki in k)\n            key_suffix: Label = k\n        elif k is None:\n            key_suffix = ()\n        else:\n            assert isinstance(k, str)\n            key_suffix = (k,)\n        key: Label = key_prefix + key_suffix\n        if isinstance(value, (TimerArgs, GroupedBenchmark)):\n            assert key not in result, f\"duplicate key: {key}\"\n            result[key] = value\n        else:\n            assert isinstance(value, dict)\n            _flatten(key_prefix=key, sub_schema=value, result=result)", "target": "def test_changes_and_restores(self):\n        start = Path.cwd()\n        with tempfile.TemporaryDirectory() as td:\n            target = Path(td) / \"wd\"\n            target.mkdir()\n            with working_directory(str(target)):\n                self.assertEqual(Path.cwd().resolve(), target.resolve())\n        self.assertEqual(Path.cwd(), start)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004903", "source": "def dataclass(\n        *,\n        init: bool = True,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool = False,\n        config: Union[ConfigDict, Type[object], None] = None,\n        validate_on_init: Optional[bool] = None,\n        use_proxy: Optional[bool] = None,\n    ) -> Callable[[Type[_T]], 'DataclassClassOrWrapper']:\n        ...", "target": "def test_extra_custom_serializer():\n    @dataclasses.dataclass\n    class Model:\n        pass\n    schema = core_schema.dataclass_schema(\n        Model,\n        core_schema.dataclass_args_schema(\n            'Model',\n            [],\n            extra_behavior='allow',\n        ),\n        [],\n    )\n    s = SchemaSerializer(schema)\n    v = SchemaValidator(schema)\n    m = v.validate_python({'extra': 'extra'})\n    assert s.to_python(m) == {'extra': 'extra bam!'}"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004904", "source": "def temp_environ(updates: dict[str, str]):\n    missing = object()\n    old: dict[str, str | object] = {k: os.environ.get(k, missing) for k in updates}\n    try:\n        os.environ.update(updates)\n        yield\n    finally:\n        for k, v in old.items():\n            if v is missing:\n                os.environ.pop(k, None)\n            else:\n                os.environ[k] = v", "target": "def test_merged_lastfailed_content_with_overlap(self) -> None:\n        last_failed_source = {\n            \"tools/tests/test_foo.py::test_num1\": True,\n            \"tools/tests/test_foo.py::test_num2\": True,\n            \"tools/tests/test_bar.py::test_num1\": True,\n        }\n        last_failed_dest = {\n            \"tools/tests/test_foo.py::test_num1\": True,\n            \"tools/tests/test_car.py::test_num1\": True,\n            \"tools/tests/test_car.py::test_num2\": True,\n        }\n        last_failed_merged = {\n            \"tools/tests/test_foo.py::test_num1\": True,\n            \"tools/tests/test_foo.py::test_num2\": True,\n            \"tools/tests/test_bar.py::test_num1\": True,\n            \"tools/tests/test_car.py::test_num1\": True,\n            \"tools/tests/test_car.py::test_num2\": True,\n        }\n        merged = _merged_lastfailed_content(last_failed_source, last_failed_dest)\n        self.assertEqual(merged, last_failed_merged)"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004905", "source": "def ensure_dir_exists(path: Union[str, Path]) -> Path:\n    path_obj = get_path(path)\n    path_obj.mkdir(parents=True, exist_ok=True)\n    return path_obj", "target": "def test_get_path_returns_path_for_str(self):\n        rel_str = \"sub/f.txt\"\n        os.chdir(self.tmp_path)\n        p = get_path(rel_str, resolve=False)\n        self.assertIsInstance(p, Path)\n        self.assertFalse(p.is_absolute())\n        self.assertEqual(str(p), rel_str)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004906", "source": "def rebuild_dataclass(\n    cls: type[PydanticDataclass],\n    *,\n    force: bool = False,\n    raise_errors: bool = True,\n    _parent_namespace_depth: int = 2,\n    _types_namespace: MappingNamespace | None = None,\n) -> bool | None:\n    if not force and cls.__pydantic_complete__:\n        return None\n    for attr in ('__pydantic_core_schema__', '__pydantic_validator__', '__pydantic_serializer__'):\n        if attr in cls.__dict__ and not isinstance(getattr(cls, attr), _mock_val_ser.MockValSer):\n            delattr(cls, attr)\n    cls.__pydantic_complete__ = False\n    if _types_namespace is not None:\n        rebuild_ns = _types_namespace\n    elif _parent_namespace_depth > 0:\n        rebuild_ns = _typing_extra.parent_frame_namespace(parent_depth=_parent_namespace_depth, force=True) or {}\n    else:\n        rebuild_ns = {}\n    ns_resolver = _namespace_utils.NsResolver(\n        parent_namespace=rebuild_ns,\n    )\n    return _pydantic_dataclasses.complete_dataclass(\n        cls,\n        _config.ConfigWrapper(cls.__pydantic_config__, check=False),\n        raise_errors=raise_errors,\n        ns_resolver=ns_resolver,\n        _force_build=True,\n    )", "target": "def test_dataclass_args_init_only_no_fields(py_and_json: PyAndJson, input_value, expected):\n    schema = core_schema.dataclass_args_schema(\n        'MyDataclass', [core_schema.dataclass_field(name='a', schema=core_schema.str_schema())], collect_init_only=True\n    )\n    v = py_and_json(schema)\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=expected.message) as exc_info:\n            v.validate_test(input_value)\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        assert v.validate_test(input_value) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004907", "source": "def ip_v6_network_validator(v: Any) -> IPv6Network:\n    if isinstance(v, IPv6Network):\n        return v\n    try:\n        return IPv6Network(v)\n    except ValueError:\n        raise errors.IPv6NetworkError()", "target": "def test_validators_build(benchmark) -> None:\n    class Base1(BaseModel):\n        a: int\n        @field_validator('a', mode='after')\n        @classmethod\n        def val_a(cls, value: int) -> int: ...\n        @computed_field\n        def prop(self) -> int: ...\n    class Bare:\n        @computed_field\n        def prop_bare(self) -> int: ...\n    class Sub1(Base1):\n        @computed_field\n        def prop_2(self) -> int: ...\n        @computed_field\n        def prop_3(self) -> int: ...\n        @computed_field\n        def prop_4(self) -> int: ...\n    @benchmark\n    def bench() -> None:\n        class SubS(Sub1, Bare, defer_build=True):\n            @computed_field\n            def prop_5(self) -> int: ...\n            @computed_field\n            def prop_6(self) -> int: ...\n            @computed_field\n            def prop_7(self) -> int: ..."}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004908", "source": "def _get_validators(cls: 'DataclassClassOrWrapper') -> 'CallableGenerator':\n    yield cls.__validate__", "target": "def test_alias_allow_pop(py_and_json: PyAndJson):\n    schema = core_schema.dataclass_schema(\n        BasicDataclass,\n        core_schema.dataclass_args_schema(\n            'BasicDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), validation_alias='FieldA'),\n            ],\n        ),\n        ['a'],\n        config=core_schema.CoreConfig(validate_by_name=True, validate_by_alias=True),\n    )\n    v = py_and_json(schema)\n    assert v.validate_test({'FieldA': 'hello'}) == BasicDataclass(a='hello')\n    assert v.validate_test({'a': 'hello'}) == BasicDataclass(a='hello')\n    assert v.validate_test(\n        {\n            'FieldA': 'hello',\n            'a': 'world',\n        }\n    ) == BasicDataclass(a='hello')\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'foobar': 'hello'})"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004909", "source": "def is_path_exist(path: Union[str, Path, None]) -> bool:\n    return bool(path and get_path(path).exists())", "target": "def test_ensure_dir_exists_creates_and_is_idempotent(self):\n        d = self.tmp_path / \"made\"\n        ensure_dir_exists(d)\n        self.assertTrue(d.exists() and d.is_dir())\n        ensure_dir_exists(d)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004910", "source": "def dataclass(\n        *,\n        init: bool = True,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool = False,\n        config: Union[ConfigDict, Type[object], None] = None,\n        validate_on_init: Optional[bool] = None,\n        use_proxy: Optional[bool] = None,\n        kw_only: bool = ...,\n    ) -> Callable[[Type[_T]], 'DataclassClassOrWrapper']:\n        ...", "target": "def test_serialization_exclude():\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(\n                    name='a', schema=core_schema.str_schema(), serialization_exclude_if=lambda x: x == 'bye'\n                ),\n                core_schema.dataclass_field(name='b', schema=core_schema.bytes_schema(), serialization_exclude=True),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    s = SchemaSerializer(schema)\n    assert s.to_python(Foo(a='hello', b=b'more')) == {'a': 'hello'}\n    assert s.to_python(Foo(a='hello', b=b'more'), mode='json') == {'a': 'hello'}\n    assert s.to_python(Foo(a='bye', b=b'more'), mode='json') == {}\n    j = s.to_json(Foo(a='hello', b=b'more'))\n    if on_pypy:\n        assert json.loads(j) == {'a': 'hello'}\n    else:\n        assert j == b'{\"a\":\"hello\"}'\n    j = s.to_json(Foo(a='bye', b=b'more'))\n    if on_pypy:\n        assert json.loads(j) == {}\n    else:\n        assert j == b'{}'"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004911", "source": "def _get_validators(cls: 'DataclassClassOrWrapper') -> 'CallableGenerator':\n    yield cls.__validate__", "target": "def test_dataclass_initvar_not_required_on_union_ser() -> None:\n    @dataclasses.dataclass\n    class Foo:\n        x: int\n        init_var: dataclasses.InitVar[int] = 1\n    @dataclasses.dataclass\n    class Bar:\n        x: int\n    schema = core_schema.union_schema(\n        [\n            core_schema.dataclass_schema(\n                Foo,\n                core_schema.dataclass_args_schema(\n                    'Foo',\n                    [\n                        core_schema.dataclass_field(name='x', schema=core_schema.int_schema()),\n                        core_schema.dataclass_field(\n                            name='init_var',\n                            init_only=True,\n                            schema=core_schema.with_default_schema(core_schema.int_schema(), default=1),\n                        ),\n                    ],\n                ),\n                ['x'],\n                post_init=True,\n            ),\n            core_schema.dataclass_schema(\n                Bar,\n                core_schema.dataclass_args_schema(\n                    'Bar', [core_schema.dataclass_field(name='x', schema=core_schema.int_schema())]\n                ),\n                ['x'],\n            ),\n        ]\n    )\n    s = SchemaSerializer(schema)\n    assert s.to_python(Foo(x=1), warnings='error') == {'x': 1}\n    assert s.to_python(Foo(x=1, init_var=2), warnings='error') == {'x': 1}"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004912", "source": "def run_command(\n    cmd: str,\n    use_shell: bool = False,\n    log_cmd: bool = True,\n    cwd: Optional[str] = None,\n    env: Optional[dict] = None,\n    check: bool = True,\n) -> int:\n    if use_shell:\n        args = cmd\n        log_prefix = \"[shell]\"\n        executable = \"/bin/bash\"\n    else:\n        args = shlex.split(cmd)\n        log_prefix = \"[cmd]\"\n        executable = None\n    if log_cmd:\n        display_cmd = cmd if use_shell else \" \".join(args)\n        logger.info(\"%s %s\", log_prefix, display_cmd)\n    run_env = {**os.environ, **(env or {})}\n    proc = subprocess.run(\n        args,\n        shell=use_shell,\n        executable=executable,\n        stdout=sys.stdout,\n        stderr=sys.stderr,\n        cwd=cwd,\n        env=run_env,\n        check=False,\n    )\n    if check and proc.returncode != 0:\n        logger.error(\n            \"%s Command failed (exit %s): %s\", log_prefix, proc.returncode, cmd\n        )\n        raise subprocess.CalledProcessError(\n            proc.returncode, args if not use_shell else cmd\n        )\n    return proc.returncode", "target": "def test_merged_lastfailed_content_without_overlap(self) -> None:\n        last_failed_source = {\n            \"tools/tests/test_foo.py::test_num1\": True,\n            \"tools/tests/test_foo.py::test_num2\": True,\n            \"tools/tests/test_bar.py::test_num1\": True,\n        }\n        last_failed_dest = {\n            \"tools/tests/test_car.py::test_num1\": True,\n            \"tools/tests/test_car.py::test_num2\": True,\n        }\n        last_failed_merged = {\n            \"tools/tests/test_foo.py::test_num1\": True,\n            \"tools/tests/test_foo.py::test_num2\": True,\n            \"tools/tests/test_bar.py::test_num1\": True,\n            \"tools/tests/test_car.py::test_num1\": True,\n            \"tools/tests/test_car.py::test_num2\": True,\n        }\n        merged = _merged_lastfailed_content(last_failed_source, last_failed_dest)\n        self.assertEqual(merged, last_failed_merged)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004913", "source": "def _is_field_cached_property(obj: 'Dataclass', k: str) -> bool:\n        return isinstance(getattr(type(obj), k, None), cached_property)", "target": "def test_slots_dataclass_subclass(revalidate_instances, input_value, expected):\n    schema = core_schema.dataclass_schema(\n        FooDataclassSlots,\n        core_schema.dataclass_args_schema(\n            'FooDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n            ],\n            extra_behavior='forbid',\n        ),\n        ['a', 'b'],\n        revalidate_instances=revalidate_instances,\n        slots=True,\n    )\n    v = SchemaValidator(schema)\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=expected.message) as exc_info:\n            print(v.validate_python(input_value))\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        dc = v.validate_python(input_value)\n        assert dataclasses.is_dataclass(dc)\n        assert dataclasses.asdict(dc) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004914", "source": "def create_pydantic_model_from_dataclass(\n    dc_cls: Type['Dataclass'],\n    config: Type[Any] = BaseConfig,\n    dc_cls_doc: Optional[str] = None,\n) -> Type['BaseModel']:\n    field_definitions: Dict[str, Any] = {}\n    for field in dataclasses.fields(dc_cls):\n        default: Any = Undefined\n        default_factory: Optional['NoArgAnyCallable'] = None\n        field_info: FieldInfo\n        if field.default is not dataclasses.MISSING:\n            default = field.default\n        elif field.default_factory is not dataclasses.MISSING:\n            default_factory = field.default_factory\n        else:\n            default = Required\n        if isinstance(default, FieldInfo):\n            field_info = default\n            dc_cls.__pydantic_has_field_info_default__ = True\n        else:\n            field_info = Field(default=default, default_factory=default_factory, **field.metadata)\n        field_definitions[field.name] = (field.type, field_info)\n    validators = gather_all_validators(dc_cls)\n    model: Type['BaseModel'] = create_model(\n        dc_cls.__name__,\n        __config__=config,\n        __module__=dc_cls.__module__,\n        __validators__=validators,\n        __cls_kwargs__={'__resolve_forward_refs__': False},\n        **field_definitions,\n    )\n    model.__doc__ = dc_cls_doc if dc_cls_doc is not None else dc_cls.__doc__ or ''\n    return model", "target": "def test_dataclass_self_init_alias():\n    @dataclasses.dataclass(init=False)\n    class Foo:\n        a: str\n        b: bool\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), validation_alias='aAlias'),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), validation_alias=['bAlias', 0]),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    def __init__(self, *args, **kwargs):\n        v.validate_python(ArgsKwargs(args, kwargs), self_instance=self)\n    Foo.__init__ = __init__\n    foo = Foo(aAlias=b'hello', bAlias=['True'])\n    assert dataclasses.is_dataclass(foo)\n    assert dataclasses.asdict(foo) == {'a': 'hello', 'b': True}\n    with pytest.raises(ValidationError) as exc_info:\n        Foo(aAlias=b'hello', bAlias=['wrong'])\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'bool_parsing',\n            'loc': ('bAlias', 0),\n            'msg': 'Input should be a valid boolean, unable to interpret input',\n            'input': 'wrong',\n        }\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004915", "source": "def dataclass(\n        _cls: Type[_T],\n        *,\n        init: bool = True,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool = False,\n        config: Union[ConfigDict, Type[object], None] = None,\n        validate_on_init: Optional[bool] = None,\n        use_proxy: Optional[bool] = None,\n        kw_only: bool = ...,\n    ) -> 'DataclassClassOrWrapper':\n        ...", "target": "def test_dataclass_exact_validation(revalidate_instances, input_value, expected):\n    schema = core_schema.dataclass_schema(\n        FooDataclass,\n        core_schema.dataclass_args_schema(\n            'FooDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n            ],\n        ),\n        ['a', 'b'],\n        revalidate_instances=revalidate_instances,\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python(input_value)\n    assert dataclasses.asdict(foo) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004916", "source": "def copy(src: Union[str, Path], dst: Union[str, Path]) -> None:\n    src_path = get_path(src, resolve=True)\n    dst_path = get_path(dst, resolve=True)\n    if not src_path.exists():\n        raise FileNotFoundError(f\"Source does not exist: {src_path}\")\n    dst_path.parent.mkdir(parents=True, exist_ok=True)\n    if src_path.is_file():\n        shutil.copy2(src_path, dst_path)\n    elif src_path.is_dir():\n        shutil.copytree(src_path, dst_path, dirs_exist_ok=True)\n    else:\n        raise ValueError(f\"Unsupported path type: {src_path}\")", "target": "def test_copy_file_to_file(self):\n        src = self.tmp_path / \"src.txt\"\n        dst = self.tmp_path / \"out\" / \"dst.txt\"\n        src.write_text(\"hello\")\n        copy(src, dst)\n        self.assertEqual(dst.read_text(), \"hello\")"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004917", "source": "def _check_finalvar(v: Optional[Type[Any]]) -> bool:\n    if v is None:\n        return False\n    return v.__class__ == Final.__class__ and (sys.version_info < (3, 8) or getattr(v, '_name', None) == 'Final')", "target": "def test_schema_validator() -> None:\n    SchemaValidator({'type': 'int'})"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004918", "source": "def get_origin(tp: Type[Any]) -> Optional[Type[Any]]:\n        if type(tp).__name__ in AnnotatedTypeNames:\n            return cast(Type[Any], Annotated)\n        return _typing_get_origin(tp) or getattr(tp, '__origin__', None)", "target": "def test_schema_typing_error() -> None:\n    _: CoreSchema = {'type': 'wrong'}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004919", "source": "def _apply_alias_generator_to_field_info(\n    alias_generator: Callable[[str], str] | AliasGenerator, field_name: str, field_info: FieldInfo\n):\n    if (\n        field_info.alias_priority is None\n        or field_info.alias_priority <= 1\n        or field_info.alias is None\n        or field_info.validation_alias is None\n        or field_info.serialization_alias is None\n    ):\n        alias, validation_alias, serialization_alias = None, None, None\n        if isinstance(alias_generator, AliasGenerator):\n            alias, validation_alias, serialization_alias = alias_generator.generate_aliases(field_name)\n        elif callable(alias_generator):\n            alias = alias_generator(field_name)\n            if not isinstance(alias, str):\n                raise TypeError(f'alias_generator {alias_generator} must return str, not {alias.__class__}')\n        if field_info.alias_priority is None or field_info.alias_priority <= 1:\n            field_info.alias_priority = 1\n        if field_info.alias_priority == 1:\n            field_info.serialization_alias = get_first_not_none(serialization_alias, alias)\n            field_info.validation_alias = get_first_not_none(validation_alias, alias)\n            field_info.alias = alias\n        if field_info.alias is None:\n            field_info.alias = alias\n        if field_info.serialization_alias is None:\n            field_info.serialization_alias = get_first_not_none(serialization_alias, alias)\n        if field_info.validation_alias is None:\n            field_info.validation_alias = get_first_not_none(validation_alias, alias)", "target": "def test_from_attributes_error_error():\n    class BadError(Exception):\n        def __str__(self):\n            raise RuntimeError('intentional error inside error')\n    class Foobar:\n        @property\n        def x(self):\n            raise BadError('intentional error')\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={'x': core_schema.model_field(schema=core_schema.int_schema())}, from_attributes=True\n        )\n    )\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python(Foobar())\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'get_attribute_error',\n            'loc': ('x',),\n            'msg': IsStr(regex=r'Error extracting attribute: \\S+\\.<locals>\\.BadError: <exception str\\(\\) failed>'),\n            'input': HasRepr(IsStr(regex='.+Foobar object at.+')),\n            'ctx': {'error': IsStr(regex=r'\\S+\\.<locals>\\.BadError: <exception str\\(\\) failed>')},\n        }\n    ]\n    class UnInitError:\n        @property\n        def x(self):\n            raise RuntimeError\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python(UnInitError())\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'get_attribute_error',\n            'loc': ('x',),\n            'msg': 'Error extracting attribute: RuntimeError',\n            'input': HasRepr(IsStr(regex='.+UnInitError object at.+')),\n            'ctx': {'error': 'RuntimeError'},\n        }\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004920", "source": "def dataclass(\n        *,\n        init: bool = True,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool = False,\n        config: Union[ConfigDict, Type[object], None] = None,\n        validate_on_init: Optional[bool] = None,\n        use_proxy: Optional[bool] = None,\n    ) -> Callable[[Type[_T]], 'DataclassClassOrWrapper']:\n        ...", "target": "def test_dataclass():\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bytes_schema()),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    s = SchemaSerializer(schema)\n    assert s.to_python(Foo(a='hello', b=b'more')) == IsStrictDict(a='hello', b=b'more')\n    assert s.to_python(Foo(a='hello', b=b'more'), mode='json') == IsStrictDict(a='hello', b='more')\n    j = s.to_json(Foo(a='hello', b=b'more'))\n    if on_pypy:\n        assert json.loads(j) == {'a': 'hello', 'b': 'more'}\n    else:\n        assert j == b'{\"a\":\"hello\",\"b\":\"more\"}'\n    assert s.to_python(Foo(a='hello', b=b'more'), exclude={'b'}) == IsStrictDict(a='hello')\n    assert s.to_json(Foo(a='hello', b=b'more'), include={'a'}) == b'{\"a\":\"hello\"}'"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004921", "source": "def make_dataclass_validator(dc_cls: Type['Dataclass'], config: Type[BaseConfig]) -> 'CallableGenerator':\n    yield from _get_validators(dataclass(dc_cls, config=config, use_proxy=True))", "target": "def test_dataclass_args_init_with_default(input_value, extra_behavior, expected):\n    @dataclasses.dataclass\n    class Foo:\n        a: str\n        b: str\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(\n                    name='b',\n                    schema=core_schema.with_default_schema(schema=core_schema.str_schema(), default='HELLO'),\n                    init=False,\n                ),\n            ],\n            extra_behavior=extra_behavior,\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)) as exc_info:\n            v.validate_python(input_value)\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        assert dataclasses.asdict(v.validate_python(input_value)) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004922", "source": "def dataclass(\n        *,\n        init: Literal[False] = False,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool | None = None,\n        config: ConfigDict | type[object] | None = None,\n        validate_on_init: bool | None = None,\n    ) -> Callable[[type[_T]], type[PydanticDataclass]]:\n        ...", "target": "def test_extra_behavior_ignore(\n    config: Union[core_schema.CoreConfig, None],\n    schema_extra_behavior_kw: dict[str, Any],\n    validate_fn_extra_kw: Union[ExtraBehavior, None],\n):\n    @dataclasses.dataclass\n    class MyModel:\n        f: str\n    v = SchemaValidator(\n        core_schema.dataclass_schema(\n            MyModel,\n            core_schema.dataclass_args_schema(\n                'MyModel', [core_schema.dataclass_field('f', core_schema.str_schema())], **schema_extra_behavior_kw\n            ),\n            ['f'],\n        ),\n        config=config,\n    )\n    m: MyModel = v.validate_python({'f': 'x', 'extra_field': 123}, extra=validate_fn_extra_kw)\n    assert m.f == 'x'\n    assert not hasattr(m, 'extra_field')\n    v.validate_assignment(m, 'f', 'y', extra=validate_fn_extra_kw)\n    assert m.f == 'y'\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_assignment(m, 'not_f', 'xyz', extra=validate_fn_extra_kw)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'no_such_attribute',\n            'loc': ('not_f',),\n            'msg': \"Object has no attribute 'not_f'\",\n            'input': 'xyz',\n            'ctx': {'attribute': 'not_f'},\n        }\n    ]\n    assert not hasattr(m, 'not_f')"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004923", "source": "def _add_pydantic_validation_attributes(\n    dc_cls: Type['Dataclass'],\n    config: Type[BaseConfig],\n    validate_on_init: bool,\n    dc_cls_doc: str,\n) -> None:\n    init = dc_cls.__init__\n    @wraps(init)\n    def handle_extra_init(self: 'Dataclass', *args: Any, **kwargs: Any) -> None:\n        if config.extra == Extra.ignore:\n            init(self, *args, **{k: v for k, v in kwargs.items() if k in self.__dataclass_fields__})\n        elif config.extra == Extra.allow:\n            for k, v in kwargs.items():\n                self.__dict__.setdefault(k, v)\n            init(self, *args, **{k: v for k, v in kwargs.items() if k in self.__dataclass_fields__})\n        else:\n            init(self, *args, **kwargs)\n    if hasattr(dc_cls, '__post_init__'):\n        try:\n            post_init = dc_cls.__post_init__.__wrapped__\n        except AttributeError:\n            post_init = dc_cls.__post_init__\n        @wraps(post_init)\n        def new_post_init(self: 'Dataclass', *args: Any, **kwargs: Any) -> None:\n            if config.post_init_call == 'before_validation':\n                post_init(self, *args, **kwargs)\n            if self.__class__.__pydantic_run_validation__:\n                self.__pydantic_validate_values__()\n                if hasattr(self, '__post_init_post_parse__'):\n                    self.__post_init_post_parse__(*args, **kwargs)\n            if config.post_init_call == 'after_validation':\n                post_init(self, *args, **kwargs)\n        setattr(dc_cls, '__init__', handle_extra_init)\n        setattr(dc_cls, '__post_init__', new_post_init)\n    else:\n        @wraps(init)\n        def new_init(self: 'Dataclass', *args: Any, **kwargs: Any) -> None:\n            handle_extra_init(self, *args, **kwargs)\n            if self.__class__.__pydantic_run_validation__:\n                self.__pydantic_validate_values__()\n            if hasattr(self, '__post_init_post_parse__'):\n                initvars_and_values: Dict[str, Any] = {}\n                for i, f in enumerate(self.__class__.__dataclass_fields__.values()):\n                    if f._field_type is dataclasses._FIELD_INITVAR:\n                        try:\n                            initvars_and_values[f.name] = args[i]\n                        except IndexError:\n                            initvars_and_values[f.name] = kwargs.get(f.name, f.default)\n                self.__post_init_post_parse__(**initvars_and_values)\n        setattr(dc_cls, '__init__', new_init)\n    setattr(dc_cls, '__pydantic_run_validation__', ClassAttribute('__pydantic_run_validation__', validate_on_init))\n    setattr(dc_cls, '__pydantic_initialised__', False)\n    setattr(dc_cls, '__pydantic_model__', create_pydantic_model_from_dataclass(dc_cls, config, dc_cls_doc))\n    setattr(dc_cls, '__pydantic_validate_values__', _dataclass_validate_values)\n    setattr(dc_cls, '__validate__', classmethod(_validate_dataclass))\n    setattr(dc_cls, '__get_validators__', classmethod(_get_validators))\n    if dc_cls.__pydantic_model__.__config__.validate_assignment and not dc_cls.__dataclass_params__.frozen:\n        setattr(dc_cls, '__setattr__', _dataclass_validate_assignment_setattr)", "target": "def test_dataclass_args_init_only(py_and_json: PyAndJson, input_value, expected):\n    schema = core_schema.dataclass_args_schema(\n        'MyDataclass',\n        [\n            core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), kw_only=False),\n            core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), kw_only=False, init_only=True),\n        ],\n        collect_init_only=True,\n    )\n    v = py_and_json(schema)\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)) as exc_info:\n            v.validate_test(input_value)\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        assert v.validate_test(input_value) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004924", "source": "def decorator(f: Callable[..., T]) -> Callable[..., T]:\n        @wraps(f)\n        def wrapper(*args: list[Any], **kwargs: dict[str, Any]) -> T:\n            for idx in range(num_retries):\n                try:\n                    return f(*args, **kwargs)\n                except Exception as e:\n                    print(\n                        f'Attempt {idx} of {num_retries} to call {f.__name__} failed with \"{e}\"'\n                    )\n            return cast(T, rc)\n        return wrapper", "target": "def test_ghstack_branches_in_sync(self) -> None:\n        head_ref = \"gh/SS-JIA/206/head\"\n        self._skip_if_ref_does_not_exist(head_ref)\n        self.assertTrue(are_ghstack_branches_in_sync(self.repo, head_ref))"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004925", "source": "def env_bool(\n    name: str,\n    default: bool = False,\n) -> bool:\n    val = get_env(name)\n    if not val:\n        return default\n    return str2bool(val)", "target": "def test_env_bool_uses_str2bool_when_set(self):\n        def fake_str2bool(s: str) -> bool:\n            return s.lower() in {\"1\", \"true\", \"yes\", \"on\", \"y\"}\n        with (\n            patch.dict(os.environ, {\"FLAG\": \"yEs\"}, clear=True),\n            patch.object(m, \"str2bool\", fake_str2bool),\n        ):\n            self.assertTrue(m.env_bool(\"FLAG\", default=False))"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004926", "source": "def _get_validators(cls: 'DataclassClassOrWrapper') -> 'CallableGenerator':\n    yield cls.__validate__", "target": "def test_dataclass_subclass_strict_never_revalidate():\n    v = SchemaValidator(\n        core_schema.dataclass_schema(\n            FooDataclass,\n            core_schema.dataclass_args_schema(\n                'FooDataclass',\n                [\n                    core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                    core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n                ],\n            ),\n            ['a', 'b'],\n            revalidate_instances='never',\n            strict=True,\n        )\n    )\n    foo = FooDataclass(a='hello', b=True)\n    assert v.validate_python(foo) is foo\n    sub_foo = FooDataclassSame(a='hello', b=True)\n    assert v.validate_python(sub_foo) is sub_foo\n    with pytest.raises(ValidationError, match='Input should be an instance of FooDataclass'):\n        v.validate_python(ArgsKwargs((), {'a': 'hello', 'b': True}))"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004927", "source": "def is_stdlib_dataclass(cls: type[Any], /) -> TypeIs[type[StandardDataclass]]:\n    return '__dataclass_fields__' in cls.__dict__ and not hasattr(cls, '__pydantic_validator__')", "target": "def test_dataclass_wrap_json():\n    schema = core_schema.no_info_wrap_validator_function(\n        lambda v, handler: handler(v),\n        core_schema.dataclass_schema(\n            FooDataclass,\n            core_schema.dataclass_args_schema(\n                'FooDataclass',\n                [\n                    core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                    core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n                ],\n            ),\n            ['a', 'b'],\n        ),\n    )\n    v = SchemaValidator(schema)\n    assert v.validate_json('{\"a\": \"hello\", \"b\": true}') == FooDataclass(a='hello', b=True)\n    assert v.validate_json('{\"a\": \"hello\", \"b\": true}', strict=True) == FooDataclass(a='hello', b=True)"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004928", "source": "def _flatten(\n    key_prefix: Label, sub_schema: Definition, result: FlatIntermediateDefinition\n) -> None:\n    for k, value in sub_schema.items():\n        if isinstance(k, tuple):\n            assert all(isinstance(ki, str) for ki in k)\n            key_suffix: Label = k\n        elif k is None:\n            key_suffix = ()\n        else:\n            assert isinstance(k, str)\n            key_suffix = (k,)\n        key: Label = key_prefix + key_suffix\n        if isinstance(value, (TimerArgs, GroupedBenchmark)):\n            assert key not in result, f\"duplicate key: {key}\"\n            result[key] = value\n        else:\n            assert isinstance(value, dict)\n            _flatten(key_prefix=key, sub_schema=value, result=result)", "target": "def test_merged_lastfailed_content_without_overlap(self) -> None:\n        last_failed_source = {\n            \"tools/tests/test_foo.py::test_num1\": True,\n            \"tools/tests/test_foo.py::test_num2\": True,\n            \"tools/tests/test_bar.py::test_num1\": True,\n        }\n        last_failed_dest = {\n            \"tools/tests/test_car.py::test_num1\": True,\n            \"tools/tests/test_car.py::test_num2\": True,\n        }\n        last_failed_merged = {\n            \"tools/tests/test_foo.py::test_num1\": True,\n            \"tools/tests/test_foo.py::test_num2\": True,\n            \"tools/tests/test_bar.py::test_num1\": True,\n            \"tools/tests/test_car.py::test_num1\": True,\n            \"tools/tests/test_car.py::test_num2\": True,\n        }\n        merged = _merged_lastfailed_content(last_failed_source, last_failed_dest)\n        self.assertEqual(merged, last_failed_merged)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004929", "source": "def dataclass(\n        *,\n        init: Literal[False] = False,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool | None = None,\n        config: ConfigDict | type[object] | None = None,\n        validate_on_init: bool | None = None,\n    ) -> Callable[[type[_T]], type[PydanticDataclass]]:\n        ...", "target": "def test_dataclass_subclass_strict_never_revalidate():\n    v = SchemaValidator(\n        core_schema.dataclass_schema(\n            FooDataclass,\n            core_schema.dataclass_args_schema(\n                'FooDataclass',\n                [\n                    core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                    core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n                ],\n            ),\n            ['a', 'b'],\n            revalidate_instances='never',\n            strict=True,\n        )\n    )\n    foo = FooDataclass(a='hello', b=True)\n    assert v.validate_python(foo) is foo\n    sub_foo = FooDataclassSame(a='hello', b=True)\n    assert v.validate_python(sub_foo) is sub_foo\n    with pytest.raises(ValidationError, match='Input should be an instance of FooDataclass'):\n        v.validate_python(ArgsKwargs((), {'a': 'hello', 'b': True}))"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004930", "source": "def benchmark_module(config, module, use_throughput_benchmark=False):\n    if use_throughput_benchmark:\n        return benchmark_using_throughput_benchmark(config, module)\n    module.forward(config.num_warmup_iters)\n    print(f\"Running module for {config.num_iters} iterations\")\n    start = time.time()\n    module.forward(config.num_iters)\n    end = time.time()\n    time_elapsed_s = end - start\n    return secs_to_ms(time_elapsed_s) / config.num_iters / NUM_LOOP_ITERS", "target": "def test_changes_and_restores(self):\n        start = Path.cwd()\n        with tempfile.TemporaryDirectory() as td:\n            target = Path(td) / \"wd\"\n            target.mkdir()\n            with working_directory(str(target)):\n                self.assertEqual(Path.cwd().resolve(), target.resolve())\n        self.assertEqual(Path.cwd(), start)"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004931", "source": "def powspace(start, stop, pow, step):\n    start = math.log(start, pow)\n    stop = math.log(stop, pow)\n    steps = int((stop - start + 1) // step)\n    ret = torch.pow(pow, torch.linspace(start, stop, steps))\n    ret = torch.unique(ret)\n    return list(map(int, ret))", "target": "def test_noop_when_empty_path(self):\n        start = Path.cwd()\n        with working_directory(\"\"):\n            self.assertEqual(Path.cwd(), start)\n        self.assertEqual(Path.cwd(), start)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004932", "source": "def _recreate_field_info(\n    field_info: FieldInfo,\n    ns_resolver: NsResolver,\n    typevars_map: Mapping[TypeVar, Any],\n    *,\n    lenient: bool,\n) -> FieldInfo:\n    FieldInfo_ = import_cached_field_info()\n    existing_desc = field_info.description\n    if lenient:\n        ann = _generics.replace_types(field_info._original_annotation, typevars_map)\n        ann, evaluated = _typing_extra.try_eval_type(\n            ann,\n            *ns_resolver.types_namespace,\n        )\n    else:\n        ann = _typing_extra.eval_type(\n            field_info._original_annotation,\n            *ns_resolver.types_namespace,\n        )\n        ann = _generics.replace_types(ann, typevars_map)\n        ann = _typing_extra.eval_type(\n            ann,\n            *ns_resolver.types_namespace,\n        )\n        evaluated = True\n    if (assign := field_info._original_assignment) is PydanticUndefined:\n        new_field = FieldInfo_.from_annotation(ann, _source=AnnotationSource.CLASS)\n    else:\n        new_field = FieldInfo_.from_annotated_attribute(ann, assign, _source=AnnotationSource.CLASS)\n        new_field._original_assignment = assign\n    new_field._original_annotation = ann\n    new_field.description = new_field.description if new_field.description is not None else existing_desc\n    if not evaluated:\n        new_field._complete = False\n    return new_field", "target": "def test_model_fields_deep():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'field_a': core_schema.model_field(schema=core_schema.str_schema()),\n                'field_b': core_schema.model_field(\n                    schema=core_schema.model_fields_schema(\n                        fields={\n                            'field_c': core_schema.model_field(schema=core_schema.str_schema()),\n                            'field_d': core_schema.model_field(\n                                schema=core_schema.model_fields_schema(\n                                    fields={\n                                        'field_e': core_schema.model_field(schema=core_schema.str_schema()),\n                                        'field_f': core_schema.model_field(schema=core_schema.int_schema()),\n                                    }\n                                )\n                            ),\n                        }\n                    )\n                ),\n            }\n        )\n    )\n    model_dict, model_extra, fields_set = v.validate_python(\n        {'field_a': '1', 'field_b': {'field_c': '2', 'field_d': {'field_e': '4', 'field_f': 4}}}\n    )\n    assert model_dict == {\n        'field_a': '1',\n        'field_b': (\n            {'field_c': '2', 'field_d': ({'field_e': '4', 'field_f': 4}, None, {'field_f', 'field_e'})},\n            None,\n            {'field_d', 'field_c'},\n        ),\n    }\n    assert model_extra is None\n    assert fields_set == {'field_a', 'field_b'}\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python({'field_a': '1', 'field_b': {'field_c': '2', 'field_d': {'field_e': '4', 'field_f': 'xx'}}})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('field_b', 'field_d', 'field_f'),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'xx',\n        }\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004933", "source": "def is_valid_privateattr_name(name: str) -> bool:\n    return name.startswith('_') and not name.startswith('__')", "target": "def test_validate_assignment_ignore_extra():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(fields={'field_a': core_schema.model_field(schema=core_schema.str_schema())})\n    )\n    assert v.validate_python({'field_a': 'test'}) == ({'field_a': 'test'}, None, {'field_a'})\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_assignment({'field_a': 'test'}, 'other_field', 456)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'no_such_attribute',\n            'loc': ('other_field',),\n            'msg': \"Object has no attribute 'other_field'\",\n            'input': 456,\n            'ctx': {'attribute': 'other_field'},\n        }\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004934", "source": "def make_dataclass_validator(dc_cls: Type['Dataclass'], config: Type[BaseConfig]) -> 'CallableGenerator':\n    yield from _get_validators(dataclass(dc_cls, config=config, use_proxy=True))", "target": "def test_dataclass_validate_assignment():\n    schema = core_schema.dataclass_schema(\n        FooDataclass,\n        core_schema.dataclass_args_schema(\n            'FooDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), kw_only=False),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), kw_only=False),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 'hello', 'b': 'True'})\n    assert dataclasses.asdict(foo) == {'a': 'hello', 'b': True}\n    v.validate_assignment(foo, 'a', b'world')\n    assert dataclasses.asdict(foo) == {'a': 'world', 'b': True}\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_assignment(foo, 'a', 123)\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'string_type', 'loc': ('a',), 'msg': 'Input should be a valid string', 'input': 123}\n    ]\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_assignment(foo, 'c', '123')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'no_such_attribute',\n            'loc': ('c',),\n            'msg': \"Object has no attribute 'c'\",\n            'input': '123',\n            'ctx': {'attribute': 'c'},\n        }\n    ]\n    assert not hasattr(foo, 'c')\n    with pytest.raises(AttributeError, match=\"'str' object has no attribute 'a'\"):\n        v.validate_assignment('field_a', 'c', 123)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004935", "source": "def as_dataclass_field(pydantic_field: FieldInfo) -> dataclasses.Field[Any]:\n    field_args: dict[str, Any] = {'default': pydantic_field}\n    if sys.version_info >= (3, 14) and pydantic_field.description is not None:\n        field_args['doc'] = pydantic_field.description\n    if sys.version_info >= (3, 10) and pydantic_field.kw_only:\n        field_args['kw_only'] = True\n    if pydantic_field.repr is not True:\n        field_args['repr'] = pydantic_field.repr\n    return dataclasses.field(**field_args)", "target": "def test_dataclass_subclass(revalidate_instances, input_value, expected):\n    schema = core_schema.dataclass_schema(\n        FooDataclass,\n        core_schema.dataclass_args_schema(\n            'FooDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n            ],\n            extra_behavior='forbid',\n        ),\n        ['a', 'b'],\n        revalidate_instances=revalidate_instances,\n    )\n    v = SchemaValidator(schema)\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=expected.message) as exc_info:\n            print(v.validate_python(input_value))\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        dc = v.validate_python(input_value)\n        assert dataclasses.is_dataclass(dc)\n        assert dataclasses.asdict(dc) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004936", "source": "def _check_protected_namespaces(\n    protected_namespaces: tuple[str | Pattern[str], ...],\n    ann_name: str,\n    bases: tuple[type[Any], ...],\n    cls_name: str,\n) -> None:\n    BaseModel = import_cached_base_model()\n    for protected_namespace in protected_namespaces:\n        ns_violation = False\n        if isinstance(protected_namespace, Pattern):\n            ns_violation = protected_namespace.match(ann_name) is not None\n        elif isinstance(protected_namespace, str):\n            ns_violation = ann_name.startswith(protected_namespace)\n        if ns_violation:\n            for b in bases:\n                if hasattr(b, ann_name):\n                    if not (issubclass(b, BaseModel) and ann_name in getattr(b, '__pydantic_fields__', {})):\n                        raise ValueError(\n                            f'Field {ann_name!r} conflicts with member {getattr(b, ann_name)}'\n                            f' of protected namespace {protected_namespace!r}.'\n                        )\n            else:\n                valid_namespaces: list[str] = []\n                for pn in protected_namespaces:\n                    if isinstance(pn, Pattern):\n                        if not pn.match(ann_name):\n                            valid_namespaces.append(f're.compile({pn.pattern!r})')\n                    else:\n                        if not ann_name.startswith(pn):\n                            valid_namespaces.append(f\"'{pn}'\")\n                valid_namespaces_str = f'({\", \".join(valid_namespaces)}{\",)\" if len(valid_namespaces) == 1 else \")\"}'\n                warnings.warn(\n                    f'Field {ann_name!r} in {cls_name!r} conflicts with protected namespace {protected_namespace!r}.\\n\\n'\n                    f\"You may be able to solve this by setting the 'protected_namespaces' configuration to {valid_namespaces_str}.\",\n                    UserWarning,\n                    stacklevel=5,\n                )", "target": "def test_from_attributes_extra_ignore_no_attributes_accessed() -> None:\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={'a': core_schema.model_field(schema=core_schema.int_schema())},\n            from_attributes=True,\n            extra_behavior='ignore',\n        )\n    )\n    accessed: list[str] = []\n    class Source:\n        a = 1\n        b = 2\n        def __getattribute__(self, name: str, /) -> Any:\n            accessed.append(name)\n            return super().__getattribute__(name)\n    assert v.validate_python(Source()) == ({'a': 1}, None, {'a'})\n    assert 'a' in accessed and 'b' not in accessed"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004937", "source": "def pydantic_general_metadata(**metadata: Any) -> BaseMetadata:\n    return _general_metadata_cls()(metadata)", "target": "def test_aliases_debug():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'field_a': core_schema.model_field(\n                    validation_alias=[['foo', 'bar', 'bat'], ['foo', 3]], schema=core_schema.int_schema()\n                )\n            }\n        )\n    )\n    print(repr(v))\n    assert repr(v).startswith('SchemaValidator(title=\"model-fields\", validator=ModelFields(')\n    assert 'PathChoices(' in repr(v)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004938", "source": "def decimal_encoder(dec_value: Decimal) -> Union[int, float]:\n    if dec_value.as_tuple().exponent >= 0:\n        return int(dec_value)\n    else:\n        return float(dec_value)", "target": "def test_float_no_remainder():\n    v = SchemaValidator(core_schema.int_schema())\n    assert v.validate_json('123.0') == 123"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004939", "source": "def dataclass(\n        _cls: Type[_T],\n        *,\n        init: bool = True,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool = False,\n        config: Union[ConfigDict, Type[object], None] = None,\n        validate_on_init: Optional[bool] = None,\n        use_proxy: Optional[bool] = None,\n    ) -> 'DataclassClassOrWrapper':\n        ...", "target": "def test_dataclass_self_init_alias():\n    @dataclasses.dataclass(init=False)\n    class Foo:\n        a: str\n        b: bool\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), validation_alias='aAlias'),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), validation_alias=['bAlias', 0]),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    def __init__(self, *args, **kwargs):\n        v.validate_python(ArgsKwargs(args, kwargs), self_instance=self)\n    Foo.__init__ = __init__\n    foo = Foo(aAlias=b'hello', bAlias=['True'])\n    assert dataclasses.is_dataclass(foo)\n    assert dataclasses.asdict(foo) == {'a': 'hello', 'b': True}\n    with pytest.raises(ValidationError) as exc_info:\n        Foo(aAlias=b'hello', bAlias=['wrong'])\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'bool_parsing',\n            'loc': ('bAlias', 0),\n            'msg': 'Input should be a valid boolean, unable to interpret input',\n            'input': 'wrong',\n        }\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004940", "source": "def get_git_repo_dir() -> str:\n    from pathlib import Path\n    return os.getenv(\"GIT_REPO_DIR\", str(Path(__file__).resolve().parents[2]))", "target": "def test_peek(self, input_: str = \"abcdef\") -> None:\n        iter_ = PeekableIterator(input_)\n        for idx, c in enumerate(iter_):\n            if idx + 1 < len(input_):\n                self.assertEqual(iter_.peek(), input_[idx + 1])\n            else:\n                self.assertTrue(iter_.peek() is None)"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004941", "source": "def load_weights(mod: nn.Module, names: list[str], params: tuple[Tensor, ...]) -> None:\n    for name, p in zip(names, params):\n        _set_nested_attr(mod, name.split(\".\"), p)", "target": "def test_sets_and_restores_new_var(self):\n        var = \"TEST_TMP_ENV_NEW\"\n        self.assertNotIn(var, os.environ)\n        with temp_environ({var: \"123\"}):\n            self.assertEqual(os.environ[var], \"123\")\n        self.assertNotIn(var, os.environ)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004942", "source": "def make_dataclass_validator(dc_cls: Type['Dataclass'], config: Type[BaseConfig]) -> 'CallableGenerator':\n    yield from _get_validators(dataclass(dc_cls, config=config, use_proxy=True))", "target": "def test_dataclass_post_init_args_multiple():\n    dc_args = None\n    @dataclasses.dataclass\n    class Foo:\n        a: str\n        b: dataclasses.InitVar[bool]\n        c: dataclasses.InitVar[int]\n        def __post_init__(self, *args):\n            nonlocal dc_args\n            dc_args = args\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), init_only=True),\n                core_schema.dataclass_field(name='c', schema=core_schema.int_schema(), init_only=True),\n            ],\n            collect_init_only=True,\n        ),\n        ['a', 'b'],\n        post_init=True,\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': b'hello', 'b': 'true', 'c': '42'})\n    assert dataclasses.asdict(foo) == {'a': 'hello'}\n    assert dc_args == (True, 42)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004943", "source": "def set_cached_generic_type(\n    parent: type[BaseModel],\n    typevar_values: tuple[Any, ...],\n    type_: type[BaseModel],\n    origin: type[BaseModel] | None = None,\n    args: tuple[Any, ...] | None = None,\n) -> None:\n    _GENERIC_TYPES_CACHE[_early_cache_key(parent, typevar_values)] = type_\n    if len(typevar_values) == 1:\n        _GENERIC_TYPES_CACHE[_early_cache_key(parent, typevar_values[0])] = type_\n    if origin and args:\n        _GENERIC_TYPES_CACHE[_late_cache_key(origin, args, typevar_values)] = type_", "target": "def test_fastapi_startup_perf(benchmark: Any):\n    data_models = create_data_models()\n    T = TypeVar('T')\n    class GetModel(BaseModel, Generic[T]):\n        res: T\n    class GetModel2(GetModel[T], Generic[T]):\n        foo: str\n        bar: str\n    class GetManyModel(BaseModel, Generic[T]):\n        res: list[T]\n    class GetManyModel2(GetManyModel[T], Generic[T]):\n        foo: str\n        bar: str\n    class GetManyModel3(BaseModel, Generic[T]):\n        res: dict[str, T]\n    class GetManyModel4(BaseModel, Generic[T]):\n        res: dict[str, list[T]]\n    class PutModel(BaseModel, Generic[T]):\n        data: T\n    class PutModel2(PutModel[T], Generic[T]):\n        foo: str\n        bar: str\n    class PutManyModel(BaseModel, Generic[T]):\n        data: list[T]\n    class PutManyModel2(PutManyModel[T], Generic[T]):\n        foo: str\n        bar: str\n    api_models: list[Any] = [\n        GetModel,\n        GetModel2,\n        GetManyModel,\n        GetManyModel2,\n        GetManyModel3,\n        GetManyModel4,\n        PutModel,\n        PutModel2,\n        PutManyModel,\n        PutManyModel2,\n    ]\n    assert len(data_models) == INNER_DATA_MODEL_COUNT + OUTER_DATA_MODEL_COUNT\n    def bench():\n        concrete_api_models = []\n        adapters = []\n        for outer_api_model in api_models:\n            for data_model in data_models:\n                concrete_api_model = outer_api_model[\n                    data_model\n                ]\n                concrete_api_models.append(concrete_api_model)\n                adapt = TypeAdapter(Annotated[concrete_api_model, FieldInfo(description='foo')])\n                adapters.append(adapt)\n                adapt = TypeAdapter(Annotated[concrete_api_model, FieldInfo(description='bar')])\n                adapters.append(adapt)\n        assert len(concrete_api_models) == len(data_models) * len(api_models)\n        assert len(adapters) == len(concrete_api_models) * 2\n    benchmark(bench)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004944", "source": "def as_dataclass_field(pydantic_field: FieldInfo) -> dataclasses.Field[Any]:\n    field_args: dict[str, Any] = {'default': pydantic_field}\n    if sys.version_info >= (3, 14) and pydantic_field.description is not None:\n        field_args['doc'] = pydantic_field.description\n    if sys.version_info >= (3, 10) and pydantic_field.kw_only:\n        field_args['kw_only'] = True\n    if pydantic_field.repr is not True:\n        field_args['repr'] = pydantic_field.repr\n    return dataclasses.field(**field_args)", "target": "def test_dataclass_args(py_and_json: PyAndJson, input_value, expected):\n    schema = core_schema.dataclass_args_schema(\n        'MyDataclass',\n        [\n            core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), kw_only=False),\n            core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), kw_only=False),\n        ],\n    )\n    v = py_and_json(schema)\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)) as exc_info:\n            v.validate_test(input_value)\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        assert v.validate_test(input_value) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004945", "source": "def is_pydantic_dataclass(class_: type[Any], /) -> TypeGuard[type[PydanticDataclass]]:\n    try:\n        return '__is_pydantic_dataclass__' in class_.__dict__ and dataclasses.is_dataclass(class_)\n    except AttributeError:\n        return False", "target": "def test_custom_dataclass_names():\n    schema = core_schema.dataclass_schema(\n        FooParentDataclass,\n        core_schema.dataclass_args_schema(\n            'FooParentDataclass',\n            [\n                core_schema.dataclass_field(\n                    name='foo',\n                    schema=core_schema.union_schema(\n                        [\n                            core_schema.dataclass_schema(\n                                FooDataclass,\n                                core_schema.dataclass_args_schema(\n                                    'FooDataclass[dataclass_args_schema]',\n                                    [\n                                        core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                                        core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n                                    ],\n                                ),\n                                ['a', 'b'],\n                                cls_name='FooDataclass[cls_name]',\n                            ),\n                            core_schema.none_schema(),\n                        ]\n                    ),\n                )\n            ],\n        ),\n        ['foo'],\n    )\n    v = SchemaValidator(schema)\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python({'foo': 123})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'class_name': 'FooDataclass[dataclass_args_schema]'},\n            'input': 123,\n            'loc': ('foo', 'FooDataclass[cls_name]'),\n            'msg': 'Input should be a dictionary or an instance of FooDataclass[dataclass_args_schema]',\n            'type': 'dataclass_type',\n        },\n        {'input': 123, 'loc': ('foo', 'none'), 'msg': 'Input should be None', 'type': 'none_required'},\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004946", "source": "def env_path_field(\n    name: str,\n    default: Union[str, Path] = \"\",\n    *,\n    resolve: bool = True,\n) -> Path:\n    return field(default_factory=lambda: env_path(name, default, resolve=resolve))", "target": "def test_env_path_optional_unset_returns_default_str(self):\n        default_str = \"x/y\"\n        with patch.dict(os.environ, {}, clear=True):\n            p = m.env_path_optional(\"P\", default=default_str)\n            self.assertIsInstance(p, Path)\n            self.assertIsNotNone(p)\n            if p:\n                self.assertTrue(p.is_absolute())\n                self.assertEqual(p.parts[-2:], (\"x\", \"y\"))"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004947", "source": "def branches_containing_ref(\n        self, ref: str, *, include_remote: bool = True\n    ) -> list[str]:\n        rc = (\n            self._run_git(\"branch\", \"--remote\", \"--contains\", ref)\n            if include_remote\n            else self._run_git(\"branch\", \"--contains\", ref)\n        )\n        return [x.strip() for x in rc.split(\"\\n\") if x.strip()] if len(rc) > 0 else []", "target": "def test_compute_diff(self) -> None:\n        diff = self.repo.diff(\"HEAD\")\n        sha = _shasum(diff)\n        self.assertEqual(len(sha), 64)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004948", "source": "def _validate_dataclass(cls: Type['DataclassT'], v: Any) -> 'DataclassT':\n    with set_validation(cls, True):\n        if isinstance(v, cls):\n            v.__pydantic_validate_values__()\n            return v\n        elif isinstance(v, (list, tuple)):\n            return cls(*v)\n        elif isinstance(v, dict):\n            return cls(**v)\n        else:\n            raise DataclassTypeError(class_name=cls.__name__)", "target": "def test_aliases(py_and_json: PyAndJson):\n    schema = core_schema.dataclass_args_schema(\n        'MyDataclass',\n        [\n            core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), validation_alias='Apple'),\n            core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), validation_alias=['Banana', 1]),\n            core_schema.dataclass_field(\n                name='c', schema=core_schema.int_schema(), validation_alias=['Carrot', 'v'], init_only=True\n            ),\n        ],\n        collect_init_only=True,\n    )\n    v = py_and_json(schema)\n    assert v.validate_test({'Apple': 'a', 'Banana': ['x', 'false'], 'Carrot': {'v': '42'}}) == (\n        {'a': 'a', 'b': False},\n        (42,),\n    )"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004949", "source": "def is_path_exist(path: Union[str, Path, None]) -> bool:\n    return bool(path and get_path(path).exists())", "target": "def test_copy_dir_into_existing_dir_overwrite_true_merges(self):\n        src = self.tmp_path / \"srcdir\"\n        dst = self.tmp_path / \"destdir\"\n        (src / \"x\").mkdir(parents=True)\n        (src / \"x\" / \"new.txt\").write_text(\"new\")\n        dst.mkdir()\n        (dst / \"existing.txt\").write_text(\"old\")\n        copy(src, dst)\n        self.assertEqual((dst / \"existing.txt\").read_text(), \"old\")\n        self.assertEqual((dst / \"x\" / \"new.txt\").read_text(), \"new\")"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004950", "source": "def new_init(self: 'Dataclass', *args: Any, **kwargs: Any) -> None:\n            handle_extra_init(self, *args, **kwargs)\n            if self.__class__.__pydantic_run_validation__:\n                self.__pydantic_validate_values__()\n            if hasattr(self, '__post_init_post_parse__'):\n                initvars_and_values: Dict[str, Any] = {}\n                for i, f in enumerate(self.__class__.__dataclass_fields__.values()):\n                    if f._field_type is dataclasses._FIELD_INITVAR:\n                        try:\n                            initvars_and_values[f.name] = args[i]\n                        except IndexError:\n                            initvars_and_values[f.name] = kwargs.get(f.name, f.default)\n                self.__post_init_post_parse__(**initvars_and_values)", "target": "def test_dataclass_self_init():\n    @dataclasses.dataclass(init=False)\n    class Foo:\n        a: str\n        b: bool\n        def __init__(self, *args, **kwargs):\n            v.validate_python(ArgsKwargs(args, kwargs), self_instance=self)\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), kw_only=False),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), kw_only=False),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    foo = Foo(b'hello', 'True')\n    assert dataclasses.is_dataclass(foo)\n    assert dataclasses.asdict(foo) == {'a': 'hello', 'b': True}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004951", "source": "def dataclass(\n    _cls: type[_T] | None = None,\n    *,\n    init: Literal[False] = False,\n    repr: bool = True,\n    eq: bool = True,\n    order: bool = False,\n    unsafe_hash: bool = False,\n    frozen: bool | None = None,\n    config: ConfigDict | type[object] | None = None,\n    validate_on_init: bool | None = None,\n    kw_only: bool = False,\n    slots: bool = False,\n) -> Callable[[type[_T]], type[PydanticDataclass]] | type[PydanticDataclass]:\n    assert init is False, 'pydantic.dataclasses.dataclass only supports init=False'\n    assert validate_on_init is not False, 'validate_on_init=False is no longer supported'\n    if sys.version_info >= (3, 10):\n        kwargs = {'kw_only': kw_only, 'slots': slots}\n    else:\n        kwargs = {}\n    def create_dataclass(cls: type[Any]) -> type[PydanticDataclass]:\n        from ._internal._utils import is_model_class\n        if is_model_class(cls):\n            raise PydanticUserError(\n                f'Cannot create a Pydantic dataclass from {cls.__name__} as it is already a Pydantic model',\n                code='dataclass-on-model',\n            )\n        original_cls = cls\n        has_dataclass_base = any(dataclasses.is_dataclass(base) for base in cls.__bases__)\n        if not has_dataclass_base and config is not None and hasattr(cls, '__pydantic_config__'):\n            warn(\n                f'`config` is set via both the `dataclass` decorator and `__pydantic_config__` for dataclass {cls.__name__}. '\n                f'The `config` specification from `dataclass` decorator will take priority.',\n                category=UserWarning,\n                stacklevel=2,\n            )\n        config_dict = config if config is not None else getattr(cls, '__pydantic_config__', None)\n        config_wrapper = _config.ConfigWrapper(config_dict)\n        decorators = _decorators.DecoratorInfos.build(cls, replace_wrapped_methods=True)\n        decorators.update_from_config(config_wrapper)\n        original_doc = cls.__doc__\n        if _pydantic_dataclasses.is_stdlib_dataclass(cls):\n            original_doc = None\n            bases = (cls,)\n            if issubclass(cls, Generic):\n                generic_base = Generic[cls.__parameters__]\n                bases = bases + (generic_base,)\n            cls = types.new_class(cls.__name__, bases)\n        if frozen is not None:\n            frozen_ = frozen\n            if config_wrapper.frozen:\n                warn(\n                    f'`frozen` is set via both the `dataclass` decorator and `config` for dataclass {cls.__name__!r}.'\n                    'This is not recommended. The `frozen` specification on `dataclass` will take priority.',\n                    category=UserWarning,\n                    stacklevel=2,\n                )\n        else:\n            frozen_ = config_wrapper.frozen or False\n        cls_anns = _typing_extra.safe_get_annotations(cls)\n        for field_name in cls_anns:\n            field_value = getattr(cls, field_name, None)\n            if isinstance(field_value, FieldInfo):\n                setattr(cls, field_name, _pydantic_dataclasses.as_dataclass_field(field_value))\n        with _pydantic_dataclasses.patch_base_fields(cls):\n            cls = dataclasses.dataclass(\n                cls,\n                init=True,\n                repr=repr,\n                eq=eq,\n                order=order,\n                unsafe_hash=unsafe_hash,\n                frozen=frozen_,\n                **kwargs,\n            )\n        if config_wrapper.validate_assignment:\n            original_setattr = cls.__setattr__\n            @functools.wraps(cls.__setattr__)\n            def validated_setattr(instance: PydanticDataclass, name: str, value: Any, /) -> None:\n                if frozen_:\n                    return original_setattr(instance, name, value)\n                inst_cls = type(instance)\n                attr = getattr(inst_cls, name, None)\n                if isinstance(attr, property):\n                    attr.__set__(instance, value)\n                elif isinstance(attr, functools.cached_property):\n                    instance.__dict__.__setitem__(name, value)\n                else:\n                    inst_cls.__pydantic_validator__.validate_assignment(instance, name, value)\n            cls.__setattr__ = validated_setattr.__get__(None, cls)\n            if slots and not hasattr(cls, '__setstate__'):\n                def _dataclass_getstate(self: Any) -> list[Any]:\n                    return [getattr(self, f.name) for f in dataclasses.fields(self)]\n                def _dataclass_setstate(self: Any, state: list[Any]) -> None:\n                    for field, value in zip(dataclasses.fields(self), state):\n                        object.__setattr__(self, field.name, value)\n                cls.__getstate__ = _dataclass_getstate\n                cls.__setstate__ = _dataclass_setstate\n        cls.__is_pydantic_dataclass__ = True\n        cls.__pydantic_decorators__ = decorators\n        cls.__doc__ = original_doc\n        firstlineno = getattr(original_cls, '__firstlineno__', None)\n        cls.__module__ = original_cls.__module__\n        if sys.version_info >= (3, 13) and firstlineno is not None:\n            original_cls.__firstlineno__ = firstlineno\n            cls.__firstlineno__ = firstlineno\n        cls.__qualname__ = original_cls.__qualname__\n        cls.__pydantic_fields_complete__ = classmethod(_pydantic_fields_complete)\n        cls.__pydantic_complete__ = False\n        _pydantic_dataclasses.complete_dataclass(cls, config_wrapper, raise_errors=False)\n        return cls\n    return create_dataclass if _cls is None else create_dataclass(_cls)", "target": "def test_dataclass_args(py_and_json: PyAndJson, input_value, expected):\n    schema = core_schema.dataclass_args_schema(\n        'MyDataclass',\n        [\n            core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), kw_only=False),\n            core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), kw_only=False),\n        ],\n    )\n    v = py_and_json(schema)\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)) as exc_info:\n            v.validate_test(input_value)\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        assert v.validate_test(input_value) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004952", "source": "def dataclass(\n        _cls: Type[_T],\n        *,\n        init: bool = True,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool = False,\n        config: Union[ConfigDict, Type[object], None] = None,\n        validate_on_init: Optional[bool] = None,\n        use_proxy: Optional[bool] = None,\n        kw_only: bool = ...,\n    ) -> 'DataclassClassOrWrapper':\n        ...", "target": "def test_serialization_exclude():\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(\n                    name='a', schema=core_schema.str_schema(), serialization_exclude_if=lambda x: x == 'bye'\n                ),\n                core_schema.dataclass_field(name='b', schema=core_schema.bytes_schema(), serialization_exclude=True),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    s = SchemaSerializer(schema)\n    assert s.to_python(Foo(a='hello', b=b'more')) == {'a': 'hello'}\n    assert s.to_python(Foo(a='hello', b=b'more'), mode='json') == {'a': 'hello'}\n    assert s.to_python(Foo(a='bye', b=b'more'), mode='json') == {}\n    j = s.to_json(Foo(a='hello', b=b'more'))\n    if on_pypy:\n        assert json.loads(j) == {'a': 'hello'}\n    else:\n        assert j == b'{\"a\":\"hello\"}'\n    j = s.to_json(Foo(a='bye', b=b'more'))\n    if on_pypy:\n        assert json.loads(j) == {}\n    else:\n        assert j == b'{}'"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004953", "source": "def check_for_functorch():\n    try:\n        import functorch\n        return True\n    except ImportError:\n        return False", "target": "def test_noop_when_empty_path(self):\n        start = Path.cwd()\n        with working_directory(\"\"):\n            self.assertEqual(Path.cwd(), start)\n        self.assertEqual(Path.cwd(), start)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004954", "source": "def is_pydantic_dataclass(class_: type[Any], /) -> TypeGuard[type[PydanticDataclass]]:\n    try:\n        return '__is_pydantic_dataclass__' in class_.__dict__ and dataclasses.is_dataclass(class_)\n    except AttributeError:\n        return False", "target": "def test_properties():\n    @dataclasses.dataclass\n    class FooProp:\n        a: str\n        b: bytes\n        @property\n        def c(self) -> str:\n            return f'{self.a} {self.b.decode()}'\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'FooProp',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bytes_schema()),\n            ],\n            computed_fields=[core_schema.computed_field('c', core_schema.str_schema())],\n        ),\n        ['a', 'b'],\n    )\n    s = SchemaSerializer(schema)\n    assert s.to_python(FooProp(a='hello', b=b'more')) == IsStrictDict(a='hello', b=b'more', c='hello more')\n    assert s.to_python(FooProp(a='hello', b=b'more'), mode='json') == IsStrictDict(a='hello', b='more', c='hello more')\n    j = s.to_json(FooProp(a='hello', b=b'more'))\n    if on_pypy:\n        assert json.loads(j) == {'a': 'hello', 'b': 'more', 'c': 'hello more'}\n    else:\n        assert j == b'{\"a\":\"hello\",\"b\":\"more\",\"c\":\"hello more\"}'\n    assert s.to_python(FooProp(a='hello', b=b'more'), exclude={'b'}) == IsStrictDict(a='hello', c='hello more')\n    assert s.to_json(FooProp(a='hello', b=b'more'), include={'a'}) == b'{\"a\":\"hello\"}'"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004955", "source": "def _is_field_cached_property(obj: 'Dataclass', k: str) -> bool:\n        return False", "target": "def test_serialization_alias():\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bytes_schema(), serialization_alias='BAR'),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    s = SchemaSerializer(schema)\n    assert s.to_python(Foo(a='hello', b=b'more'), by_alias=True) == IsStrictDict(a='hello', BAR=b'more')\n    assert s.to_python(Foo(a='hello', b=b'more'), mode='json', by_alias=True) == IsStrictDict(a='hello', BAR='more')\n    j = s.to_json(Foo(a='hello', b=b'more'), by_alias=True)\n    if on_pypy:\n        assert json.loads(j) == {'a': 'hello', 'BAR': 'more'}\n    else:\n        assert j == b'{\"a\":\"hello\",\"BAR\":\"more\"}'"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004956", "source": "def timedelta_isoformat(td: datetime.timedelta) -> str:\n    warnings.warn('`timedelta_isoformat` is deprecated.', category=PydanticDeprecatedSince20, stacklevel=2)\n    minutes, seconds = divmod(td.seconds, 60)\n    hours, minutes = divmod(minutes, 60)\n    return f'{\"-\" if td.days < 0 else \"\"}P{abs(td.days)}DT{hours:d}H{minutes:d}M{seconds:d}.{td.microseconds:06d}S'", "target": "def test_json_bytes_base64_round_trip():\n    data = b'\\xd8\\x07\\xc1Tx$\\x91F%\\xf3\\xf3I\\xca\\xd8@\\x0c\\xee\\xc3\\xab\\xff\\x7f\\xd3\\xcd\\xcd\\xf9\\xc2\\x10\\xe4\\xa1\\xb01e'\n    encoded_std = b'\"2AfBVHgkkUYl8/NJythADO7Dq/9/083N+cIQ5KGwMWU=\"'\n    encoded_url = b'\"2AfBVHgkkUYl8_NJythADO7Dq_9_083N-cIQ5KGwMWU=\"'\n    assert to_json(data, bytes_mode='base64') == encoded_url\n    v = SchemaValidator(core_schema.bytes_schema(), config=CoreConfig(val_json_bytes='base64'))\n    assert v.validate_json(encoded_url) == data\n    assert v.validate_json(encoded_std) == data\n    with pytest.raises(ValidationError) as exc:\n        v.validate_json('\"wrong!\"')\n    [details] = exc.value.errors()\n    assert details['type'] == 'bytes_invalid_encoding'\n    assert to_json({'key': data}, bytes_mode='base64') == b'{\"key\":' + encoded_url + b'}'\n    v = SchemaValidator(\n        core_schema.dict_schema(keys_schema=core_schema.str_schema(), values_schema=core_schema.bytes_schema()),\n        config=CoreConfig(val_json_bytes='base64'),\n    )\n    assert v.validate_json(b'{\"key\":' + encoded_url + b'}') == {'key': data}"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004957", "source": "def fetch(self, ref: Optional[str] = None, branch: Optional[str] = None) -> None:\n        if branch is None and ref is None:\n            self._run_git(\"fetch\", self.remote)\n        elif branch is None:\n            self._run_git(\"fetch\", self.remote, ref)\n        else:\n            self._run_git(\"fetch\", self.remote, f\"{ref}:{branch}\")", "target": "def test_iterator(self, input_: str = \"abcdef\") -> None:\n        iter_ = PeekableIterator(input_)\n        for idx, c in enumerate(iter_):\n            self.assertEqual(c, input_[idx])"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004958", "source": "def _dataclass_getstate(self: Any) -> list[Any]:\n                    return [getattr(self, f.name) for f in dataclasses.fields(self)]", "target": "def test_frozen_field():\n    @dataclasses.dataclass\n    class MyModel:\n        f: str\n    v = SchemaValidator(\n        core_schema.dataclass_schema(\n            MyModel,\n            core_schema.dataclass_args_schema(\n                'MyModel', [core_schema.dataclass_field('f', core_schema.str_schema(), frozen=True)]\n            ),\n            ['f'],\n        )\n    )\n    m = v.validate_python({'f': 'x'})\n    assert m.f == 'x'\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_assignment(m, 'f', 'y')\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'frozen_field', 'loc': ('f',), 'msg': 'Field is frozen', 'input': 'y'}\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004959", "source": "def run_command(\n    cmd: str,\n    use_shell: bool = False,\n    log_cmd: bool = True,\n    cwd: Optional[str] = None,\n    env: Optional[dict] = None,\n    check: bool = True,\n) -> int:\n    if use_shell:\n        args = cmd\n        log_prefix = \"[shell]\"\n        executable = \"/bin/bash\"\n    else:\n        args = shlex.split(cmd)\n        log_prefix = \"[cmd]\"\n        executable = None\n    if log_cmd:\n        display_cmd = cmd if use_shell else \" \".join(args)\n        logger.info(\"%s %s\", log_prefix, display_cmd)\n    run_env = {**os.environ, **(env or {})}\n    proc = subprocess.run(\n        args,\n        shell=use_shell,\n        executable=executable,\n        stdout=sys.stdout,\n        stderr=sys.stderr,\n        cwd=cwd,\n        env=run_env,\n        check=False,\n    )\n    if check and proc.returncode != 0:\n        logger.error(\n            \"%s Command failed (exit %s): %s\", log_prefix, proc.returncode, cmd\n        )\n        raise subprocess.CalledProcessError(\n            proc.returncode, args if not use_shell else cmd\n        )\n    return proc.returncode", "target": "def test_sets_and_restores_new_var(self):\n        var = \"TEST_TMP_ENV_NEW\"\n        self.assertNotIn(var, os.environ)\n        with temp_environ({var: \"123\"}):\n            self.assertEqual(os.environ[var], \"123\")\n        self.assertNotIn(var, os.environ)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004960", "source": "def is_stdlib_dataclass(cls: type[Any], /) -> TypeIs[type[StandardDataclass]]:\n    return '__dataclass_fields__' in cls.__dict__ and not hasattr(cls, '__pydantic_validator__')", "target": "def test_dataclass_exact_validation(revalidate_instances, input_value, expected):\n    schema = core_schema.dataclass_schema(\n        FooDataclass,\n        core_schema.dataclass_args_schema(\n            'FooDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n            ],\n        ),\n        ['a', 'b'],\n        revalidate_instances=revalidate_instances,\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python(input_value)\n    assert dataclasses.asdict(foo) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004961", "source": "def is_pydantic_dataclass(class_: type[Any], /) -> TypeGuard[type[PydanticDataclass]]:\n    try:\n        return '__is_pydantic_dataclass__' in class_.__dict__ and dataclasses.is_dataclass(class_)\n    except AttributeError:\n        return False", "target": "def test_slots_mixed():\n    @dataclasses.dataclass(slots=True)\n    class Model:\n        x: int\n        y: dataclasses.InitVar[str]\n        z: ClassVar[str] = 'z-classvar'\n    @dataclasses.dataclass\n    class SubModel(Model):\n        x2: int\n        y2: dataclasses.InitVar[str]\n        z2: ClassVar[str] = 'z2-classvar'\n    schema = core_schema.dataclass_schema(\n        SubModel,\n        core_schema.dataclass_args_schema(\n            'SubModel',\n            [\n                core_schema.dataclass_field(name='x', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(name='y', init_only=True, schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='x2', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(name='y2', init_only=True, schema=core_schema.str_schema()),\n            ],\n        ),\n        ['x'],\n        slots=True,\n    )\n    v = SchemaValidator(schema)\n    dc = v.validate_python({'x': 1, 'y': 'a', 'x2': 2, 'y2': 'b'})\n    assert dc.x == 1\n    assert dc.x2 == 2\n    assert dataclasses.asdict(dc) == {'x': 1, 'x2': 2}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004962", "source": "def dataclass(\n        _cls: Type[_T],\n        *,\n        init: bool = True,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool = False,\n        config: Union[ConfigDict, Type[object], None] = None,\n        validate_on_init: Optional[bool] = None,\n        use_proxy: Optional[bool] = None,\n    ) -> 'DataclassClassOrWrapper':\n        ...", "target": "def test_extra_behavior_ignore(\n    config: Union[core_schema.CoreConfig, None],\n    schema_extra_behavior_kw: dict[str, Any],\n    validate_fn_extra_kw: Union[ExtraBehavior, None],\n):\n    @dataclasses.dataclass\n    class MyModel:\n        f: str\n    v = SchemaValidator(\n        core_schema.dataclass_schema(\n            MyModel,\n            core_schema.dataclass_args_schema(\n                'MyModel', [core_schema.dataclass_field('f', core_schema.str_schema())], **schema_extra_behavior_kw\n            ),\n            ['f'],\n        ),\n        config=config,\n    )\n    m: MyModel = v.validate_python({'f': 'x', 'extra_field': 123}, extra=validate_fn_extra_kw)\n    assert m.f == 'x'\n    assert not hasattr(m, 'extra_field')\n    v.validate_assignment(m, 'f', 'y', extra=validate_fn_extra_kw)\n    assert m.f == 'y'\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_assignment(m, 'not_f', 'xyz', extra=validate_fn_extra_kw)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'no_such_attribute',\n            'loc': ('not_f',),\n            'msg': \"Object has no attribute 'not_f'\",\n            'input': 'xyz',\n            'ctx': {'attribute': 'not_f'},\n        }\n    ]\n    assert not hasattr(m, 'not_f')"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004963", "source": "def pydantic_encoder(obj: Any) -> Any:\n    warnings.warn(\n        '`pydantic_encoder` is deprecated, use `pydantic_core.to_jsonable_python` instead.',\n        category=PydanticDeprecatedSince20,\n        stacklevel=2,\n    )\n    from dataclasses import asdict, is_dataclass\n    BaseModel = import_cached_base_model()\n    if isinstance(obj, BaseModel):\n        return obj.model_dump()\n    elif is_dataclass(obj):\n        return asdict(obj)\n    for base in obj.__class__.__mro__[:-1]:\n        try:\n            encoder = ENCODERS_BY_TYPE[base]\n        except KeyError:\n            continue\n        return encoder(obj)\n    else:\n        raise TypeError(f\"Object of type '{obj.__class__.__name__}' is not JSON serializable\")", "target": "def test_input_types(input_value):\n    v = SchemaValidator(core_schema.list_schema(items_schema=core_schema.int_schema()))\n    assert v.validate_json(input_value) == [1, 2, 3]"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004964", "source": "def working_directory(path: str):\n    if not path:\n        yield\n        return\n    prev_cwd = os.getcwd()\n    try:\n        os.chdir(path)\n        yield\n    finally:\n        os.chdir(prev_cwd)", "target": "def test_noop_when_empty_path(self):\n        start = Path.cwd()\n        with working_directory(\"\"):\n            self.assertEqual(Path.cwd(), start)\n        self.assertEqual(Path.cwd(), start)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004965", "source": "def _recreate_field_info(\n    field_info: FieldInfo,\n    ns_resolver: NsResolver,\n    typevars_map: Mapping[TypeVar, Any],\n    *,\n    lenient: bool,\n) -> FieldInfo:\n    FieldInfo_ = import_cached_field_info()\n    existing_desc = field_info.description\n    if lenient:\n        ann = _generics.replace_types(field_info._original_annotation, typevars_map)\n        ann, evaluated = _typing_extra.try_eval_type(\n            ann,\n            *ns_resolver.types_namespace,\n        )\n    else:\n        ann = _typing_extra.eval_type(\n            field_info._original_annotation,\n            *ns_resolver.types_namespace,\n        )\n        ann = _generics.replace_types(ann, typevars_map)\n        ann = _typing_extra.eval_type(\n            ann,\n            *ns_resolver.types_namespace,\n        )\n        evaluated = True\n    if (assign := field_info._original_assignment) is PydanticUndefined:\n        new_field = FieldInfo_.from_annotation(ann, _source=AnnotationSource.CLASS)\n    else:\n        new_field = FieldInfo_.from_annotated_attribute(ann, assign, _source=AnnotationSource.CLASS)\n        new_field._original_assignment = assign\n    new_field._original_annotation = ann\n    new_field.description = new_field.description if new_field.description is not None else existing_desc\n    if not evaluated:\n        new_field._complete = False\n    return new_field", "target": "def test_wrap_on_error(self, py_and_json: PyAndJson):\n        def wrap_function(input_value, validator, info):\n            try:\n                return validator(input_value)\n            except ValidationError:\n                if isinstance(input_value, list):\n                    return str(len(input_value))\n                else:\n                    return repr(input_value)\n        v = py_and_json(\n            {\n                'type': 'model-fields',\n                'fields': {\n                    'x': {\n                        'type': 'model-field',\n                        'schema': {\n                            'type': 'default',\n                            'on_error': 'raise',\n                            'schema': {\n                                'type': 'function-wrap',\n                                'function': {'type': 'with-info', 'function': wrap_function},\n                                'schema': {'type': 'str'},\n                            },\n                        },\n                    }\n                },\n            }\n        )\n        assert v.validate_test({'x': 'foo'}) == ({'x': 'foo'}, None, {'x'})\n        assert v.validate_test({'x': ['foo']}) == ({'x': '1'}, None, {'x'})\n        assert v.validate_test({'x': ['foo', 'bar']}) == ({'x': '2'}, None, {'x'})\n        assert v.validate_test({'x': {'a': 'b'}}) == ({'x': \"{'a': 'b'}\"}, None, {'x'})"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004966", "source": "def secs_to_us(time_s):\n    return time_s * 1e6", "target": "def test_pr_with_not_user_facing_label(\n        self, mocked_rn_labels: Any, mocked_gql: Any\n    ) -> None:\n        \"Test PR with 'topic: not user facing' label\"\n        pr = GitHubPR(\"pytorch\", \"pytorch\", 75095)\n        self.assertTrue(has_required_labels(pr))"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004967", "source": "def are_ghstack_branches_in_sync(\n    repo: GitRepo, head_ref: str, base_ref: Optional[str] = None\n) -> bool:\n    orig_ref = re.sub(r\"/head$\", \"/orig\", head_ref)\n    if base_ref is None:\n        base_ref = re.sub(r\"/head$\", \"/base\", head_ref)\n    orig_diff_sha = _shasum(repo.diff(f\"{repo.remote}/{orig_ref}\"))\n    head_diff_sha = _shasum(\n        repo.diff(\n            base_ref if is_commit_hash(base_ref) else f\"{repo.remote}/{base_ref}\",\n            f\"{repo.remote}/{head_ref}\",\n        )\n    )\n    return orig_diff_sha == head_diff_sha", "target": "def test_compute_diff(self) -> None:\n        diff = self.repo.diff(\"HEAD\")\n        sha = _shasum(diff)\n        self.assertEqual(len(sha), 64)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004968", "source": "def create_dataclass(cls: type[Any]) -> type[PydanticDataclass]:\n        from ._internal._utils import is_model_class\n        if is_model_class(cls):\n            raise PydanticUserError(\n                f'Cannot create a Pydantic dataclass from {cls.__name__} as it is already a Pydantic model',\n                code='dataclass-on-model',\n            )\n        original_cls = cls\n        has_dataclass_base = any(dataclasses.is_dataclass(base) for base in cls.__bases__)\n        if not has_dataclass_base and config is not None and hasattr(cls, '__pydantic_config__'):\n            warn(\n                f'`config` is set via both the `dataclass` decorator and `__pydantic_config__` for dataclass {cls.__name__}. '\n                f'The `config` specification from `dataclass` decorator will take priority.',\n                category=UserWarning,\n                stacklevel=2,\n            )\n        config_dict = config if config is not None else getattr(cls, '__pydantic_config__', None)\n        config_wrapper = _config.ConfigWrapper(config_dict)\n        decorators = _decorators.DecoratorInfos.build(cls, replace_wrapped_methods=True)\n        decorators.update_from_config(config_wrapper)\n        original_doc = cls.__doc__\n        if _pydantic_dataclasses.is_stdlib_dataclass(cls):\n            original_doc = None\n            bases = (cls,)\n            if issubclass(cls, Generic):\n                generic_base = Generic[cls.__parameters__]\n                bases = bases + (generic_base,)\n            cls = types.new_class(cls.__name__, bases)\n        if frozen is not None:\n            frozen_ = frozen\n            if config_wrapper.frozen:\n                warn(\n                    f'`frozen` is set via both the `dataclass` decorator and `config` for dataclass {cls.__name__!r}.'\n                    'This is not recommended. The `frozen` specification on `dataclass` will take priority.',\n                    category=UserWarning,\n                    stacklevel=2,\n                )\n        else:\n            frozen_ = config_wrapper.frozen or False\n        cls_anns = _typing_extra.safe_get_annotations(cls)\n        for field_name in cls_anns:\n            field_value = getattr(cls, field_name, None)\n            if isinstance(field_value, FieldInfo):\n                setattr(cls, field_name, _pydantic_dataclasses.as_dataclass_field(field_value))\n        with _pydantic_dataclasses.patch_base_fields(cls):\n            cls = dataclasses.dataclass(\n                cls,\n                init=True,\n                repr=repr,\n                eq=eq,\n                order=order,\n                unsafe_hash=unsafe_hash,\n                frozen=frozen_,\n                **kwargs,\n            )\n        if config_wrapper.validate_assignment:\n            original_setattr = cls.__setattr__\n            @functools.wraps(cls.__setattr__)\n            def validated_setattr(instance: PydanticDataclass, name: str, value: Any, /) -> None:\n                if frozen_:\n                    return original_setattr(instance, name, value)\n                inst_cls = type(instance)\n                attr = getattr(inst_cls, name, None)\n                if isinstance(attr, property):\n                    attr.__set__(instance, value)\n                elif isinstance(attr, functools.cached_property):\n                    instance.__dict__.__setitem__(name, value)\n                else:\n                    inst_cls.__pydantic_validator__.validate_assignment(instance, name, value)\n            cls.__setattr__ = validated_setattr.__get__(None, cls)\n            if slots and not hasattr(cls, '__setstate__'):\n                def _dataclass_getstate(self: Any) -> list[Any]:\n                    return [getattr(self, f.name) for f in dataclasses.fields(self)]\n                def _dataclass_setstate(self: Any, state: list[Any]) -> None:\n                    for field, value in zip(dataclasses.fields(self), state):\n                        object.__setattr__(self, field.name, value)\n                cls.__getstate__ = _dataclass_getstate\n                cls.__setstate__ = _dataclass_setstate\n        cls.__is_pydantic_dataclass__ = True\n        cls.__pydantic_decorators__ = decorators\n        cls.__doc__ = original_doc\n        firstlineno = getattr(original_cls, '__firstlineno__', None)\n        cls.__module__ = original_cls.__module__\n        if sys.version_info >= (3, 13) and firstlineno is not None:\n            original_cls.__firstlineno__ = firstlineno\n            cls.__firstlineno__ = firstlineno\n        cls.__qualname__ = original_cls.__qualname__\n        cls.__pydantic_fields_complete__ = classmethod(_pydantic_fields_complete)\n        cls.__pydantic_complete__ = False\n        _pydantic_dataclasses.complete_dataclass(cls, config_wrapper, raise_errors=False)\n        return cls", "target": "def test_dataclass_post_init_args():\n    c_value = None\n    @dataclasses.dataclass\n    class Foo:\n        a: str\n        b: bool\n        c: dataclasses.InitVar[int]\n        def __post_init__(self, c: int):\n            nonlocal c_value\n            c_value = c\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n                core_schema.dataclass_field(name='c', schema=core_schema.int_schema(), init_only=True),\n            ],\n            collect_init_only=True,\n        ),\n        ['a', 'b'],\n        post_init=True,\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': b'hello', 'b': 'true', 'c': '42'})\n    assert foo.a == 'hello'\n    assert foo.b is True\n    assert not hasattr(foo, 'c')\n    assert c_value == 42"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004969", "source": "def _recreate_field_info(\n    field_info: FieldInfo,\n    ns_resolver: NsResolver,\n    typevars_map: Mapping[TypeVar, Any],\n    *,\n    lenient: bool,\n) -> FieldInfo:\n    FieldInfo_ = import_cached_field_info()\n    existing_desc = field_info.description\n    if lenient:\n        ann = _generics.replace_types(field_info._original_annotation, typevars_map)\n        ann, evaluated = _typing_extra.try_eval_type(\n            ann,\n            *ns_resolver.types_namespace,\n        )\n    else:\n        ann = _typing_extra.eval_type(\n            field_info._original_annotation,\n            *ns_resolver.types_namespace,\n        )\n        ann = _generics.replace_types(ann, typevars_map)\n        ann = _typing_extra.eval_type(\n            ann,\n            *ns_resolver.types_namespace,\n        )\n        evaluated = True\n    if (assign := field_info._original_assignment) is PydanticUndefined:\n        new_field = FieldInfo_.from_annotation(ann, _source=AnnotationSource.CLASS)\n    else:\n        new_field = FieldInfo_.from_annotated_attribute(ann, assign, _source=AnnotationSource.CLASS)\n        new_field._original_assignment = assign\n    new_field._original_annotation = ann\n    new_field.description = new_field.description if new_field.description is not None else existing_desc\n    if not evaluated:\n        new_field._complete = False\n    return new_field", "target": "def test_alias_extra_from_attributes():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            extra_behavior='allow',\n            from_attributes=True,\n            fields={\n                'field_a': core_schema.model_field(\n                    validation_alias=[['FieldA'], ['foo', 2]], schema=core_schema.int_schema()\n                )\n            },\n        )\n    )\n    assert v.validate_python({'FieldA': 1}) == ({'field_a': 1}, {}, {'field_a'})\n    assert v.validate_python(Cls(FieldA=1)) == ({'field_a': 1}, {}, {'field_a'})\n    assert v.validate_python(Cls(foo=[1, 2, 3])) == ({'field_a': 3}, {}, {'field_a'})\n    assert v.validate_python({'foo': [1, 2, 3]}) == ({'field_a': 3}, {}, {'field_a'})"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004970", "source": "def _call_initvar(*args: Any, **kwargs: Any) -> NoReturn:\n        raise TypeError(\"'InitVar' object is not callable\")", "target": "def test_dataclass_self_init():\n    @dataclasses.dataclass(init=False)\n    class Foo:\n        a: str\n        b: bool\n        def __init__(self, *args, **kwargs):\n            v.validate_python(ArgsKwargs(args, kwargs), self_instance=self)\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), kw_only=False),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), kw_only=False),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    foo = Foo(b'hello', 'True')\n    assert dataclasses.is_dataclass(foo)\n    assert dataclasses.asdict(foo) == {'a': 'hello', 'b': True}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004971", "source": "def _dataclass_setstate(self: Any, state: list[Any]) -> None:\n                    for field, value in zip(dataclasses.fields(self), state):\n                        object.__setattr__(self, field.name, value)", "target": "def test_leak_dataclass(validator):\n    def fn():\n        @dataclasses.dataclass\n        class Dataclass:\n            a: int\n            @classmethod\n            def _validator(cls, v, info):\n                return v\n            @classmethod\n            def _wrap_validator(cls, v, validator, info):\n                return validator(v)\n        field_schema = core_schema.int_schema()\n        if validator == 'field':\n            field_schema = core_schema.with_info_before_validator_function(Dataclass._validator, field_schema)\n            field_schema = core_schema.with_info_wrap_validator_function(Dataclass._wrap_validator, field_schema)\n            field_schema = core_schema.with_info_after_validator_function(Dataclass._validator, field_schema)\n        dataclass_schema = core_schema.dataclass_schema(\n            Dataclass,\n            core_schema.dataclass_args_schema('Dataclass', [core_schema.dataclass_field('a', field_schema)]),\n            ['a'],\n        )\n        if validator == 'dataclass':\n            dataclass_schema = core_schema.with_info_before_validator_function(Dataclass._validator, dataclass_schema)\n            dataclass_schema = core_schema.with_info_wrap_validator_function(\n                Dataclass._wrap_validator, dataclass_schema\n            )\n            dataclass_schema = core_schema.with_info_after_validator_function(Dataclass._validator, dataclass_schema)\n        Dataclass.__pydantic_validator__ = SchemaValidator(dataclass_schema)\n        return Dataclass\n    klass = fn()\n    ref = weakref.ref(klass)\n    assert ref() is not None\n    del klass\n    assert_gc(lambda: ref() is None)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004972", "source": "def dataclass(\n        *,\n        init: Literal[False] = False,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool = False,\n        config: ConfigDict | type[object] | None = None,\n        validate_on_init: bool | None = None,\n        kw_only: bool = ...,\n        slots: bool = ...,\n    ) -> Callable[[type[_T]], type[PydanticDataclass]]:\n        ...", "target": "def test_alias_allow_pop(py_and_json: PyAndJson):\n    schema = core_schema.dataclass_schema(\n        BasicDataclass,\n        core_schema.dataclass_args_schema(\n            'BasicDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), validation_alias='FieldA'),\n            ],\n        ),\n        ['a'],\n        config=core_schema.CoreConfig(validate_by_name=True, validate_by_alias=True),\n    )\n    v = py_and_json(schema)\n    assert v.validate_test({'FieldA': 'hello'}) == BasicDataclass(a='hello')\n    assert v.validate_test({'a': 'hello'}) == BasicDataclass(a='hello')\n    assert v.validate_test(\n        {\n            'FieldA': 'hello',\n            'a': 'world',\n        }\n    ) == BasicDataclass(a='hello')\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'foobar': 'hello'})"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004973", "source": "def complete_dataclass(\n    cls: type[Any],\n    config_wrapper: _config.ConfigWrapper,\n    *,\n    raise_errors: bool = True,\n    ns_resolver: NsResolver | None = None,\n    _force_build: bool = False,\n) -> bool:\n    original_init = cls.__init__\n    def __init__(__dataclass_self__: PydanticDataclass, *args: Any, **kwargs: Any) -> None:\n        __tracebackhide__ = True\n        s = __dataclass_self__\n        s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)\n    __init__.__qualname__ = f'{cls.__qualname__}.__init__'\n    cls.__init__ = __init__\n    cls.__pydantic_config__ = config_wrapper.config_dict\n    set_dataclass_fields(cls, config_wrapper=config_wrapper, ns_resolver=ns_resolver)\n    if not _force_build and config_wrapper.defer_build:\n        set_dataclass_mocks(cls)\n        return False\n    if hasattr(cls, '__post_init_post_parse__'):\n        warnings.warn(\n            'Support for `__post_init_post_parse__` has been dropped, the method will not be called',\n            PydanticDeprecatedSince20,\n        )\n    typevars_map = get_standard_typevars_map(cls)\n    gen_schema = GenerateSchema(\n        config_wrapper,\n        ns_resolver=ns_resolver,\n        typevars_map=typevars_map,\n    )\n    cls.__signature__ = LazyClassAttribute(\n        '__signature__',\n        partial(\n            generate_pydantic_signature,\n            init=original_init,\n            fields=cls.__pydantic_fields__,\n            validate_by_name=config_wrapper.validate_by_name,\n            extra=config_wrapper.extra,\n            is_dataclass=True,\n        ),\n    )\n    try:\n        schema = gen_schema.generate_schema(cls)\n    except PydanticUndefinedAnnotation as e:\n        if raise_errors:\n            raise\n        set_dataclass_mocks(cls, f'`{e.name}`')\n        return False\n    core_config = config_wrapper.core_config(title=cls.__name__)\n    try:\n        schema = gen_schema.clean_schema(schema)\n    except InvalidSchemaError:\n        set_dataclass_mocks(cls)\n        return False\n    cls = cast('type[PydanticDataclass]', cls)\n    cls.__pydantic_core_schema__ = schema\n    cls.__pydantic_validator__ = create_schema_validator(\n        schema, cls, cls.__module__, cls.__qualname__, 'dataclass', core_config, config_wrapper.plugin_settings\n    )\n    cls.__pydantic_serializer__ = SchemaSerializer(schema, core_config)\n    cls.__pydantic_complete__ = True\n    return True", "target": "def test_validate_assignment_function():\n    @dataclasses.dataclass\n    class MyDataclass:\n        field_a: str\n        field_b: int\n        field_c: int\n    calls = []\n    def func(x, info):\n        calls.append(str(info))\n        return x * 2\n    v = SchemaValidator(\n        core_schema.dataclass_schema(\n            MyDataclass,\n            core_schema.dataclass_args_schema(\n                'MyDataclass',\n                [\n                    core_schema.dataclass_field('field_a', core_schema.str_schema()),\n                    core_schema.dataclass_field(\n                        'field_b',\n                        core_schema.with_info_after_validator_function(func, core_schema.int_schema()),\n                    ),\n                    core_schema.dataclass_field('field_c', core_schema.int_schema()),\n                ],\n            ),\n            ['field_a', 'field_b', 'field_c'],\n        )\n    )\n    m = v.validate_python({'field_a': 'x', 'field_b': 123, 'field_c': 456})\n    assert m.field_a == 'x'\n    assert m.field_b == 246\n    assert m.field_c == 456\n    assert calls == [\"ValidationInfo(config=None, context=None, data={'field_a': 'x'}, field_name='field_b')\"]\n    v.validate_assignment(m, 'field_b', '111')\n    assert m.field_b == 222\n    assert calls == [\n        \"ValidationInfo(config=None, context=None, data={'field_a': 'x'}, field_name='field_b')\",\n        \"ValidationInfo(config=None, context=None, data={'field_a': 'x', 'field_c': 456}, field_name='field_b')\",\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004974", "source": "def _dataclass_validate_values(self: 'Dataclass') -> None:\n    if getattr(self, '__pydantic_initialised__'):\n        return\n    if getattr(self, '__pydantic_has_field_info_default__', False):\n        input_data = {\n            k: v\n            for k, v in self.__dict__.items()\n            if not (isinstance(v, FieldInfo) or _is_field_cached_property(self, k))\n        }\n    else:\n        input_data = {k: v for k, v in self.__dict__.items() if not _is_field_cached_property(self, k)}\n    d, _, validation_error = validate_model(self.__pydantic_model__, input_data, cls=self.__class__)\n    if validation_error:\n        raise validation_error\n    self.__dict__.update(d)\n    object.__setattr__(self, '__pydantic_initialised__', True)", "target": "def test_dataclass_subclass(revalidate_instances, input_value, expected):\n    schema = core_schema.dataclass_schema(\n        FooDataclass,\n        core_schema.dataclass_args_schema(\n            'FooDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n            ],\n            extra_behavior='forbid',\n        ),\n        ['a', 'b'],\n        revalidate_instances=revalidate_instances,\n    )\n    v = SchemaValidator(schema)\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=expected.message) as exc_info:\n            print(v.validate_python(input_value))\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        dc = v.validate_python(input_value)\n        assert dataclasses.is_dataclass(dc)\n        assert dataclasses.asdict(dc) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004975", "source": "def run_mixtral_8x7b_autoquant(device: str = \"cuda\"):\n    model = GPTModelConfig(\n        \"Mixtral-8x7B-v0.1\",\n        MixtralMoE,\n        \"autoquant\",\n        None,\n        175,\n        1130,\n        133,\n    )\n    token_per_sec, memory_bandwidth, compilation_time = run_experiment(\n        model, device=device\n    )\n    return [\n        Experiment(\n            model.name,\n            \"token_per_sec\",\n            model.token_per_sec,\n            f\"{token_per_sec:.02f}\",\n            model.mode,\n            device,\n            get_arch_name(),\n            True,\n        ),\n        Experiment(\n            model.name,\n            \"memory_bandwidth(GB/s)\",\n            model.memory_bandwidth,\n            f\"{memory_bandwidth:.02f}\",\n            model.mode,\n            device,\n            get_arch_name(),\n            True,\n        ),\n        Experiment(\n            model.name,\n            \"compilation_time(s)\",\n            model.compilation_time,\n            f\"{compilation_time:.02f}\",\n            model.mode,\n            device,\n            get_arch_name(),\n            True,\n        ),\n    ]", "target": "def testNonContiguous(self):\n        x = torch.tensor([100, 200, 300])[::2]\n        assert not x.is_contiguous()\n        assert x[0] == 100\n        assert x[1] == 300\n        return x"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "004976", "source": "def remote_url(self) -> str:\n        return self._run_git(\"remote\", \"get-url\", self.remote)", "target": "def test_compute_diff(self) -> None:\n        diff = self.repo.diff(\"HEAD\")\n        sha = _shasum(diff)\n        self.assertEqual(len(sha), 64)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004977", "source": "def _dataclass_validate_assignment_setattr(self: 'Dataclass', name: str, value: Any) -> None:\n    if self.__pydantic_initialised__:\n        d = dict(self.__dict__)\n        d.pop(name, None)\n        known_field = self.__pydantic_model__.__fields__.get(name, None)\n        if known_field:\n            value, error_ = known_field.validate(value, d, loc=name, cls=self.__class__)\n            if error_:\n                raise ValidationError([error_], self.__class__)\n    object.__setattr__(self, name, value)", "target": "def test_dataclass_exact_validation(revalidate_instances, input_value, expected):\n    schema = core_schema.dataclass_schema(\n        FooDataclass,\n        core_schema.dataclass_args_schema(\n            'FooDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n            ],\n        ),\n        ['a', 'b'],\n        revalidate_instances=revalidate_instances,\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python(input_value)\n    assert dataclasses.asdict(foo) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004978", "source": "def update_field_from_config(config_wrapper: ConfigWrapper, field_name: str, field_info: FieldInfo) -> None:\n    field_title_generator = field_info.field_title_generator or config_wrapper.field_title_generator\n    if field_title_generator is not None:\n        _apply_field_title_generator_to_field_info(field_title_generator, field_name, field_info)\n    if config_wrapper.alias_generator is not None:\n        _apply_alias_generator_to_field_info(config_wrapper.alias_generator, field_name, field_info)", "target": "def test_empty_string_field_name(py_and_json: PyAndJson):\n    v = py_and_json({'type': 'model-fields', 'fields': {'': {'type': 'model-field', 'schema': {'type': 'int'}}}})\n    assert v.validate_test({'': 123}) == ({'': 123}, None, {''})"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004979", "source": "def pydantic_encoder(obj: Any) -> Any:\n    from dataclasses import asdict, is_dataclass\n    from pydantic.v1.main import BaseModel\n    if isinstance(obj, BaseModel):\n        return obj.dict()\n    elif is_dataclass(obj):\n        return asdict(obj)\n    for base in obj.__class__.__mro__[:-1]:\n        try:\n            encoder = ENCODERS_BY_TYPE[base]\n        except KeyError:\n            continue\n        return encoder(obj)\n    else:\n        raise TypeError(f\"Object of type '{obj.__class__.__name__}' is not JSON serializable\")", "target": "def test_to_jsonable_python_schema_serializer():\n    class Foobar:\n        def __init__(self, my_foo: int, my_inners: list['Foobar']):\n            self.my_foo = my_foo\n            self.my_inners = my_inners\n    c = core_schema.definitions_schema(\n        core_schema.definition_reference_schema(schema_ref='foobar'),\n        [\n            core_schema.model_schema(\n                Foobar,\n                core_schema.typed_dict_schema(\n                    {\n                        'my_foo': core_schema.typed_dict_field(core_schema.int_schema(), serialization_alias='myFoo'),\n                        'my_inners': core_schema.typed_dict_field(\n                            core_schema.list_schema(core_schema.definition_reference_schema('foobar')),\n                            serialization_alias='myInners',\n                        ),\n                    }\n                ),\n                ref='foobar',\n            )\n        ],\n    )\n    v = SchemaValidator(c)\n    s = SchemaSerializer(c)\n    Foobar.__pydantic_validator__ = v\n    Foobar.__pydantic_serializer__ = s\n    instance = Foobar(my_foo=1, my_inners=[Foobar(my_foo=2, my_inners=[])])\n    assert to_jsonable_python(instance, by_alias=True) == {'myFoo': 1, 'myInners': [{'myFoo': 2, 'myInners': []}]}\n    assert to_jsonable_python(instance, by_alias=False) == {'my_foo': 1, 'my_inners': [{'my_foo': 2, 'my_inners': []}]}\n    assert to_json(instance, by_alias=True) == b'{\"myFoo\":1,\"myInners\":[{\"myFoo\":2,\"myInners\":[]}]}'\n    assert to_json(instance, by_alias=False) == b'{\"my_foo\":1,\"my_inners\":[{\"my_foo\":2,\"my_inners\":[]}]}'"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004980", "source": "def as_dataclass_field(pydantic_field: FieldInfo) -> dataclasses.Field[Any]:\n    field_args: dict[str, Any] = {'default': pydantic_field}\n    if sys.version_info >= (3, 14) and pydantic_field.description is not None:\n        field_args['doc'] = pydantic_field.description\n    if sys.version_info >= (3, 10) and pydantic_field.kw_only:\n        field_args['kw_only'] = True\n    if pydantic_field.repr is not True:\n        field_args['repr'] = pydantic_field.repr\n    return dataclasses.field(**field_args)", "target": "def test_dataclass_self_init_post_init():\n    calls = []\n    @dataclasses.dataclass(init=False)\n    class Foo:\n        a: str\n        b: bool\n        c: dataclasses.InitVar[int]\n        def __init__(self, *args, **kwargs):\n            v.validate_python(ArgsKwargs(args, kwargs), self_instance=self)\n        def __post_init__(self, c):\n            calls.append(c)\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), kw_only=False),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), kw_only=False),\n                core_schema.dataclass_field(name='c', schema=core_schema.int_schema(), init_only=True),\n            ],\n            collect_init_only=True,\n        ),\n        ['a', 'b', 'c'],\n        post_init=True,\n    )\n    v = SchemaValidator(schema)\n    foo = Foo(b'hello', 'True', c='123')\n    assert dataclasses.is_dataclass(foo)\n    assert dataclasses.asdict(foo) == {'a': 'hello', 'b': True}\n    assert calls == [123]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004981", "source": "def is_none_type(type_: Any) -> bool:\n        return type_ in NONE_TYPES", "target": "def test_ser_function_wrap():\n    def f(\n        input: Any, serialize: core_schema.SerializerFunctionWrapHandler, info: core_schema.SerializationInfo, /\n    ) -> str:\n        return f'{serialize} {info}'\n    s = SchemaSerializer(\n        core_schema.any_schema(\n            serialization=core_schema.wrap_serializer_function_ser_schema(\n                f, info_arg=True, schema=core_schema.str_schema(), when_used='json'\n            )\n        )\n    )\n    assert s.to_python(123, mode='json') == (\n        'SerializationCallable(serializer=str) '\n        \"SerializationInfo(include=None, exclude=None, context=None, mode='json', by_alias=False, exclude_unset=False, \"\n        'exclude_defaults=False, exclude_none=False, exclude_computed_fields=False, round_trip=False, serialize_as_any=False)'\n    )"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004982", "source": "def dataclass(\n        _cls: type[_T],\n        *,\n        init: Literal[False] = False,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool | None = None,\n        config: ConfigDict | type[object] | None = None,\n        validate_on_init: bool | None = None,\n        kw_only: bool = ...,\n        slots: bool = ...,\n    ) -> type[PydanticDataclass]: ...", "target": "def test_frozen():\n    @dataclasses.dataclass\n    class MyModel:\n        f: str\n    v = SchemaValidator(\n        core_schema.dataclass_schema(\n            MyModel,\n            core_schema.dataclass_args_schema('MyModel', [core_schema.dataclass_field('f', core_schema.str_schema())]),\n            ['f'],\n            frozen=True,\n        )\n    )\n    m = v.validate_python({'f': 'x'})\n    assert m.f == 'x'\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_assignment(m, 'f', 'y')\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'frozen_instance', 'loc': (), 'msg': 'Instance is frozen', 'input': 'y'}\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004983", "source": "def with_config(**config: Unpack[ConfigDict]) -> Callable[[_TypeT], _TypeT]: ...", "target": "def test_hide_input_in_errors(config, input_str):\n    v = SchemaValidator(\n        cs.model_schema(\n            cls=MyModel, schema=cs.model_fields_schema(fields={'f': cs.model_field(schema=cs.str_schema())})\n        ),\n        config=config,\n    )\n    with pytest.raises(ValidationError, match=re.escape(f'Input should be a valid string [{input_str}]')):\n        assert v.validate_python({'f': 123})"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "004984", "source": "def _apply_to_root(self, schema: core_schema.CoreSchema) -> core_schema.CoreSchema:\n        if schema['type'] == 'nullable':\n            self._is_nullable = True\n            wrapped = self._apply_to_root(schema['schema'])\n            nullable_wrapper = schema.copy()\n            nullable_wrapper['schema'] = wrapped\n            return nullable_wrapper\n        if schema['type'] == 'definitions':\n            wrapped = self._apply_to_root(schema['schema'])\n            definitions_wrapper = schema.copy()\n            definitions_wrapper['schema'] = wrapped\n            return definitions_wrapper\n        if schema['type'] != 'union':\n            schema = core_schema.union_schema([schema])\n        choices_schemas = [v[0] if isinstance(v, tuple) else v for v in schema['choices'][::-1]]\n        self._choices_to_handle.extend(choices_schemas)\n        while self._choices_to_handle:\n            choice = self._choices_to_handle.pop()\n            self._handle_choice(choice)\n        if self._discriminator_alias is not None and self._discriminator_alias != self.discriminator:\n            discriminator: str | list[list[str | int]] = [[self.discriminator], [self._discriminator_alias]]\n        else:\n            discriminator = self.discriminator\n        return core_schema.tagged_union_schema(\n            choices=self._tagged_union_choices,\n            discriminator=discriminator,\n            custom_error_type=schema.get('custom_error_type'),\n            custom_error_message=schema.get('custom_error_message'),\n            custom_error_context=schema.get('custom_error_context'),\n            strict=False,\n            from_attributes=True,\n            ref=schema.get('ref'),\n            metadata=schema.get('metadata'),\n            serialization=schema.get('serialization'),\n        )", "target": "def test_schema_build(benchmark) -> None:\n    @benchmark\n    def run():\n        adapter = TypeAdapter(AnyState)\n        assert adapter.core_schema['schema']['type'] == 'tagged-union'"}
