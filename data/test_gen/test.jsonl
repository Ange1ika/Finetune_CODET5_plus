{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "004985", "source": "def _general_metadata_cls() -> type[BaseMetadata]:\n    from annotated_types import BaseMetadata\n    class _PydanticGeneralMetadata(PydanticMetadata, BaseMetadata):\n        def __init__(self, metadata: Any):\n            self.__dict__ = metadata\n    return _PydanticGeneralMetadata", "target": "def test_on_error_raise_explicit(self, py_and_json: PyAndJson):\n        v = py_and_json(\n            {\n                'type': 'model-fields',\n                'fields': {\n                    'x': {\n                        'type': 'model-field',\n                        'schema': {'type': 'default', 'schema': {'type': 'str'}, 'on_error': 'raise'},\n                    }\n                },\n            }\n        )\n        assert v.validate_test({'x': 'foo'}) == ({'x': 'foo'}, None, {'x'})\n        with pytest.raises(ValidationError) as exc_info:\n            v.validate_test({'x': ['foo']})\n        assert exc_info.value.errors(include_url=False) == [\n            {'input': ['foo'], 'type': 'string_type', 'loc': ('x',), 'msg': 'Input should be a valid string'}\n        ]"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "004986", "source": "def to_markdown_table(\n    res: TimingResultType, header: Optional[tuple[str, ...]] = None\n) -> str:\n    if header is None:\n        header = (\"model\", \"task\", \"mean\", \"var\")\n    out = \"\"\n    def write_line(*args):\n        nonlocal out\n        out += f\"| {' | '.join(str(a) for a in args)} |\\n\"\n    write_line(*header)\n    write_line(*[\"--\"] * len(header))\n    for model, tasks in res.items():\n        for task, line in tasks.items():\n            write_line(*(model, task) + line)\n    return out", "target": "def test_multiple_vars_and_missing_cleanup(self):\n        v1, v2 = \"TEST_ENV_V1\", \"TEST_ENV_V2\"\n        os.environ.pop(v1, None)\n        os.environ[v2] = \"keep\"\n        with temp_environ({v1: \"a\", v2: \"b\"}):\n            self.assertEqual(os.environ[v1], \"a\")\n            self.assertEqual(os.environ[v2], \"b\")\n        self.assertNotIn(v1, os.environ)\n        self.assertEqual(os.environ[v2], \"keep\")"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "004987", "source": "def is_union(tp: Optional[Type[Any]]) -> bool:\n        return tp is Union or tp is types.UnionType", "target": "def test_type_error():\n    try:\n        PydanticKnownError('foobar')\n    except KeyError as exc:\n        assert str(exc) == '\"Invalid error type: \\'foobar\\'\"'\n    else:\n        raise AssertionError(\"PydanticKnownError('foobar') did not raise KeyError\")\n    e = PydanticKnownError('recursion_loop')\n    assert isinstance(e, PydanticKnownError)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "004988", "source": "def _apply_alias_generator_to_field_info(\n    alias_generator: Callable[[str], str] | AliasGenerator, field_name: str, field_info: FieldInfo\n):\n    if (\n        field_info.alias_priority is None\n        or field_info.alias_priority <= 1\n        or field_info.alias is None\n        or field_info.validation_alias is None\n        or field_info.serialization_alias is None\n    ):\n        alias, validation_alias, serialization_alias = None, None, None\n        if isinstance(alias_generator, AliasGenerator):\n            alias, validation_alias, serialization_alias = alias_generator.generate_aliases(field_name)\n        elif callable(alias_generator):\n            alias = alias_generator(field_name)\n            if not isinstance(alias, str):\n                raise TypeError(f'alias_generator {alias_generator} must return str, not {alias.__class__}')\n        if field_info.alias_priority is None or field_info.alias_priority <= 1:\n            field_info.alias_priority = 1\n        if field_info.alias_priority == 1:\n            field_info.serialization_alias = get_first_not_none(serialization_alias, alias)\n            field_info.validation_alias = get_first_not_none(validation_alias, alias)\n            field_info.alias = alias\n        if field_info.alias is None:\n            field_info.alias = alias\n        if field_info.serialization_alias is None:\n            field_info.serialization_alias = get_first_not_none(serialization_alias, alias)\n        if field_info.validation_alias is None:\n            field_info.validation_alias = get_first_not_none(validation_alias, alias)", "target": "def test_empty_string_aliases(py_and_json: PyAndJson):\n    v = py_and_json(\n        {\n            'type': 'model-fields',\n            'fields': {'field_a': {'validation_alias': '', 'type': 'model-field', 'schema': {'type': 'int'}}},\n        }\n    )\n    assert v.validate_test({'': 123}) == ({'field_a': 123}, None, {'field_a'})\n    v = py_and_json(\n        {\n            'type': 'model-fields',\n            'fields': {'field_a': {'validation_alias': ['', ''], 'type': 'model-field', 'schema': {'type': 'int'}}},\n        }\n    )\n    assert v.validate_test({'': {'': 123}}) == ({'field_a': 123}, None, {'field_a'})"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "004989", "source": "def is_pydantic_dataclass(class_: type[Any], /) -> TypeGuard[type[PydanticDataclass]]:\n    try:\n        return '__is_pydantic_dataclass__' in class_.__dict__ and dataclasses.is_dataclass(class_)\n    except AttributeError:\n        return False", "target": "def test_by_alias_and_name_config_interaction(\n    config_by_alias: Union[bool, None],\n    config_by_name: Union[bool, None],\n    runtime_by_alias: Union[bool, None],\n    runtime_by_name: Union[bool, None],\n) -> None:\n    if config_by_alias is False and config_by_name is False and runtime_by_alias is False and runtime_by_name is False:\n        pytest.skip(\"Can't have both by_alias and by_name as effectively False\")\n    core_config = {\n        **({'validate_by_alias': config_by_alias} if config_by_alias is not None else {}),\n        **({'validate_by_name': config_by_name} if config_by_name is not None else {}),\n    }\n    @dataclasses.dataclass\n    class MyDataclass:\n        my_field: int\n    schema = core_schema.dataclass_schema(\n        MyDataclass,\n        core_schema.dataclass_args_schema(\n            'MyDataclass',\n            [\n                core_schema.dataclass_field(\n                    name='my_field', schema=core_schema.int_schema(), validation_alias='my_alias'\n                ),\n            ],\n        ),\n        ['my_field'],\n        config=core_schema.CoreConfig(**core_config),\n    )\n    s = SchemaValidator(schema)\n    alias_allowed = next(x for x in (runtime_by_alias, config_by_alias, True) if x is not None)\n    name_allowed = next(x for x in (runtime_by_name, config_by_name, False) if x is not None)\n    if alias_allowed:\n        assert dataclasses.asdict(\n            s.validate_python({'my_alias': 1}, by_alias=runtime_by_alias, by_name=runtime_by_name)\n        ) == {'my_field': 1}\n    if name_allowed:\n        assert dataclasses.asdict(\n            s.validate_python({'my_field': 1}, by_alias=runtime_by_alias, by_name=runtime_by_name)\n        ) == {'my_field': 1}"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "004990", "source": "def write_line(*args):\n        nonlocal out\n        out += f\"| {' | '.join(str(a) for a in args)} |\\n\"", "target": "def test_gh_get_labels_raises_with_no_pages(\n        self,\n        mock_request_for_labels: Any,\n        get_last_page_num_from_header: Any,\n    ) -> None:\n        with self.assertRaises(AssertionError) as err:\n            gh_get_labels(\"foo\", \"bar\")\n        self.assertIn(\"number of pages of labels\", str(err.exception))"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "004991", "source": "def _check_protected_namespaces(\n    protected_namespaces: tuple[str | Pattern[str], ...],\n    ann_name: str,\n    bases: tuple[type[Any], ...],\n    cls_name: str,\n) -> None:\n    BaseModel = import_cached_base_model()\n    for protected_namespace in protected_namespaces:\n        ns_violation = False\n        if isinstance(protected_namespace, Pattern):\n            ns_violation = protected_namespace.match(ann_name) is not None\n        elif isinstance(protected_namespace, str):\n            ns_violation = ann_name.startswith(protected_namespace)\n        if ns_violation:\n            for b in bases:\n                if hasattr(b, ann_name):\n                    if not (issubclass(b, BaseModel) and ann_name in getattr(b, '__pydantic_fields__', {})):\n                        raise ValueError(\n                            f'Field {ann_name!r} conflicts with member {getattr(b, ann_name)}'\n                            f' of protected namespace {protected_namespace!r}.'\n                        )\n            else:\n                valid_namespaces: list[str] = []\n                for pn in protected_namespaces:\n                    if isinstance(pn, Pattern):\n                        if not pn.match(ann_name):\n                            valid_namespaces.append(f're.compile({pn.pattern!r})')\n                    else:\n                        if not ann_name.startswith(pn):\n                            valid_namespaces.append(f\"'{pn}'\")\n                valid_namespaces_str = f'({\", \".join(valid_namespaces)}{\",)\" if len(valid_namespaces) == 1 else \")\"}'\n                warnings.warn(\n                    f'Field {ann_name!r} in {cls_name!r} conflicts with protected namespace {protected_namespace!r}.\\n\\n'\n                    f\"You may be able to solve this by setting the 'protected_namespaces' configuration to {valid_namespaces_str}.\",\n                    UserWarning,\n                    stacklevel=5,\n                )", "target": "def test_aliases_path_negative(input_value, expected):\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={'field_a': core_schema.model_field(validation_alias=['foo', -2], schema=core_schema.int_schema())}\n        ),\n        config=CoreConfig(loc_by_alias=False),\n    )\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=expected.message):\n            val = v.validate_python(input_value)\n            print(f'UNEXPECTED OUTPUT: {val!r}')\n    else:\n        output = v.validate_python(input_value)\n        assert output == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "004992", "source": "def new_post_init(self: 'Dataclass', *args: Any, **kwargs: Any) -> None:\n            if config.post_init_call == 'before_validation':\n                post_init(self, *args, **kwargs)\n            if self.__class__.__pydantic_run_validation__:\n                self.__pydantic_validate_values__()\n                if hasattr(self, '__post_init_post_parse__'):\n                    self.__post_init_post_parse__(*args, **kwargs)\n            if config.post_init_call == 'after_validation':\n                post_init(self, *args, **kwargs)", "target": "def test_only_allow_alias(py_and_json) -> None:\n    schema = core_schema.dataclass_schema(\n        BasicDataclass,\n        core_schema.dataclass_args_schema(\n            'BasicDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), validation_alias='FieldA'),\n            ],\n        ),\n        ['a'],\n        config=core_schema.CoreConfig(validate_by_name=False, validate_by_alias=True),\n    )\n    v = py_and_json(schema)\n    assert v.validate_test({'FieldA': 'hello'}) == BasicDataclass(a='hello')\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'a': 'hello'})"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "004993", "source": "def dataclass(\n        _cls: Type[_T],\n        *,\n        init: bool = True,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool = False,\n        config: Union[ConfigDict, Type[object], None] = None,\n        validate_on_init: Optional[bool] = None,\n        use_proxy: Optional[bool] = None,\n    ) -> 'DataclassClassOrWrapper':\n        ...", "target": "def test_dataclass_field_before_validator():\n    @dataclasses.dataclass\n    class Foo:\n        a: int\n        b: str\n        @classmethod\n        def validate_b(cls, v: bytes, info: core_schema.ValidationInfo) -> bytes:\n            assert v == b'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return b'hello world!'\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(\n                    name='b',\n                    schema=core_schema.with_info_before_validator_function(Foo.validate_b, core_schema.str_schema()),\n                ),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 1, 'b': b'hello'})\n    assert dataclasses.asdict(foo) == {'a': 1, 'b': 'hello world!'}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "004994", "source": "def new_init(self: 'Dataclass', *args: Any, **kwargs: Any) -> None:\n            handle_extra_init(self, *args, **kwargs)\n            if self.__class__.__pydantic_run_validation__:\n                self.__pydantic_validate_values__()\n            if hasattr(self, '__post_init_post_parse__'):\n                initvars_and_values: Dict[str, Any] = {}\n                for i, f in enumerate(self.__class__.__dataclass_fields__.values()):\n                    if f._field_type is dataclasses._FIELD_INITVAR:\n                        try:\n                            initvars_and_values[f.name] = args[i]\n                        except IndexError:\n                            initvars_and_values[f.name] = kwargs.get(f.name, f.default)\n                self.__post_init_post_parse__(**initvars_and_values)", "target": "def test_dataclass_args_init_only(py_and_json: PyAndJson, input_value, expected):\n    schema = core_schema.dataclass_args_schema(\n        'MyDataclass',\n        [\n            core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), kw_only=False),\n            core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), kw_only=False, init_only=True),\n        ],\n        collect_init_only=True,\n    )\n    v = py_and_json(schema)\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)) as exc_info:\n            v.validate_test(input_value)\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        assert v.validate_test(input_value) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "004995", "source": "def takes_validated_data_argument(\n    default_factory: Callable[[], Any] | Callable[[dict[str, Any]], Any],\n) -> TypeIs[Callable[[dict[str, Any]], Any]]:\n    try:\n        sig = signature(default_factory)\n    except (ValueError, TypeError):\n        return False\n    parameters = list(sig.parameters.values())\n    return len(parameters) == 1 and can_be_positional(parameters[0]) and parameters[0].default is Parameter.empty", "target": "def test_extra_behavior_allow_with_validate_fn_override(\n    config: Union[core_schema.CoreConfig, None],\n    schema_extra_behavior_kw: dict[str, Any],\n):\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            {'f': core_schema.model_field(core_schema.str_schema())}, **schema_extra_behavior_kw\n        ),\n        config=config,\n    )\n    m, model_extra, fields_set = v.validate_python({'f': 'x', 'extra_field': '123'}, extra='allow')\n    assert m == {'f': 'x'}\n    assert model_extra == {'extra_field': '123'}\n    assert fields_set == {'f', 'extra_field'}\n    v.validate_assignment(m, 'f', 'y', extra='allow')\n    assert m == {'f': 'y'}\n    new_m, new_model_extra, new_fields_set = v.validate_assignment({**m, **model_extra}, 'not_f', '123', extra='allow')\n    assert new_m == {'f': 'y'}\n    assert new_model_extra == {'extra_field': '123', 'not_f': '123'}\n    assert new_fields_set == {'not_f'}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "004996", "source": "def _update_fields_from_docstrings(cls: type[Any], fields: dict[str, FieldInfo], use_inspect: bool = False) -> None:\n    fields_docs = extract_docstrings_from_cls(cls, use_inspect=use_inspect)\n    for ann_name, field_info in fields.items():\n        if field_info.description is None and ann_name in fields_docs:\n            field_info.description = fields_docs[ann_name]", "target": "def test_empty_string_field_name(py_and_json: PyAndJson):\n    v = py_and_json({'type': 'model-fields', 'fields': {'': {'type': 'model-field', 'schema': {'type': 'int'}}}})\n    assert v.validate_test({'': 123}) == ({'': 123}, None, {''})"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "004997", "source": "def as_dataclass_field(pydantic_field: FieldInfo) -> dataclasses.Field[Any]:\n    field_args: dict[str, Any] = {'default': pydantic_field}\n    if sys.version_info >= (3, 14) and pydantic_field.description is not None:\n        field_args['doc'] = pydantic_field.description\n    if sys.version_info >= (3, 10) and pydantic_field.kw_only:\n        field_args['kw_only'] = True\n    if pydantic_field.repr is not True:\n        field_args['repr'] = pydantic_field.repr\n    return dataclasses.field(**field_args)", "target": "def test_dataclass_args_init_only_no_fields(py_and_json: PyAndJson, input_value, expected):\n    schema = core_schema.dataclass_args_schema(\n        'MyDataclass', [core_schema.dataclass_field(name='a', schema=core_schema.str_schema())], collect_init_only=True\n    )\n    v = py_and_json(schema)\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=expected.message) as exc_info:\n            v.validate_test(input_value)\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        assert v.validate_test(input_value) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "004998", "source": "def is_union(tp: Optional[Type[Any]]) -> bool:\n        return tp is Union", "target": "def test_schema_typing_error() -> None:\n    _: CoreSchema = {'type': 'wrong'}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "004999", "source": "def dataclass(\n        *,\n        init: bool = True,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool = False,\n        config: Union[ConfigDict, Type[object], None] = None,\n        validate_on_init: Optional[bool] = None,\n        use_proxy: Optional[bool] = None,\n    ) -> Callable[[Type[_T]], 'DataclassClassOrWrapper']:\n        ...", "target": "def test_dataclass_field_plain_validator():\n    @dataclasses.dataclass\n    class Foo:\n        a: int\n        b: str\n        @classmethod\n        def validate_b(cls, v: bytes, info: core_schema.ValidationInfo) -> str:\n            assert v == b'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return 'hello world!'\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(\n                    name='b', schema=core_schema.with_info_plain_validator_function(Foo.validate_b)\n                ),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 1, 'b': b'hello'})\n    assert dataclasses.asdict(foo) == {'a': 1, 'b': 'hello world!'}"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005000", "source": "def ms_to_us(time_ms):\n    return time_ms * 1e3", "target": "def test_gh_get_labels(\n        self,\n        mock_request_for_labels: Any,\n        mock_get_last_page_num_from_header: Any,\n    ) -> None:\n        res = gh_get_labels(\"mock_org\", \"mock_repo\")\n        mock_get_last_page_num_from_header.assert_called_once()\n        self.assertEqual(res, [\"foo\"] * 3)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005001", "source": "def decimal_encoder(dec_value: Decimal) -> Union[int, float]:\n    exponent = dec_value.as_tuple().exponent\n    if isinstance(exponent, int) and exponent >= 0:\n        return int(dec_value)\n    else:\n        return float(dec_value)", "target": "def test_bad_repr():\n    b = BadRepr()\n    error_msg = '^Unable to serialize unknown type: <unprintable BedReprMeta object>$'\n    with pytest.raises(PydanticSerializationError, match=error_msg):\n        to_jsonable_python(b)\n    assert to_jsonable_python(b, serialize_unknown=True) == '<Unserializable BadRepr object>'\n    with pytest.raises(PydanticSerializationError, match=error_msg):\n        to_json(b)\n    assert to_json(b, serialize_unknown=True) == b'\"<Unserializable BadRepr object>\"'"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005002", "source": "def isoformat(o: Union[datetime.date, datetime.time]) -> str:\n    return o.isoformat()", "target": "def test_to_jsonable_python_fallback():\n    with pytest.raises(PydanticSerializationError, match=r'Unable to serialize unknown type: <.+\\.Foobar'):\n        to_jsonable_python(Foobar())\n    assert to_jsonable_python(Foobar(), serialize_unknown=True) == 'Foobar.__str__'\n    assert to_jsonable_python(Foobar(), serialize_unknown=True, fallback=fallback_func) == 'fallback:Foobar'\n    assert to_jsonable_python(Foobar(), fallback=fallback_func) == 'fallback:Foobar'"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005003", "source": "def dataclass(\n        _cls: Type[_T],\n        *,\n        init: bool = True,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool = False,\n        config: Union[ConfigDict, Type[object], None] = None,\n        validate_on_init: Optional[bool] = None,\n        use_proxy: Optional[bool] = None,\n        kw_only: bool = ...,\n    ) -> 'DataclassClassOrWrapper':\n        ...", "target": "def test_dataclass():\n    schema = core_schema.dataclass_schema(\n        FooDataclass,\n        core_schema.dataclass_args_schema(\n            'FooDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 'hello', 'b': True})\n    assert dataclasses.is_dataclass(foo)\n    assert foo.a == 'hello'\n    assert foo.b is True\n    assert dataclasses.asdict(v.validate_python(FooDataclass(a='hello', b=True))) == {'a': 'hello', 'b': True}\n    with pytest.raises(ValidationError, match='Input should be an instance of FooDataclass') as exc_info:\n        v.validate_python({'a': 'hello', 'b': True}, strict=True)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'dataclass_exact_type',\n            'loc': (),\n            'msg': 'Input should be an instance of FooDataclass',\n            'input': {'a': 'hello', 'b': True},\n            'ctx': {'class_name': 'FooDataclass'},\n        }\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005004", "source": "def update_field_from_config(config_wrapper: ConfigWrapper, field_name: str, field_info: FieldInfo) -> None:\n    field_title_generator = field_info.field_title_generator or config_wrapper.field_title_generator\n    if field_title_generator is not None:\n        _apply_field_title_generator_to_field_info(field_title_generator, field_name, field_info)\n    if config_wrapper.alias_generator is not None:\n        _apply_alias_generator_to_field_info(config_wrapper.alias_generator, field_name, field_info)", "target": "def test_allow_extra_wrong():\n    with pytest.raises(SchemaError, match='Invalid extra_behavior: `wrong`'):\n        SchemaValidator(\n            schema=core_schema.model_fields_schema(fields={}), config=CoreConfig(extra_fields_behavior='wrong')\n        )"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005005", "source": "def timedelta_isoformat(td: datetime.timedelta) -> str:\n    minutes, seconds = divmod(td.seconds, 60)\n    hours, minutes = divmod(minutes, 60)\n    return f'{\"-\" if td.days < 0 else \"\"}P{abs(td.days)}DT{hours:d}H{minutes:d}M{seconds:d}.{td.microseconds:06d}S'", "target": "def test_enum() -> None:\n    class MyEnum(Enum):\n        a = 'a'\n        b = 'b'\n    enum_schema = core_schema.lax_or_strict_schema(\n        core_schema.no_info_after_validator_function(MyEnum, core_schema.str_schema()),\n        core_schema.is_instance_schema(MyEnum),\n    )\n    v = core_schema.json_schema(enum_schema)\n    v = SchemaValidator(v)\n    assert v.validate_python('\"a\"') == MyEnum.a\n    assert v.validate_python('\"b\"') == MyEnum.b\n    with pytest.raises(ValidationError):\n        v.validate_python('\"c\"')"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005006", "source": "def dataclass(\n        *,\n        init: Literal[False] = False,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool = False,\n        config: ConfigDict | type[object] | None = None,\n        validate_on_init: bool | None = None,\n        kw_only: bool = ...,\n        slots: bool = ...,\n    ) -> Callable[[type[_T]], type[PydanticDataclass]]:\n        ...", "target": "def test_function_validator_wrapping_args_schema_after() -> None:\n    calls: list[Any] = []\n    def func(*args: Any) -> Any:\n        calls.append(args)\n        return args[0]\n    @dataclasses.dataclass\n    class Model:\n        number: int = 1\n    cs = core_schema.dataclass_schema(\n        Model,\n        core_schema.no_info_after_validator_function(\n            func,\n            core_schema.dataclass_args_schema(\n                'Model', [core_schema.dataclass_field('number', core_schema.int_schema())]\n            ),\n        ),\n        ['number'],\n    )\n    v = SchemaValidator(cs)\n    instance: Model = v.validate_python({'number': 1})\n    assert instance.number == 1\n    assert calls == [(({'number': 1}, None),)]\n    v.validate_assignment(instance, 'number', 2)\n    assert instance.number == 2\n    assert calls == [(({'number': 1}, None),), (({'number': 2}, None),)]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005007", "source": "def as_dataclass_field(pydantic_field: FieldInfo) -> dataclasses.Field[Any]:\n    field_args: dict[str, Any] = {'default': pydantic_field}\n    if sys.version_info >= (3, 14) and pydantic_field.description is not None:\n        field_args['doc'] = pydantic_field.description\n    if sys.version_info >= (3, 10) and pydantic_field.kw_only:\n        field_args['kw_only'] = True\n    if pydantic_field.repr is not True:\n        field_args['repr'] = pydantic_field.repr\n    return dataclasses.field(**field_args)", "target": "def test_dataclass_json_duplicate_keys():\n    @dataclasses.dataclass\n    class MyDataclass:\n        name: str\n        age: int\n    schema = core_schema.dataclass_schema(\n        MyDataclass,\n        core_schema.dataclass_args_schema(\n            'MyDataclass',\n            [\n                core_schema.dataclass_field(name='name', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='age', schema=core_schema.int_schema()),\n            ],\n        ),\n        ['name', 'age'],\n    )\n    v = SchemaValidator(schema)\n    json_with_duplicates = '{\"name\": \"Alice\", \"age\": 30, \"name\": \"Bob\", \"age\": 25}'\n    result = v.validate_json(json_with_duplicates)\n    assert result.name == 'Bob', \"Last value for 'name' should win\"\n    assert result.age == 25, \"Last value for 'age' should win\"\n    assert dataclasses.asdict(result) == {'name': 'Bob', 'age': 25}\n    json_multiple_duplicates = '{\"name\": \"First\", \"age\": 1, \"name\": \"Second\", \"name\": \"Third\", \"age\": 3}'\n    result2 = v.validate_json(json_multiple_duplicates)\n    assert result2.name == 'Third', 'Last value among multiple duplicates should win'\n    assert result2.age == 3\n    assert dataclasses.asdict(result2) == {'name': 'Third', 'age': 3}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005008", "source": "def is_pydantic_dataclass(class_: type[Any], /) -> TypeGuard[type[PydanticDataclass]]:\n    try:\n        return '__is_pydantic_dataclass__' in class_.__dict__ and dataclasses.is_dataclass(class_)\n    except AttributeError:\n        return False", "target": "def test_dataclass_exact_validation(revalidate_instances, input_value, expected):\n    schema = core_schema.dataclass_schema(\n        FooDataclass,\n        core_schema.dataclass_args_schema(\n            'FooDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n            ],\n        ),\n        ['a', 'b'],\n        revalidate_instances=revalidate_instances,\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python(input_value)\n    assert dataclasses.asdict(foo) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005009", "source": "def _is_field_cached_property(obj: 'Dataclass', k: str) -> bool:\n        return False", "target": "def test_validate_assignment_function():\n    @dataclasses.dataclass\n    class MyDataclass:\n        field_a: str\n        field_b: int\n        field_c: int\n    calls = []\n    def func(x, info):\n        calls.append(str(info))\n        return x * 2\n    v = SchemaValidator(\n        core_schema.dataclass_schema(\n            MyDataclass,\n            core_schema.dataclass_args_schema(\n                'MyDataclass',\n                [\n                    core_schema.dataclass_field('field_a', core_schema.str_schema()),\n                    core_schema.dataclass_field(\n                        'field_b',\n                        core_schema.with_info_after_validator_function(func, core_schema.int_schema()),\n                    ),\n                    core_schema.dataclass_field('field_c', core_schema.int_schema()),\n                ],\n            ),\n            ['field_a', 'field_b', 'field_c'],\n        )\n    )\n    m = v.validate_python({'field_a': 'x', 'field_b': 123, 'field_c': 456})\n    assert m.field_a == 'x'\n    assert m.field_b == 246\n    assert m.field_c == 456\n    assert calls == [\"ValidationInfo(config=None, context=None, data={'field_a': 'x'}, field_name='field_b')\"]\n    v.validate_assignment(m, 'field_b', '111')\n    assert m.field_b == 222\n    assert calls == [\n        \"ValidationInfo(config=None, context=None, data={'field_a': 'x'}, field_name='field_b')\",\n        \"ValidationInfo(config=None, context=None, data={'field_a': 'x', 'field_c': 456}, field_name='field_b')\",\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005010", "source": "def benchmark_module(config, module, use_throughput_benchmark=False):\n    if use_throughput_benchmark:\n        return benchmark_using_throughput_benchmark(config, module)\n    module.forward(config.num_warmup_iters)\n    print(f\"Running module for {config.num_iters} iterations\")\n    start = time.time()\n    module.forward(config.num_iters)\n    end = time.time()\n    time_elapsed_s = end - start\n    return secs_to_ms(time_elapsed_s) / config.num_iters / NUM_LOOP_ITERS", "target": "def test_iterator(self, input_: str = \"abcdef\") -> None:\n        iter_ = PeekableIterator(input_)\n        for idx, c in enumerate(iter_):\n            self.assertEqual(c, input_[idx])"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005011", "source": "def create_dataclass(cls: type[Any]) -> type[PydanticDataclass]:\n        from ._internal._utils import is_model_class\n        if is_model_class(cls):\n            raise PydanticUserError(\n                f'Cannot create a Pydantic dataclass from {cls.__name__} as it is already a Pydantic model',\n                code='dataclass-on-model',\n            )\n        original_cls = cls\n        has_dataclass_base = any(dataclasses.is_dataclass(base) for base in cls.__bases__)\n        if not has_dataclass_base and config is not None and hasattr(cls, '__pydantic_config__'):\n            warn(\n                f'`config` is set via both the `dataclass` decorator and `__pydantic_config__` for dataclass {cls.__name__}. '\n                f'The `config` specification from `dataclass` decorator will take priority.',\n                category=UserWarning,\n                stacklevel=2,\n            )\n        config_dict = config if config is not None else getattr(cls, '__pydantic_config__', None)\n        config_wrapper = _config.ConfigWrapper(config_dict)\n        decorators = _decorators.DecoratorInfos.build(cls, replace_wrapped_methods=True)\n        decorators.update_from_config(config_wrapper)\n        original_doc = cls.__doc__\n        if _pydantic_dataclasses.is_stdlib_dataclass(cls):\n            original_doc = None\n            bases = (cls,)\n            if issubclass(cls, Generic):\n                generic_base = Generic[cls.__parameters__]\n                bases = bases + (generic_base,)\n            cls = types.new_class(cls.__name__, bases)\n        if frozen is not None:\n            frozen_ = frozen\n            if config_wrapper.frozen:\n                warn(\n                    f'`frozen` is set via both the `dataclass` decorator and `config` for dataclass {cls.__name__!r}.'\n                    'This is not recommended. The `frozen` specification on `dataclass` will take priority.',\n                    category=UserWarning,\n                    stacklevel=2,\n                )\n        else:\n            frozen_ = config_wrapper.frozen or False\n        cls_anns = _typing_extra.safe_get_annotations(cls)\n        for field_name in cls_anns:\n            field_value = getattr(cls, field_name, None)\n            if isinstance(field_value, FieldInfo):\n                setattr(cls, field_name, _pydantic_dataclasses.as_dataclass_field(field_value))\n        with _pydantic_dataclasses.patch_base_fields(cls):\n            cls = dataclasses.dataclass(\n                cls,\n                init=True,\n                repr=repr,\n                eq=eq,\n                order=order,\n                unsafe_hash=unsafe_hash,\n                frozen=frozen_,\n                **kwargs,\n            )\n        if config_wrapper.validate_assignment:\n            original_setattr = cls.__setattr__\n            @functools.wraps(cls.__setattr__)\n            def validated_setattr(instance: PydanticDataclass, name: str, value: Any, /) -> None:\n                if frozen_:\n                    return original_setattr(instance, name, value)\n                inst_cls = type(instance)\n                attr = getattr(inst_cls, name, None)\n                if isinstance(attr, property):\n                    attr.__set__(instance, value)\n                elif isinstance(attr, functools.cached_property):\n                    instance.__dict__.__setitem__(name, value)\n                else:\n                    inst_cls.__pydantic_validator__.validate_assignment(instance, name, value)\n            cls.__setattr__ = validated_setattr.__get__(None, cls)\n            if slots and not hasattr(cls, '__setstate__'):\n                def _dataclass_getstate(self: Any) -> list[Any]:\n                    return [getattr(self, f.name) for f in dataclasses.fields(self)]\n                def _dataclass_setstate(self: Any, state: list[Any]) -> None:\n                    for field, value in zip(dataclasses.fields(self), state):\n                        object.__setattr__(self, field.name, value)\n                cls.__getstate__ = _dataclass_getstate\n                cls.__setstate__ = _dataclass_setstate\n        cls.__is_pydantic_dataclass__ = True\n        cls.__pydantic_decorators__ = decorators\n        cls.__doc__ = original_doc\n        firstlineno = getattr(original_cls, '__firstlineno__', None)\n        cls.__module__ = original_cls.__module__\n        if sys.version_info >= (3, 13) and firstlineno is not None:\n            original_cls.__firstlineno__ = firstlineno\n            cls.__firstlineno__ = firstlineno\n        cls.__qualname__ = original_cls.__qualname__\n        cls.__pydantic_fields_complete__ = classmethod(_pydantic_fields_complete)\n        cls.__pydantic_complete__ = False\n        _pydantic_dataclasses.complete_dataclass(cls, config_wrapper, raise_errors=False)\n        return cls", "target": "def test_properties():\n    @dataclasses.dataclass\n    class FooProp:\n        a: str\n        b: bytes\n        @property\n        def c(self) -> str:\n            return f'{self.a} {self.b.decode()}'\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'FooProp',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bytes_schema()),\n            ],\n            computed_fields=[core_schema.computed_field('c', core_schema.str_schema())],\n        ),\n        ['a', 'b'],\n    )\n    s = SchemaSerializer(schema)\n    assert s.to_python(FooProp(a='hello', b=b'more')) == IsStrictDict(a='hello', b=b'more', c='hello more')\n    assert s.to_python(FooProp(a='hello', b=b'more'), mode='json') == IsStrictDict(a='hello', b='more', c='hello more')\n    j = s.to_json(FooProp(a='hello', b=b'more'))\n    if on_pypy:\n        assert json.loads(j) == {'a': 'hello', 'b': 'more', 'c': 'hello more'}\n    else:\n        assert j == b'{\"a\":\"hello\",\"b\":\"more\",\"c\":\"hello more\"}'\n    assert s.to_python(FooProp(a='hello', b=b'more'), exclude={'b'}) == IsStrictDict(a='hello', c='hello more')\n    assert s.to_json(FooProp(a='hello', b=b'more'), include={'a'}) == b'{\"a\":\"hello\"}'"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005012", "source": "def _update_fields_from_docstrings(cls: type[Any], fields: dict[str, FieldInfo], use_inspect: bool = False) -> None:\n    fields_docs = extract_docstrings_from_cls(cls, use_inspect=use_inspect)\n    for ann_name, field_info in fields.items():\n        if field_info.description is None and ann_name in fields_docs:\n            field_info.description = fields_docs[ann_name]", "target": "def test_validate_assignment_strict_field():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={'field_a': core_schema.model_field(schema=core_schema.str_schema(strict=True))}\n        )\n    )\n    assert v.validate_python({'field_a': 'test'}) == ({'field_a': 'test'}, None, {'field_a'})\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_assignment({'field_a': 'test'}, 'field_a', b'abc')\n    assert exc_info.value.errors(include_url=False) == [\n        {'input': b'abc', 'type': 'string_type', 'loc': ('field_a',), 'msg': 'Input should be a valid string'}\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005013", "source": "def commits_resolving_gh_pr(self, pr_num: int) -> list[str]:\n        owner, name = self.gh_owner_and_name()\n        msg = f\"Pull Request resolved: https://github.com/{owner}/{name}/pull/{pr_num}\"\n        rc = self._run_git(\"log\", \"--format=%H\", \"--grep\", msg).strip()\n        return rc.split(\"\\n\") if len(rc) > 0 else []", "target": "def test_ghstack_branches_in_sync(self) -> None:\n        head_ref = \"gh/SS-JIA/206/head\"\n        self._skip_if_ref_does_not_exist(head_ref)\n        self.assertTrue(are_ghstack_branches_in_sync(self.repo, head_ref))"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005014", "source": "def takes_validated_data_argument(\n    default_factory: Callable[[], Any] | Callable[[dict[str, Any]], Any],\n) -> TypeIs[Callable[[dict[str, Any]], Any]]:\n    try:\n        sig = signature(default_factory)\n    except (ValueError, TypeError):\n        return False\n    parameters = list(sig.parameters.values())\n    return len(parameters) == 1 and can_be_positional(parameters[0]) and parameters[0].default is Parameter.empty", "target": "def test_bad_default_factory(default_factory, error_message):\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'x': core_schema.model_field(\n                    schema=core_schema.with_default_schema(\n                        schema=core_schema.str_schema(), default_factory=default_factory\n                    )\n                )\n            }\n        )\n    )\n    with pytest.raises(TypeError, match=re.escape(error_message)):\n        v.validate_python({})"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005015", "source": "def extract_weights(mod: nn.Module) -> tuple[tuple[Tensor, ...], list[str]]:\n    orig_params = tuple(mod.parameters())\n    names = []\n    for name, p in list(mod.named_parameters()):\n        _del_nested_attr(mod, name.split(\".\"))\n        names.append(name)\n    params = tuple(p.detach().requires_grad_() for p in orig_params)\n    return params, names", "target": "def test_overwrites_and_restores_existing_var(self):\n        var = \"TEST_TMP_ENV_OVERWRITE\"\n        os.environ[var] = \"orig\"\n        with temp_environ({var: \"override\"}):\n            self.assertEqual(os.environ[var], \"override\")\n        self.assertEqual(os.environ[var], \"orig\")"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005016", "source": "def dataclass(\n        *,\n        init: bool = True,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool = False,\n        config: Union[ConfigDict, Type[object], None] = None,\n        validate_on_init: Optional[bool] = None,\n        use_proxy: Optional[bool] = None,\n    ) -> Callable[[Type[_T]], 'DataclassClassOrWrapper']:\n        ...", "target": "def test_dataclass_json_duplicate_keys():\n    @dataclasses.dataclass\n    class MyDataclass:\n        name: str\n        age: int\n    schema = core_schema.dataclass_schema(\n        MyDataclass,\n        core_schema.dataclass_args_schema(\n            'MyDataclass',\n            [\n                core_schema.dataclass_field(name='name', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='age', schema=core_schema.int_schema()),\n            ],\n        ),\n        ['name', 'age'],\n    )\n    v = SchemaValidator(schema)\n    json_with_duplicates = '{\"name\": \"Alice\", \"age\": 30, \"name\": \"Bob\", \"age\": 25}'\n    result = v.validate_json(json_with_duplicates)\n    assert result.name == 'Bob', \"Last value for 'name' should win\"\n    assert result.age == 25, \"Last value for 'age' should win\"\n    assert dataclasses.asdict(result) == {'name': 'Bob', 'age': 25}\n    json_multiple_duplicates = '{\"name\": \"First\", \"age\": 1, \"name\": \"Second\", \"name\": \"Third\", \"age\": 3}'\n    result2 = v.validate_json(json_multiple_duplicates)\n    assert result2.name == 'Third', 'Last value among multiple duplicates should win'\n    assert result2.age == 3\n    assert dataclasses.asdict(result2) == {'name': 'Third', 'age': 3}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005017", "source": "def dataclass(\n    _cls: type[_T] | None = None,\n    *,\n    init: Literal[False] = False,\n    repr: bool = True,\n    eq: bool = True,\n    order: bool = False,\n    unsafe_hash: bool = False,\n    frozen: bool | None = None,\n    config: ConfigDict | type[object] | None = None,\n    validate_on_init: bool | None = None,\n    kw_only: bool = False,\n    slots: bool = False,\n) -> Callable[[type[_T]], type[PydanticDataclass]] | type[PydanticDataclass]:\n    assert init is False, 'pydantic.dataclasses.dataclass only supports init=False'\n    assert validate_on_init is not False, 'validate_on_init=False is no longer supported'\n    if sys.version_info >= (3, 10):\n        kwargs = {'kw_only': kw_only, 'slots': slots}\n    else:\n        kwargs = {}\n    def create_dataclass(cls: type[Any]) -> type[PydanticDataclass]:\n        from ._internal._utils import is_model_class\n        if is_model_class(cls):\n            raise PydanticUserError(\n                f'Cannot create a Pydantic dataclass from {cls.__name__} as it is already a Pydantic model',\n                code='dataclass-on-model',\n            )\n        original_cls = cls\n        has_dataclass_base = any(dataclasses.is_dataclass(base) for base in cls.__bases__)\n        if not has_dataclass_base and config is not None and hasattr(cls, '__pydantic_config__'):\n            warn(\n                f'`config` is set via both the `dataclass` decorator and `__pydantic_config__` for dataclass {cls.__name__}. '\n                f'The `config` specification from `dataclass` decorator will take priority.',\n                category=UserWarning,\n                stacklevel=2,\n            )\n        config_dict = config if config is not None else getattr(cls, '__pydantic_config__', None)\n        config_wrapper = _config.ConfigWrapper(config_dict)\n        decorators = _decorators.DecoratorInfos.build(cls, replace_wrapped_methods=True)\n        decorators.update_from_config(config_wrapper)\n        original_doc = cls.__doc__\n        if _pydantic_dataclasses.is_stdlib_dataclass(cls):\n            original_doc = None\n            bases = (cls,)\n            if issubclass(cls, Generic):\n                generic_base = Generic[cls.__parameters__]\n                bases = bases + (generic_base,)\n            cls = types.new_class(cls.__name__, bases)\n        if frozen is not None:\n            frozen_ = frozen\n            if config_wrapper.frozen:\n                warn(\n                    f'`frozen` is set via both the `dataclass` decorator and `config` for dataclass {cls.__name__!r}.'\n                    'This is not recommended. The `frozen` specification on `dataclass` will take priority.',\n                    category=UserWarning,\n                    stacklevel=2,\n                )\n        else:\n            frozen_ = config_wrapper.frozen or False\n        cls_anns = _typing_extra.safe_get_annotations(cls)\n        for field_name in cls_anns:\n            field_value = getattr(cls, field_name, None)\n            if isinstance(field_value, FieldInfo):\n                setattr(cls, field_name, _pydantic_dataclasses.as_dataclass_field(field_value))\n        with _pydantic_dataclasses.patch_base_fields(cls):\n            cls = dataclasses.dataclass(\n                cls,\n                init=True,\n                repr=repr,\n                eq=eq,\n                order=order,\n                unsafe_hash=unsafe_hash,\n                frozen=frozen_,\n                **kwargs,\n            )\n        if config_wrapper.validate_assignment:\n            original_setattr = cls.__setattr__\n            @functools.wraps(cls.__setattr__)\n            def validated_setattr(instance: PydanticDataclass, name: str, value: Any, /) -> None:\n                if frozen_:\n                    return original_setattr(instance, name, value)\n                inst_cls = type(instance)\n                attr = getattr(inst_cls, name, None)\n                if isinstance(attr, property):\n                    attr.__set__(instance, value)\n                elif isinstance(attr, functools.cached_property):\n                    instance.__dict__.__setitem__(name, value)\n                else:\n                    inst_cls.__pydantic_validator__.validate_assignment(instance, name, value)\n            cls.__setattr__ = validated_setattr.__get__(None, cls)\n            if slots and not hasattr(cls, '__setstate__'):\n                def _dataclass_getstate(self: Any) -> list[Any]:\n                    return [getattr(self, f.name) for f in dataclasses.fields(self)]\n                def _dataclass_setstate(self: Any, state: list[Any]) -> None:\n                    for field, value in zip(dataclasses.fields(self), state):\n                        object.__setattr__(self, field.name, value)\n                cls.__getstate__ = _dataclass_getstate\n                cls.__setstate__ = _dataclass_setstate\n        cls.__is_pydantic_dataclass__ = True\n        cls.__pydantic_decorators__ = decorators\n        cls.__doc__ = original_doc\n        firstlineno = getattr(original_cls, '__firstlineno__', None)\n        cls.__module__ = original_cls.__module__\n        if sys.version_info >= (3, 13) and firstlineno is not None:\n            original_cls.__firstlineno__ = firstlineno\n            cls.__firstlineno__ = firstlineno\n        cls.__qualname__ = original_cls.__qualname__\n        cls.__pydantic_fields_complete__ = classmethod(_pydantic_fields_complete)\n        cls.__pydantic_complete__ = False\n        _pydantic_dataclasses.complete_dataclass(cls, config_wrapper, raise_errors=False)\n        return cls\n    return create_dataclass if _cls is None else create_dataclass(_cls)", "target": "def test_function_validator_wrapping_args_schema_wrap() -> None:\n    calls: list[Any] = []\n    def func(*args: Any) -> Any:\n        assert len(args) == 2\n        input, handler = args\n        output = handler(input)\n        calls.append((input, output))\n        return output\n    @dataclasses.dataclass\n    class Model:\n        number: int = 1\n    cs = core_schema.dataclass_schema(\n        Model,\n        core_schema.no_info_wrap_validator_function(\n            func,\n            core_schema.dataclass_args_schema(\n                'Model', [core_schema.dataclass_field('number', core_schema.int_schema())]\n            ),\n        ),\n        ['number'],\n    )\n    v = SchemaValidator(cs)\n    instance: Model = v.validate_python({'number': 1})\n    assert instance.number == 1\n    assert calls == [({'number': 1}, ({'number': 1}, None))]\n    v.validate_assignment(instance, 'number', 2)\n    assert instance.number == 2\n    assert calls == [({'number': 1}, ({'number': 1}, None)), ({'number': 2}, ({'number': 2}, None))]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005018", "source": "def with_config(config: ConfigDict, /) -> Callable[[_TypeT], _TypeT]: ...", "target": "def test_hide_input_in_errors(config, input_str):\n    v = SchemaValidator(\n        cs.model_schema(\n            cls=MyModel, schema=cs.model_fields_schema(fields={'f': cs.model_field(schema=cs.str_schema())})\n        ),\n        config=config,\n    )\n    with pytest.raises(ValidationError, match=re.escape(f'Input should be a valid string [{input_str}]')):\n        assert v.validate_python({'f': 123})"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005019", "source": "def get_args(t: Type[Any]) -> Tuple[Any, ...]:\n        if type(t).__name__ in AnnotatedTypeNames:\n            return t.__args__ + t.__metadata__\n        if isinstance(t, _GenericAlias):\n            res = t.__args__\n            if t.__origin__ is Callable and res and res[0] is not Ellipsis:\n                res = (list(res[:-1]), res[-1])\n            return res\n        return getattr(t, '__args__', ())", "target": "def test_ser_function_wrap():\n    def f(\n        input: Any, serialize: core_schema.SerializerFunctionWrapHandler, info: core_schema.SerializationInfo, /\n    ) -> str:\n        return f'{serialize} {info}'\n    s = SchemaSerializer(\n        core_schema.any_schema(\n            serialization=core_schema.wrap_serializer_function_ser_schema(\n                f, info_arg=True, schema=core_schema.str_schema(), when_used='json'\n            )\n        )\n    )\n    assert s.to_python(123, mode='json') == (\n        'SerializationCallable(serializer=str) '\n        \"SerializationInfo(include=None, exclude=None, context=None, mode='json', by_alias=False, exclude_unset=False, \"\n        'exclude_defaults=False, exclude_none=False, exclude_computed_fields=False, round_trip=False, serialize_as_any=False)'\n    )"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005020", "source": "def dataclass(\n    _cls: Optional[Type[_T]] = None,\n    *,\n    init: bool = True,\n    repr: bool = True,\n    eq: bool = True,\n    order: bool = False,\n    unsafe_hash: bool = False,\n    frozen: bool = False,\n    config: Union[ConfigDict, Type[object], None] = None,\n    validate_on_init: Optional[bool] = None,\n    use_proxy: Optional[bool] = None,\n    kw_only: bool = False,\n) -> Union[Callable[[Type[_T]], 'DataclassClassOrWrapper'], 'DataclassClassOrWrapper']:\n    the_config = get_config(config)\n    def wrap(cls: Type[Any]) -> 'DataclassClassOrWrapper':\n        should_use_proxy = (\n            use_proxy\n            if use_proxy is not None\n            else (\n                is_builtin_dataclass(cls)\n                and (cls.__bases__[0] is object or set(dir(cls)) == set(dir(cls.__bases__[0])))\n            )\n        )\n        if should_use_proxy:\n            dc_cls_doc = ''\n            dc_cls = DataclassProxy(cls)\n            default_validate_on_init = False\n        else:\n            dc_cls_doc = cls.__doc__ or ''\n            if sys.version_info >= (3, 10):\n                dc_cls = dataclasses.dataclass(\n                    cls,\n                    init=init,\n                    repr=repr,\n                    eq=eq,\n                    order=order,\n                    unsafe_hash=unsafe_hash,\n                    frozen=frozen,\n                    kw_only=kw_only,\n                )\n            else:\n                dc_cls = dataclasses.dataclass(\n                    cls, init=init, repr=repr, eq=eq, order=order, unsafe_hash=unsafe_hash, frozen=frozen\n                )\n            default_validate_on_init = True\n        should_validate_on_init = default_validate_on_init if validate_on_init is None else validate_on_init\n        _add_pydantic_validation_attributes(cls, the_config, should_validate_on_init, dc_cls_doc)\n        dc_cls.__pydantic_model__.__try_update_forward_refs__(**{cls.__name__: cls})\n        return dc_cls\n    if _cls is None:\n        return wrap\n    return wrap(_cls)", "target": "def test_leak_dataclass(validator):\n    def fn():\n        @dataclasses.dataclass\n        class Dataclass:\n            a: int\n            @classmethod\n            def _validator(cls, v, info):\n                return v\n            @classmethod\n            def _wrap_validator(cls, v, validator, info):\n                return validator(v)\n        field_schema = core_schema.int_schema()\n        if validator == 'field':\n            field_schema = core_schema.with_info_before_validator_function(Dataclass._validator, field_schema)\n            field_schema = core_schema.with_info_wrap_validator_function(Dataclass._wrap_validator, field_schema)\n            field_schema = core_schema.with_info_after_validator_function(Dataclass._validator, field_schema)\n        dataclass_schema = core_schema.dataclass_schema(\n            Dataclass,\n            core_schema.dataclass_args_schema('Dataclass', [core_schema.dataclass_field('a', field_schema)]),\n            ['a'],\n        )\n        if validator == 'dataclass':\n            dataclass_schema = core_schema.with_info_before_validator_function(Dataclass._validator, dataclass_schema)\n            dataclass_schema = core_schema.with_info_wrap_validator_function(\n                Dataclass._wrap_validator, dataclass_schema\n            )\n            dataclass_schema = core_schema.with_info_after_validator_function(Dataclass._validator, dataclass_schema)\n        Dataclass.__pydantic_validator__ = SchemaValidator(dataclass_schema)\n        return Dataclass\n    klass = fn()\n    ref = weakref.ref(klass)\n    assert ref() is not None\n    del klass\n    assert_gc(lambda: ref() is None)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005021", "source": "def _dataclass_validate_assignment_setattr(self: 'Dataclass', name: str, value: Any) -> None:\n    if self.__pydantic_initialised__:\n        d = dict(self.__dict__)\n        d.pop(name, None)\n        known_field = self.__pydantic_model__.__fields__.get(name, None)\n        if known_field:\n            value, error_ = known_field.validate(value, d, loc=name, cls=self.__class__)\n            if error_:\n                raise ValidationError([error_], self.__class__)\n    object.__setattr__(self, name, value)", "target": "def test_dataclass_args_init_only_no_fields(py_and_json: PyAndJson, input_value, expected):\n    schema = core_schema.dataclass_args_schema(\n        'MyDataclass', [core_schema.dataclass_field(name='a', schema=core_schema.str_schema())], collect_init_only=True\n    )\n    v = py_and_json(schema)\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=expected.message) as exc_info:\n            v.validate_test(input_value)\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        assert v.validate_test(input_value) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005022", "source": "def as_dataclass_field(pydantic_field: FieldInfo) -> dataclasses.Field[Any]:\n    field_args: dict[str, Any] = {'default': pydantic_field}\n    if sys.version_info >= (3, 14) and pydantic_field.description is not None:\n        field_args['doc'] = pydantic_field.description\n    if sys.version_info >= (3, 10) and pydantic_field.kw_only:\n        field_args['kw_only'] = True\n    if pydantic_field.repr is not True:\n        field_args['repr'] = pydantic_field.repr\n    return dataclasses.field(**field_args)", "target": "def test_frozen_field():\n    @dataclasses.dataclass\n    class MyModel:\n        f: str\n    v = SchemaValidator(\n        core_schema.dataclass_schema(\n            MyModel,\n            core_schema.dataclass_args_schema(\n                'MyModel', [core_schema.dataclass_field('f', core_schema.str_schema(), frozen=True)]\n            ),\n            ['f'],\n        )\n    )\n    m = v.validate_python({'f': 'x'})\n    assert m.f == 'x'\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_assignment(m, 'f', 'y')\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'frozen_field', 'loc': ('f',), 'msg': 'Field is frozen', 'input': 'y'}\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005023", "source": "def create_pydantic_model_from_dataclass(\n    dc_cls: Type['Dataclass'],\n    config: Type[Any] = BaseConfig,\n    dc_cls_doc: Optional[str] = None,\n) -> Type['BaseModel']:\n    field_definitions: Dict[str, Any] = {}\n    for field in dataclasses.fields(dc_cls):\n        default: Any = Undefined\n        default_factory: Optional['NoArgAnyCallable'] = None\n        field_info: FieldInfo\n        if field.default is not dataclasses.MISSING:\n            default = field.default\n        elif field.default_factory is not dataclasses.MISSING:\n            default_factory = field.default_factory\n        else:\n            default = Required\n        if isinstance(default, FieldInfo):\n            field_info = default\n            dc_cls.__pydantic_has_field_info_default__ = True\n        else:\n            field_info = Field(default=default, default_factory=default_factory, **field.metadata)\n        field_definitions[field.name] = (field.type, field_info)\n    validators = gather_all_validators(dc_cls)\n    model: Type['BaseModel'] = create_model(\n        dc_cls.__name__,\n        __config__=config,\n        __module__=dc_cls.__module__,\n        __validators__=validators,\n        __cls_kwargs__={'__resolve_forward_refs__': False},\n        **field_definitions,\n    )\n    model.__doc__ = dc_cls_doc if dc_cls_doc is not None else dc_cls.__doc__ or ''\n    return model", "target": "def test_dataclass_subclass(revalidate_instances, input_value, expected):\n    schema = core_schema.dataclass_schema(\n        FooDataclass,\n        core_schema.dataclass_args_schema(\n            'FooDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n            ],\n            extra_behavior='forbid',\n        ),\n        ['a', 'b'],\n        revalidate_instances=revalidate_instances,\n    )\n    v = SchemaValidator(schema)\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=expected.message) as exc_info:\n            print(v.validate_python(input_value))\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        dc = v.validate_python(input_value)\n        assert dataclasses.is_dataclass(dc)\n        assert dataclasses.asdict(dc) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005024", "source": "def _recreate_field_info(\n    field_info: FieldInfo,\n    ns_resolver: NsResolver,\n    typevars_map: Mapping[TypeVar, Any],\n    *,\n    lenient: bool,\n) -> FieldInfo:\n    FieldInfo_ = import_cached_field_info()\n    existing_desc = field_info.description\n    if lenient:\n        ann = _generics.replace_types(field_info._original_annotation, typevars_map)\n        ann, evaluated = _typing_extra.try_eval_type(\n            ann,\n            *ns_resolver.types_namespace,\n        )\n    else:\n        ann = _typing_extra.eval_type(\n            field_info._original_annotation,\n            *ns_resolver.types_namespace,\n        )\n        ann = _generics.replace_types(ann, typevars_map)\n        ann = _typing_extra.eval_type(\n            ann,\n            *ns_resolver.types_namespace,\n        )\n        evaluated = True\n    if (assign := field_info._original_assignment) is PydanticUndefined:\n        new_field = FieldInfo_.from_annotation(ann, _source=AnnotationSource.CLASS)\n    else:\n        new_field = FieldInfo_.from_annotated_attribute(ann, assign, _source=AnnotationSource.CLASS)\n        new_field._original_assignment = assign\n    new_field._original_annotation = ann\n    new_field.description = new_field.description if new_field.description is not None else existing_desc\n    if not evaluated:\n        new_field._complete = False\n    return new_field", "target": "def test_from_attributes_missing():\n    class Foobar:\n        def __init__(self):\n            self.a = 1\n            self.b = 2\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'a': core_schema.model_field(schema=core_schema.int_schema()),\n                'b': core_schema.model_field(schema=core_schema.int_schema()),\n                'c': core_schema.model_field(schema=core_schema.str_schema()),\n            },\n            from_attributes=True,\n        )\n    )\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python(Foobar())\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'missing',\n            'loc': ('c',),\n            'msg': 'Field required',\n            'input': HasRepr(IsStr(regex='.+Foobar object at.+')),\n        }\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005025", "source": "def convert_generics(tp: Type[Any]) -> Type[Any]:\n        return tp", "target": "def test_wrong_function_signature() -> None:\n    def wrong_validator(value: Any) -> Any:\n        return value\n    v = SchemaValidator(core_schema.with_info_plain_validator_function(wrong_validator))\n    try:\n        v.validate_python(1)\n    except TypeError as exc:\n        assert 'takes 1 positional argument but 2 were given' in str(exc)\n    else:\n        raise AssertionError('v.validate_python(1) did not raise TypeError')"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005026", "source": "def is_finalvar(ann_type: Type[Any]) -> bool:\n    return _check_finalvar(ann_type) or _check_finalvar(get_origin(ann_type))", "target": "def test_schema_typing() -> None:\n    schema: CoreSchema = {\n        'type': 'union',\n        'choices': [{'type': 'int'}, {'type': 'int', 'ge': 1}, {'type': 'float', 'lt': 1.0}],\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = {\n        'type': 'tagged-union',\n        'discriminator': 'type',\n        'choices': {\n            'apple': {\n                'type': 'typed-dict',\n                'fields': {'pips': {'type': 'typed-dict-field', 'schema': {'type': 'int'}}},\n            },\n            'banana': {\n                'type': 'typed-dict',\n                'fields': {'curvature': {'type': 'typed-dict-field', 'schema': {'type': 'float'}}},\n            },\n        },\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'int', 'ge': 1}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'float', 'lt': 1.0}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'str', 'pattern': r'http://.*'}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'bool', 'strict': False}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'literal', 'expected': [1, '1']}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'any'}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'none'}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'bytes'}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'list', 'items_schema': {'type': 'str'}, 'min_length': 3}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'set', 'items_schema': {'type': 'str'}, 'max_length': 3}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'tuple', 'items_schema': [{'type': 'str'}], 'variadic_item_index': 0, 'max_length': 3}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'tuple', 'items_schema': [{'type': 'str'}, {'type': 'int'}]}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'frozenset', 'items_schema': {'type': 'str'}, 'max_length': 3}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'dict', 'keys_schema': {'type': 'str'}, 'values_schema': {'type': 'any'}}\n    SchemaValidator(schema)\n    schema: CoreSchema = {\n        'type': 'typed-dict',\n        'fields': {'bar': {'type': 'typed-dict-field', 'schema': {'type': 'str'}}},\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = {\n        'type': 'model',\n        'cls': Foo,\n        'schema': {'type': 'model-fields', 'fields': {'bar': {'type': 'model-field', 'schema': {'type': 'str'}}}},\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = {\n        'type': 'typed-dict',\n        'fields': {\n            'a': {'type': 'typed-dict-field', 'schema': {'type': 'str'}},\n            'b': {'type': 'typed-dict-field', 'schema': {'type': 'str'}, 'validation_alias': 'foobar'},\n            'c': {\n                'type': 'typed-dict-field',\n                'schema': {'type': 'str'},\n                'validation_alias': [['foobar', 0, 'bar'], ['foo']],\n            },\n            'd': {\n                'type': 'typed-dict-field',\n                'schema': {'type': 'default', 'schema': {'type': 'str'}, 'default': 'spam'},\n            },\n        },\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = {\n        'type': 'function-wrap',\n        'function': {'type': 'with-info', 'function': wrap_validator, 'field_name': 'foobar'},\n        'schema': {'type': 'str'},\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = core_schema.with_info_plain_validator_function(validator)\n    SchemaValidator(schema)\n    schema: CoreSchema = {\n        'type': 'definitions',\n        'schema': {'type': 'definition-ref', 'schema_ref': 'Branch'},\n        'definitions': [\n            {\n                'type': 'typed-dict',\n                'fields': {\n                    'name': {'type': 'typed-dict-field', 'schema': {'type': 'str'}},\n                    'sub_branch': {\n                        'type': 'typed-dict-field',\n                        'schema': {\n                            'type': 'default',\n                            'schema': {\n                                'type': 'nullable',\n                                'schema': {'type': 'definition-ref', 'schema_ref': 'Branch'},\n                            },\n                            'default': None,\n                        },\n                    },\n                },\n                'ref': 'Branch',\n            }\n        ],\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'date', 'le': date.today()}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'time', 'lt': time(12, 13, 14)}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'datetime', 'ge': datetime.now()}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'is-instance', 'cls': Foo}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'callable'}\n    SchemaValidator(schema)\n    schema: CoreSchema = {\n        'type': 'arguments',\n        'arguments_schema': [\n            {'name': 'a', 'mode': 'positional_only', 'schema': {'type': 'int'}},\n            {'name': 'b', 'schema': {'type': 'str'}},\n            {'name': 'c', 'mode': 'keyword_only', 'schema': {'type': 'bool'}},\n        ],\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'call', 'arguments_schema': {'type': 'any'}, 'function': foo}\n    SchemaValidator(schema)"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005027", "source": "def secs_to_ms(time_s):\n    return time_s * 1e3", "target": "def test_restores_even_on_exception(self):\n        var = \"TEST_TMP_ENV_EXCEPTION\"\n        self.assertNotIn(var, os.environ)\n        with self.assertRaises(RuntimeError):\n            with temp_environ({var: \"x\"}):\n                self.assertEqual(os.environ[var], \"x\")\n                raise RuntimeError(\"boom\")\n        self.assertNotIn(var, os.environ)"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005028", "source": "def generate_dataclass_help(cls) -> str:\n    if not is_dataclass(cls):\n        raise TypeError(f\"{cls} is not a dataclass\")\n    def get_value(f):\n        if f.default is not MISSING:\n            return f.default\n        if f.default_factory is not MISSING:\n            try:\n                return f.default_factory()\n            except Exception as e:\n                return f\"<error: {e}>\"\n        return \"<required>\"\n    lines = [f\"{f.name:<22} = {repr(get_value(f))}\" for f in fields(cls)]\n    return indent(\"\\n\".join(lines), \"    \")", "target": "def test_get_env_not_exist_returns_default(self):\n        with patch.dict(os.environ, {\"FOO\": \"bar\"}, clear=True):\n            self.assertEqual(m.get_env(\"TEST_NOT_EXIST\", \"default\"), \"default\")"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005029", "source": "def extract_weights(mod: nn.Module) -> tuple[tuple[Tensor, ...], list[str]]:\n    orig_params = tuple(mod.parameters())\n    names = []\n    for name, p in list(mod.named_parameters()):\n        _del_nested_attr(mod, name.split(\".\"))\n        names.append(name)\n    params = tuple(p.detach().requires_grad_() for p in orig_params)\n    return params, names", "target": "def test_get_last_page_num_from_header(self) -> None:\n        for (\n            expected_page_num,\n            mock_header,\n        ) in self.MOCK_HEADER_LINKS_TO_PAGE_NUMS.items():\n            self.assertEqual(\n                get_last_page_num_from_header(mock_header), expected_page_num\n            )"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005030", "source": "def decimal_encoder(dec_value: Decimal) -> Union[int, float]:\n    if dec_value.as_tuple().exponent >= 0:\n        return int(dec_value)\n    else:\n        return float(dec_value)", "target": "def test_list_json():\n    s = SchemaSerializer(core_schema.list_schema(core_schema.json_schema()))\n    v = ['a', [1, 2], None]\n    assert s.to_python(v) == v\n    assert s.to_python(v, round_trip=True) == ['\"a\"', '[1,2]', 'null']\n    assert s.to_python(v, mode='json') == v\n    assert s.to_python(v, mode='json', round_trip=True) == ['\"a\"', '[1,2]', 'null']\n    assert s.to_json(v) == b'[\"a\",[1,2],null]'\n    assert s.to_json(v, round_trip=True) == b'[\"\\\\\"a\\\\\"\",\"[1,2]\",\"null\"]'"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005031", "source": "def dataclass(\n    _cls: Optional[Type[_T]] = None,\n    *,\n    init: bool = True,\n    repr: bool = True,\n    eq: bool = True,\n    order: bool = False,\n    unsafe_hash: bool = False,\n    frozen: bool = False,\n    config: Union[ConfigDict, Type[object], None] = None,\n    validate_on_init: Optional[bool] = None,\n    use_proxy: Optional[bool] = None,\n    kw_only: bool = False,\n) -> Union[Callable[[Type[_T]], 'DataclassClassOrWrapper'], 'DataclassClassOrWrapper']:\n    the_config = get_config(config)\n    def wrap(cls: Type[Any]) -> 'DataclassClassOrWrapper':\n        should_use_proxy = (\n            use_proxy\n            if use_proxy is not None\n            else (\n                is_builtin_dataclass(cls)\n                and (cls.__bases__[0] is object or set(dir(cls)) == set(dir(cls.__bases__[0])))\n            )\n        )\n        if should_use_proxy:\n            dc_cls_doc = ''\n            dc_cls = DataclassProxy(cls)\n            default_validate_on_init = False\n        else:\n            dc_cls_doc = cls.__doc__ or ''\n            if sys.version_info >= (3, 10):\n                dc_cls = dataclasses.dataclass(\n                    cls,\n                    init=init,\n                    repr=repr,\n                    eq=eq,\n                    order=order,\n                    unsafe_hash=unsafe_hash,\n                    frozen=frozen,\n                    kw_only=kw_only,\n                )\n            else:\n                dc_cls = dataclasses.dataclass(\n                    cls, init=init, repr=repr, eq=eq, order=order, unsafe_hash=unsafe_hash, frozen=frozen\n                )\n            default_validate_on_init = True\n        should_validate_on_init = default_validate_on_init if validate_on_init is None else validate_on_init\n        _add_pydantic_validation_attributes(cls, the_config, should_validate_on_init, dc_cls_doc)\n        dc_cls.__pydantic_model__.__try_update_forward_refs__(**{cls.__name__: cls})\n        return dc_cls\n    if _cls is None:\n        return wrap\n    return wrap(_cls)", "target": "def test_dataclass_args_init_with_default(input_value, extra_behavior, expected):\n    @dataclasses.dataclass\n    class Foo:\n        a: str\n        b: str\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(\n                    name='b',\n                    schema=core_schema.with_default_schema(schema=core_schema.str_schema(), default='HELLO'),\n                    init=False,\n                ),\n            ],\n            extra_behavior=extra_behavior,\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)) as exc_info:\n            v.validate_python(input_value)\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        assert dataclasses.asdict(v.validate_python(input_value)) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005032", "source": "def get_wheels(\n    output_dir: Path,\n    max_depth: Optional[int] = None,\n) -> list[str]:\n    root = Path(output_dir)\n    if not root.exists():\n        return []\n    items = []\n    for dirpath, _, filenames in os.walk(root):\n        depth = Path(dirpath).relative_to(root).parts\n        if max_depth is not None and len(depth) > max_depth:\n            continue\n        for fname in sorted(filenames):\n            if fname.endswith(\".whl\"):\n                pkg = fname.split(\"-\")[0]\n                relpath = str((Path(dirpath) / fname).relative_to(root))\n                items.append({\"pkg\": pkg, \"relpath\": relpath})\n    return items", "target": "def test_merged_lastfailed_content_with_empty_dest(self) -> None:\n        last_failed_source = {\n            \"tools/tests/test_car.py::test_num1\": True,\n            \"tools/tests/test_car.py::test_num2\": True,\n        }\n        last_failed_dest = {\n            \"\": True,\n        }\n        last_failed_merged = {\n            \"tools/tests/test_car.py::test_num1\": True,\n            \"tools/tests/test_car.py::test_num2\": True,\n        }\n        merged = _merged_lastfailed_content(last_failed_source, last_failed_dest)\n        self.assertEqual(merged, last_failed_merged)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005033", "source": "def dataclass(\n        *,\n        init: bool = True,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool = False,\n        config: Union[ConfigDict, Type[object], None] = None,\n        validate_on_init: Optional[bool] = None,\n        use_proxy: Optional[bool] = None,\n    ) -> Callable[[Type[_T]], 'DataclassClassOrWrapper']:\n        ...", "target": "def test_only_validate_by_name(py_and_json) -> None:\n    schema = core_schema.dataclass_schema(\n        BasicDataclass,\n        core_schema.dataclass_args_schema(\n            'BasicDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), validation_alias='FieldA'),\n            ],\n        ),\n        ['a'],\n        config=core_schema.CoreConfig(validate_by_name=True, validate_by_alias=False),\n    )\n    v = py_and_json(schema)\n    assert v.validate_test({'a': 'hello'}) == BasicDataclass(a='hello')\n    with pytest.raises(ValidationError, match=r'a\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'FieldA': 'hello'})"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005034", "source": "def update_field_from_config(config_wrapper: ConfigWrapper, field_name: str, field_info: FieldInfo) -> None:\n    field_title_generator = field_info.field_title_generator or config_wrapper.field_title_generator\n    if field_title_generator is not None:\n        _apply_field_title_generator_to_field_info(field_title_generator, field_name, field_info)\n    if config_wrapper.alias_generator is not None:\n        _apply_alias_generator_to_field_info(config_wrapper.alias_generator, field_name, field_info)", "target": "def test_by_alias_and_name_config_interaction(\n    config_by_alias: Union[bool, None],\n    config_by_name: Union[bool, None],\n    runtime_by_alias: Union[bool, None],\n    runtime_by_name: Union[bool, None],\n) -> None:\n    if config_by_alias is False and config_by_name is False and runtime_by_alias is False and runtime_by_name is False:\n        pytest.skip(\"Can't have both by_alias and by_name as effectively False\")\n    class Model:\n        def __init__(self, my_field: int) -> None:\n            self.my_field = my_field\n    core_config = {\n        **({'validate_by_alias': config_by_alias} if config_by_alias is not None else {}),\n        **({'validate_by_name': config_by_name} if config_by_name is not None else {}),\n    }\n    schema = core_schema.model_schema(\n        Model,\n        core_schema.model_fields_schema(\n            {\n                'my_field': core_schema.model_field(core_schema.int_schema(), validation_alias='my_alias'),\n            }\n        ),\n        config=core_schema.CoreConfig(**core_config),\n    )\n    s = SchemaValidator(schema)\n    alias_allowed = next(x for x in (runtime_by_alias, config_by_alias, True) if x is not None)\n    name_allowed = next(x for x in (runtime_by_name, config_by_name, False) if x is not None)\n    if alias_allowed:\n        assert s.validate_python({'my_alias': 1}, by_alias=runtime_by_alias, by_name=runtime_by_name).my_field == 1\n    if name_allowed:\n        assert s.validate_python({'my_field': 1}, by_alias=runtime_by_alias, by_name=runtime_by_name).my_field == 1"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005035", "source": "def new_post_init(self: 'Dataclass', *args: Any, **kwargs: Any) -> None:\n            if config.post_init_call == 'before_validation':\n                post_init(self, *args, **kwargs)\n            if self.__class__.__pydantic_run_validation__:\n                self.__pydantic_validate_values__()\n                if hasattr(self, '__post_init_post_parse__'):\n                    self.__post_init_post_parse__(*args, **kwargs)\n            if config.post_init_call == 'after_validation':\n                post_init(self, *args, **kwargs)", "target": "def test_dataclass_args_init(input_value, extra_behavior, expected):\n    @dataclasses.dataclass\n    class Foo:\n        a: str\n        b: str\n        def __post_init__(self):\n            self.b = self.a.upper()\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.str_schema(), init=False),\n            ],\n            extra_behavior=extra_behavior,\n        ),\n        ['a', 'b'],\n        post_init=True,\n    )\n    v = SchemaValidator(schema)\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)) as exc_info:\n            v.validate_python(input_value)\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        assert dataclasses.asdict(v.validate_python(input_value)) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005036", "source": "def revert(self, ref: str) -> None:\n        self._run_git(\"revert\", \"--no-edit\", ref)", "target": "def test_iterator(self, input_: str = \"abcdef\") -> None:\n        iter_ = PeekableIterator(input_)\n        for idx, c in enumerate(iter_):\n            self.assertEqual(c, input_[idx])"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005037", "source": "def isoformat(o: Union[datetime.date, datetime.time]) -> str:\n    return o.isoformat()", "target": "def test_to_jsonable_python():\n    assert to_jsonable_python([1, 2]) == [1, 2]\n    assert to_jsonable_python({1, 2}) == IsList(1, 2, check_order=False)\n    assert to_jsonable_python([1, b'x']) == [1, 'x']\n    assert to_jsonable_python([0, 1, 2, 3, 4], exclude={1, 3}) == [0, 2, 4]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005038", "source": "def dataclass(\n    _cls: Optional[Type[_T]] = None,\n    *,\n    init: bool = True,\n    repr: bool = True,\n    eq: bool = True,\n    order: bool = False,\n    unsafe_hash: bool = False,\n    frozen: bool = False,\n    config: Union[ConfigDict, Type[object], None] = None,\n    validate_on_init: Optional[bool] = None,\n    use_proxy: Optional[bool] = None,\n    kw_only: bool = False,\n) -> Union[Callable[[Type[_T]], 'DataclassClassOrWrapper'], 'DataclassClassOrWrapper']:\n    the_config = get_config(config)\n    def wrap(cls: Type[Any]) -> 'DataclassClassOrWrapper':\n        should_use_proxy = (\n            use_proxy\n            if use_proxy is not None\n            else (\n                is_builtin_dataclass(cls)\n                and (cls.__bases__[0] is object or set(dir(cls)) == set(dir(cls.__bases__[0])))\n            )\n        )\n        if should_use_proxy:\n            dc_cls_doc = ''\n            dc_cls = DataclassProxy(cls)\n            default_validate_on_init = False\n        else:\n            dc_cls_doc = cls.__doc__ or ''\n            if sys.version_info >= (3, 10):\n                dc_cls = dataclasses.dataclass(\n                    cls,\n                    init=init,\n                    repr=repr,\n                    eq=eq,\n                    order=order,\n                    unsafe_hash=unsafe_hash,\n                    frozen=frozen,\n                    kw_only=kw_only,\n                )\n            else:\n                dc_cls = dataclasses.dataclass(\n                    cls, init=init, repr=repr, eq=eq, order=order, unsafe_hash=unsafe_hash, frozen=frozen\n                )\n            default_validate_on_init = True\n        should_validate_on_init = default_validate_on_init if validate_on_init is None else validate_on_init\n        _add_pydantic_validation_attributes(cls, the_config, should_validate_on_init, dc_cls_doc)\n        dc_cls.__pydantic_model__.__try_update_forward_refs__(**{cls.__name__: cls})\n        return dc_cls\n    if _cls is None:\n        return wrap\n    return wrap(_cls)", "target": "def test_dataclass_self_init_alias():\n    @dataclasses.dataclass(init=False)\n    class Foo:\n        a: str\n        b: bool\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), validation_alias='aAlias'),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), validation_alias=['bAlias', 0]),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    def __init__(self, *args, **kwargs):\n        v.validate_python(ArgsKwargs(args, kwargs), self_instance=self)\n    Foo.__init__ = __init__\n    foo = Foo(aAlias=b'hello', bAlias=['True'])\n    assert dataclasses.is_dataclass(foo)\n    assert dataclasses.asdict(foo) == {'a': 'hello', 'b': True}\n    with pytest.raises(ValidationError) as exc_info:\n        Foo(aAlias=b'hello', bAlias=['wrong'])\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'bool_parsing',\n            'loc': ('bAlias', 0),\n            'msg': 'Input should be a valid boolean, unable to interpret input',\n            'input': 'wrong',\n        }\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005039", "source": "def patch_base_fields(cls: type[Any]) -> Generator[None]:\n    original_fields_list: list[tuple[DcFields, DcFields]] = []\n    for base in cls.__mro__[1:]:\n        dc_fields: dict[str, dataclasses.Field[Any]] = base.__dict__.get('__dataclass_fields__', {})\n        dc_fields_with_pydantic_field_defaults = {\n            field_name: field\n            for field_name, field in dc_fields.items()\n            if isinstance(field.default, FieldInfo)\n            and (field.default.description is not None or field.default.kw_only or field.default.repr is not True)\n        }\n        if dc_fields_with_pydantic_field_defaults:\n            original_fields_list.append((dc_fields, dc_fields_with_pydantic_field_defaults))\n            for field_name, field in dc_fields_with_pydantic_field_defaults.items():\n                default = cast(FieldInfo, field.default)\n                new_dc_field = copy.copy(field)\n                if sys.version_info >= (3, 10) and default.kw_only:\n                    new_dc_field.kw_only = True\n                if default.repr is not True:\n                    new_dc_field.repr = default.repr\n                dc_fields[field_name] = new_dc_field\n    try:\n        yield\n    finally:\n        for fields, original_fields in original_fields_list:\n            for field_name, original_field in original_fields.items():\n                fields[field_name] = original_field", "target": "def test_custom_dataclass_names():\n    schema = core_schema.dataclass_schema(\n        FooParentDataclass,\n        core_schema.dataclass_args_schema(\n            'FooParentDataclass',\n            [\n                core_schema.dataclass_field(\n                    name='foo',\n                    schema=core_schema.union_schema(\n                        [\n                            core_schema.dataclass_schema(\n                                FooDataclass,\n                                core_schema.dataclass_args_schema(\n                                    'FooDataclass[dataclass_args_schema]',\n                                    [\n                                        core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                                        core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n                                    ],\n                                ),\n                                ['a', 'b'],\n                                cls_name='FooDataclass[cls_name]',\n                            ),\n                            core_schema.none_schema(),\n                        ]\n                    ),\n                )\n            ],\n        ),\n        ['foo'],\n    )\n    v = SchemaValidator(schema)\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python({'foo': 123})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'class_name': 'FooDataclass[dataclass_args_schema]'},\n            'input': 123,\n            'loc': ('foo', 'FooDataclass[cls_name]'),\n            'msg': 'Input should be a dictionary or an instance of FooDataclass[dataclass_args_schema]',\n            'type': 'dataclass_type',\n        },\n        {'input': 123, 'loc': ('foo', 'none'), 'msg': 'Input should be None', 'type': 'none_required'},\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005040", "source": "def as_dataclass_field(pydantic_field: FieldInfo) -> dataclasses.Field[Any]:\n    field_args: dict[str, Any] = {'default': pydantic_field}\n    if sys.version_info >= (3, 14) and pydantic_field.description is not None:\n        field_args['doc'] = pydantic_field.description\n    if sys.version_info >= (3, 10) and pydantic_field.kw_only:\n        field_args['kw_only'] = True\n    if pydantic_field.repr is not True:\n        field_args['repr'] = pydantic_field.repr\n    return dataclasses.field(**field_args)", "target": "def test_dataclass_json():\n    schema = core_schema.dataclass_schema(\n        FooDataclass,\n        core_schema.dataclass_args_schema(\n            'FooDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    assert v.validate_json('{\"a\": \"hello\", \"b\": true}') == FooDataclass(a='hello', b=True)\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_json('[\"a\", \"b\"]')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'class_name': 'FooDataclass'},\n            'input': ['a', 'b'],\n            'loc': (),\n            'msg': 'Input should be an object',\n            'type': 'dataclass_type',\n        }\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005041", "source": "def _validate_dataclass(cls: Type['DataclassT'], v: Any) -> 'DataclassT':\n    with set_validation(cls, True):\n        if isinstance(v, cls):\n            v.__pydantic_validate_values__()\n            return v\n        elif isinstance(v, (list, tuple)):\n            return cls(*v)\n        elif isinstance(v, dict):\n            return cls(**v)\n        else:\n            raise DataclassTypeError(class_name=cls.__name__)", "target": "def test_dataclass_field_before_validator():\n    @dataclasses.dataclass\n    class Foo:\n        a: int\n        b: str\n        @classmethod\n        def validate_b(cls, v: bytes, info: core_schema.ValidationInfo) -> bytes:\n            assert v == b'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return b'hello world!'\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(\n                    name='b',\n                    schema=core_schema.with_info_before_validator_function(Foo.validate_b, core_schema.str_schema()),\n                ),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 1, 'b': b'hello'})\n    assert dataclasses.asdict(foo) == {'a': 1, 'b': 'hello world!'}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005042", "source": "def dataclass(\n        _cls: type[_T],\n        *,\n        init: Literal[False] = False,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool | None = None,\n        config: ConfigDict | type[object] | None = None,\n        validate_on_init: bool | None = None,\n        kw_only: bool = ...,\n        slots: bool = ...,\n    ) -> type[PydanticDataclass]: ...", "target": "def test_by_alias_and_name_config_interaction(\n    config_by_alias: Union[bool, None],\n    config_by_name: Union[bool, None],\n    runtime_by_alias: Union[bool, None],\n    runtime_by_name: Union[bool, None],\n) -> None:\n    if config_by_alias is False and config_by_name is False and runtime_by_alias is False and runtime_by_name is False:\n        pytest.skip(\"Can't have both by_alias and by_name as effectively False\")\n    core_config = {\n        **({'validate_by_alias': config_by_alias} if config_by_alias is not None else {}),\n        **({'validate_by_name': config_by_name} if config_by_name is not None else {}),\n    }\n    @dataclasses.dataclass\n    class MyDataclass:\n        my_field: int\n    schema = core_schema.dataclass_schema(\n        MyDataclass,\n        core_schema.dataclass_args_schema(\n            'MyDataclass',\n            [\n                core_schema.dataclass_field(\n                    name='my_field', schema=core_schema.int_schema(), validation_alias='my_alias'\n                ),\n            ],\n        ),\n        ['my_field'],\n        config=core_schema.CoreConfig(**core_config),\n    )\n    s = SchemaValidator(schema)\n    alias_allowed = next(x for x in (runtime_by_alias, config_by_alias, True) if x is not None)\n    name_allowed = next(x for x in (runtime_by_name, config_by_name, False) if x is not None)\n    if alias_allowed:\n        assert dataclasses.asdict(\n            s.validate_python({'my_alias': 1}, by_alias=runtime_by_alias, by_name=runtime_by_name)\n        ) == {'my_field': 1}\n    if name_allowed:\n        assert dataclasses.asdict(\n            s.validate_python({'my_field': 1}, by_alias=runtime_by_alias, by_name=runtime_by_name)\n        ) == {'my_field': 1}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005043", "source": "def new_post_init(self: 'Dataclass', *args: Any, **kwargs: Any) -> None:\n            if config.post_init_call == 'before_validation':\n                post_init(self, *args, **kwargs)\n            if self.__class__.__pydantic_run_validation__:\n                self.__pydantic_validate_values__()\n                if hasattr(self, '__post_init_post_parse__'):\n                    self.__post_init_post_parse__(*args, **kwargs)\n            if config.post_init_call == 'after_validation':\n                post_init(self, *args, **kwargs)", "target": "def test_dataclass_subclass_strict_never_revalidate():\n    v = SchemaValidator(\n        core_schema.dataclass_schema(\n            FooDataclass,\n            core_schema.dataclass_args_schema(\n                'FooDataclass',\n                [\n                    core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                    core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n                ],\n            ),\n            ['a', 'b'],\n            revalidate_instances='never',\n            strict=True,\n        )\n    )\n    foo = FooDataclass(a='hello', b=True)\n    assert v.validate_python(foo) is foo\n    sub_foo = FooDataclassSame(a='hello', b=True)\n    assert v.validate_python(sub_foo) is sub_foo\n    with pytest.raises(ValidationError, match='Input should be an instance of FooDataclass'):\n        v.validate_python(ArgsKwargs((), {'a': 'hello', 'b': True}))"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005044", "source": "def get_config(config: Union[ConfigDict, Type[object], None]) -> Type[BaseConfig]:\n    if config is None:\n        return BaseConfig\n    else:\n        config_dict = (\n            config\n            if isinstance(config, dict)\n            else {k: getattr(config, k) for k in dir(config) if not k.startswith('__')}\n        )\n        class Config(BaseConfig):\n            ...\n        for k, v in config_dict.items():\n            setattr(Config, k, v)\n        return Config", "target": "def test_field_priority_arg():\n    v = SchemaValidator(cs.str_schema(max_length=5), config=CoreConfig(str_max_length=10))\n    assert 'max_length:Some(5)' in plain_repr(v)\n    assert v.isinstance_python('test') is True\n    assert v.isinstance_python('test long') is False"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005045", "source": "def _is_field_cached_property(obj: 'Dataclass', k: str) -> bool:\n        return False", "target": "def test_slots() -> None:\n    @dataclasses.dataclass(slots=True)\n    class Model:\n        x: int\n    schema = core_schema.dataclass_schema(\n        Model,\n        core_schema.dataclass_args_schema(\n            'Model', [core_schema.dataclass_field(name='x', schema=core_schema.int_schema())]\n        ),\n        ['x'],\n        slots=True,\n    )\n    val = SchemaValidator(schema)\n    m: Model\n    m = val.validate_python({'x': 123})\n    assert m == Model(x=123)\n    with pytest.raises(ValidationError):\n        val.validate_python({'x': 'abc'})\n    val.validate_assignment(m, 'x', 456)\n    assert m.x == 456\n    with pytest.raises(ValidationError):\n        val.validate_assignment(m, 'x', 'abc')"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005046", "source": "def _recreate_field_info(\n    field_info: FieldInfo,\n    ns_resolver: NsResolver,\n    typevars_map: Mapping[TypeVar, Any],\n    *,\n    lenient: bool,\n) -> FieldInfo:\n    FieldInfo_ = import_cached_field_info()\n    existing_desc = field_info.description\n    if lenient:\n        ann = _generics.replace_types(field_info._original_annotation, typevars_map)\n        ann, evaluated = _typing_extra.try_eval_type(\n            ann,\n            *ns_resolver.types_namespace,\n        )\n    else:\n        ann = _typing_extra.eval_type(\n            field_info._original_annotation,\n            *ns_resolver.types_namespace,\n        )\n        ann = _generics.replace_types(ann, typevars_map)\n        ann = _typing_extra.eval_type(\n            ann,\n            *ns_resolver.types_namespace,\n        )\n        evaluated = True\n    if (assign := field_info._original_assignment) is PydanticUndefined:\n        new_field = FieldInfo_.from_annotation(ann, _source=AnnotationSource.CLASS)\n    else:\n        new_field = FieldInfo_.from_annotated_attribute(ann, assign, _source=AnnotationSource.CLASS)\n        new_field._original_assignment = assign\n    new_field._original_annotation = ann\n    new_field.description = new_field.description if new_field.description is not None else existing_desc\n    if not evaluated:\n        new_field._complete = False\n    return new_field", "target": "def test_alias(py_and_json: PyAndJson):\n    v = py_and_json(\n        {\n            'type': 'model-fields',\n            'fields': {'field_a': {'validation_alias': 'FieldA', 'type': 'model-field', 'schema': {'type': 'int'}}},\n        }\n    )\n    assert v.validate_test({'FieldA': '123'}) == ({'field_a': 123}, None, {'field_a'})\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'foobar': '123'})\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'field_a': '123'})"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005047", "source": "def check_for_functorch():\n    try:\n        import functorch\n        return True\n    except ImportError:\n        return False", "target": "def test_merged_lastfailed_content_with_overlap(self) -> None:\n        last_failed_source = {\n            \"tools/tests/test_foo.py::test_num1\": True,\n            \"tools/tests/test_foo.py::test_num2\": True,\n            \"tools/tests/test_bar.py::test_num1\": True,\n        }\n        last_failed_dest = {\n            \"tools/tests/test_foo.py::test_num1\": True,\n            \"tools/tests/test_car.py::test_num1\": True,\n            \"tools/tests/test_car.py::test_num2\": True,\n        }\n        last_failed_merged = {\n            \"tools/tests/test_foo.py::test_num1\": True,\n            \"tools/tests/test_foo.py::test_num2\": True,\n            \"tools/tests/test_bar.py::test_num1\": True,\n            \"tools/tests/test_car.py::test_num1\": True,\n            \"tools/tests/test_car.py::test_num2\": True,\n        }\n        merged = _merged_lastfailed_content(last_failed_source, last_failed_dest)\n        self.assertEqual(merged, last_failed_merged)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005048", "source": "def isoformat(o: Union[datetime.date, datetime.time]) -> str:\n    return o.isoformat()", "target": "def test_cycle_same():\n    def fallback_func_passthrough(obj):\n        return obj\n    f = Foobar()\n    with pytest.raises(ValueError, match=r'Circular reference detected \\(id repeated\\)'):\n        to_jsonable_python(f, fallback=fallback_func_passthrough)\n    with pytest.raises(ValueError, match=r'Circular reference detected \\(id repeated\\)'):\n        to_json(f, fallback=fallback_func_passthrough)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005049", "source": "def _general_metadata_cls() -> type[BaseMetadata]:\n    from annotated_types import BaseMetadata\n    class _PydanticGeneralMetadata(PydanticMetadata, BaseMetadata):\n        def __init__(self, metadata: Any):\n            self.__dict__ = metadata\n    return _PydanticGeneralMetadata", "target": "def test_from_attributes_error_error():\n    class BadError(Exception):\n        def __str__(self):\n            raise RuntimeError('intentional error inside error')\n    class Foobar:\n        @property\n        def x(self):\n            raise BadError('intentional error')\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={'x': core_schema.model_field(schema=core_schema.int_schema())}, from_attributes=True\n        )\n    )\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python(Foobar())\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'get_attribute_error',\n            'loc': ('x',),\n            'msg': IsStr(regex=r'Error extracting attribute: \\S+\\.<locals>\\.BadError: <exception str\\(\\) failed>'),\n            'input': HasRepr(IsStr(regex='.+Foobar object at.+')),\n            'ctx': {'error': IsStr(regex=r'\\S+\\.<locals>\\.BadError: <exception str\\(\\) failed>')},\n        }\n    ]\n    class UnInitError:\n        @property\n        def x(self):\n            raise RuntimeError\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python(UnInitError())\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'get_attribute_error',\n            'loc': ('x',),\n            'msg': 'Error extracting attribute: RuntimeError',\n            'input': HasRepr(IsStr(regex='.+UnInitError object at.+')),\n            'ctx': {'error': 'RuntimeError'},\n        }\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005050", "source": "def env_str_field(\n    name: str,\n    default: str = \"\",\n) -> str:\n    return field(default_factory=lambda: get_env(name, default))", "target": "def test_get_env_not_exist_returns_default(self):\n        with patch.dict(os.environ, {\"FOO\": \"bar\"}, clear=True):\n            self.assertEqual(m.get_env(\"TEST_NOT_EXIST\", \"default\"), \"default\")"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005051", "source": "def dataclass(\n    _cls: Optional[Type[_T]] = None,\n    *,\n    init: bool = True,\n    repr: bool = True,\n    eq: bool = True,\n    order: bool = False,\n    unsafe_hash: bool = False,\n    frozen: bool = False,\n    config: Union[ConfigDict, Type[object], None] = None,\n    validate_on_init: Optional[bool] = None,\n    use_proxy: Optional[bool] = None,\n    kw_only: bool = False,\n) -> Union[Callable[[Type[_T]], 'DataclassClassOrWrapper'], 'DataclassClassOrWrapper']:\n    the_config = get_config(config)\n    def wrap(cls: Type[Any]) -> 'DataclassClassOrWrapper':\n        should_use_proxy = (\n            use_proxy\n            if use_proxy is not None\n            else (\n                is_builtin_dataclass(cls)\n                and (cls.__bases__[0] is object or set(dir(cls)) == set(dir(cls.__bases__[0])))\n            )\n        )\n        if should_use_proxy:\n            dc_cls_doc = ''\n            dc_cls = DataclassProxy(cls)\n            default_validate_on_init = False\n        else:\n            dc_cls_doc = cls.__doc__ or ''\n            if sys.version_info >= (3, 10):\n                dc_cls = dataclasses.dataclass(\n                    cls,\n                    init=init,\n                    repr=repr,\n                    eq=eq,\n                    order=order,\n                    unsafe_hash=unsafe_hash,\n                    frozen=frozen,\n                    kw_only=kw_only,\n                )\n            else:\n                dc_cls = dataclasses.dataclass(\n                    cls, init=init, repr=repr, eq=eq, order=order, unsafe_hash=unsafe_hash, frozen=frozen\n                )\n            default_validate_on_init = True\n        should_validate_on_init = default_validate_on_init if validate_on_init is None else validate_on_init\n        _add_pydantic_validation_attributes(cls, the_config, should_validate_on_init, dc_cls_doc)\n        dc_cls.__pydantic_model__.__try_update_forward_refs__(**{cls.__name__: cls})\n        return dc_cls\n    if _cls is None:\n        return wrap\n    return wrap(_cls)", "target": "def test_by_alias_and_name_config_interaction(config, runtime, expected) -> None:\n    @dataclasses.dataclass\n    class Foo:\n        my_field: str\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(\n                    name='my_field', schema=core_schema.str_schema(), serialization_alias='my_alias'\n                ),\n            ],\n        ),\n        ['my_field'],\n        config=core_schema.CoreConfig(serialize_by_alias=config or False),\n    )\n    s = SchemaSerializer(schema)\n    assert s.to_python(Foo(my_field='hello'), by_alias=runtime) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005052", "source": "def update_field_from_config(config_wrapper: ConfigWrapper, field_name: str, field_info: FieldInfo) -> None:\n    field_title_generator = field_info.field_title_generator or config_wrapper.field_title_generator\n    if field_title_generator is not None:\n        _apply_field_title_generator_to_field_info(field_title_generator, field_name, field_info)\n    if config_wrapper.alias_generator is not None:\n        _apply_alias_generator_to_field_info(config_wrapper.alias_generator, field_name, field_info)", "target": "def test_alias_extra_from_attributes():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            extra_behavior='allow',\n            from_attributes=True,\n            fields={\n                'field_a': core_schema.model_field(\n                    validation_alias=[['FieldA'], ['foo', 2]], schema=core_schema.int_schema()\n                )\n            },\n        )\n    )\n    assert v.validate_python({'FieldA': 1}) == ({'field_a': 1}, {}, {'field_a'})\n    assert v.validate_python(Cls(FieldA=1)) == ({'field_a': 1}, {}, {'field_a'})\n    assert v.validate_python(Cls(foo=[1, 2, 3])) == ({'field_a': 3}, {}, {'field_a'})\n    assert v.validate_python({'foo': [1, 2, 3]}) == ({'field_a': 3}, {}, {'field_a'})"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005053", "source": "def dataclass(\n        *,\n        init: bool = True,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool = False,\n        config: Union[ConfigDict, Type[object], None] = None,\n        validate_on_init: Optional[bool] = None,\n        use_proxy: Optional[bool] = None,\n    ) -> Callable[[Type[_T]], 'DataclassClassOrWrapper']:\n        ...", "target": "def test_by_alias_and_name_config_interaction(config, runtime, expected) -> None:\n    @dataclasses.dataclass\n    class Foo:\n        my_field: str\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(\n                    name='my_field', schema=core_schema.str_schema(), serialization_alias='my_alias'\n                ),\n            ],\n        ),\n        ['my_field'],\n        config=core_schema.CoreConfig(serialize_by_alias=config or False),\n    )\n    s = SchemaSerializer(schema)\n    assert s.to_python(Foo(my_field='hello'), by_alias=runtime) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005054", "source": "def dataclass(\n        _cls: type[_T],\n        *,\n        init: Literal[False] = False,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool | None = None,\n        config: ConfigDict | type[object] | None = None,\n        validate_on_init: bool | None = None,\n    ) -> type[PydanticDataclass]: ...", "target": "def test_dataclass_field_wrap_validator2():\n    @dataclasses.dataclass\n    class Foo:\n        a: int\n        b: str\n        @classmethod\n        def validate_b(\n            cls, v: bytes, nxt: core_schema.ValidatorFunctionWrapHandler, info: core_schema.ValidationInfo\n        ) -> bytes:\n            assert v == b'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return nxt(b'hello world!')\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(\n                    name='b',\n                    schema=core_schema.with_info_wrap_validator_function(Foo.validate_b, core_schema.str_schema()),\n                ),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 1, 'b': b'hello'})\n    assert dataclasses.asdict(foo) == {'a': 1, 'b': 'hello world!'}"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005055", "source": "def rounded_linspace(low, high, steps, div):\n    ret = torch.linspace(low, high, steps)\n    ret = (ret.int() + div - 1) // div * div\n    ret = torch.unique(ret)\n    return list(map(int, ret))", "target": "def test_merged_lastfailed_content_with_empty_source(self) -> None:\n        last_failed_source = {\n            \"\": True,\n        }\n        last_failed_dest = {\n            \"tools/tests/test_car.py::test_num1\": True,\n            \"tools/tests/test_car.py::test_num2\": True,\n        }\n        last_failed_merged = {\n            \"tools/tests/test_car.py::test_num1\": True,\n            \"tools/tests/test_car.py::test_num2\": True,\n        }\n        merged = _merged_lastfailed_content(last_failed_source, last_failed_dest)\n        self.assertEqual(merged, last_failed_merged)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005056", "source": "def collect_dataclass_fields(\n    cls: type[StandardDataclass],\n    *,\n    config_wrapper: ConfigWrapper,\n    ns_resolver: NsResolver | None = None,\n    typevars_map: dict[Any, Any] | None = None,\n) -> dict[str, FieldInfo]:\n    FieldInfo_ = import_cached_field_info()\n    fields: dict[str, FieldInfo] = {}\n    ns_resolver = ns_resolver or NsResolver()\n    dataclass_fields = cls.__dataclass_fields__\n    for base in reversed(cls.__mro__):\n        if not dataclasses.is_dataclass(base):\n            continue\n        with ns_resolver.push(base):\n            for ann_name, dataclass_field in dataclass_fields.items():\n                base_anns = _typing_extra.safe_get_annotations(base)\n                if ann_name not in base_anns:\n                    continue\n                globalns, localns = ns_resolver.types_namespace\n                ann_type, evaluated = _typing_extra.try_eval_type(dataclass_field.type, globalns, localns)\n                if _typing_extra.is_classvar_annotation(ann_type):\n                    continue\n                if (\n                    not dataclass_field.init\n                    and dataclass_field.default is dataclasses.MISSING\n                    and dataclass_field.default_factory is dataclasses.MISSING\n                ):\n                    continue\n                if isinstance(dataclass_field.default, FieldInfo_):\n                    if dataclass_field.default.init_var:\n                        if dataclass_field.default.init is False:\n                            raise PydanticUserError(\n                                f'Dataclass field {ann_name} has init=False and init_var=True, but these are mutually exclusive.',\n                                code='clashing-init-and-init-var',\n                            )\n                        continue\n                    field_info = FieldInfo_.from_annotated_attribute(\n                        ann_type, dataclass_field.default, _source=AnnotationSource.DATACLASS\n                    )\n                    field_info._original_assignment = dataclass_field.default\n                else:\n                    field_info = FieldInfo_.from_annotated_attribute(\n                        ann_type, dataclass_field, _source=AnnotationSource.DATACLASS\n                    )\n                    field_info._original_assignment = dataclass_field\n                if not evaluated:\n                    field_info._complete = False\n                    field_info._original_annotation = ann_type\n                fields[ann_name] = field_info\n                update_field_from_config(config_wrapper, ann_name, field_info)\n                if field_info.default is not PydanticUndefined and isinstance(\n                    getattr(cls, ann_name, field_info), FieldInfo_\n                ):\n                    setattr(cls, ann_name, field_info.default)\n    if typevars_map:\n        for field in fields.values():\n            field.apply_typevars_map(typevars_map)\n    if config_wrapper.use_attribute_docstrings:\n        _update_fields_from_docstrings(\n            cls,\n            fields,\n            use_inspect=not hasattr(cls, '__is_pydantic_dataclass__'),\n        )\n    return fields", "target": "def test_from_attributes_error_error():\n    class BadError(Exception):\n        def __str__(self):\n            raise RuntimeError('intentional error inside error')\n    class Foobar:\n        @property\n        def x(self):\n            raise BadError('intentional error')\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={'x': core_schema.model_field(schema=core_schema.int_schema())}, from_attributes=True\n        )\n    )\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python(Foobar())\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'get_attribute_error',\n            'loc': ('x',),\n            'msg': IsStr(regex=r'Error extracting attribute: \\S+\\.<locals>\\.BadError: <exception str\\(\\) failed>'),\n            'input': HasRepr(IsStr(regex='.+Foobar object at.+')),\n            'ctx': {'error': IsStr(regex=r'\\S+\\.<locals>\\.BadError: <exception str\\(\\) failed>')},\n        }\n    ]\n    class UnInitError:\n        @property\n        def x(self):\n            raise RuntimeError\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python(UnInitError())\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'get_attribute_error',\n            'loc': ('x',),\n            'msg': 'Error extracting attribute: RuntimeError',\n            'input': HasRepr(IsStr(regex='.+UnInitError object at.+')),\n            'ctx': {'error': 'RuntimeError'},\n        }\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005057", "source": "def _is_field_cached_property(obj: 'Dataclass', k: str) -> bool:\n        return isinstance(getattr(type(obj), k, None), cached_property)", "target": "def test_dataclass_field_wrap_validator1():\n    @dataclasses.dataclass\n    class Foo:\n        a: int\n        b: str\n        @classmethod\n        def validate_b(\n            cls, v: bytes, nxt: core_schema.ValidatorFunctionWrapHandler, info: core_schema.ValidationInfo\n        ) -> str:\n            assert v == b'hello'\n            v = nxt(v)\n            assert v == 'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return 'hello world!'\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(\n                    name='b',\n                    schema=core_schema.with_info_wrap_validator_function(Foo.validate_b, core_schema.str_schema()),\n                ),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 1, 'b': b'hello'})\n    assert dataclasses.asdict(foo) == {'a': 1, 'b': 'hello world!'}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005058", "source": "def _get_validators(cls: 'DataclassClassOrWrapper') -> 'CallableGenerator':\n    yield cls.__validate__", "target": "def test_dataclass_slots_field_after_validator():\n    @dataclasses.dataclass(slots=True)\n    class Foo:\n        a: int\n        b: str\n        @classmethod\n        def validate_b(cls, v: str, info: core_schema.ValidationInfo) -> str:\n            assert v == 'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return 'hello world!'\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(\n                    name='b',\n                    schema=core_schema.with_info_after_validator_function(Foo.validate_b, core_schema.str_schema()),\n                ),\n            ],\n        ),\n        ['a', 'b'],\n        slots=True,\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 1, 'b': b'hello'})\n    assert dataclasses.asdict(foo) == {'a': 1, 'b': 'hello world!'}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005059", "source": "def dataclass(\n    _cls: Optional[Type[_T]] = None,\n    *,\n    init: bool = True,\n    repr: bool = True,\n    eq: bool = True,\n    order: bool = False,\n    unsafe_hash: bool = False,\n    frozen: bool = False,\n    config: Union[ConfigDict, Type[object], None] = None,\n    validate_on_init: Optional[bool] = None,\n    use_proxy: Optional[bool] = None,\n    kw_only: bool = False,\n) -> Union[Callable[[Type[_T]], 'DataclassClassOrWrapper'], 'DataclassClassOrWrapper']:\n    the_config = get_config(config)\n    def wrap(cls: Type[Any]) -> 'DataclassClassOrWrapper':\n        should_use_proxy = (\n            use_proxy\n            if use_proxy is not None\n            else (\n                is_builtin_dataclass(cls)\n                and (cls.__bases__[0] is object or set(dir(cls)) == set(dir(cls.__bases__[0])))\n            )\n        )\n        if should_use_proxy:\n            dc_cls_doc = ''\n            dc_cls = DataclassProxy(cls)\n            default_validate_on_init = False\n        else:\n            dc_cls_doc = cls.__doc__ or ''\n            if sys.version_info >= (3, 10):\n                dc_cls = dataclasses.dataclass(\n                    cls,\n                    init=init,\n                    repr=repr,\n                    eq=eq,\n                    order=order,\n                    unsafe_hash=unsafe_hash,\n                    frozen=frozen,\n                    kw_only=kw_only,\n                )\n            else:\n                dc_cls = dataclasses.dataclass(\n                    cls, init=init, repr=repr, eq=eq, order=order, unsafe_hash=unsafe_hash, frozen=frozen\n                )\n            default_validate_on_init = True\n        should_validate_on_init = default_validate_on_init if validate_on_init is None else validate_on_init\n        _add_pydantic_validation_attributes(cls, the_config, should_validate_on_init, dc_cls_doc)\n        dc_cls.__pydantic_model__.__try_update_forward_refs__(**{cls.__name__: cls})\n        return dc_cls\n    if _cls is None:\n        return wrap\n    return wrap(_cls)", "target": "def test_aliases(py_and_json: PyAndJson):\n    schema = core_schema.dataclass_args_schema(\n        'MyDataclass',\n        [\n            core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), validation_alias='Apple'),\n            core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), validation_alias=['Banana', 1]),\n            core_schema.dataclass_field(\n                name='c', schema=core_schema.int_schema(), validation_alias=['Carrot', 'v'], init_only=True\n            ),\n        ],\n        collect_init_only=True,\n    )\n    v = py_and_json(schema)\n    assert v.validate_test({'Apple': 'a', 'Banana': ['x', 'false'], 'Carrot': {'v': '42'}}) == (\n        {'a': 'a', 'b': False},\n        (42,),\n    )"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005060", "source": "def get_field_info(cls, name: str) -> Dict[str, Any]:\n        fields_value = cls.fields.get(name)\n        if isinstance(fields_value, str):\n            field_info: Dict[str, Any] = {'alias': fields_value}\n        elif isinstance(fields_value, dict):\n            field_info = fields_value\n        else:\n            field_info = {}\n        if 'alias' in field_info:\n            field_info.setdefault('alias_priority', 2)\n        if field_info.get('alias_priority', 0) <= 1 and cls.alias_generator:\n            alias = cls.alias_generator(name)\n            if not isinstance(alias, str):\n                raise TypeError(f'Config.alias_generator must return str, not {alias.__class__}')\n            field_info.update(alias=alias, alias_priority=1)\n        return field_info", "target": "def test_field_priority_arg():\n    v = SchemaValidator(cs.str_schema(max_length=5), config=CoreConfig(str_max_length=10))\n    assert 'max_length:Some(5)' in plain_repr(v)\n    assert v.isinstance_python('test') is True\n    assert v.isinstance_python('test long') is False"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005061", "source": "def rebuild_dataclass(\n    cls: type[PydanticDataclass],\n    *,\n    force: bool = False,\n    raise_errors: bool = True,\n    _parent_namespace_depth: int = 2,\n    _types_namespace: MappingNamespace | None = None,\n) -> bool | None:\n    if not force and cls.__pydantic_complete__:\n        return None\n    for attr in ('__pydantic_core_schema__', '__pydantic_validator__', '__pydantic_serializer__'):\n        if attr in cls.__dict__ and not isinstance(getattr(cls, attr), _mock_val_ser.MockValSer):\n            delattr(cls, attr)\n    cls.__pydantic_complete__ = False\n    if _types_namespace is not None:\n        rebuild_ns = _types_namespace\n    elif _parent_namespace_depth > 0:\n        rebuild_ns = _typing_extra.parent_frame_namespace(parent_depth=_parent_namespace_depth, force=True) or {}\n    else:\n        rebuild_ns = {}\n    ns_resolver = _namespace_utils.NsResolver(\n        parent_namespace=rebuild_ns,\n    )\n    return _pydantic_dataclasses.complete_dataclass(\n        cls,\n        _config.ConfigWrapper(cls.__pydantic_config__, check=False),\n        raise_errors=raise_errors,\n        ns_resolver=ns_resolver,\n        _force_build=True,\n    )", "target": "def test_frozen():\n    @dataclasses.dataclass\n    class MyModel:\n        f: str\n    v = SchemaValidator(\n        core_schema.dataclass_schema(\n            MyModel,\n            core_schema.dataclass_args_schema('MyModel', [core_schema.dataclass_field('f', core_schema.str_schema())]),\n            ['f'],\n            frozen=True,\n        )\n    )\n    m = v.validate_python({'f': 'x'})\n    assert m.f == 'x'\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_assignment(m, 'f', 'y')\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'frozen_instance', 'loc': (), 'msg': 'Instance is frozen', 'input': 'y'}\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005062", "source": "def _set_nested_attr(obj: nn.Module, names: list[str], value: Tensor) -> None:\n    if len(names) == 1:\n        setattr(obj, names[0], value)\n    else:\n        _set_nested_attr(getattr(obj, names[0]), names[1:], value)", "target": "def test_simple(self) -> None:\n        @retries_decorator()\n        def foo(x: int, y: int) -> int:\n            return x + y\n        self.assertEqual(foo(3, 4), 7)"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005063", "source": "def env_str_field(\n    name: str,\n    default: str = \"\",\n) -> str:\n    return field(default_factory=lambda: get_env(name, default))", "target": "def test_env_path_optional_respects_resolve_false(self):\n        with patch.dict(os.environ, {\"P\": \"rel/dir\"}, clear=True):\n            p = m.env_path_optional(\"P\", resolve=False)\n            self.assertEqual(p, Path(\"rel/dir\"))\n            if p:\n                self.assertFalse(p.is_absolute())"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005064", "source": "def wrap(cls: Type[Any]) -> 'DataclassClassOrWrapper':\n        should_use_proxy = (\n            use_proxy\n            if use_proxy is not None\n            else (\n                is_builtin_dataclass(cls)\n                and (cls.__bases__[0] is object or set(dir(cls)) == set(dir(cls.__bases__[0])))\n            )\n        )\n        if should_use_proxy:\n            dc_cls_doc = ''\n            dc_cls = DataclassProxy(cls)\n            default_validate_on_init = False\n        else:\n            dc_cls_doc = cls.__doc__ or ''\n            if sys.version_info >= (3, 10):\n                dc_cls = dataclasses.dataclass(\n                    cls,\n                    init=init,\n                    repr=repr,\n                    eq=eq,\n                    order=order,\n                    unsafe_hash=unsafe_hash,\n                    frozen=frozen,\n                    kw_only=kw_only,\n                )\n            else:\n                dc_cls = dataclasses.dataclass(\n                    cls, init=init, repr=repr, eq=eq, order=order, unsafe_hash=unsafe_hash, frozen=frozen\n                )\n            default_validate_on_init = True\n        should_validate_on_init = default_validate_on_init if validate_on_init is None else validate_on_init\n        _add_pydantic_validation_attributes(cls, the_config, should_validate_on_init, dc_cls_doc)\n        dc_cls.__pydantic_model__.__try_update_forward_refs__(**{cls.__name__: cls})\n        return dc_cls", "target": "def test_dataclass_field_before_validator():\n    @dataclasses.dataclass\n    class Foo:\n        a: int\n        b: str\n        @classmethod\n        def validate_b(cls, v: bytes, info: core_schema.ValidationInfo) -> bytes:\n            assert v == b'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return b'hello world!'\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(\n                    name='b',\n                    schema=core_schema.with_info_before_validator_function(Foo.validate_b, core_schema.str_schema()),\n                ),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 1, 'b': b'hello'})\n    assert dataclasses.asdict(foo) == {'a': 1, 'b': 'hello world!'}"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005065", "source": "def _flatten(\n    key_prefix: Label, sub_schema: Definition, result: FlatIntermediateDefinition\n) -> None:\n    for k, value in sub_schema.items():\n        if isinstance(k, tuple):\n            assert all(isinstance(ki, str) for ki in k)\n            key_suffix: Label = k\n        elif k is None:\n            key_suffix = ()\n        else:\n            assert isinstance(k, str)\n            key_suffix = (k,)\n        key: Label = key_prefix + key_suffix\n        if isinstance(value, (TimerArgs, GroupedBenchmark)):\n            assert key not in result, f\"duplicate key: {key}\"\n            result[key] = value\n        else:\n            assert isinstance(value, dict)\n            _flatten(key_prefix=key, sub_schema=value, result=result)", "target": "def test_merged_lastfailed_content_with_overlap(self) -> None:\n        last_failed_source = {\n            \"tools/tests/test_foo.py::test_num1\": True,\n            \"tools/tests/test_foo.py::test_num2\": True,\n            \"tools/tests/test_bar.py::test_num1\": True,\n        }\n        last_failed_dest = {\n            \"tools/tests/test_foo.py::test_num1\": True,\n            \"tools/tests/test_car.py::test_num1\": True,\n            \"tools/tests/test_car.py::test_num2\": True,\n        }\n        last_failed_merged = {\n            \"tools/tests/test_foo.py::test_num1\": True,\n            \"tools/tests/test_foo.py::test_num2\": True,\n            \"tools/tests/test_bar.py::test_num1\": True,\n            \"tools/tests/test_car.py::test_num1\": True,\n            \"tools/tests/test_car.py::test_num2\": True,\n        }\n        merged = _merged_lastfailed_content(last_failed_source, last_failed_dest)\n        self.assertEqual(merged, last_failed_merged)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005066", "source": "def dataclass(\n    _cls: type[_T] | None = None,\n    *,\n    init: Literal[False] = False,\n    repr: bool = True,\n    eq: bool = True,\n    order: bool = False,\n    unsafe_hash: bool = False,\n    frozen: bool | None = None,\n    config: ConfigDict | type[object] | None = None,\n    validate_on_init: bool | None = None,\n    kw_only: bool = False,\n    slots: bool = False,\n) -> Callable[[type[_T]], type[PydanticDataclass]] | type[PydanticDataclass]:\n    assert init is False, 'pydantic.dataclasses.dataclass only supports init=False'\n    assert validate_on_init is not False, 'validate_on_init=False is no longer supported'\n    if sys.version_info >= (3, 10):\n        kwargs = {'kw_only': kw_only, 'slots': slots}\n    else:\n        kwargs = {}\n    def create_dataclass(cls: type[Any]) -> type[PydanticDataclass]:\n        from ._internal._utils import is_model_class\n        if is_model_class(cls):\n            raise PydanticUserError(\n                f'Cannot create a Pydantic dataclass from {cls.__name__} as it is already a Pydantic model',\n                code='dataclass-on-model',\n            )\n        original_cls = cls\n        has_dataclass_base = any(dataclasses.is_dataclass(base) for base in cls.__bases__)\n        if not has_dataclass_base and config is not None and hasattr(cls, '__pydantic_config__'):\n            warn(\n                f'`config` is set via both the `dataclass` decorator and `__pydantic_config__` for dataclass {cls.__name__}. '\n                f'The `config` specification from `dataclass` decorator will take priority.',\n                category=UserWarning,\n                stacklevel=2,\n            )\n        config_dict = config if config is not None else getattr(cls, '__pydantic_config__', None)\n        config_wrapper = _config.ConfigWrapper(config_dict)\n        decorators = _decorators.DecoratorInfos.build(cls, replace_wrapped_methods=True)\n        decorators.update_from_config(config_wrapper)\n        original_doc = cls.__doc__\n        if _pydantic_dataclasses.is_stdlib_dataclass(cls):\n            original_doc = None\n            bases = (cls,)\n            if issubclass(cls, Generic):\n                generic_base = Generic[cls.__parameters__]\n                bases = bases + (generic_base,)\n            cls = types.new_class(cls.__name__, bases)\n        if frozen is not None:\n            frozen_ = frozen\n            if config_wrapper.frozen:\n                warn(\n                    f'`frozen` is set via both the `dataclass` decorator and `config` for dataclass {cls.__name__!r}.'\n                    'This is not recommended. The `frozen` specification on `dataclass` will take priority.',\n                    category=UserWarning,\n                    stacklevel=2,\n                )\n        else:\n            frozen_ = config_wrapper.frozen or False\n        cls_anns = _typing_extra.safe_get_annotations(cls)\n        for field_name in cls_anns:\n            field_value = getattr(cls, field_name, None)\n            if isinstance(field_value, FieldInfo):\n                setattr(cls, field_name, _pydantic_dataclasses.as_dataclass_field(field_value))\n        with _pydantic_dataclasses.patch_base_fields(cls):\n            cls = dataclasses.dataclass(\n                cls,\n                init=True,\n                repr=repr,\n                eq=eq,\n                order=order,\n                unsafe_hash=unsafe_hash,\n                frozen=frozen_,\n                **kwargs,\n            )\n        if config_wrapper.validate_assignment:\n            original_setattr = cls.__setattr__\n            @functools.wraps(cls.__setattr__)\n            def validated_setattr(instance: PydanticDataclass, name: str, value: Any, /) -> None:\n                if frozen_:\n                    return original_setattr(instance, name, value)\n                inst_cls = type(instance)\n                attr = getattr(inst_cls, name, None)\n                if isinstance(attr, property):\n                    attr.__set__(instance, value)\n                elif isinstance(attr, functools.cached_property):\n                    instance.__dict__.__setitem__(name, value)\n                else:\n                    inst_cls.__pydantic_validator__.validate_assignment(instance, name, value)\n            cls.__setattr__ = validated_setattr.__get__(None, cls)\n            if slots and not hasattr(cls, '__setstate__'):\n                def _dataclass_getstate(self: Any) -> list[Any]:\n                    return [getattr(self, f.name) for f in dataclasses.fields(self)]\n                def _dataclass_setstate(self: Any, state: list[Any]) -> None:\n                    for field, value in zip(dataclasses.fields(self), state):\n                        object.__setattr__(self, field.name, value)\n                cls.__getstate__ = _dataclass_getstate\n                cls.__setstate__ = _dataclass_setstate\n        cls.__is_pydantic_dataclass__ = True\n        cls.__pydantic_decorators__ = decorators\n        cls.__doc__ = original_doc\n        firstlineno = getattr(original_cls, '__firstlineno__', None)\n        cls.__module__ = original_cls.__module__\n        if sys.version_info >= (3, 13) and firstlineno is not None:\n            original_cls.__firstlineno__ = firstlineno\n            cls.__firstlineno__ = firstlineno\n        cls.__qualname__ = original_cls.__qualname__\n        cls.__pydantic_fields_complete__ = classmethod(_pydantic_fields_complete)\n        cls.__pydantic_complete__ = False\n        _pydantic_dataclasses.complete_dataclass(cls, config_wrapper, raise_errors=False)\n        return cls\n    return create_dataclass if _cls is None else create_dataclass(_cls)", "target": "def test_dataclass_field_plain_validator():\n    @dataclasses.dataclass\n    class Foo:\n        a: int\n        b: str\n        @classmethod\n        def validate_b(cls, v: bytes, info: core_schema.ValidationInfo) -> str:\n            assert v == b'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return 'hello world!'\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(\n                    name='b', schema=core_schema.with_info_plain_validator_function(Foo.validate_b)\n                ),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 1, 'b': b'hello'})\n    assert dataclasses.asdict(foo) == {'a': 1, 'b': 'hello world!'}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005067", "source": "def get_args(tp: Type[Any]) -> Tuple[Any, ...]:\n        if type(tp).__name__ in AnnotatedTypeNames:\n            return tp.__args__ + tp.__metadata__\n        return _typing_get_args(tp) or getattr(tp, '__args__', ()) or _generic_get_args(tp)", "target": "def test_correct_function_signature() -> None:\n    def my_validator(value: Any, info: Any) -> str:\n        return str(value)\n    v = SchemaValidator(core_schema.with_info_plain_validator_function(my_validator))\n    assert v.validate_python(1) == '1'"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005068", "source": "def _apply_alias_generator_to_field_info(\n    alias_generator: Callable[[str], str] | AliasGenerator, field_name: str, field_info: FieldInfo\n):\n    if (\n        field_info.alias_priority is None\n        or field_info.alias_priority <= 1\n        or field_info.alias is None\n        or field_info.validation_alias is None\n        or field_info.serialization_alias is None\n    ):\n        alias, validation_alias, serialization_alias = None, None, None\n        if isinstance(alias_generator, AliasGenerator):\n            alias, validation_alias, serialization_alias = alias_generator.generate_aliases(field_name)\n        elif callable(alias_generator):\n            alias = alias_generator(field_name)\n            if not isinstance(alias, str):\n                raise TypeError(f'alias_generator {alias_generator} must return str, not {alias.__class__}')\n        if field_info.alias_priority is None or field_info.alias_priority <= 1:\n            field_info.alias_priority = 1\n        if field_info.alias_priority == 1:\n            field_info.serialization_alias = get_first_not_none(serialization_alias, alias)\n            field_info.validation_alias = get_first_not_none(validation_alias, alias)\n            field_info.alias = alias\n        if field_info.alias is None:\n            field_info.alias = alias\n        if field_info.serialization_alias is None:\n            field_info.serialization_alias = get_first_not_none(serialization_alias, alias)\n        if field_info.validation_alias is None:\n            field_info.validation_alias = get_first_not_none(validation_alias, alias)", "target": "def test_ignore_extra():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'field_a': core_schema.model_field(schema=core_schema.str_schema()),\n                'field_b': core_schema.model_field(schema=core_schema.int_schema()),\n            }\n        )\n    )\n    assert v.validate_python({'field_a': b'123', 'field_b': 1, 'field_c': 123}) == (\n        {'field_a': '123', 'field_b': 1},\n        None,\n        {'field_b', 'field_a'},\n    )"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005069", "source": "def run_llama2_7b_int8(device: str = \"cuda\"):\n    model = GPTModelConfig(\n        \"Llama-2-7b-chat-hf\",\n        LLaMA,\n        \"int8\",\n        LLaMAWeightOnlyInt8QuantHandler,\n        144,\n        957,\n        136,\n    )\n    token_per_sec, memory_bandwidth, compilation_time = run_experiment(\n        model, device=device\n    )\n    return [\n        Experiment(\n            model.name,\n            \"token_per_sec\",\n            model.token_per_sec,\n            f\"{token_per_sec:.02f}\",\n            model.mode,\n            device,\n            get_arch_name(),\n            True,\n        ),\n        Experiment(\n            model.name,\n            \"memory_bandwidth(GB/s)\",\n            model.memory_bandwidth,\n            f\"{memory_bandwidth:.02f}\",\n            model.mode,\n            device,\n            get_arch_name(),\n            True,\n        ),\n        Experiment(\n            model.name,\n            \"compilation_time(s)\",\n            model.compilation_time,\n            f\"{compilation_time:.02f}\",\n            model.mode,\n            device,\n            get_arch_name(),\n            True,\n        ),\n    ]", "target": "def testAliasWithOffset(self) -> list[Tensor]:\n        x = torch.tensor([100, 200])\n        a = [x[0], x[1]]\n        return a"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005070", "source": "def _update_fields_from_docstrings(cls: type[Any], fields: dict[str, FieldInfo], use_inspect: bool = False) -> None:\n    fields_docs = extract_docstrings_from_cls(cls, use_inspect=use_inspect)\n    for ann_name, field_info in fields.items():\n        if field_info.description is None and ann_name in fields_docs:\n            field_info.description = fields_docs[ann_name]", "target": "def test_from_attributes_path(input_value, expected):\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'my_field': core_schema.model_field(\n                    validation_alias=[['foo', 'bar', 'bat'], ['foo', 3], ['spam']], schema=core_schema.int_schema()\n                )\n            },\n            from_attributes=True,\n        ),\n        config=CoreConfig(loc_by_alias=False),\n    )\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=expected.message):\n            val = v.validate_python(input_value)\n            print(f'UNEXPECTED OUTPUT: {val!r}')\n    else:\n        model_dict, model_extra, fields_set = v.validate_python(input_value)\n        assert model_dict == expected\n        assert model_extra is None\n        assert fields_set == {'my_field'}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005071", "source": "def _general_metadata_cls() -> type[BaseMetadata]:\n    from annotated_types import BaseMetadata\n    class _PydanticGeneralMetadata(PydanticMetadata, BaseMetadata):\n        def __init__(self, metadata: Any):\n            self.__dict__ = metadata\n    return _PydanticGeneralMetadata", "target": "def test_with_default_factory():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'x': core_schema.model_field(\n                    schema=core_schema.with_default_schema(\n                        schema=core_schema.str_schema(), default_factory=lambda: 'pikachu'\n                    )\n                )\n            }\n        )\n    )\n    assert v.validate_python({}) == ({'x': 'pikachu'}, None, set())\n    assert v.validate_python({'x': 'bulbi'}) == ({'x': 'bulbi'}, None, {'x'})"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005072", "source": "def _call_initvar(*args: Any, **kwargs: Any) -> NoReturn:\n        raise TypeError(\"'InitVar' object is not callable\")", "target": "def test_dataclass_args(py_and_json: PyAndJson, input_value, expected):\n    schema = core_schema.dataclass_args_schema(\n        'MyDataclass',\n        [\n            core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), kw_only=False),\n            core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), kw_only=False),\n        ],\n    )\n    v = py_and_json(schema)\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)) as exc_info:\n            v.validate_test(input_value)\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        assert v.validate_test(input_value) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005073", "source": "def _get_validators(cls: 'DataclassClassOrWrapper') -> 'CallableGenerator':\n    yield cls.__validate__", "target": "def test_slots() -> None:\n    @dataclasses.dataclass(slots=True)\n    class Model:\n        x: int\n    schema = core_schema.dataclass_schema(\n        Model,\n        core_schema.dataclass_args_schema(\n            'Model', [core_schema.dataclass_field(name='x', schema=core_schema.int_schema())]\n        ),\n        ['x'],\n        slots=True,\n    )\n    val = SchemaValidator(schema)\n    m: Model\n    m = val.validate_python({'x': 123})\n    assert m == Model(x=123)\n    with pytest.raises(ValidationError):\n        val.validate_python({'x': 'abc'})\n    val.validate_assignment(m, 'x', 456)\n    assert m.x == 456\n    with pytest.raises(ValidationError):\n        val.validate_assignment(m, 'x', 'abc')"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005074", "source": "def dataclass(\n        _cls: type[_T],\n        *,\n        init: Literal[False] = False,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool | None = None,\n        config: ConfigDict | type[object] | None = None,\n        validate_on_init: bool | None = None,\n        kw_only: bool = ...,\n        slots: bool = ...,\n    ) -> type[PydanticDataclass]: ...", "target": "def test_dataclass_post_init_args_multiple():\n    dc_args = None\n    @dataclasses.dataclass\n    class Foo:\n        a: str\n        b: dataclasses.InitVar[bool]\n        c: dataclasses.InitVar[int]\n        def __post_init__(self, *args):\n            nonlocal dc_args\n            dc_args = args\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), init_only=True),\n                core_schema.dataclass_field(name='c', schema=core_schema.int_schema(), init_only=True),\n            ],\n            collect_init_only=True,\n        ),\n        ['a', 'b'],\n        post_init=True,\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': b'hello', 'b': 'true', 'c': '42'})\n    assert dataclasses.asdict(foo) == {'a': 'hello'}\n    assert dc_args == (True, 42)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005075", "source": "def is_valid_field_name(name: str) -> bool:\n    return not name.startswith('_')", "target": "def test_on_error_default(self, py_and_json: PyAndJson):\n        v = py_and_json(\n            {\n                'type': 'model-fields',\n                'fields': {\n                    'x': {\n                        'type': 'model-field',\n                        'schema': {\n                            'type': 'default',\n                            'schema': {'type': 'str'},\n                            'on_error': 'default',\n                            'default': 'pika',\n                        },\n                    }\n                },\n            }\n        )\n        assert v.validate_test({'x': 'foo'}) == ({'x': 'foo'}, None, {'x'})\n        assert v.validate_test({'x': ['foo']}) == ({'x': 'pika'}, None, {'x'})"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005076", "source": "def show_ref(self, name: str) -> str:\n        refs = self._run_git(\"show-ref\", \"-s\", name).strip().split(\"\\n\")\n        if not all(refs[i] == refs[0] for i in range(1, len(refs))):\n            raise RuntimeError(f\"reference {name} is ambiguous\")\n        return refs[0]", "target": "def test_double_asterisks(self) -> None:\n        allowed_patterns = [\n            \"aten/src/ATen/native/**LinearAlgebra*\",\n        ]\n        patterns_re = patterns_to_regex(allowed_patterns)\n        fnames = [\n            \"aten/src/ATen/native/LinearAlgebra.cpp\",\n            \"aten/src/ATen/native/cpu/LinearAlgebraKernel.cpp\",\n        ]\n        for filename in fnames:\n            self.assertTrue(patterns_re.match(filename))"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005077", "source": "def _check_classvar(v: Optional[Type[Any]]) -> bool:\n    if v is None:\n        return False\n    return v.__class__ == ClassVar.__class__ and getattr(v, '_name', None) == 'ClassVar'", "target": "def test_ser_function_wrap():\n    def f(\n        input: Any, serialize: core_schema.SerializerFunctionWrapHandler, info: core_schema.SerializationInfo, /\n    ) -> str:\n        return f'{serialize} {info}'\n    s = SchemaSerializer(\n        core_schema.any_schema(\n            serialization=core_schema.wrap_serializer_function_ser_schema(\n                f, info_arg=True, schema=core_schema.str_schema(), when_used='json'\n            )\n        )\n    )\n    assert s.to_python(123, mode='json') == (\n        'SerializationCallable(serializer=str) '\n        \"SerializationInfo(include=None, exclude=None, context=None, mode='json', by_alias=False, exclude_unset=False, \"\n        'exclude_defaults=False, exclude_none=False, exclude_computed_fields=False, round_trip=False, serialize_as_any=False)'\n    )"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005078", "source": "def dataclass(\n    _cls: type[_T] | None = None,\n    *,\n    init: Literal[False] = False,\n    repr: bool = True,\n    eq: bool = True,\n    order: bool = False,\n    unsafe_hash: bool = False,\n    frozen: bool | None = None,\n    config: ConfigDict | type[object] | None = None,\n    validate_on_init: bool | None = None,\n    kw_only: bool = False,\n    slots: bool = False,\n) -> Callable[[type[_T]], type[PydanticDataclass]] | type[PydanticDataclass]:\n    assert init is False, 'pydantic.dataclasses.dataclass only supports init=False'\n    assert validate_on_init is not False, 'validate_on_init=False is no longer supported'\n    if sys.version_info >= (3, 10):\n        kwargs = {'kw_only': kw_only, 'slots': slots}\n    else:\n        kwargs = {}\n    def create_dataclass(cls: type[Any]) -> type[PydanticDataclass]:\n        from ._internal._utils import is_model_class\n        if is_model_class(cls):\n            raise PydanticUserError(\n                f'Cannot create a Pydantic dataclass from {cls.__name__} as it is already a Pydantic model',\n                code='dataclass-on-model',\n            )\n        original_cls = cls\n        has_dataclass_base = any(dataclasses.is_dataclass(base) for base in cls.__bases__)\n        if not has_dataclass_base and config is not None and hasattr(cls, '__pydantic_config__'):\n            warn(\n                f'`config` is set via both the `dataclass` decorator and `__pydantic_config__` for dataclass {cls.__name__}. '\n                f'The `config` specification from `dataclass` decorator will take priority.',\n                category=UserWarning,\n                stacklevel=2,\n            )\n        config_dict = config if config is not None else getattr(cls, '__pydantic_config__', None)\n        config_wrapper = _config.ConfigWrapper(config_dict)\n        decorators = _decorators.DecoratorInfos.build(cls, replace_wrapped_methods=True)\n        decorators.update_from_config(config_wrapper)\n        original_doc = cls.__doc__\n        if _pydantic_dataclasses.is_stdlib_dataclass(cls):\n            original_doc = None\n            bases = (cls,)\n            if issubclass(cls, Generic):\n                generic_base = Generic[cls.__parameters__]\n                bases = bases + (generic_base,)\n            cls = types.new_class(cls.__name__, bases)\n        if frozen is not None:\n            frozen_ = frozen\n            if config_wrapper.frozen:\n                warn(\n                    f'`frozen` is set via both the `dataclass` decorator and `config` for dataclass {cls.__name__!r}.'\n                    'This is not recommended. The `frozen` specification on `dataclass` will take priority.',\n                    category=UserWarning,\n                    stacklevel=2,\n                )\n        else:\n            frozen_ = config_wrapper.frozen or False\n        cls_anns = _typing_extra.safe_get_annotations(cls)\n        for field_name in cls_anns:\n            field_value = getattr(cls, field_name, None)\n            if isinstance(field_value, FieldInfo):\n                setattr(cls, field_name, _pydantic_dataclasses.as_dataclass_field(field_value))\n        with _pydantic_dataclasses.patch_base_fields(cls):\n            cls = dataclasses.dataclass(\n                cls,\n                init=True,\n                repr=repr,\n                eq=eq,\n                order=order,\n                unsafe_hash=unsafe_hash,\n                frozen=frozen_,\n                **kwargs,\n            )\n        if config_wrapper.validate_assignment:\n            original_setattr = cls.__setattr__\n            @functools.wraps(cls.__setattr__)\n            def validated_setattr(instance: PydanticDataclass, name: str, value: Any, /) -> None:\n                if frozen_:\n                    return original_setattr(instance, name, value)\n                inst_cls = type(instance)\n                attr = getattr(inst_cls, name, None)\n                if isinstance(attr, property):\n                    attr.__set__(instance, value)\n                elif isinstance(attr, functools.cached_property):\n                    instance.__dict__.__setitem__(name, value)\n                else:\n                    inst_cls.__pydantic_validator__.validate_assignment(instance, name, value)\n            cls.__setattr__ = validated_setattr.__get__(None, cls)\n            if slots and not hasattr(cls, '__setstate__'):\n                def _dataclass_getstate(self: Any) -> list[Any]:\n                    return [getattr(self, f.name) for f in dataclasses.fields(self)]\n                def _dataclass_setstate(self: Any, state: list[Any]) -> None:\n                    for field, value in zip(dataclasses.fields(self), state):\n                        object.__setattr__(self, field.name, value)\n                cls.__getstate__ = _dataclass_getstate\n                cls.__setstate__ = _dataclass_setstate\n        cls.__is_pydantic_dataclass__ = True\n        cls.__pydantic_decorators__ = decorators\n        cls.__doc__ = original_doc\n        firstlineno = getattr(original_cls, '__firstlineno__', None)\n        cls.__module__ = original_cls.__module__\n        if sys.version_info >= (3, 13) and firstlineno is not None:\n            original_cls.__firstlineno__ = firstlineno\n            cls.__firstlineno__ = firstlineno\n        cls.__qualname__ = original_cls.__qualname__\n        cls.__pydantic_fields_complete__ = classmethod(_pydantic_fields_complete)\n        cls.__pydantic_complete__ = False\n        _pydantic_dataclasses.complete_dataclass(cls, config_wrapper, raise_errors=False)\n        return cls\n    return create_dataclass if _cls is None else create_dataclass(_cls)", "target": "def test_dataclass_self_init_alias():\n    @dataclasses.dataclass(init=False)\n    class Foo:\n        a: str\n        b: bool\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), validation_alias='aAlias'),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), validation_alias=['bAlias', 0]),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    def __init__(self, *args, **kwargs):\n        v.validate_python(ArgsKwargs(args, kwargs), self_instance=self)\n    Foo.__init__ = __init__\n    foo = Foo(aAlias=b'hello', bAlias=['True'])\n    assert dataclasses.is_dataclass(foo)\n    assert dataclasses.asdict(foo) == {'a': 'hello', 'b': True}\n    with pytest.raises(ValidationError) as exc_info:\n        Foo(aAlias=b'hello', bAlias=['wrong'])\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'bool_parsing',\n            'loc': ('bAlias', 0),\n            'msg': 'Input should be a valid boolean, unable to interpret input',\n            'input': 'wrong',\n        }\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005079", "source": "def parse_stmts(stmts: str) -> tuple[str, str]:\n    stmts = textwrap.dedent(stmts).strip()\n    lines: list[str] = stmts.splitlines(keepends=False)\n    assert len(lines) >= 3, f\"Invalid string:\\n{stmts}\"\n    column_header_pattern = r\"^Python\\s{35}\\| C\\+\\+(\\s*)$\"\n    signature_pattern = r\"^: f\\((.*)\\)( -> (.+))?\\s*$\"\n    separation_pattern = r\"^[-]{40} | [-]{40}$\"\n    code_pattern = r\"^(.{40}) \\|($| (.*)$)\"\n    column_match = re.search(column_header_pattern, lines[0])\n    if column_match is None:\n        raise ValueError(\n            f\"Column header `{lines[0]}` \"\n            f\"does not match pattern `{column_header_pattern}`\"\n        )\n    assert re.search(separation_pattern, lines[1])\n    py_lines: list[str] = []\n    cpp_lines: list[str] = []\n    for l in lines[2:]:\n        l_match = re.search(code_pattern, l)\n        if l_match is None:\n            raise ValueError(f\"Invalid line `{l}`\")\n        py_lines.append(l_match.groups()[0])\n        cpp_lines.append(l_match.groups()[2] or \"\")\n        l_from_stmts = f\"{py_lines[-1]:<40} | {cpp_lines[-1]:<40}\".rstrip()\n        assert l_from_stmts == l.rstrip(), f\"Failed to round trip `{l}`\"\n    return \"\\n\".join(py_lines), \"\\n\".join(cpp_lines)", "target": "def test_restores_on_exception(self):\n        start = Path.cwd()\n        with tempfile.TemporaryDirectory() as td:\n            target = Path(td) / \"wd_exc\"\n            target.mkdir()\n            with self.assertRaises(ValueError):\n                with working_directory(str(target)):\n                    self.assertEqual(Path.cwd().resolve(), target.resolve())\n                    raise ValueError(\"boom\")\n        self.assertEqual(Path.cwd().resolve(), start.resolve())"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005080", "source": "def dataclass(\n        *,\n        init: Literal[False] = False,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool | None = None,\n        config: ConfigDict | type[object] | None = None,\n        validate_on_init: bool | None = None,\n    ) -> Callable[[type[_T]], type[PydanticDataclass]]:\n        ...", "target": "def test_validate_assignment_function():\n    @dataclasses.dataclass\n    class MyDataclass:\n        field_a: str\n        field_b: int\n        field_c: int\n    calls = []\n    def func(x, info):\n        calls.append(str(info))\n        return x * 2\n    v = SchemaValidator(\n        core_schema.dataclass_schema(\n            MyDataclass,\n            core_schema.dataclass_args_schema(\n                'MyDataclass',\n                [\n                    core_schema.dataclass_field('field_a', core_schema.str_schema()),\n                    core_schema.dataclass_field(\n                        'field_b',\n                        core_schema.with_info_after_validator_function(func, core_schema.int_schema()),\n                    ),\n                    core_schema.dataclass_field('field_c', core_schema.int_schema()),\n                ],\n            ),\n            ['field_a', 'field_b', 'field_c'],\n        )\n    )\n    m = v.validate_python({'field_a': 'x', 'field_b': 123, 'field_c': 456})\n    assert m.field_a == 'x'\n    assert m.field_b == 246\n    assert m.field_c == 456\n    assert calls == [\"ValidationInfo(config=None, context=None, data={'field_a': 'x'}, field_name='field_b')\"]\n    v.validate_assignment(m, 'field_b', '111')\n    assert m.field_b == 222\n    assert calls == [\n        \"ValidationInfo(config=None, context=None, data={'field_a': 'x'}, field_name='field_b')\",\n        \"ValidationInfo(config=None, context=None, data={'field_a': 'x', 'field_c': 456}, field_name='field_b')\",\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005081", "source": "def secs_to_ms(time_s):\n    return time_s * 1e3", "target": "def test_get_last_page_num_from_header(self) -> None:\n        for (\n            expected_page_num,\n            mock_header,\n        ) in self.MOCK_HEADER_LINKS_TO_PAGE_NUMS.items():\n            self.assertEqual(\n                get_last_page_num_from_header(mock_header), expected_page_num\n            )"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005082", "source": "def evaluate_forwardref(type_: ForwardRef, globalns: Any, localns: Any) -> Any:\n        return cast(Any, type_)._evaluate(globalns, localns, type_params=(), recursive_guard=set())", "target": "def test_schema_typing() -> None:\n    schema: CoreSchema = {\n        'type': 'union',\n        'choices': [{'type': 'int'}, {'type': 'int', 'ge': 1}, {'type': 'float', 'lt': 1.0}],\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = {\n        'type': 'tagged-union',\n        'discriminator': 'type',\n        'choices': {\n            'apple': {\n                'type': 'typed-dict',\n                'fields': {'pips': {'type': 'typed-dict-field', 'schema': {'type': 'int'}}},\n            },\n            'banana': {\n                'type': 'typed-dict',\n                'fields': {'curvature': {'type': 'typed-dict-field', 'schema': {'type': 'float'}}},\n            },\n        },\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'int', 'ge': 1}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'float', 'lt': 1.0}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'str', 'pattern': r'http://.*'}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'bool', 'strict': False}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'literal', 'expected': [1, '1']}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'any'}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'none'}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'bytes'}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'list', 'items_schema': {'type': 'str'}, 'min_length': 3}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'set', 'items_schema': {'type': 'str'}, 'max_length': 3}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'tuple', 'items_schema': [{'type': 'str'}], 'variadic_item_index': 0, 'max_length': 3}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'tuple', 'items_schema': [{'type': 'str'}, {'type': 'int'}]}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'frozenset', 'items_schema': {'type': 'str'}, 'max_length': 3}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'dict', 'keys_schema': {'type': 'str'}, 'values_schema': {'type': 'any'}}\n    SchemaValidator(schema)\n    schema: CoreSchema = {\n        'type': 'typed-dict',\n        'fields': {'bar': {'type': 'typed-dict-field', 'schema': {'type': 'str'}}},\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = {\n        'type': 'model',\n        'cls': Foo,\n        'schema': {'type': 'model-fields', 'fields': {'bar': {'type': 'model-field', 'schema': {'type': 'str'}}}},\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = {\n        'type': 'typed-dict',\n        'fields': {\n            'a': {'type': 'typed-dict-field', 'schema': {'type': 'str'}},\n            'b': {'type': 'typed-dict-field', 'schema': {'type': 'str'}, 'validation_alias': 'foobar'},\n            'c': {\n                'type': 'typed-dict-field',\n                'schema': {'type': 'str'},\n                'validation_alias': [['foobar', 0, 'bar'], ['foo']],\n            },\n            'd': {\n                'type': 'typed-dict-field',\n                'schema': {'type': 'default', 'schema': {'type': 'str'}, 'default': 'spam'},\n            },\n        },\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = {\n        'type': 'function-wrap',\n        'function': {'type': 'with-info', 'function': wrap_validator, 'field_name': 'foobar'},\n        'schema': {'type': 'str'},\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = core_schema.with_info_plain_validator_function(validator)\n    SchemaValidator(schema)\n    schema: CoreSchema = {\n        'type': 'definitions',\n        'schema': {'type': 'definition-ref', 'schema_ref': 'Branch'},\n        'definitions': [\n            {\n                'type': 'typed-dict',\n                'fields': {\n                    'name': {'type': 'typed-dict-field', 'schema': {'type': 'str'}},\n                    'sub_branch': {\n                        'type': 'typed-dict-field',\n                        'schema': {\n                            'type': 'default',\n                            'schema': {\n                                'type': 'nullable',\n                                'schema': {'type': 'definition-ref', 'schema_ref': 'Branch'},\n                            },\n                            'default': None,\n                        },\n                    },\n                },\n                'ref': 'Branch',\n            }\n        ],\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'date', 'le': date.today()}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'time', 'lt': time(12, 13, 14)}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'datetime', 'ge': datetime.now()}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'is-instance', 'cls': Foo}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'callable'}\n    SchemaValidator(schema)\n    schema: CoreSchema = {\n        'type': 'arguments',\n        'arguments_schema': [\n            {'name': 'a', 'mode': 'positional_only', 'schema': {'type': 'int'}},\n            {'name': 'b', 'schema': {'type': 'str'}},\n            {'name': 'c', 'mode': 'keyword_only', 'schema': {'type': 'bool'}},\n        ],\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'call', 'arguments_schema': {'type': 'any'}, 'function': foo}\n    SchemaValidator(schema)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005083", "source": "def as_dataclass_field(pydantic_field: FieldInfo) -> dataclasses.Field[Any]:\n    field_args: dict[str, Any] = {'default': pydantic_field}\n    if sys.version_info >= (3, 14) and pydantic_field.description is not None:\n        field_args['doc'] = pydantic_field.description\n    if sys.version_info >= (3, 10) and pydantic_field.kw_only:\n        field_args['kw_only'] = True\n    if pydantic_field.repr is not True:\n        field_args['repr'] = pydantic_field.repr\n    return dataclasses.field(**field_args)", "target": "def test_dataclass_initvar_not_required_on_union_ser() -> None:\n    @dataclasses.dataclass\n    class Foo:\n        x: int\n        init_var: dataclasses.InitVar[int] = 1\n    @dataclasses.dataclass\n    class Bar:\n        x: int\n    schema = core_schema.union_schema(\n        [\n            core_schema.dataclass_schema(\n                Foo,\n                core_schema.dataclass_args_schema(\n                    'Foo',\n                    [\n                        core_schema.dataclass_field(name='x', schema=core_schema.int_schema()),\n                        core_schema.dataclass_field(\n                            name='init_var',\n                            init_only=True,\n                            schema=core_schema.with_default_schema(core_schema.int_schema(), default=1),\n                        ),\n                    ],\n                ),\n                ['x'],\n                post_init=True,\n            ),\n            core_schema.dataclass_schema(\n                Bar,\n                core_schema.dataclass_args_schema(\n                    'Bar', [core_schema.dataclass_field(name='x', schema=core_schema.int_schema())]\n                ),\n                ['x'],\n            ),\n        ]\n    )\n    s = SchemaSerializer(schema)\n    assert s.to_python(Foo(x=1), warnings='error') == {'x': 1}\n    assert s.to_python(Foo(x=1, init_var=2), warnings='error') == {'x': 1}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005084", "source": "def dataclass(\n        _cls: type[_T],\n        *,\n        init: Literal[False] = False,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool | None = None,\n        config: ConfigDict | type[object] | None = None,\n        validate_on_init: bool | None = None,\n    ) -> type[PydanticDataclass]: ...", "target": "def test_dataclass_args(py_and_json: PyAndJson, input_value, expected):\n    schema = core_schema.dataclass_args_schema(\n        'MyDataclass',\n        [\n            core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), kw_only=False),\n            core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), kw_only=False),\n        ],\n    )\n    v = py_and_json(schema)\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)) as exc_info:\n            v.validate_test(input_value)\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        assert v.validate_test(input_value) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005085", "source": "def is_none_type(type_: Any) -> bool:\n        return type_ in NONE_TYPES", "target": "def test_ser_function_plain():\n    def f(input: Any, info: core_schema.SerializationInfo, /) -> str:\n        return str(info)\n    s = SchemaSerializer(\n        core_schema.any_schema(\n            serialization=core_schema.plain_serializer_function_ser_schema(\n                f, info_arg=True, return_schema=core_schema.str_schema()\n            )\n        )\n    )\n    assert s.to_python(123) == (\n        \"SerializationInfo(include=None, exclude=None, context=None, mode='python', by_alias=False, exclude_unset=False, \"\n        'exclude_defaults=False, exclude_none=False, exclude_computed_fields=False, round_trip=False, serialize_as_any=False)'\n    )"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005086", "source": "def rebuild_dataclass(\n    cls: type[PydanticDataclass],\n    *,\n    force: bool = False,\n    raise_errors: bool = True,\n    _parent_namespace_depth: int = 2,\n    _types_namespace: MappingNamespace | None = None,\n) -> bool | None:\n    if not force and cls.__pydantic_complete__:\n        return None\n    for attr in ('__pydantic_core_schema__', '__pydantic_validator__', '__pydantic_serializer__'):\n        if attr in cls.__dict__ and not isinstance(getattr(cls, attr), _mock_val_ser.MockValSer):\n            delattr(cls, attr)\n    cls.__pydantic_complete__ = False\n    if _types_namespace is not None:\n        rebuild_ns = _types_namespace\n    elif _parent_namespace_depth > 0:\n        rebuild_ns = _typing_extra.parent_frame_namespace(parent_depth=_parent_namespace_depth, force=True) or {}\n    else:\n        rebuild_ns = {}\n    ns_resolver = _namespace_utils.NsResolver(\n        parent_namespace=rebuild_ns,\n    )\n    return _pydantic_dataclasses.complete_dataclass(\n        cls,\n        _config.ConfigWrapper(cls.__pydantic_config__, check=False),\n        raise_errors=raise_errors,\n        ns_resolver=ns_resolver,\n        _force_build=True,\n    )", "target": "def test_dataclass_wrap_json():\n    schema = core_schema.no_info_wrap_validator_function(\n        lambda v, handler: handler(v),\n        core_schema.dataclass_schema(\n            FooDataclass,\n            core_schema.dataclass_args_schema(\n                'FooDataclass',\n                [\n                    core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                    core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n                ],\n            ),\n            ['a', 'b'],\n        ),\n    )\n    v = SchemaValidator(schema)\n    assert v.validate_json('{\"a\": \"hello\", \"b\": true}') == FooDataclass(a='hello', b=True)\n    assert v.validate_json('{\"a\": \"hello\", \"b\": true}', strict=True) == FooDataclass(a='hello', b=True)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005087", "source": "def pydantic_encoder(obj: Any) -> Any:\n    warnings.warn(\n        '`pydantic_encoder` is deprecated, use `pydantic_core.to_jsonable_python` instead.',\n        category=PydanticDeprecatedSince20,\n        stacklevel=2,\n    )\n    from dataclasses import asdict, is_dataclass\n    BaseModel = import_cached_base_model()\n    if isinstance(obj, BaseModel):\n        return obj.model_dump()\n    elif is_dataclass(obj):\n        return asdict(obj)\n    for base in obj.__class__.__mro__[:-1]:\n        try:\n            encoder = ENCODERS_BY_TYPE[base]\n        except KeyError:\n            continue\n        return encoder(obj)\n    else:\n        raise TypeError(f\"Object of type '{obj.__class__.__name__}' is not JSON serializable\")", "target": "def test_partial_parse():\n    with pytest.raises(ValueError, match='EOF while parsing a string at line 1 column 15'):\n        from_json('[\"aa\", \"bb\", \"c')\n    assert from_json('[\"aa\", \"bb\", \"c', allow_partial=True) == ['aa', 'bb']\n    with pytest.raises(ValueError, match='EOF while parsing a string at line 1 column 15'):\n        from_json(b'[\"aa\", \"bb\", \"c')\n    assert from_json(b'[\"aa\", \"bb\", \"c', allow_partial=True) == ['aa', 'bb']"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005088", "source": "def get_commit(self, ref: str) -> GitCommit:\n        return parse_fuller_format(\n            self._run_git(\"show\", \"--format=fuller\", \"--date=unix\", \"--shortstat\", ref)\n        )", "target": "def test_is_iterable(self) -> None:\n        from collections.abc import Iterator\n        iter_ = PeekableIterator(\"\")\n        self.assertTrue(isinstance(iter_, Iterator))"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005089", "source": "def _is_field_cached_property(obj: 'Dataclass', k: str) -> bool:\n        return isinstance(getattr(type(obj), k, None), cached_property)", "target": "def test_dataclass_exact_validation(revalidate_instances, input_value, expected):\n    schema = core_schema.dataclass_schema(\n        FooDataclass,\n        core_schema.dataclass_args_schema(\n            'FooDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n            ],\n        ),\n        ['a', 'b'],\n        revalidate_instances=revalidate_instances,\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python(input_value)\n    assert dataclasses.asdict(foo) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005090", "source": "def is_pydantic_dataclass(class_: type[Any], /) -> TypeGuard[type[PydanticDataclass]]:\n    try:\n        return '__is_pydantic_dataclass__' in class_.__dict__ and dataclasses.is_dataclass(class_)\n    except AttributeError:\n        return False", "target": "def test_dataclass_field_plain_validator():\n    @dataclasses.dataclass\n    class Foo:\n        a: int\n        b: str\n        @classmethod\n        def validate_b(cls, v: bytes, info: core_schema.ValidationInfo) -> str:\n            assert v == b'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return 'hello world!'\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(\n                    name='b', schema=core_schema.with_info_plain_validator_function(Foo.validate_b)\n                ),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 1, 'b': b'hello'})\n    assert dataclasses.asdict(foo) == {'a': 1, 'b': 'hello world!'}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005091", "source": "def collect_model_fields(\n    cls: type[BaseModel],\n    config_wrapper: ConfigWrapper,\n    ns_resolver: NsResolver,\n    *,\n    typevars_map: Mapping[TypeVar, Any] | None = None,\n) -> tuple[dict[str, FieldInfo], set[str]]:\n    FieldInfo_ = import_cached_field_info()\n    BaseModel_ = import_cached_base_model()\n    bases = cls.__bases__\n    parent_fields_lookup: dict[str, FieldInfo] = {}\n    for base in reversed(bases):\n        if model_fields := getattr(base, '__pydantic_fields__', None):\n            parent_fields_lookup.update(model_fields)\n    type_hints = _typing_extra.get_model_type_hints(cls, ns_resolver=ns_resolver)\n    annotations = _typing_extra.safe_get_annotations(cls)\n    fields: dict[str, FieldInfo] = {}\n    class_vars: set[str] = set()\n    for ann_name, (ann_type, evaluated) in type_hints.items():\n        if ann_name == 'model_config':\n            continue\n        _check_protected_namespaces(\n            protected_namespaces=config_wrapper.protected_namespaces,\n            ann_name=ann_name,\n            bases=bases,\n            cls_name=cls.__name__,\n        )\n        if _typing_extra.is_classvar_annotation(ann_type):\n            class_vars.add(ann_name)\n            continue\n        assigned_value = getattr(cls, ann_name, PydanticUndefined)\n        if assigned_value is not PydanticUndefined and (\n            any(getattr(BaseModel_, depr_name, None) is assigned_value for depr_name in _deprecated_method_names)\n            or (\n                hasattr(assigned_value, '__func__')\n                and any(\n                    getattr(getattr(BaseModel_, depr_name, None), '__func__', None) is assigned_value.__func__\n                    for depr_name in _deprecated_classmethod_names\n                )\n            )\n        ):\n            assigned_value = PydanticUndefined\n        if not is_valid_field_name(ann_name):\n            continue\n        if cls.__pydantic_root_model__ and ann_name != 'root':\n            raise NameError(\n                f\"Unexpected field with name {ann_name!r}; only 'root' is allowed as a field of a `RootModel`\"\n            )\n        generic_origin = getattr(cls, '__pydantic_generic_metadata__', {}).get('origin')\n        for base in bases:\n            dataclass_fields = {\n                field.name for field in (dataclasses.fields(base) if dataclasses.is_dataclass(base) else ())\n            }\n            if hasattr(base, ann_name):\n                if base is generic_origin:\n                    continue\n                if ann_name in dataclass_fields:\n                    continue\n                if ann_name not in annotations:\n                    continue\n                warnings.warn(\n                    f'Field name \"{ann_name}\" in \"{cls.__qualname__}\" shadows an attribute in parent '\n                    f'\"{base.__qualname__}\"',\n                    UserWarning,\n                    stacklevel=4,\n                )\n        if assigned_value is PydanticUndefined:\n            if ann_name in annotations or ann_name not in parent_fields_lookup:\n                field_info = FieldInfo_.from_annotation(ann_type, _source=AnnotationSource.CLASS)\n                field_info._original_annotation = ann_type\n                if not evaluated:\n                    field_info._complete = False\n            else:\n                parent_field_info = parent_fields_lookup[ann_name]._copy()\n                if typevars_map:\n                    field_info = _recreate_field_info(\n                        parent_field_info, ns_resolver=ns_resolver, typevars_map=typevars_map, lenient=True\n                    )\n                else:\n                    field_info = parent_field_info\n        else:\n            if isinstance(assigned_value, FieldInfo_) and ismethoddescriptor(assigned_value.default):\n                default = assigned_value.default.__get__(None, cls)\n                assigned_value.default = default\n                assigned_value._attributes_set['default'] = default\n            field_info = FieldInfo_.from_annotated_attribute(ann_type, assigned_value, _source=AnnotationSource.CLASS)\n            field_info._original_assignment = assigned_value\n            field_info._original_annotation = ann_type\n            if not evaluated:\n                field_info._complete = False\n            elif 'final' in field_info._qualifiers and not field_info.is_required():\n                warnings.warn(\n                    f'Annotation {ann_name!r} is marked as final and has a default value. Pydantic treats {ann_name!r} as a '\n                    'class variable, but it will be considered as a normal field in V3 to be aligned with dataclasses. If you '\n                    f'still want {ann_name!r} to be considered as a class variable, annotate it as: `ClassVar[<type>] = <default>.`',\n                    category=PydanticDeprecatedSince211,\n                    stacklevel=4,\n                )\n                class_vars.add(ann_name)\n                continue\n            try:\n                delattr(cls, ann_name)\n            except AttributeError:\n                pass\n        decorators: DecoratorInfos = cls.__dict__['__pydantic_decorators__']\n        if ann_name in decorators.computed_fields:\n            raise TypeError(\n                f'Field {ann_name!r} of class {cls.__name__!r} overrides symbol of same name in a parent class. '\n                'This override with a computed_field is incompatible.'\n            )\n        fields[ann_name] = field_info\n        if field_info._complete:\n            update_field_from_config(config_wrapper, ann_name, field_info)\n    if config_wrapper.use_attribute_docstrings:\n        _update_fields_from_docstrings(cls, fields)\n    return fields, class_vars", "target": "def test_from_attributes_missing():\n    class Foobar:\n        def __init__(self):\n            self.a = 1\n            self.b = 2\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'a': core_schema.model_field(schema=core_schema.int_schema()),\n                'b': core_schema.model_field(schema=core_schema.int_schema()),\n                'c': core_schema.model_field(schema=core_schema.str_schema()),\n            },\n            from_attributes=True,\n        )\n    )\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python(Foobar())\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'missing',\n            'loc': ('c',),\n            'msg': 'Field required',\n            'input': HasRepr(IsStr(regex='.+Foobar object at.+')),\n        }\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005092", "source": "def validated_setattr(instance: PydanticDataclass, name: str, value: Any, /) -> None:\n                if frozen_:\n                    return original_setattr(instance, name, value)\n                inst_cls = type(instance)\n                attr = getattr(inst_cls, name, None)\n                if isinstance(attr, property):\n                    attr.__set__(instance, value)\n                elif isinstance(attr, functools.cached_property):\n                    instance.__dict__.__setitem__(name, value)\n                else:\n                    inst_cls.__pydantic_validator__.validate_assignment(instance, name, value)", "target": "def test_only_validate_by_name(py_and_json) -> None:\n    schema = core_schema.dataclass_schema(\n        BasicDataclass,\n        core_schema.dataclass_args_schema(\n            'BasicDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), validation_alias='FieldA'),\n            ],\n        ),\n        ['a'],\n        config=core_schema.CoreConfig(validate_by_name=True, validate_by_alias=False),\n    )\n    v = py_and_json(schema)\n    assert v.validate_test({'a': 'hello'}) == BasicDataclass(a='hello')\n    with pytest.raises(ValidationError, match=r'a\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'FieldA': 'hello'})"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005093", "source": "def update_field_from_config(config_wrapper: ConfigWrapper, field_name: str, field_info: FieldInfo) -> None:\n    field_title_generator = field_info.field_title_generator or config_wrapper.field_title_generator\n    if field_title_generator is not None:\n        _apply_field_title_generator_to_field_info(field_title_generator, field_name, field_info)\n    if config_wrapper.alias_generator is not None:\n        _apply_alias_generator_to_field_info(config_wrapper.alias_generator, field_name, field_info)", "target": "def test_validate_assignment_ignore_extra():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(fields={'field_a': core_schema.model_field(schema=core_schema.str_schema())})\n    )\n    assert v.validate_python({'field_a': 'test'}) == ({'field_a': 'test'}, None, {'field_a'})\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_assignment({'field_a': 'test'}, 'other_field', 456)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'no_such_attribute',\n            'loc': ('other_field',),\n            'msg': \"Object has no attribute 'other_field'\",\n            'input': 456,\n            'ctx': {'attribute': 'other_field'},\n        }\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005094", "source": "def rebuild_dataclass_fields(\n    cls: type[PydanticDataclass],\n    *,\n    config_wrapper: ConfigWrapper,\n    ns_resolver: NsResolver,\n    typevars_map: Mapping[TypeVar, Any],\n) -> dict[str, FieldInfo]:\n    FieldInfo_ = import_cached_field_info()\n    rebuilt_fields: dict[str, FieldInfo] = {}\n    with ns_resolver.push(cls):\n        for f_name, field_info in cls.__pydantic_fields__.items():\n            if field_info._complete:\n                rebuilt_fields[f_name] = field_info\n            else:\n                existing_desc = field_info.description\n                ann = _typing_extra.eval_type(\n                    field_info._original_annotation,\n                    *ns_resolver.types_namespace,\n                )\n                ann = _generics.replace_types(ann, typevars_map)\n                new_field = FieldInfo_.from_annotated_attribute(\n                    ann,\n                    field_info._original_assignment,\n                    _source=AnnotationSource.DATACLASS,\n                )\n                new_field.description = new_field.description if new_field.description is not None else existing_desc\n                update_field_from_config(config_wrapper, f_name, new_field)\n                rebuilt_fields[f_name] = new_field\n    return rebuilt_fields", "target": "def test_from_attributes_path(input_value, expected):\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'my_field': core_schema.model_field(\n                    validation_alias=[['foo', 'bar', 'bat'], ['foo', 3], ['spam']], schema=core_schema.int_schema()\n                )\n            },\n            from_attributes=True,\n        ),\n        config=CoreConfig(loc_by_alias=False),\n    )\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=expected.message):\n            val = v.validate_python(input_value)\n            print(f'UNEXPECTED OUTPUT: {val!r}')\n    else:\n        model_dict, model_extra, fields_set = v.validate_python(input_value)\n        assert model_dict == expected\n        assert model_extra is None\n        assert fields_set == {'my_field'}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005095", "source": "def new_init(self: 'Dataclass', *args: Any, **kwargs: Any) -> None:\n            handle_extra_init(self, *args, **kwargs)\n            if self.__class__.__pydantic_run_validation__:\n                self.__pydantic_validate_values__()\n            if hasattr(self, '__post_init_post_parse__'):\n                initvars_and_values: Dict[str, Any] = {}\n                for i, f in enumerate(self.__class__.__dataclass_fields__.values()):\n                    if f._field_type is dataclasses._FIELD_INITVAR:\n                        try:\n                            initvars_and_values[f.name] = args[i]\n                        except IndexError:\n                            initvars_and_values[f.name] = kwargs.get(f.name, f.default)\n                self.__post_init_post_parse__(**initvars_and_values)", "target": "def test_dataclass_field_wrap_validator1():\n    @dataclasses.dataclass\n    class Foo:\n        a: int\n        b: str\n        @classmethod\n        def validate_b(\n            cls, v: bytes, nxt: core_schema.ValidatorFunctionWrapHandler, info: core_schema.ValidationInfo\n        ) -> str:\n            assert v == b'hello'\n            v = nxt(v)\n            assert v == 'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return 'hello world!'\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(\n                    name='b',\n                    schema=core_schema.with_info_wrap_validator_function(Foo.validate_b, core_schema.str_schema()),\n                ),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 1, 'b': b'hello'})\n    assert dataclasses.asdict(foo) == {'a': 1, 'b': 'hello world!'}"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005096", "source": "def from_markdown_table(data: str) -> TimingResultType:\n    out = data.strip().split(\"\\n\")\n    out = out[2:]\n    res: TimingResultType\n    res = defaultdict(defaultdict)\n    for line in out:\n        model, task, mean, var = (f.strip() for f in line.strip().split(\"|\") if f)\n        res[model][task] = (float(mean), float(var))\n    return res", "target": "def test_pr_with_not_user_facing_label(\n        self, mocked_rn_labels: Any, mocked_gql: Any\n    ) -> None:\n        \"Test PR with 'topic: not user facing' label\"\n        pr = GitHubPR(\"pytorch\", \"pytorch\", 75095)\n        self.assertTrue(has_required_labels(pr))"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005097", "source": "def timedelta_isoformat(td: datetime.timedelta) -> str:\n    minutes, seconds = divmod(td.seconds, 60)\n    hours, minutes = divmod(minutes, 60)\n    return f'{\"-\" if td.days < 0 else \"\"}P{abs(td.days)}DT{hours:d}H{minutes:d}M{seconds:d}.{td.microseconds:06d}S'", "target": "def test_dict_key(py_and_json: PyAndJson):\n    v = py_and_json(\n        core_schema.dict_schema(\n            core_schema.json_schema(core_schema.tuple_positional_schema([core_schema.int_schema()])),\n            core_schema.int_schema(),\n        )\n    )\n    assert v.validate_test({'[1]': 4}) == {(1,): 4}\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_test({'x': 4})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'json_invalid',\n            'loc': ('x', '[key]'),\n            'msg': 'Invalid JSON: expected value at line 1 column 1',\n            'input': 'x',\n            'ctx': {'error': 'expected value at line 1 column 1'},\n        }\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005098", "source": "def _add_pydantic_validation_attributes(\n    dc_cls: Type['Dataclass'],\n    config: Type[BaseConfig],\n    validate_on_init: bool,\n    dc_cls_doc: str,\n) -> None:\n    init = dc_cls.__init__\n    @wraps(init)\n    def handle_extra_init(self: 'Dataclass', *args: Any, **kwargs: Any) -> None:\n        if config.extra == Extra.ignore:\n            init(self, *args, **{k: v for k, v in kwargs.items() if k in self.__dataclass_fields__})\n        elif config.extra == Extra.allow:\n            for k, v in kwargs.items():\n                self.__dict__.setdefault(k, v)\n            init(self, *args, **{k: v for k, v in kwargs.items() if k in self.__dataclass_fields__})\n        else:\n            init(self, *args, **kwargs)\n    if hasattr(dc_cls, '__post_init__'):\n        try:\n            post_init = dc_cls.__post_init__.__wrapped__\n        except AttributeError:\n            post_init = dc_cls.__post_init__\n        @wraps(post_init)\n        def new_post_init(self: 'Dataclass', *args: Any, **kwargs: Any) -> None:\n            if config.post_init_call == 'before_validation':\n                post_init(self, *args, **kwargs)\n            if self.__class__.__pydantic_run_validation__:\n                self.__pydantic_validate_values__()\n                if hasattr(self, '__post_init_post_parse__'):\n                    self.__post_init_post_parse__(*args, **kwargs)\n            if config.post_init_call == 'after_validation':\n                post_init(self, *args, **kwargs)\n        setattr(dc_cls, '__init__', handle_extra_init)\n        setattr(dc_cls, '__post_init__', new_post_init)\n    else:\n        @wraps(init)\n        def new_init(self: 'Dataclass', *args: Any, **kwargs: Any) -> None:\n            handle_extra_init(self, *args, **kwargs)\n            if self.__class__.__pydantic_run_validation__:\n                self.__pydantic_validate_values__()\n            if hasattr(self, '__post_init_post_parse__'):\n                initvars_and_values: Dict[str, Any] = {}\n                for i, f in enumerate(self.__class__.__dataclass_fields__.values()):\n                    if f._field_type is dataclasses._FIELD_INITVAR:\n                        try:\n                            initvars_and_values[f.name] = args[i]\n                        except IndexError:\n                            initvars_and_values[f.name] = kwargs.get(f.name, f.default)\n                self.__post_init_post_parse__(**initvars_and_values)\n        setattr(dc_cls, '__init__', new_init)\n    setattr(dc_cls, '__pydantic_run_validation__', ClassAttribute('__pydantic_run_validation__', validate_on_init))\n    setattr(dc_cls, '__pydantic_initialised__', False)\n    setattr(dc_cls, '__pydantic_model__', create_pydantic_model_from_dataclass(dc_cls, config, dc_cls_doc))\n    setattr(dc_cls, '__pydantic_validate_values__', _dataclass_validate_values)\n    setattr(dc_cls, '__validate__', classmethod(_validate_dataclass))\n    setattr(dc_cls, '__get_validators__', classmethod(_get_validators))\n    if dc_cls.__pydantic_model__.__config__.validate_assignment and not dc_cls.__dataclass_params__.frozen:\n        setattr(dc_cls, '__setattr__', _dataclass_validate_assignment_setattr)", "target": "def test_only_validate_by_name(py_and_json) -> None:\n    schema = core_schema.dataclass_schema(\n        BasicDataclass,\n        core_schema.dataclass_args_schema(\n            'BasicDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), validation_alias='FieldA'),\n            ],\n        ),\n        ['a'],\n        config=core_schema.CoreConfig(validate_by_name=True, validate_by_alias=False),\n    )\n    v = py_and_json(schema)\n    assert v.validate_test({'a': 'hello'}) == BasicDataclass(a='hello')\n    with pytest.raises(ValidationError, match=r'a\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'FieldA': 'hello'})"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005099", "source": "def _recreate_field_info(\n    field_info: FieldInfo,\n    ns_resolver: NsResolver,\n    typevars_map: Mapping[TypeVar, Any],\n    *,\n    lenient: bool,\n) -> FieldInfo:\n    FieldInfo_ = import_cached_field_info()\n    existing_desc = field_info.description\n    if lenient:\n        ann = _generics.replace_types(field_info._original_annotation, typevars_map)\n        ann, evaluated = _typing_extra.try_eval_type(\n            ann,\n            *ns_resolver.types_namespace,\n        )\n    else:\n        ann = _typing_extra.eval_type(\n            field_info._original_annotation,\n            *ns_resolver.types_namespace,\n        )\n        ann = _generics.replace_types(ann, typevars_map)\n        ann = _typing_extra.eval_type(\n            ann,\n            *ns_resolver.types_namespace,\n        )\n        evaluated = True\n    if (assign := field_info._original_assignment) is PydanticUndefined:\n        new_field = FieldInfo_.from_annotation(ann, _source=AnnotationSource.CLASS)\n    else:\n        new_field = FieldInfo_.from_annotated_attribute(ann, assign, _source=AnnotationSource.CLASS)\n        new_field._original_assignment = assign\n    new_field._original_annotation = ann\n    new_field.description = new_field.description if new_field.description is not None else existing_desc\n    if not evaluated:\n        new_field._complete = False\n    return new_field", "target": "def test_json_error():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'field_a': core_schema.model_field(\n                    schema=core_schema.list_schema(items_schema=core_schema.int_schema())\n                )\n            }\n        )\n    )\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_json('{\"field_a\": [123, \"wrong\"]}')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('field_a', 1),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'wrong',\n        }\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005100", "source": "def patch_base_fields(cls: type[Any]) -> Generator[None]:\n    original_fields_list: list[tuple[DcFields, DcFields]] = []\n    for base in cls.__mro__[1:]:\n        dc_fields: dict[str, dataclasses.Field[Any]] = base.__dict__.get('__dataclass_fields__', {})\n        dc_fields_with_pydantic_field_defaults = {\n            field_name: field\n            for field_name, field in dc_fields.items()\n            if isinstance(field.default, FieldInfo)\n            and (field.default.description is not None or field.default.kw_only or field.default.repr is not True)\n        }\n        if dc_fields_with_pydantic_field_defaults:\n            original_fields_list.append((dc_fields, dc_fields_with_pydantic_field_defaults))\n            for field_name, field in dc_fields_with_pydantic_field_defaults.items():\n                default = cast(FieldInfo, field.default)\n                new_dc_field = copy.copy(field)\n                if sys.version_info >= (3, 10) and default.kw_only:\n                    new_dc_field.kw_only = True\n                if default.repr is not True:\n                    new_dc_field.repr = default.repr\n                dc_fields[field_name] = new_dc_field\n    try:\n        yield\n    finally:\n        for fields, original_fields in original_fields_list:\n            for field_name, original_field in original_fields.items():\n                fields[field_name] = original_field", "target": "def test_only_validate_by_name(py_and_json) -> None:\n    schema = core_schema.dataclass_schema(\n        BasicDataclass,\n        core_schema.dataclass_args_schema(\n            'BasicDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), validation_alias='FieldA'),\n            ],\n        ),\n        ['a'],\n        config=core_schema.CoreConfig(validate_by_name=True, validate_by_alias=False),\n    )\n    v = py_and_json(schema)\n    assert v.validate_test({'a': 'hello'}) == BasicDataclass(a='hello')\n    with pytest.raises(ValidationError, match=r'a\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'FieldA': 'hello'})"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005101", "source": "def dataclass(\n        _cls: type[_T],\n        *,\n        init: Literal[False] = False,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool | None = None,\n        config: ConfigDict | type[object] | None = None,\n        validate_on_init: bool | None = None,\n    ) -> type[PydanticDataclass]: ...", "target": "def test_custom_dataclass_names():\n    schema = core_schema.dataclass_schema(\n        FooParentDataclass,\n        core_schema.dataclass_args_schema(\n            'FooParentDataclass',\n            [\n                core_schema.dataclass_field(\n                    name='foo',\n                    schema=core_schema.union_schema(\n                        [\n                            core_schema.dataclass_schema(\n                                FooDataclass,\n                                core_schema.dataclass_args_schema(\n                                    'FooDataclass[dataclass_args_schema]',\n                                    [\n                                        core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                                        core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n                                    ],\n                                ),\n                                ['a', 'b'],\n                                cls_name='FooDataclass[cls_name]',\n                            ),\n                            core_schema.none_schema(),\n                        ]\n                    ),\n                )\n            ],\n        ),\n        ['foo'],\n    )\n    v = SchemaValidator(schema)\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python({'foo': 123})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'class_name': 'FooDataclass[dataclass_args_schema]'},\n            'input': 123,\n            'loc': ('foo', 'FooDataclass[cls_name]'),\n            'msg': 'Input should be a dictionary or an instance of FooDataclass[dataclass_args_schema]',\n            'type': 'dataclass_type',\n        },\n        {'input': 123, 'loc': ('foo', 'none'), 'msg': 'Input should be None', 'type': 'none_required'},\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005102", "source": "def get_git_remote_name() -> str:\n    return os.getenv(\"GIT_REMOTE_NAME\", \"origin\")", "target": "def test_fails(self) -> None:\n        @retries_decorator(rc=0)\n        def foo(x: int, y: int) -> int:\n            return x + y\n        self.assertEqual(foo(\"a\", 4), 0)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005103", "source": "def validated_setattr(instance: PydanticDataclass, name: str, value: Any, /) -> None:\n                if frozen_:\n                    return original_setattr(instance, name, value)\n                inst_cls = type(instance)\n                attr = getattr(inst_cls, name, None)\n                if isinstance(attr, property):\n                    attr.__set__(instance, value)\n                elif isinstance(attr, functools.cached_property):\n                    instance.__dict__.__setitem__(name, value)\n                else:\n                    inst_cls.__pydantic_validator__.validate_assignment(instance, name, value)", "target": "def test_dataclass_args_init_only_no_fields(py_and_json: PyAndJson, input_value, expected):\n    schema = core_schema.dataclass_args_schema(\n        'MyDataclass', [core_schema.dataclass_field(name='a', schema=core_schema.str_schema())], collect_init_only=True\n    )\n    v = py_and_json(schema)\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=expected.message) as exc_info:\n            v.validate_test(input_value)\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        assert v.validate_test(input_value) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005104", "source": "def pydantic_encoder(obj: Any) -> Any:\n    from dataclasses import asdict, is_dataclass\n    from pydantic.v1.main import BaseModel\n    if isinstance(obj, BaseModel):\n        return obj.dict()\n    elif is_dataclass(obj):\n        return asdict(obj)\n    for base in obj.__class__.__mro__[:-1]:\n        try:\n            encoder = ENCODERS_BY_TYPE[base]\n        except KeyError:\n            continue\n        return encoder(obj)\n    else:\n        raise TypeError(f\"Object of type '{obj.__class__.__name__}' is not JSON serializable\")", "target": "def test_any_schema_no_schema():\n    v = SchemaValidator(core_schema.json_schema())\n    assert 'validator:None' in plain_repr(v)\n    v = SchemaValidator(core_schema.json_schema(core_schema.any_schema()))\n    assert 'validator:None' in plain_repr(v)\n    v = SchemaValidator(core_schema.json_schema(core_schema.int_schema()))\n    assert 'validator:Some(' in plain_repr(v)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005105", "source": "def pydantic_encoder(obj: Any) -> Any:\n    from dataclasses import asdict, is_dataclass\n    from pydantic.v1.main import BaseModel\n    if isinstance(obj, BaseModel):\n        return obj.dict()\n    elif is_dataclass(obj):\n        return asdict(obj)\n    for base in obj.__class__.__mro__[:-1]:\n        try:\n            encoder = ENCODERS_BY_TYPE[base]\n        except KeyError:\n            continue\n        return encoder(obj)\n    else:\n        raise TypeError(f\"Object of type '{obj.__class__.__name__}' is not JSON serializable\")", "target": "def test_cycle_same():\n    def fallback_func_passthrough(obj):\n        return obj\n    f = Foobar()\n    with pytest.raises(ValueError, match=r'Circular reference detected \\(id repeated\\)'):\n        to_jsonable_python(f, fallback=fallback_func_passthrough)\n    with pytest.raises(ValueError, match=r'Circular reference detected \\(id repeated\\)'):\n        to_json(f, fallback=fallback_func_passthrough)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005106", "source": "def _general_metadata_cls() -> type[BaseMetadata]:\n    from annotated_types import BaseMetadata\n    class _PydanticGeneralMetadata(PydanticMetadata, BaseMetadata):\n        def __init__(self, metadata: Any):\n            self.__dict__ = metadata\n    return _PydanticGeneralMetadata", "target": "def test_validate_assignment_allow_extra_validate():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={'field_a': core_schema.model_field(schema=core_schema.str_schema())},\n            extras_schema=core_schema.int_schema(),\n            extra_behavior='allow',\n        )\n    )\n    assert v.validate_assignment({'field_a': 'test'}, 'other_field', '456') == (\n        {'field_a': 'test'},\n        {'other_field': 456},\n        {'other_field'},\n    )\n    with pytest.raises(ValidationError) as exc_info:\n        assert v.validate_assignment({'field_a': 'test'}, 'other_field', 'xyz')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('other_field',),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'xyz',\n        }\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005107", "source": "def validated_setattr(instance: PydanticDataclass, name: str, value: Any, /) -> None:\n                if frozen_:\n                    return original_setattr(instance, name, value)\n                inst_cls = type(instance)\n                attr = getattr(inst_cls, name, None)\n                if isinstance(attr, property):\n                    attr.__set__(instance, value)\n                elif isinstance(attr, functools.cached_property):\n                    instance.__dict__.__setitem__(name, value)\n                else:\n                    inst_cls.__pydantic_validator__.validate_assignment(instance, name, value)", "target": "def test_dataclass_args_init_only(py_and_json: PyAndJson, input_value, expected):\n    schema = core_schema.dataclass_args_schema(\n        'MyDataclass',\n        [\n            core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), kw_only=False),\n            core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), kw_only=False, init_only=True),\n        ],\n        collect_init_only=True,\n    )\n    v = py_and_json(schema)\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)) as exc_info:\n            v.validate_test(input_value)\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        assert v.validate_test(input_value) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005108", "source": "def rebuild_model_fields(\n    cls: type[BaseModel],\n    *,\n    config_wrapper: ConfigWrapper,\n    ns_resolver: NsResolver,\n    typevars_map: Mapping[TypeVar, Any],\n) -> dict[str, FieldInfo]:\n    rebuilt_fields: dict[str, FieldInfo] = {}\n    with ns_resolver.push(cls):\n        for f_name, field_info in cls.__pydantic_fields__.items():\n            if field_info._complete:\n                rebuilt_fields[f_name] = field_info\n            else:\n                new_field = _recreate_field_info(\n                    field_info, ns_resolver=ns_resolver, typevars_map=typevars_map, lenient=False\n                )\n                update_field_from_config(config_wrapper, f_name, new_field)\n                rebuilt_fields[f_name] = new_field\n    return rebuilt_fields", "target": "def test_alias_path(py_and_json: PyAndJson, input_value, expected):\n    v = py_and_json(\n        {\n            'type': 'model-fields',\n            'fields': {\n                'field_a': {'validation_alias': ['foo', 'bar'], 'type': 'model-field', 'schema': {'type': 'int'}}\n            },\n        }\n    )\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=expected.message):\n            v.validate_test(input_value)\n    else:\n        output = v.validate_test(input_value)\n        assert output == expected"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005109", "source": "def temp_environ(updates: dict[str, str]):\n    missing = object()\n    old: dict[str, str | object] = {k: os.environ.get(k, missing) for k in updates}\n    try:\n        os.environ.update(updates)\n        yield\n    finally:\n        for k, v in old.items():\n            if v is missing:\n                os.environ.pop(k, None)\n            else:\n                os.environ[k] = v", "target": "def test_gh_get_labels_raises_with_no_pages(\n        self,\n        mock_request_for_labels: Any,\n        get_last_page_num_from_header: Any,\n    ) -> None:\n        with self.assertRaises(AssertionError) as err:\n            gh_get_labels(\"foo\", \"bar\")\n        self.assertIn(\"number of pages of labels\", str(err.exception))"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005110", "source": "def _set_nested_attr(obj: nn.Module, names: list[str], value: Tensor) -> None:\n    if len(names) == 1:\n        setattr(obj, names[0], value)\n    else:\n        _set_nested_attr(getattr(obj, names[0]), names[1:], value)", "target": "def test_compute_diff(self) -> None:\n        diff = self.repo.diff(\"HEAD\")\n        sha = _shasum(diff)\n        self.assertEqual(len(sha), 64)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005111", "source": "def collect_dataclass_fields(\n    cls: type[StandardDataclass],\n    *,\n    config_wrapper: ConfigWrapper,\n    ns_resolver: NsResolver | None = None,\n    typevars_map: dict[Any, Any] | None = None,\n) -> dict[str, FieldInfo]:\n    FieldInfo_ = import_cached_field_info()\n    fields: dict[str, FieldInfo] = {}\n    ns_resolver = ns_resolver or NsResolver()\n    dataclass_fields = cls.__dataclass_fields__\n    for base in reversed(cls.__mro__):\n        if not dataclasses.is_dataclass(base):\n            continue\n        with ns_resolver.push(base):\n            for ann_name, dataclass_field in dataclass_fields.items():\n                base_anns = _typing_extra.safe_get_annotations(base)\n                if ann_name not in base_anns:\n                    continue\n                globalns, localns = ns_resolver.types_namespace\n                ann_type, evaluated = _typing_extra.try_eval_type(dataclass_field.type, globalns, localns)\n                if _typing_extra.is_classvar_annotation(ann_type):\n                    continue\n                if (\n                    not dataclass_field.init\n                    and dataclass_field.default is dataclasses.MISSING\n                    and dataclass_field.default_factory is dataclasses.MISSING\n                ):\n                    continue\n                if isinstance(dataclass_field.default, FieldInfo_):\n                    if dataclass_field.default.init_var:\n                        if dataclass_field.default.init is False:\n                            raise PydanticUserError(\n                                f'Dataclass field {ann_name} has init=False and init_var=True, but these are mutually exclusive.',\n                                code='clashing-init-and-init-var',\n                            )\n                        continue\n                    field_info = FieldInfo_.from_annotated_attribute(\n                        ann_type, dataclass_field.default, _source=AnnotationSource.DATACLASS\n                    )\n                    field_info._original_assignment = dataclass_field.default\n                else:\n                    field_info = FieldInfo_.from_annotated_attribute(\n                        ann_type, dataclass_field, _source=AnnotationSource.DATACLASS\n                    )\n                    field_info._original_assignment = dataclass_field\n                if not evaluated:\n                    field_info._complete = False\n                    field_info._original_annotation = ann_type\n                fields[ann_name] = field_info\n                update_field_from_config(config_wrapper, ann_name, field_info)\n                if field_info.default is not PydanticUndefined and isinstance(\n                    getattr(cls, ann_name, field_info), FieldInfo_\n                ):\n                    setattr(cls, ann_name, field_info.default)\n    if typevars_map:\n        for field in fields.values():\n            field.apply_typevars_map(typevars_map)\n    if config_wrapper.use_attribute_docstrings:\n        _update_fields_from_docstrings(\n            cls,\n            fields,\n            use_inspect=not hasattr(cls, '__is_pydantic_dataclass__'),\n        )\n    return fields", "target": "def test_alias_error_loc_alias(py_and_json: PyAndJson):\n    v = py_and_json(\n        {\n            'type': 'model-fields',\n            'fields': {\n                'field_a': {\n                    'type': 'model-field',\n                    'schema': {'type': 'int'},\n                    'validation_alias': [['foo', 'x'], ['bar', 1, -1]],\n                }\n            },\n        },\n        {'loc_by_alias': True},\n    )\n    assert v.validate_test({'foo': {'x': 42}}) == ({'field_a': 42}, None, {'field_a'})\n    assert v.validate_python({'bar': ['x', {-1: 42}]}) == ({'field_a': 42}, None, {'field_a'})\n    assert v.validate_test({'bar': ['x', [1, 2, 42]]}) == ({'field_a': 42}, None, {'field_a'})\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_test({'foo': {'x': 'not_int'}})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('foo', 'x'),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'not_int',\n        }\n    ]\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_test({'bar': ['x', [1, 2, 'not_int']]})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('bar', 1, -1),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'not_int',\n        }\n    ]\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_test({})\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'missing', 'loc': ('foo', 'x'), 'msg': 'Field required', 'input': {}}\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005112", "source": "def rebuild_model_fields(\n    cls: type[BaseModel],\n    *,\n    config_wrapper: ConfigWrapper,\n    ns_resolver: NsResolver,\n    typevars_map: Mapping[TypeVar, Any],\n) -> dict[str, FieldInfo]:\n    rebuilt_fields: dict[str, FieldInfo] = {}\n    with ns_resolver.push(cls):\n        for f_name, field_info in cls.__pydantic_fields__.items():\n            if field_info._complete:\n                rebuilt_fields[f_name] = field_info\n            else:\n                new_field = _recreate_field_info(\n                    field_info, ns_resolver=ns_resolver, typevars_map=typevars_map, lenient=False\n                )\n                update_field_from_config(config_wrapper, f_name, new_field)\n                rebuilt_fields[f_name] = new_field\n    return rebuilt_fields", "target": "def test_str_config():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(fields={'field_a': core_schema.model_field(schema=core_schema.str_schema())}),\n        config=CoreConfig(str_max_length=5),\n    )\n    assert v.validate_python({'field_a': 'test'}) == ({'field_a': 'test'}, None, {'field_a'})\n    with pytest.raises(ValidationError, match='String should have at most 5 characters'):\n        v.validate_python({'field_a': 'test long'})"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005113", "source": "def rebuild_dataclass_fields(\n    cls: type[PydanticDataclass],\n    *,\n    config_wrapper: ConfigWrapper,\n    ns_resolver: NsResolver,\n    typevars_map: Mapping[TypeVar, Any],\n) -> dict[str, FieldInfo]:\n    FieldInfo_ = import_cached_field_info()\n    rebuilt_fields: dict[str, FieldInfo] = {}\n    with ns_resolver.push(cls):\n        for f_name, field_info in cls.__pydantic_fields__.items():\n            if field_info._complete:\n                rebuilt_fields[f_name] = field_info\n            else:\n                existing_desc = field_info.description\n                ann = _typing_extra.eval_type(\n                    field_info._original_annotation,\n                    *ns_resolver.types_namespace,\n                )\n                ann = _generics.replace_types(ann, typevars_map)\n                new_field = FieldInfo_.from_annotated_attribute(\n                    ann,\n                    field_info._original_assignment,\n                    _source=AnnotationSource.DATACLASS,\n                )\n                new_field.description = new_field.description if new_field.description is not None else existing_desc\n                update_field_from_config(config_wrapper, f_name, new_field)\n                rebuilt_fields[f_name] = new_field\n    return rebuilt_fields", "target": "def test_aliases_debug():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'field_a': core_schema.model_field(\n                    validation_alias=[['foo', 'bar', 'bat'], ['foo', 3]], schema=core_schema.int_schema()\n                )\n            }\n        )\n    )\n    print(repr(v))\n    assert repr(v).startswith('SchemaValidator(title=\"model-fields\", validator=ModelFields(')\n    assert 'PathChoices(' in repr(v)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005114", "source": "def literal_values(type_: Type[Any]) -> Tuple[Any, ...]:\n    return get_args(type_)", "target": "def test_schema_typing_error() -> None:\n    _: CoreSchema = {'type': 'wrong'}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005115", "source": "def complete_dataclass(\n    cls: type[Any],\n    config_wrapper: _config.ConfigWrapper,\n    *,\n    raise_errors: bool = True,\n    ns_resolver: NsResolver | None = None,\n    _force_build: bool = False,\n) -> bool:\n    original_init = cls.__init__\n    def __init__(__dataclass_self__: PydanticDataclass, *args: Any, **kwargs: Any) -> None:\n        __tracebackhide__ = True\n        s = __dataclass_self__\n        s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)\n    __init__.__qualname__ = f'{cls.__qualname__}.__init__'\n    cls.__init__ = __init__\n    cls.__pydantic_config__ = config_wrapper.config_dict\n    set_dataclass_fields(cls, config_wrapper=config_wrapper, ns_resolver=ns_resolver)\n    if not _force_build and config_wrapper.defer_build:\n        set_dataclass_mocks(cls)\n        return False\n    if hasattr(cls, '__post_init_post_parse__'):\n        warnings.warn(\n            'Support for `__post_init_post_parse__` has been dropped, the method will not be called',\n            PydanticDeprecatedSince20,\n        )\n    typevars_map = get_standard_typevars_map(cls)\n    gen_schema = GenerateSchema(\n        config_wrapper,\n        ns_resolver=ns_resolver,\n        typevars_map=typevars_map,\n    )\n    cls.__signature__ = LazyClassAttribute(\n        '__signature__',\n        partial(\n            generate_pydantic_signature,\n            init=original_init,\n            fields=cls.__pydantic_fields__,\n            validate_by_name=config_wrapper.validate_by_name,\n            extra=config_wrapper.extra,\n            is_dataclass=True,\n        ),\n    )\n    try:\n        schema = gen_schema.generate_schema(cls)\n    except PydanticUndefinedAnnotation as e:\n        if raise_errors:\n            raise\n        set_dataclass_mocks(cls, f'`{e.name}`')\n        return False\n    core_config = config_wrapper.core_config(title=cls.__name__)\n    try:\n        schema = gen_schema.clean_schema(schema)\n    except InvalidSchemaError:\n        set_dataclass_mocks(cls)\n        return False\n    cls = cast('type[PydanticDataclass]', cls)\n    cls.__pydantic_core_schema__ = schema\n    cls.__pydantic_validator__ = create_schema_validator(\n        schema, cls, cls.__module__, cls.__qualname__, 'dataclass', core_config, config_wrapper.plugin_settings\n    )\n    cls.__pydantic_serializer__ = SchemaSerializer(schema, core_config)\n    cls.__pydantic_complete__ = True\n    return True", "target": "def test_dataclass_field_wrap_validator2():\n    @dataclasses.dataclass\n    class Foo:\n        a: int\n        b: str\n        @classmethod\n        def validate_b(\n            cls, v: bytes, nxt: core_schema.ValidatorFunctionWrapHandler, info: core_schema.ValidationInfo\n        ) -> bytes:\n            assert v == b'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return nxt(b'hello world!')\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(\n                    name='b',\n                    schema=core_schema.with_info_wrap_validator_function(Foo.validate_b, core_schema.str_schema()),\n                ),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 1, 'b': b'hello'})\n    assert dataclasses.asdict(foo) == {'a': 1, 'b': 'hello world!'}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005116", "source": "def dataclass(\n        *,\n        init: bool = True,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool = False,\n        config: Union[ConfigDict, Type[object], None] = None,\n        validate_on_init: Optional[bool] = None,\n        use_proxy: Optional[bool] = None,\n        kw_only: bool = ...,\n    ) -> Callable[[Type[_T]], 'DataclassClassOrWrapper']:\n        ...", "target": "def test_dataclass_self_init_alias_field_name():\n    @dataclasses.dataclass(init=False)\n    class Foo:\n        a: str\n        b: bool\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), validation_alias='aAlias'),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), validation_alias=['bAlias', 0]),\n            ],\n        ),\n        ['a', 'b'],\n        config={'loc_by_alias': False},\n    )\n    v = SchemaValidator(schema)\n    def __init__(self, *args, **kwargs):\n        v.validate_python(ArgsKwargs(args, kwargs), self_instance=self)\n    Foo.__init__ = __init__\n    foo = Foo(aAlias=b'hello', bAlias=['True'])\n    assert dataclasses.asdict(foo) == {'a': 'hello', 'b': True}\n    with pytest.raises(ValidationError) as exc_info:\n        Foo(aAlias=b'hello', bAlias=['wrong'])\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'bool_parsing',\n            'loc': ('b',),\n            'msg': 'Input should be a valid boolean, unable to interpret input',\n            'input': 'wrong',\n        }\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005117", "source": "def dataclass(\n        *,\n        init: Literal[False] = False,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool = False,\n        config: ConfigDict | type[object] | None = None,\n        validate_on_init: bool | None = None,\n        kw_only: bool = ...,\n        slots: bool = ...,\n    ) -> Callable[[type[_T]], type[PydanticDataclass]]:\n        ...", "target": "def test_dataclass_post_init_args():\n    c_value = None\n    @dataclasses.dataclass\n    class Foo:\n        a: str\n        b: bool\n        c: dataclasses.InitVar[int]\n        def __post_init__(self, c: int):\n            nonlocal c_value\n            c_value = c\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n                core_schema.dataclass_field(name='c', schema=core_schema.int_schema(), init_only=True),\n            ],\n            collect_init_only=True,\n        ),\n        ['a', 'b'],\n        post_init=True,\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': b'hello', 'b': 'true', 'c': '42'})\n    assert foo.a == 'hello'\n    assert foo.b is True\n    assert not hasattr(foo, 'c')\n    assert c_value == 42"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005118", "source": "def gh_owner_and_name(self) -> tuple[str, str]:\n        url = os.getenv(\"GIT_REMOTE_URL\", None)\n        if url is None:\n            url = self.remote_url()\n        rc = RE_GITHUB_URL_MATCH.match(url)\n        if rc is None:\n            raise RuntimeError(f\"Unexpected url format {url}\")\n        return cast(tuple[str, str], rc.groups())", "target": "def test_compute_diff(self) -> None:\n        diff = self.repo.diff(\"HEAD\")\n        sha = _shasum(diff)\n        self.assertEqual(len(sha), 64)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005119", "source": "def complete_dataclass(\n    cls: type[Any],\n    config_wrapper: _config.ConfigWrapper,\n    *,\n    raise_errors: bool = True,\n    ns_resolver: NsResolver | None = None,\n    _force_build: bool = False,\n) -> bool:\n    original_init = cls.__init__\n    def __init__(__dataclass_self__: PydanticDataclass, *args: Any, **kwargs: Any) -> None:\n        __tracebackhide__ = True\n        s = __dataclass_self__\n        s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)\n    __init__.__qualname__ = f'{cls.__qualname__}.__init__'\n    cls.__init__ = __init__\n    cls.__pydantic_config__ = config_wrapper.config_dict\n    set_dataclass_fields(cls, config_wrapper=config_wrapper, ns_resolver=ns_resolver)\n    if not _force_build and config_wrapper.defer_build:\n        set_dataclass_mocks(cls)\n        return False\n    if hasattr(cls, '__post_init_post_parse__'):\n        warnings.warn(\n            'Support for `__post_init_post_parse__` has been dropped, the method will not be called',\n            PydanticDeprecatedSince20,\n        )\n    typevars_map = get_standard_typevars_map(cls)\n    gen_schema = GenerateSchema(\n        config_wrapper,\n        ns_resolver=ns_resolver,\n        typevars_map=typevars_map,\n    )\n    cls.__signature__ = LazyClassAttribute(\n        '__signature__',\n        partial(\n            generate_pydantic_signature,\n            init=original_init,\n            fields=cls.__pydantic_fields__,\n            validate_by_name=config_wrapper.validate_by_name,\n            extra=config_wrapper.extra,\n            is_dataclass=True,\n        ),\n    )\n    try:\n        schema = gen_schema.generate_schema(cls)\n    except PydanticUndefinedAnnotation as e:\n        if raise_errors:\n            raise\n        set_dataclass_mocks(cls, f'`{e.name}`')\n        return False\n    core_config = config_wrapper.core_config(title=cls.__name__)\n    try:\n        schema = gen_schema.clean_schema(schema)\n    except InvalidSchemaError:\n        set_dataclass_mocks(cls)\n        return False\n    cls = cast('type[PydanticDataclass]', cls)\n    cls.__pydantic_core_schema__ = schema\n    cls.__pydantic_validator__ = create_schema_validator(\n        schema, cls, cls.__module__, cls.__qualname__, 'dataclass', core_config, config_wrapper.plugin_settings\n    )\n    cls.__pydantic_serializer__ = SchemaSerializer(schema, core_config)\n    cls.__pydantic_complete__ = True\n    return True", "target": "def test_dataclass_self_init():\n    @dataclasses.dataclass(init=False)\n    class Foo:\n        a: str\n        b: bool\n        def __init__(self, *args, **kwargs):\n            v.validate_python(ArgsKwargs(args, kwargs), self_instance=self)\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), kw_only=False),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), kw_only=False),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    foo = Foo(b'hello', 'True')\n    assert dataclasses.is_dataclass(foo)\n    assert dataclasses.asdict(foo) == {'a': 'hello', 'b': True}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005120", "source": "def is_pydantic_dataclass(class_: type[Any], /) -> TypeGuard[type[PydanticDataclass]]:\n    try:\n        return '__is_pydantic_dataclass__' in class_.__dict__ and dataclasses.is_dataclass(class_)\n    except AttributeError:\n        return False", "target": "def test_dataclass_args(py_and_json: PyAndJson, input_value, expected):\n    schema = core_schema.dataclass_args_schema(\n        'MyDataclass',\n        [\n            core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), kw_only=False),\n            core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), kw_only=False),\n        ],\n    )\n    v = py_and_json(schema)\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)) as exc_info:\n            v.validate_test(input_value)\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        assert v.validate_test(input_value) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005121", "source": "def decimal_encoder(dec_value: Decimal) -> Union[int, float]:\n    exponent = dec_value.as_tuple().exponent\n    if isinstance(exponent, int) and exponent >= 0:\n        return int(dec_value)\n    else:\n        return float(dec_value)", "target": "def test_json_or_python():\n    class Foo(str):\n        def __eq__(self, o: object) -> bool:\n            if isinstance(o, Foo) and super().__eq__(o):\n                return True\n            return False\n    s = cs.json_or_python_schema(\n        json_schema=cs.no_info_after_validator_function(Foo, cs.str_schema()), python_schema=cs.is_instance_schema(Foo)\n    )\n    v = SchemaValidator(s)\n    assert v.validate_python(Foo('abc')) == Foo('abc')\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python('abc')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'is_instance_of',\n            'loc': (),\n            'msg': 'Input should be an instance of test_json_or_python.<locals>.Foo',\n            'input': 'abc',\n            'ctx': {'class': 'test_json_or_python.<locals>.Foo'},\n        }\n    ]\n    assert v.validate_json('\"abc\"') == Foo('abc')"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005122", "source": "def _recreate_field_info(\n    field_info: FieldInfo,\n    ns_resolver: NsResolver,\n    typevars_map: Mapping[TypeVar, Any],\n    *,\n    lenient: bool,\n) -> FieldInfo:\n    FieldInfo_ = import_cached_field_info()\n    existing_desc = field_info.description\n    if lenient:\n        ann = _generics.replace_types(field_info._original_annotation, typevars_map)\n        ann, evaluated = _typing_extra.try_eval_type(\n            ann,\n            *ns_resolver.types_namespace,\n        )\n    else:\n        ann = _typing_extra.eval_type(\n            field_info._original_annotation,\n            *ns_resolver.types_namespace,\n        )\n        ann = _generics.replace_types(ann, typevars_map)\n        ann = _typing_extra.eval_type(\n            ann,\n            *ns_resolver.types_namespace,\n        )\n        evaluated = True\n    if (assign := field_info._original_assignment) is PydanticUndefined:\n        new_field = FieldInfo_.from_annotation(ann, _source=AnnotationSource.CLASS)\n    else:\n        new_field = FieldInfo_.from_annotated_attribute(ann, assign, _source=AnnotationSource.CLASS)\n        new_field._original_assignment = assign\n    new_field._original_annotation = ann\n    new_field.description = new_field.description if new_field.description is not None else existing_desc\n    if not evaluated:\n        new_field._complete = False\n    return new_field", "target": "def test_fields_required_by_default_with_default():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'x': core_schema.model_field(schema=core_schema.str_schema()),\n                'y': core_schema.model_field(\n                    schema=core_schema.with_default_schema(schema=core_schema.str_schema(), default='bulbi')\n                ),\n            }\n        )\n    )\n    assert v.validate_python({'x': 'pika', 'y': 'chu'}) == ({'x': 'pika', 'y': 'chu'}, None, {'x', 'y'})\n    assert v.validate_python({'x': 'pika'}) == ({'x': 'pika', 'y': 'bulbi'}, None, {'x'})"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005123", "source": "def with_config(*, config: ConfigDict) -> Callable[[_TypeT], _TypeT]: ...", "target": "def test_field_priority_model():\n    v = SchemaValidator(\n        cs.model_schema(\n            cls=MyModel,\n            config=CoreConfig(str_max_length=10),\n            schema=cs.model_fields_schema(fields={'f': cs.model_field(schema=cs.str_schema(max_length=5))}),\n        )\n    )\n    assert 'max_length:Some(5)' in plain_repr(v)\n    assert v.isinstance_python({'f': 'test'}) is True\n    assert v.isinstance_python({'f': 'test long'}) is False"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005124", "source": "def create_branch_and_checkout(self, branch: str) -> None:\n        self._run_git(\"checkout\", \"-b\", branch)", "target": "def test_ghstack_branches_in_sync(self) -> None:\n        head_ref = \"gh/SS-JIA/206/head\"\n        self._skip_if_ref_does_not_exist(head_ref)\n        self.assertTrue(are_ghstack_branches_in_sync(self.repo, head_ref))"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005125", "source": "def complete_dataclass(\n    cls: type[Any],\n    config_wrapper: _config.ConfigWrapper,\n    *,\n    raise_errors: bool = True,\n    ns_resolver: NsResolver | None = None,\n    _force_build: bool = False,\n) -> bool:\n    original_init = cls.__init__\n    def __init__(__dataclass_self__: PydanticDataclass, *args: Any, **kwargs: Any) -> None:\n        __tracebackhide__ = True\n        s = __dataclass_self__\n        s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)\n    __init__.__qualname__ = f'{cls.__qualname__}.__init__'\n    cls.__init__ = __init__\n    cls.__pydantic_config__ = config_wrapper.config_dict\n    set_dataclass_fields(cls, config_wrapper=config_wrapper, ns_resolver=ns_resolver)\n    if not _force_build and config_wrapper.defer_build:\n        set_dataclass_mocks(cls)\n        return False\n    if hasattr(cls, '__post_init_post_parse__'):\n        warnings.warn(\n            'Support for `__post_init_post_parse__` has been dropped, the method will not be called',\n            PydanticDeprecatedSince20,\n        )\n    typevars_map = get_standard_typevars_map(cls)\n    gen_schema = GenerateSchema(\n        config_wrapper,\n        ns_resolver=ns_resolver,\n        typevars_map=typevars_map,\n    )\n    cls.__signature__ = LazyClassAttribute(\n        '__signature__',\n        partial(\n            generate_pydantic_signature,\n            init=original_init,\n            fields=cls.__pydantic_fields__,\n            validate_by_name=config_wrapper.validate_by_name,\n            extra=config_wrapper.extra,\n            is_dataclass=True,\n        ),\n    )\n    try:\n        schema = gen_schema.generate_schema(cls)\n    except PydanticUndefinedAnnotation as e:\n        if raise_errors:\n            raise\n        set_dataclass_mocks(cls, f'`{e.name}`')\n        return False\n    core_config = config_wrapper.core_config(title=cls.__name__)\n    try:\n        schema = gen_schema.clean_schema(schema)\n    except InvalidSchemaError:\n        set_dataclass_mocks(cls)\n        return False\n    cls = cast('type[PydanticDataclass]', cls)\n    cls.__pydantic_core_schema__ = schema\n    cls.__pydantic_validator__ = create_schema_validator(\n        schema, cls, cls.__module__, cls.__qualname__, 'dataclass', core_config, config_wrapper.plugin_settings\n    )\n    cls.__pydantic_serializer__ = SchemaSerializer(schema, core_config)\n    cls.__pydantic_complete__ = True\n    return True", "target": "def test_function_validator_wrapping_args_schema_after() -> None:\n    calls: list[Any] = []\n    def func(*args: Any) -> Any:\n        calls.append(args)\n        return args[0]\n    @dataclasses.dataclass\n    class Model:\n        number: int = 1\n    cs = core_schema.dataclass_schema(\n        Model,\n        core_schema.no_info_after_validator_function(\n            func,\n            core_schema.dataclass_args_schema(\n                'Model', [core_schema.dataclass_field('number', core_schema.int_schema())]\n            ),\n        ),\n        ['number'],\n    )\n    v = SchemaValidator(cs)\n    instance: Model = v.validate_python({'number': 1})\n    assert instance.number == 1\n    assert calls == [(({'number': 1}, None),)]\n    v.validate_assignment(instance, 'number', 2)\n    assert instance.number == 2\n    assert calls == [(({'number': 1}, None),), (({'number': 2}, None),)]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005126", "source": "def handle_extra_init(self: 'Dataclass', *args: Any, **kwargs: Any) -> None:\n        if config.extra == Extra.ignore:\n            init(self, *args, **{k: v for k, v in kwargs.items() if k in self.__dataclass_fields__})\n        elif config.extra == Extra.allow:\n            for k, v in kwargs.items():\n                self.__dict__.setdefault(k, v)\n            init(self, *args, **{k: v for k, v in kwargs.items() if k in self.__dataclass_fields__})\n        else:\n            init(self, *args, **kwargs)", "target": "def test_dataclass_self_init_post_init():\n    calls = []\n    @dataclasses.dataclass(init=False)\n    class Foo:\n        a: str\n        b: bool\n        c: dataclasses.InitVar[int]\n        def __init__(self, *args, **kwargs):\n            v.validate_python(ArgsKwargs(args, kwargs), self_instance=self)\n        def __post_init__(self, c):\n            calls.append(c)\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), kw_only=False),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), kw_only=False),\n                core_schema.dataclass_field(name='c', schema=core_schema.int_schema(), init_only=True),\n            ],\n            collect_init_only=True,\n        ),\n        ['a', 'b', 'c'],\n        post_init=True,\n    )\n    v = SchemaValidator(schema)\n    foo = Foo(b'hello', 'True', c='123')\n    assert dataclasses.is_dataclass(foo)\n    assert dataclasses.asdict(foo) == {'a': 'hello', 'b': True}\n    assert calls == [123]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005127", "source": "def rebuild_model_fields(\n    cls: type[BaseModel],\n    *,\n    config_wrapper: ConfigWrapper,\n    ns_resolver: NsResolver,\n    typevars_map: Mapping[TypeVar, Any],\n) -> dict[str, FieldInfo]:\n    rebuilt_fields: dict[str, FieldInfo] = {}\n    with ns_resolver.push(cls):\n        for f_name, field_info in cls.__pydantic_fields__.items():\n            if field_info._complete:\n                rebuilt_fields[f_name] = field_info\n            else:\n                new_field = _recreate_field_info(\n                    field_info, ns_resolver=ns_resolver, typevars_map=typevars_map, lenient=False\n                )\n                update_field_from_config(config_wrapper, f_name, new_field)\n                rebuilt_fields[f_name] = new_field\n    return rebuilt_fields", "target": "def test_validate_assignment_allow_extra():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={'field_a': core_schema.model_field(schema=core_schema.str_schema())}, extra_behavior='allow'\n        )\n    )\n    assert v.validate_python({'field_a': 'test'}) == ({'field_a': 'test'}, {}, {'field_a'})\n    assert v.validate_assignment({'field_a': 'test'}, 'other_field', 456) == (\n        {'field_a': 'test'},\n        {'other_field': 456},\n        {'other_field'},\n    )"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005128", "source": "def _apply_field_title_generator_to_field_info(\n    title_generator: Callable[[str, FieldInfo], str],\n    field_name: str,\n    field_info: FieldInfo,\n):\n    if field_info.title is None:\n        title = title_generator(field_name, field_info)\n        if not isinstance(title, str):\n            raise TypeError(f'field_title_generator {title_generator} must return str, not {title.__class__}')\n        field_info.title = title", "target": "def test_validate_assignment_allow_extra():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={'field_a': core_schema.model_field(schema=core_schema.str_schema())}, extra_behavior='allow'\n        )\n    )\n    assert v.validate_python({'field_a': 'test'}) == ({'field_a': 'test'}, {}, {'field_a'})\n    assert v.validate_assignment({'field_a': 'test'}, 'other_field', 456) == (\n        {'field_a': 'test'},\n        {'other_field': 456},\n        {'other_field'},\n    )"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005129", "source": "def ip_v6_interface_validator(v: Any) -> IPv6Interface:\n    if isinstance(v, IPv6Interface):\n        return v\n    try:\n        return IPv6Interface(v)\n    except ValueError:\n        raise errors.IPv6InterfaceError()", "target": "def test_validators_build(benchmark) -> None:\n    class Base1(BaseModel):\n        a: int\n        @field_validator('a', mode='after')\n        @classmethod\n        def val_a(cls, value: int) -> int: ...\n        @computed_field\n        def prop(self) -> int: ...\n    class Bare:\n        @computed_field\n        def prop_bare(self) -> int: ...\n    class Sub1(Base1):\n        @computed_field\n        def prop_2(self) -> int: ...\n        @computed_field\n        def prop_3(self) -> int: ...\n        @computed_field\n        def prop_4(self) -> int: ...\n    @benchmark\n    def bench() -> None:\n        class SubS(Sub1, Bare, defer_build=True):\n            @computed_field\n            def prop_5(self) -> int: ...\n            @computed_field\n            def prop_6(self) -> int: ...\n            @computed_field\n            def prop_7(self) -> int: ..."}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005130", "source": "def _dataclass_getstate(self: Any) -> list[Any]:\n                    return [getattr(self, f.name) for f in dataclasses.fields(self)]", "target": "def test_extra_behavior_ignore(\n    config: Union[core_schema.CoreConfig, None],\n    schema_extra_behavior_kw: dict[str, Any],\n    validate_fn_extra_kw: Union[ExtraBehavior, None],\n):\n    @dataclasses.dataclass\n    class MyModel:\n        f: str\n    v = SchemaValidator(\n        core_schema.dataclass_schema(\n            MyModel,\n            core_schema.dataclass_args_schema(\n                'MyModel', [core_schema.dataclass_field('f', core_schema.str_schema())], **schema_extra_behavior_kw\n            ),\n            ['f'],\n        ),\n        config=config,\n    )\n    m: MyModel = v.validate_python({'f': 'x', 'extra_field': 123}, extra=validate_fn_extra_kw)\n    assert m.f == 'x'\n    assert not hasattr(m, 'extra_field')\n    v.validate_assignment(m, 'f', 'y', extra=validate_fn_extra_kw)\n    assert m.f == 'y'\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_assignment(m, 'not_f', 'xyz', extra=validate_fn_extra_kw)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'no_such_attribute',\n            'loc': ('not_f',),\n            'msg': \"Object has no attribute 'not_f'\",\n            'input': 'xyz',\n            'ctx': {'attribute': 'not_f'},\n        }\n    ]\n    assert not hasattr(m, 'not_f')"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005131", "source": "def dataclass(\n        _cls: type[_T],\n        *,\n        init: Literal[False] = False,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool | None = None,\n        config: ConfigDict | type[object] | None = None,\n        validate_on_init: bool | None = None,\n    ) -> type[PydanticDataclass]: ...", "target": "def test_dataclass_field_after_validator():\n    @dataclasses.dataclass\n    class Foo:\n        a: int\n        b: str\n        @classmethod\n        def validate_b(cls, v: str, info: core_schema.ValidationInfo) -> str:\n            assert v == 'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return 'hello world!'\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(\n                    name='b',\n                    schema=core_schema.with_info_after_validator_function(Foo.validate_b, core_schema.str_schema()),\n                ),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 1, 'b': b'hello'})\n    assert dataclasses.asdict(foo) == {'a': 1, 'b': 'hello world!'}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005132", "source": "def _is_field_cached_property(obj: 'Dataclass', k: str) -> bool:\n        return isinstance(getattr(type(obj), k, None), cached_property)", "target": "def test_dataclass_field_before_validator():\n    @dataclasses.dataclass\n    class Foo:\n        a: int\n        b: str\n        @classmethod\n        def validate_b(cls, v: bytes, info: core_schema.ValidationInfo) -> bytes:\n            assert v == b'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return b'hello world!'\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(\n                    name='b',\n                    schema=core_schema.with_info_before_validator_function(Foo.validate_b, core_schema.str_schema()),\n                ),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 1, 'b': b'hello'})\n    assert dataclasses.asdict(foo) == {'a': 1, 'b': 'hello world!'}"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005133", "source": "def rounded_linspace(low, high, steps, div):\n    ret = torch.linspace(low, high, steps)\n    ret = (ret.int() + div - 1) // div * div\n    ret = torch.unique(ret)\n    return list(map(int, ret))", "target": "def test_get_last_page_num_from_header(self) -> None:\n        for (\n            expected_page_num,\n            mock_header,\n        ) in self.MOCK_HEADER_LINKS_TO_PAGE_NUMS.items():\n            self.assertEqual(\n                get_last_page_num_from_header(mock_header), expected_page_num\n            )"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005134", "source": "def isoformat(o: Union[datetime.date, datetime.time]) -> str:\n    return o.isoformat()", "target": "def test_input_type_invalid():\n    v = SchemaValidator(core_schema.list_schema(items_schema=core_schema.int_schema()))\n    with pytest.raises(ValidationError, match=r'JSON input should be string, bytes or bytearray \\[type=json_type,'):\n        v.validate_json([])"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005135", "source": "def set_validation(cls: Type['DataclassT'], value: bool) -> Generator[Type['DataclassT'], None, None]:\n    original_run_validation = cls.__pydantic_run_validation__\n    try:\n        cls.__pydantic_run_validation__ = value\n        yield cls\n    finally:\n        cls.__pydantic_run_validation__ = original_run_validation", "target": "def test_dataclass_post_init():\n    @dataclasses.dataclass\n    class Foo:\n        a: str\n        b: bool\n        def __post_init__(self):\n            self.a = self.a.upper()\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n            ],\n        ),\n        ['a', 'b'],\n        post_init=True,\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 'hello', 'b': True})\n    assert foo.a == 'HELLO'\n    assert foo.b is True"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005136", "source": "def custom_pydantic_encoder(type_encoders: Dict[Any, Callable[[Type[Any]], Any]], obj: Any) -> Any:\n    for base in obj.__class__.__mro__[:-1]:\n        try:\n            encoder = type_encoders[base]\n        except KeyError:\n            continue\n        return encoder(obj)\n    else:\n        return pydantic_encoder(obj)", "target": "def test_null():\n    assert SchemaValidator(core_schema.none_schema()).validate_json('null') is None"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005137", "source": "def with_params_help(params_cls: type, title: str = \"Parameter defaults\"):\n    if not is_dataclass(params_cls):\n        raise TypeError(f\"{params_cls} must be a dataclass\")\n    def _decorator(cls: type) -> type:\n        block = generate_dataclass_help(params_cls)\n        cls.__doc__ = (cls.__doc__ or \"\") + f\"\\n\\n{title}:\\n{block}\"\n        return cls\n    return _decorator", "target": "def test_env_path_raises_when_missing_and_default_none(self):\n        with patch.dict(os.environ, {}, clear=True):\n            with self.assertRaises(ValueError):\n                m.env_path(\"P\", None, resolve=True)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005138", "source": "def _dataclass_getstate(self: Any) -> list[Any]:\n                    return [getattr(self, f.name) for f in dataclasses.fields(self)]", "target": "def test_dataclass_slots_field_before_validator():\n    @dataclasses.dataclass(slots=True)\n    class Foo:\n        a: int\n        b: str\n        @classmethod\n        def validate_b(cls, v: bytes, info: core_schema.ValidationInfo) -> bytes:\n            assert v == b'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return b'hello world!'\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(\n                    name='b',\n                    schema=core_schema.with_info_before_validator_function(Foo.validate_b, core_schema.str_schema()),\n                ),\n            ],\n        ),\n        ['a', 'b'],\n        slots=True,\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 1, 'b': b'hello'})\n    assert dataclasses.asdict(foo) == {'a': 1, 'b': 'hello world!'}"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005139", "source": "def branches_containing_ref(\n        self, ref: str, *, include_remote: bool = True\n    ) -> list[str]:\n        rc = (\n            self._run_git(\"branch\", \"--remote\", \"--contains\", ref)\n            if include_remote\n            else self._run_git(\"branch\", \"--contains\", ref)\n        )\n        return [x.strip() for x in rc.split(\"\\n\") if x.strip()] if len(rc) > 0 else []", "target": "def test_ghstack_branches_in_sync(self) -> None:\n        head_ref = \"gh/SS-JIA/206/head\"\n        self._skip_if_ref_does_not_exist(head_ref)\n        self.assertTrue(are_ghstack_branches_in_sync(self.repo, head_ref))"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005140", "source": "def env_bool(\n    name: str,\n    default: bool = False,\n) -> bool:\n    val = get_env(name)\n    if not val:\n        return default\n    return str2bool(val)", "target": "def test_env_path_optional_unset_returns_none_by_default(self):\n        with patch.dict(os.environ, {}, clear=True):\n            self.assertIsNone(m.env_path_optional(\"P\"))"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005141", "source": "def local_image_exists(\n    image_name: str, client: Optional[docker.DockerClient] = None\n) -> bool:\n    if not image_name:\n        return False\n    client = client or _get_client()\n    try:\n        client.images.get(image_name)\n        return True\n    except (NotFound, APIError) as e:\n        logger.error(\n            \"Error when checking Docker image '%s': %s\",\n            image_name,\n            e.explanation if hasattr(e, \"explanation\") else str(e),\n        )\n        return False", "target": "def test_local_image_exists_uses_lazy_singleton(self):\n        with mock.patch(\n            \"cli.lib.common.docker_helper.docker.from_env\"\n        ) as mock_from_env:\n            mock_docker_client = MagicMock()\n            mock_from_env.return_value = mock_docker_client\n            c1 = _get_client()\n            self.assertIs(c1, mock_docker_client)\n            mock_from_env.assert_called_once()\n            c2 = _get_client()\n            self.assertIs(c2, mock_docker_client)\n            mock_from_env.assert_called_once()"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005142", "source": "def get_wheels(\n    output_dir: Path,\n    max_depth: Optional[int] = None,\n) -> list[str]:\n    root = Path(output_dir)\n    if not root.exists():\n        return []\n    items = []\n    for dirpath, _, filenames in os.walk(root):\n        depth = Path(dirpath).relative_to(root).parts\n        if max_depth is not None and len(depth) > max_depth:\n            continue\n        for fname in sorted(filenames):\n            if fname.endswith(\".whl\"):\n                pkg = fname.split(\"-\")[0]\n                relpath = str((Path(dirpath) / fname).relative_to(root))\n                items.append({\"pkg\": pkg, \"relpath\": relpath})\n    return items", "target": "def test_restores_even_on_exception(self):\n        var = \"TEST_TMP_ENV_EXCEPTION\"\n        self.assertNotIn(var, os.environ)\n        with self.assertRaises(RuntimeError):\n            with temp_environ({var: \"x\"}):\n                self.assertEqual(os.environ[var], \"x\")\n                raise RuntimeError(\"boom\")\n        self.assertNotIn(var, os.environ)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005143", "source": "def _dataclass_getstate(self: Any) -> list[Any]:\n                    return [getattr(self, f.name) for f in dataclasses.fields(self)]", "target": "def test_slots() -> None:\n    @dataclasses.dataclass(slots=True)\n    class Model:\n        x: int\n    schema = core_schema.dataclass_schema(\n        Model,\n        core_schema.dataclass_args_schema(\n            'Model', [core_schema.dataclass_field(name='x', schema=core_schema.int_schema())]\n        ),\n        ['x'],\n        slots=True,\n    )\n    val = SchemaValidator(schema)\n    m: Model\n    m = val.validate_python({'x': 123})\n    assert m == Model(x=123)\n    with pytest.raises(ValidationError):\n        val.validate_python({'x': 'abc'})\n    val.validate_assignment(m, 'x', 456)\n    assert m.x == 456\n    with pytest.raises(ValidationError):\n        val.validate_assignment(m, 'x', 'abc')"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005144", "source": "def env_bool(\n    name: str,\n    default: bool = False,\n) -> bool:\n    val = get_env(name)\n    if not val:\n        return default\n    return str2bool(val)", "target": "def test_get_env_set_returns_value(self):\n        with patch.dict(os.environ, {\"FOO\": \"bar\"}, clear=True):\n            self.assertEqual(m.get_env(\"FOO\", \"default\"), \"bar\")"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005145", "source": "def _add_pydantic_validation_attributes(\n    dc_cls: Type['Dataclass'],\n    config: Type[BaseConfig],\n    validate_on_init: bool,\n    dc_cls_doc: str,\n) -> None:\n    init = dc_cls.__init__\n    @wraps(init)\n    def handle_extra_init(self: 'Dataclass', *args: Any, **kwargs: Any) -> None:\n        if config.extra == Extra.ignore:\n            init(self, *args, **{k: v for k, v in kwargs.items() if k in self.__dataclass_fields__})\n        elif config.extra == Extra.allow:\n            for k, v in kwargs.items():\n                self.__dict__.setdefault(k, v)\n            init(self, *args, **{k: v for k, v in kwargs.items() if k in self.__dataclass_fields__})\n        else:\n            init(self, *args, **kwargs)\n    if hasattr(dc_cls, '__post_init__'):\n        try:\n            post_init = dc_cls.__post_init__.__wrapped__\n        except AttributeError:\n            post_init = dc_cls.__post_init__\n        @wraps(post_init)\n        def new_post_init(self: 'Dataclass', *args: Any, **kwargs: Any) -> None:\n            if config.post_init_call == 'before_validation':\n                post_init(self, *args, **kwargs)\n            if self.__class__.__pydantic_run_validation__:\n                self.__pydantic_validate_values__()\n                if hasattr(self, '__post_init_post_parse__'):\n                    self.__post_init_post_parse__(*args, **kwargs)\n            if config.post_init_call == 'after_validation':\n                post_init(self, *args, **kwargs)\n        setattr(dc_cls, '__init__', handle_extra_init)\n        setattr(dc_cls, '__post_init__', new_post_init)\n    else:\n        @wraps(init)\n        def new_init(self: 'Dataclass', *args: Any, **kwargs: Any) -> None:\n            handle_extra_init(self, *args, **kwargs)\n            if self.__class__.__pydantic_run_validation__:\n                self.__pydantic_validate_values__()\n            if hasattr(self, '__post_init_post_parse__'):\n                initvars_and_values: Dict[str, Any] = {}\n                for i, f in enumerate(self.__class__.__dataclass_fields__.values()):\n                    if f._field_type is dataclasses._FIELD_INITVAR:\n                        try:\n                            initvars_and_values[f.name] = args[i]\n                        except IndexError:\n                            initvars_and_values[f.name] = kwargs.get(f.name, f.default)\n                self.__post_init_post_parse__(**initvars_and_values)\n        setattr(dc_cls, '__init__', new_init)\n    setattr(dc_cls, '__pydantic_run_validation__', ClassAttribute('__pydantic_run_validation__', validate_on_init))\n    setattr(dc_cls, '__pydantic_initialised__', False)\n    setattr(dc_cls, '__pydantic_model__', create_pydantic_model_from_dataclass(dc_cls, config, dc_cls_doc))\n    setattr(dc_cls, '__pydantic_validate_values__', _dataclass_validate_values)\n    setattr(dc_cls, '__validate__', classmethod(_validate_dataclass))\n    setattr(dc_cls, '__get_validators__', classmethod(_get_validators))\n    if dc_cls.__pydantic_model__.__config__.validate_assignment and not dc_cls.__dataclass_params__.frozen:\n        setattr(dc_cls, '__setattr__', _dataclass_validate_assignment_setattr)", "target": "def test_only_allow_alias(py_and_json) -> None:\n    schema = core_schema.dataclass_schema(\n        BasicDataclass,\n        core_schema.dataclass_args_schema(\n            'BasicDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), validation_alias='FieldA'),\n            ],\n        ),\n        ['a'],\n        config=core_schema.CoreConfig(validate_by_name=False, validate_by_alias=True),\n    )\n    v = py_and_json(schema)\n    assert v.validate_test({'FieldA': 'hello'}) == BasicDataclass(a='hello')\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'a': 'hello'})"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005146", "source": "def set_validation(cls: Type['DataclassT'], value: bool) -> Generator[Type['DataclassT'], None, None]:\n    original_run_validation = cls.__pydantic_run_validation__\n    try:\n        cls.__pydantic_run_validation__ = value\n        yield cls\n    finally:\n        cls.__pydantic_run_validation__ = original_run_validation", "target": "def test_extra_custom_serializer():\n    @dataclasses.dataclass\n    class Model:\n        pass\n    schema = core_schema.dataclass_schema(\n        Model,\n        core_schema.dataclass_args_schema(\n            'Model',\n            [],\n            extra_behavior='allow',\n        ),\n        [],\n    )\n    s = SchemaSerializer(schema)\n    v = SchemaValidator(schema)\n    m = v.validate_python({'extra': 'extra'})\n    assert s.to_python(m) == {'extra': 'extra bam!'}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005147", "source": "def pydantic_encoder(obj: Any) -> Any:\n    from dataclasses import asdict, is_dataclass\n    from pydantic.v1.main import BaseModel\n    if isinstance(obj, BaseModel):\n        return obj.dict()\n    elif is_dataclass(obj):\n        return asdict(obj)\n    for base in obj.__class__.__mro__[:-1]:\n        try:\n            encoder = ENCODERS_BY_TYPE[base]\n        except KeyError:\n            continue\n        return encoder(obj)\n    else:\n        raise TypeError(f\"Object of type '{obj.__class__.__name__}' is not JSON serializable\")", "target": "def test_partial_parse():\n    with pytest.raises(ValueError, match='EOF while parsing a string at line 1 column 15'):\n        from_json('[\"aa\", \"bb\", \"c')\n    assert from_json('[\"aa\", \"bb\", \"c', allow_partial=True) == ['aa', 'bb']\n    with pytest.raises(ValueError, match='EOF while parsing a string at line 1 column 15'):\n        from_json(b'[\"aa\", \"bb\", \"c')\n    assert from_json(b'[\"aa\", \"bb\", \"c', allow_partial=True) == ['aa', 'bb']"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005148", "source": "def wrap(cls: Type[Any]) -> 'DataclassClassOrWrapper':\n        should_use_proxy = (\n            use_proxy\n            if use_proxy is not None\n            else (\n                is_builtin_dataclass(cls)\n                and (cls.__bases__[0] is object or set(dir(cls)) == set(dir(cls.__bases__[0])))\n            )\n        )\n        if should_use_proxy:\n            dc_cls_doc = ''\n            dc_cls = DataclassProxy(cls)\n            default_validate_on_init = False\n        else:\n            dc_cls_doc = cls.__doc__ or ''\n            if sys.version_info >= (3, 10):\n                dc_cls = dataclasses.dataclass(\n                    cls,\n                    init=init,\n                    repr=repr,\n                    eq=eq,\n                    order=order,\n                    unsafe_hash=unsafe_hash,\n                    frozen=frozen,\n                    kw_only=kw_only,\n                )\n            else:\n                dc_cls = dataclasses.dataclass(\n                    cls, init=init, repr=repr, eq=eq, order=order, unsafe_hash=unsafe_hash, frozen=frozen\n                )\n            default_validate_on_init = True\n        should_validate_on_init = default_validate_on_init if validate_on_init is None else validate_on_init\n        _add_pydantic_validation_attributes(cls, the_config, should_validate_on_init, dc_cls_doc)\n        dc_cls.__pydantic_model__.__try_update_forward_refs__(**{cls.__name__: cls})\n        return dc_cls", "target": "def test_extra_behavior_ignore(\n    config: Union[core_schema.CoreConfig, None],\n    schema_extra_behavior_kw: dict[str, Any],\n    validate_fn_extra_kw: Union[ExtraBehavior, None],\n):\n    @dataclasses.dataclass\n    class MyModel:\n        f: str\n    v = SchemaValidator(\n        core_schema.dataclass_schema(\n            MyModel,\n            core_schema.dataclass_args_schema(\n                'MyModel', [core_schema.dataclass_field('f', core_schema.str_schema())], **schema_extra_behavior_kw\n            ),\n            ['f'],\n        ),\n        config=config,\n    )\n    m: MyModel = v.validate_python({'f': 'x', 'extra_field': 123}, extra=validate_fn_extra_kw)\n    assert m.f == 'x'\n    assert not hasattr(m, 'extra_field')\n    v.validate_assignment(m, 'f', 'y', extra=validate_fn_extra_kw)\n    assert m.f == 'y'\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_assignment(m, 'not_f', 'xyz', extra=validate_fn_extra_kw)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'no_such_attribute',\n            'loc': ('not_f',),\n            'msg': \"Object has no attribute 'not_f'\",\n            'input': 'xyz',\n            'ctx': {'attribute': 'not_f'},\n        }\n    ]\n    assert not hasattr(m, 'not_f')"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005149", "source": "def secs_to_us(time_s):\n    return time_s * 1e6", "target": "def test_peek(self, input_: str = \"abcdef\") -> None:\n        iter_ = PeekableIterator(input_)\n        for idx, c in enumerate(iter_):\n            if idx + 1 < len(input_):\n                self.assertEqual(iter_.peek(), input_[idx + 1])\n            else:\n                self.assertTrue(iter_.peek() is None)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005150", "source": "def custom_pydantic_encoder(type_encoders: dict[Any, Callable[[type[Any]], Any]], obj: Any) -> Any:\n    warnings.warn(\n        '`custom_pydantic_encoder` is deprecated, use `BaseModel.model_dump` instead.',\n        category=PydanticDeprecatedSince20,\n        stacklevel=2,\n    )\n    for base in obj.__class__.__mro__[:-1]:\n        try:\n            encoder = type_encoders[base]\n        except KeyError:\n            continue\n        return encoder(obj)\n    else:\n        return pydantic_encoder(obj)", "target": "def test_json_or_python():\n    def s1(v: int) -> int:\n        return v + 1\n    def s2(v: int) -> int:\n        return v + 2\n    s = SchemaSerializer(\n        core_schema.json_or_python_schema(\n            core_schema.int_schema(serialization=core_schema.plain_serializer_function_ser_schema(s1)),\n            core_schema.int_schema(serialization=core_schema.plain_serializer_function_ser_schema(s2)),\n        )\n    )\n    assert s.to_json(0) == b'1'\n    assert s.to_python(0) == 2"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005151", "source": "def takes_validated_data_argument(\n    default_factory: Callable[[], Any] | Callable[[dict[str, Any]], Any],\n) -> TypeIs[Callable[[dict[str, Any]], Any]]:\n    try:\n        sig = signature(default_factory)\n    except (ValueError, TypeError):\n        return False\n    parameters = list(sig.parameters.values())\n    return len(parameters) == 1 and can_be_positional(parameters[0]) and parameters[0].default is Parameter.empty", "target": "def test_validate_assignment_strict_field():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={'field_a': core_schema.model_field(schema=core_schema.str_schema(strict=True))}\n        )\n    )\n    assert v.validate_python({'field_a': 'test'}) == ({'field_a': 'test'}, None, {'field_a'})\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_assignment({'field_a': 'test'}, 'field_a', b'abc')\n    assert exc_info.value.errors(include_url=False) == [\n        {'input': b'abc', 'type': 'string_type', 'loc': ('field_a',), 'msg': 'Input should be a valid string'}\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005152", "source": "def _call_initvar(*args: Any, **kwargs: Any) -> NoReturn:\n        raise TypeError(\"'InitVar' object is not callable\")", "target": "def test_alias_allow_pop(py_and_json: PyAndJson):\n    schema = core_schema.dataclass_schema(\n        BasicDataclass,\n        core_schema.dataclass_args_schema(\n            'BasicDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), validation_alias='FieldA'),\n            ],\n        ),\n        ['a'],\n        config=core_schema.CoreConfig(validate_by_name=True, validate_by_alias=True),\n    )\n    v = py_and_json(schema)\n    assert v.validate_test({'FieldA': 'hello'}) == BasicDataclass(a='hello')\n    assert v.validate_test({'a': 'hello'}) == BasicDataclass(a='hello')\n    assert v.validate_test(\n        {\n            'FieldA': 'hello',\n            'a': 'world',\n        }\n    ) == BasicDataclass(a='hello')\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'foobar': 'hello'})"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005153", "source": "def dataclass(\n        _cls: Type[_T],\n        *,\n        init: bool = True,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool = False,\n        config: Union[ConfigDict, Type[object], None] = None,\n        validate_on_init: Optional[bool] = None,\n        use_proxy: Optional[bool] = None,\n    ) -> 'DataclassClassOrWrapper':\n        ...", "target": "def test_dataclass_args(py_and_json: PyAndJson, input_value, expected):\n    schema = core_schema.dataclass_args_schema(\n        'MyDataclass',\n        [\n            core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), kw_only=False),\n            core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), kw_only=False),\n        ],\n    )\n    v = py_and_json(schema)\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)) as exc_info:\n            v.validate_test(input_value)\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        assert v.validate_test(input_value) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005154", "source": "def _general_metadata_cls() -> type[BaseMetadata]:\n    from annotated_types import BaseMetadata\n    class _PydanticGeneralMetadata(PydanticMetadata, BaseMetadata):\n        def __init__(self, metadata: Any):\n            self.__dict__ = metadata\n    return _PydanticGeneralMetadata", "target": "def test_wrap_on_error(self, py_and_json: PyAndJson):\n        def wrap_function(input_value, validator, info):\n            try:\n                return validator(input_value)\n            except ValidationError:\n                if isinstance(input_value, list):\n                    return str(len(input_value))\n                else:\n                    return repr(input_value)\n        v = py_and_json(\n            {\n                'type': 'model-fields',\n                'fields': {\n                    'x': {\n                        'type': 'model-field',\n                        'schema': {\n                            'type': 'default',\n                            'on_error': 'raise',\n                            'schema': {\n                                'type': 'function-wrap',\n                                'function': {'type': 'with-info', 'function': wrap_function},\n                                'schema': {'type': 'str'},\n                            },\n                        },\n                    }\n                },\n            }\n        )\n        assert v.validate_test({'x': 'foo'}) == ({'x': 'foo'}, None, {'x'})\n        assert v.validate_test({'x': ['foo']}) == ({'x': '1'}, None, {'x'})\n        assert v.validate_test({'x': ['foo', 'bar']}) == ({'x': '2'}, None, {'x'})\n        assert v.validate_test({'x': {'a': 'b'}}) == ({'x': \"{'a': 'b'}\"}, None, {'x'})"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005155", "source": "def _general_metadata_cls() -> type[BaseMetadata]:\n    from annotated_types import BaseMetadata\n    class _PydanticGeneralMetadata(PydanticMetadata, BaseMetadata):\n        def __init__(self, metadata: Any):\n            self.__dict__ = metadata\n    return _PydanticGeneralMetadata", "target": "def test_aliases_path_negative(input_value, expected):\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={'field_a': core_schema.model_field(validation_alias=['foo', -2], schema=core_schema.int_schema())}\n        ),\n        config=CoreConfig(loc_by_alias=False),\n    )\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=expected.message):\n            val = v.validate_python(input_value)\n            print(f'UNEXPECTED OUTPUT: {val!r}')\n    else:\n        output = v.validate_python(input_value)\n        assert output == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005156", "source": "def _call_initvar(*args: Any, **kwargs: Any) -> NoReturn:\n        raise TypeError(\"'InitVar' object is not callable\")", "target": "def test_dataclass_field_wrap_validator1():\n    @dataclasses.dataclass\n    class Foo:\n        a: int\n        b: str\n        @classmethod\n        def validate_b(\n            cls, v: bytes, nxt: core_schema.ValidatorFunctionWrapHandler, info: core_schema.ValidationInfo\n        ) -> str:\n            assert v == b'hello'\n            v = nxt(v)\n            assert v == 'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return 'hello world!'\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(\n                    name='b',\n                    schema=core_schema.with_info_wrap_validator_function(Foo.validate_b, core_schema.str_schema()),\n                ),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 1, 'b': b'hello'})\n    assert dataclasses.asdict(foo) == {'a': 1, 'b': 'hello world!'}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005157", "source": "def dataclass(\n        _cls: Type[_T],\n        *,\n        init: bool = True,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool = False,\n        config: Union[ConfigDict, Type[object], None] = None,\n        validate_on_init: Optional[bool] = None,\n        use_proxy: Optional[bool] = None,\n    ) -> 'DataclassClassOrWrapper':\n        ...", "target": "def test_slots_dataclass_subclass(revalidate_instances, input_value, expected):\n    schema = core_schema.dataclass_schema(\n        FooDataclassSlots,\n        core_schema.dataclass_args_schema(\n            'FooDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n            ],\n            extra_behavior='forbid',\n        ),\n        ['a', 'b'],\n        revalidate_instances=revalidate_instances,\n        slots=True,\n    )\n    v = SchemaValidator(schema)\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=expected.message) as exc_info:\n            print(v.validate_python(input_value))\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        dc = v.validate_python(input_value)\n        assert dataclasses.is_dataclass(dc)\n        assert dataclasses.asdict(dc) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005158", "source": "def dataclass(\n    _cls: type[_T] | None = None,\n    *,\n    init: Literal[False] = False,\n    repr: bool = True,\n    eq: bool = True,\n    order: bool = False,\n    unsafe_hash: bool = False,\n    frozen: bool | None = None,\n    config: ConfigDict | type[object] | None = None,\n    validate_on_init: bool | None = None,\n    kw_only: bool = False,\n    slots: bool = False,\n) -> Callable[[type[_T]], type[PydanticDataclass]] | type[PydanticDataclass]:\n    assert init is False, 'pydantic.dataclasses.dataclass only supports init=False'\n    assert validate_on_init is not False, 'validate_on_init=False is no longer supported'\n    if sys.version_info >= (3, 10):\n        kwargs = {'kw_only': kw_only, 'slots': slots}\n    else:\n        kwargs = {}\n    def create_dataclass(cls: type[Any]) -> type[PydanticDataclass]:\n        from ._internal._utils import is_model_class\n        if is_model_class(cls):\n            raise PydanticUserError(\n                f'Cannot create a Pydantic dataclass from {cls.__name__} as it is already a Pydantic model',\n                code='dataclass-on-model',\n            )\n        original_cls = cls\n        has_dataclass_base = any(dataclasses.is_dataclass(base) for base in cls.__bases__)\n        if not has_dataclass_base and config is not None and hasattr(cls, '__pydantic_config__'):\n            warn(\n                f'`config` is set via both the `dataclass` decorator and `__pydantic_config__` for dataclass {cls.__name__}. '\n                f'The `config` specification from `dataclass` decorator will take priority.',\n                category=UserWarning,\n                stacklevel=2,\n            )\n        config_dict = config if config is not None else getattr(cls, '__pydantic_config__', None)\n        config_wrapper = _config.ConfigWrapper(config_dict)\n        decorators = _decorators.DecoratorInfos.build(cls, replace_wrapped_methods=True)\n        decorators.update_from_config(config_wrapper)\n        original_doc = cls.__doc__\n        if _pydantic_dataclasses.is_stdlib_dataclass(cls):\n            original_doc = None\n            bases = (cls,)\n            if issubclass(cls, Generic):\n                generic_base = Generic[cls.__parameters__]\n                bases = bases + (generic_base,)\n            cls = types.new_class(cls.__name__, bases)\n        if frozen is not None:\n            frozen_ = frozen\n            if config_wrapper.frozen:\n                warn(\n                    f'`frozen` is set via both the `dataclass` decorator and `config` for dataclass {cls.__name__!r}.'\n                    'This is not recommended. The `frozen` specification on `dataclass` will take priority.',\n                    category=UserWarning,\n                    stacklevel=2,\n                )\n        else:\n            frozen_ = config_wrapper.frozen or False\n        cls_anns = _typing_extra.safe_get_annotations(cls)\n        for field_name in cls_anns:\n            field_value = getattr(cls, field_name, None)\n            if isinstance(field_value, FieldInfo):\n                setattr(cls, field_name, _pydantic_dataclasses.as_dataclass_field(field_value))\n        with _pydantic_dataclasses.patch_base_fields(cls):\n            cls = dataclasses.dataclass(\n                cls,\n                init=True,\n                repr=repr,\n                eq=eq,\n                order=order,\n                unsafe_hash=unsafe_hash,\n                frozen=frozen_,\n                **kwargs,\n            )\n        if config_wrapper.validate_assignment:\n            original_setattr = cls.__setattr__\n            @functools.wraps(cls.__setattr__)\n            def validated_setattr(instance: PydanticDataclass, name: str, value: Any, /) -> None:\n                if frozen_:\n                    return original_setattr(instance, name, value)\n                inst_cls = type(instance)\n                attr = getattr(inst_cls, name, None)\n                if isinstance(attr, property):\n                    attr.__set__(instance, value)\n                elif isinstance(attr, functools.cached_property):\n                    instance.__dict__.__setitem__(name, value)\n                else:\n                    inst_cls.__pydantic_validator__.validate_assignment(instance, name, value)\n            cls.__setattr__ = validated_setattr.__get__(None, cls)\n            if slots and not hasattr(cls, '__setstate__'):\n                def _dataclass_getstate(self: Any) -> list[Any]:\n                    return [getattr(self, f.name) for f in dataclasses.fields(self)]\n                def _dataclass_setstate(self: Any, state: list[Any]) -> None:\n                    for field, value in zip(dataclasses.fields(self), state):\n                        object.__setattr__(self, field.name, value)\n                cls.__getstate__ = _dataclass_getstate\n                cls.__setstate__ = _dataclass_setstate\n        cls.__is_pydantic_dataclass__ = True\n        cls.__pydantic_decorators__ = decorators\n        cls.__doc__ = original_doc\n        firstlineno = getattr(original_cls, '__firstlineno__', None)\n        cls.__module__ = original_cls.__module__\n        if sys.version_info >= (3, 13) and firstlineno is not None:\n            original_cls.__firstlineno__ = firstlineno\n            cls.__firstlineno__ = firstlineno\n        cls.__qualname__ = original_cls.__qualname__\n        cls.__pydantic_fields_complete__ = classmethod(_pydantic_fields_complete)\n        cls.__pydantic_complete__ = False\n        _pydantic_dataclasses.complete_dataclass(cls, config_wrapper, raise_errors=False)\n        return cls\n    return create_dataclass if _cls is None else create_dataclass(_cls)", "target": "def test_dataclass_args_init_only_no_fields(py_and_json: PyAndJson, input_value, expected):\n    schema = core_schema.dataclass_args_schema(\n        'MyDataclass', [core_schema.dataclass_field(name='a', schema=core_schema.str_schema())], collect_init_only=True\n    )\n    v = py_and_json(schema)\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=expected.message) as exc_info:\n            v.validate_test(input_value)\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        assert v.validate_test(input_value) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005159", "source": "def decorator(f: Callable[..., T]) -> Callable[..., T]:\n        @wraps(f)\n        def wrapper(*args: list[Any], **kwargs: dict[str, Any]) -> T:\n            for idx in range(num_retries):\n                try:\n                    return f(*args, **kwargs)\n                except Exception as e:\n                    print(\n                        f'Attempt {idx} of {num_retries} to call {f.__name__} failed with \"{e}\"'\n                    )\n            return cast(T, rc)\n        return wrapper", "target": "def test_compute_diff(self) -> None:\n        diff = self.repo.diff(\"HEAD\")\n        sha = _shasum(diff)\n        self.assertEqual(len(sha), 64)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005160", "source": "def is_valid_field_name(name: str) -> bool:\n    return not name.startswith('_')", "target": "def test_allow_extra_wrong():\n    with pytest.raises(SchemaError, match='Invalid extra_behavior: `wrong`'):\n        SchemaValidator(\n            schema=core_schema.model_fields_schema(fields={}), config=CoreConfig(extra_fields_behavior='wrong')\n        )"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005161", "source": "def get_origin(t: Type[Any]) -> Optional[Type[Any]]:\n        if type(t).__name__ in AnnotatedTypeNames:\n            return cast(Type[Any], Annotated)\n        return getattr(t, '__origin__', None)", "target": "def test_schema_typing() -> None:\n    schema: CoreSchema = {\n        'type': 'union',\n        'choices': [{'type': 'int'}, {'type': 'int', 'ge': 1}, {'type': 'float', 'lt': 1.0}],\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = {\n        'type': 'tagged-union',\n        'discriminator': 'type',\n        'choices': {\n            'apple': {\n                'type': 'typed-dict',\n                'fields': {'pips': {'type': 'typed-dict-field', 'schema': {'type': 'int'}}},\n            },\n            'banana': {\n                'type': 'typed-dict',\n                'fields': {'curvature': {'type': 'typed-dict-field', 'schema': {'type': 'float'}}},\n            },\n        },\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'int', 'ge': 1}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'float', 'lt': 1.0}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'str', 'pattern': r'http://.*'}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'bool', 'strict': False}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'literal', 'expected': [1, '1']}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'any'}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'none'}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'bytes'}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'list', 'items_schema': {'type': 'str'}, 'min_length': 3}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'set', 'items_schema': {'type': 'str'}, 'max_length': 3}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'tuple', 'items_schema': [{'type': 'str'}], 'variadic_item_index': 0, 'max_length': 3}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'tuple', 'items_schema': [{'type': 'str'}, {'type': 'int'}]}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'frozenset', 'items_schema': {'type': 'str'}, 'max_length': 3}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'dict', 'keys_schema': {'type': 'str'}, 'values_schema': {'type': 'any'}}\n    SchemaValidator(schema)\n    schema: CoreSchema = {\n        'type': 'typed-dict',\n        'fields': {'bar': {'type': 'typed-dict-field', 'schema': {'type': 'str'}}},\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = {\n        'type': 'model',\n        'cls': Foo,\n        'schema': {'type': 'model-fields', 'fields': {'bar': {'type': 'model-field', 'schema': {'type': 'str'}}}},\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = {\n        'type': 'typed-dict',\n        'fields': {\n            'a': {'type': 'typed-dict-field', 'schema': {'type': 'str'}},\n            'b': {'type': 'typed-dict-field', 'schema': {'type': 'str'}, 'validation_alias': 'foobar'},\n            'c': {\n                'type': 'typed-dict-field',\n                'schema': {'type': 'str'},\n                'validation_alias': [['foobar', 0, 'bar'], ['foo']],\n            },\n            'd': {\n                'type': 'typed-dict-field',\n                'schema': {'type': 'default', 'schema': {'type': 'str'}, 'default': 'spam'},\n            },\n        },\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = {\n        'type': 'function-wrap',\n        'function': {'type': 'with-info', 'function': wrap_validator, 'field_name': 'foobar'},\n        'schema': {'type': 'str'},\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = core_schema.with_info_plain_validator_function(validator)\n    SchemaValidator(schema)\n    schema: CoreSchema = {\n        'type': 'definitions',\n        'schema': {'type': 'definition-ref', 'schema_ref': 'Branch'},\n        'definitions': [\n            {\n                'type': 'typed-dict',\n                'fields': {\n                    'name': {'type': 'typed-dict-field', 'schema': {'type': 'str'}},\n                    'sub_branch': {\n                        'type': 'typed-dict-field',\n                        'schema': {\n                            'type': 'default',\n                            'schema': {\n                                'type': 'nullable',\n                                'schema': {'type': 'definition-ref', 'schema_ref': 'Branch'},\n                            },\n                            'default': None,\n                        },\n                    },\n                },\n                'ref': 'Branch',\n            }\n        ],\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'date', 'le': date.today()}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'time', 'lt': time(12, 13, 14)}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'datetime', 'ge': datetime.now()}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'is-instance', 'cls': Foo}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'callable'}\n    SchemaValidator(schema)\n    schema: CoreSchema = {\n        'type': 'arguments',\n        'arguments_schema': [\n            {'name': 'a', 'mode': 'positional_only', 'schema': {'type': 'int'}},\n            {'name': 'b', 'schema': {'type': 'str'}},\n            {'name': 'c', 'mode': 'keyword_only', 'schema': {'type': 'bool'}},\n        ],\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'call', 'arguments_schema': {'type': 'any'}, 'function': foo}\n    SchemaValidator(schema)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005162", "source": "def dataclass(\n        _cls: Type[_T],\n        *,\n        init: bool = True,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool = False,\n        config: Union[ConfigDict, Type[object], None] = None,\n        validate_on_init: Optional[bool] = None,\n        use_proxy: Optional[bool] = None,\n    ) -> 'DataclassClassOrWrapper':\n        ...", "target": "def test_dataclass_field_plain_validator():\n    @dataclasses.dataclass\n    class Foo:\n        a: int\n        b: str\n        @classmethod\n        def validate_b(cls, v: bytes, info: core_schema.ValidationInfo) -> str:\n            assert v == b'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return 'hello world!'\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(\n                    name='b', schema=core_schema.with_info_plain_validator_function(Foo.validate_b)\n                ),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 1, 'b': b'hello'})\n    assert dataclasses.asdict(foo) == {'a': 1, 'b': 'hello world!'}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005163", "source": "def path_exists_validator(v: Any) -> Path:\n    if not v.exists():\n        raise errors.PathNotExistsError(path=v)\n    return v", "target": "def test_validators_build(benchmark) -> None:\n    class Base1(BaseModel):\n        a: int\n        @field_validator('a', mode='after')\n        @classmethod\n        def val_a(cls, value: int) -> int: ...\n        @computed_field\n        def prop(self) -> int: ...\n    class Bare:\n        @computed_field\n        def prop_bare(self) -> int: ...\n    class Sub1(Base1):\n        @computed_field\n        def prop_2(self) -> int: ...\n        @computed_field\n        def prop_3(self) -> int: ...\n        @computed_field\n        def prop_4(self) -> int: ...\n    @benchmark\n    def bench() -> None:\n        class SubS(Sub1, Bare, defer_build=True):\n            @computed_field\n            def prop_5(self) -> int: ...\n            @computed_field\n            def prop_6(self) -> int: ...\n            @computed_field\n            def prop_7(self) -> int: ..."}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005164", "source": "def dataclass(\n        _cls: type[_T],\n        *,\n        init: Literal[False] = False,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool | None = None,\n        config: ConfigDict | type[object] | None = None,\n        validate_on_init: bool | None = None,\n    ) -> type[PydanticDataclass]: ...", "target": "def test_dataclass_self_init_alias_field_name():\n    @dataclasses.dataclass(init=False)\n    class Foo:\n        a: str\n        b: bool\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), validation_alias='aAlias'),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), validation_alias=['bAlias', 0]),\n            ],\n        ),\n        ['a', 'b'],\n        config={'loc_by_alias': False},\n    )\n    v = SchemaValidator(schema)\n    def __init__(self, *args, **kwargs):\n        v.validate_python(ArgsKwargs(args, kwargs), self_instance=self)\n    Foo.__init__ = __init__\n    foo = Foo(aAlias=b'hello', bAlias=['True'])\n    assert dataclasses.asdict(foo) == {'a': 'hello', 'b': True}\n    with pytest.raises(ValidationError) as exc_info:\n        Foo(aAlias=b'hello', bAlias=['wrong'])\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'bool_parsing',\n            'loc': ('b',),\n            'msg': 'Input should be a valid boolean, unable to interpret input',\n            'input': 'wrong',\n        }\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005165", "source": "def patch_base_fields(cls: type[Any]) -> Generator[None]:\n    original_fields_list: list[tuple[DcFields, DcFields]] = []\n    for base in cls.__mro__[1:]:\n        dc_fields: dict[str, dataclasses.Field[Any]] = base.__dict__.get('__dataclass_fields__', {})\n        dc_fields_with_pydantic_field_defaults = {\n            field_name: field\n            for field_name, field in dc_fields.items()\n            if isinstance(field.default, FieldInfo)\n            and (field.default.description is not None or field.default.kw_only or field.default.repr is not True)\n        }\n        if dc_fields_with_pydantic_field_defaults:\n            original_fields_list.append((dc_fields, dc_fields_with_pydantic_field_defaults))\n            for field_name, field in dc_fields_with_pydantic_field_defaults.items():\n                default = cast(FieldInfo, field.default)\n                new_dc_field = copy.copy(field)\n                if sys.version_info >= (3, 10) and default.kw_only:\n                    new_dc_field.kw_only = True\n                if default.repr is not True:\n                    new_dc_field.repr = default.repr\n                dc_fields[field_name] = new_dc_field\n    try:\n        yield\n    finally:\n        for fields, original_fields in original_fields_list:\n            for field_name, original_field in original_fields.items():\n                fields[field_name] = original_field", "target": "def test_function_validator_wrapping_args_schema_before() -> None:\n    calls: list[Any] = []\n    def func(*args: Any) -> Any:\n        calls.append(args)\n        return args[0]\n    @dataclasses.dataclass\n    class Model:\n        number: int = 1\n    cs = core_schema.dataclass_schema(\n        Model,\n        core_schema.no_info_before_validator_function(\n            func,\n            core_schema.dataclass_args_schema(\n                'Model', [core_schema.dataclass_field('number', core_schema.int_schema())]\n            ),\n        ),\n        ['number'],\n    )\n    v = SchemaValidator(cs)\n    instance: Model = v.validate_python({'number': 1})\n    assert instance.number == 1\n    assert calls == [({'number': 1},)]\n    v.validate_assignment(instance, 'number', 2)\n    assert instance.number == 2\n    assert calls == [({'number': 1},), ({'number': 2},)]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005166", "source": "def pydantic_encoder(obj: Any) -> Any:\n    from dataclasses import asdict, is_dataclass\n    from pydantic.v1.main import BaseModel\n    if isinstance(obj, BaseModel):\n        return obj.dict()\n    elif is_dataclass(obj):\n        return asdict(obj)\n    for base in obj.__class__.__mro__[:-1]:\n        try:\n            encoder = ENCODERS_BY_TYPE[base]\n        except KeyError:\n            continue\n        return encoder(obj)\n    else:\n        raise TypeError(f\"Object of type '{obj.__class__.__name__}' is not JSON serializable\")", "target": "def test_typed_dict():\n    v = SchemaValidator(\n        core_schema.typed_dict_schema(\n            fields={\n                'field_a': core_schema.typed_dict_field(schema=core_schema.str_schema()),\n                'field_b': core_schema.typed_dict_field(schema=core_schema.int_schema()),\n            }\n        )\n    )\n    input_str = '{\"field_a\": \"abc\", \"field_b\": 1}'\n    assert v.validate_json(input_str) == {'field_a': 'abc', 'field_b': 1}\n    input_str = '{\"field_a\": \"a\", \"field_a\": \"b\", \"field_b\": 1}'\n    assert v.validate_json(input_str) == {'field_a': 'b', 'field_b': 1}\n    assert v.validate_json(input_str) == {'field_a': 'b', 'field_b': 1}"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005167", "source": "def secs_to_us(time_s):\n    return time_s * 1e6", "target": "def test_noop_when_empty_path(self):\n        start = Path.cwd()\n        with working_directory(\"\"):\n            self.assertEqual(Path.cwd(), start)\n        self.assertEqual(Path.cwd(), start)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005168", "source": "def set_dataclass_fields(\n    cls: type[StandardDataclass],\n    config_wrapper: _config.ConfigWrapper,\n    ns_resolver: NsResolver | None = None,\n) -> None:\n    typevars_map = get_standard_typevars_map(cls)\n    fields = collect_dataclass_fields(\n        cls, ns_resolver=ns_resolver, typevars_map=typevars_map, config_wrapper=config_wrapper\n    )\n    cls.__pydantic_fields__ = fields", "target": "def test_dataclass_slots_field_after_validator():\n    @dataclasses.dataclass(slots=True)\n    class Foo:\n        a: int\n        b: str\n        @classmethod\n        def validate_b(cls, v: str, info: core_schema.ValidationInfo) -> str:\n            assert v == 'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return 'hello world!'\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(\n                    name='b',\n                    schema=core_schema.with_info_after_validator_function(Foo.validate_b, core_schema.str_schema()),\n                ),\n            ],\n        ),\n        ['a', 'b'],\n        slots=True,\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 1, 'b': b'hello'})\n    assert dataclasses.asdict(foo) == {'a': 1, 'b': 'hello world!'}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005169", "source": "def _dataclass_validate_assignment_setattr(self: 'Dataclass', name: str, value: Any) -> None:\n    if self.__pydantic_initialised__:\n        d = dict(self.__dict__)\n        d.pop(name, None)\n        known_field = self.__pydantic_model__.__fields__.get(name, None)\n        if known_field:\n            value, error_ = known_field.validate(value, d, loc=name, cls=self.__class__)\n            if error_:\n                raise ValidationError([error_], self.__class__)\n    object.__setattr__(self, name, value)", "target": "def test_dataclass_self_init_post_init():\n    calls = []\n    @dataclasses.dataclass(init=False)\n    class Foo:\n        a: str\n        b: bool\n        c: dataclasses.InitVar[int]\n        def __init__(self, *args, **kwargs):\n            v.validate_python(ArgsKwargs(args, kwargs), self_instance=self)\n        def __post_init__(self, c):\n            calls.append(c)\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), kw_only=False),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), kw_only=False),\n                core_schema.dataclass_field(name='c', schema=core_schema.int_schema(), init_only=True),\n            ],\n            collect_init_only=True,\n        ),\n        ['a', 'b', 'c'],\n        post_init=True,\n    )\n    v = SchemaValidator(schema)\n    foo = Foo(b'hello', 'True', c='123')\n    assert dataclasses.is_dataclass(foo)\n    assert dataclasses.asdict(foo) == {'a': 'hello', 'b': True}\n    assert calls == [123]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005170", "source": "def as_dataclass_field(pydantic_field: FieldInfo) -> dataclasses.Field[Any]:\n    field_args: dict[str, Any] = {'default': pydantic_field}\n    if sys.version_info >= (3, 14) and pydantic_field.description is not None:\n        field_args['doc'] = pydantic_field.description\n    if sys.version_info >= (3, 10) and pydantic_field.kw_only:\n        field_args['kw_only'] = True\n    if pydantic_field.repr is not True:\n        field_args['repr'] = pydantic_field.repr\n    return dataclasses.field(**field_args)", "target": "def test_frozen():\n    @dataclasses.dataclass\n    class MyModel:\n        f: str\n    v = SchemaValidator(\n        core_schema.dataclass_schema(\n            MyModel,\n            core_schema.dataclass_args_schema('MyModel', [core_schema.dataclass_field('f', core_schema.str_schema())]),\n            ['f'],\n            frozen=True,\n        )\n    )\n    m = v.validate_python({'f': 'x'})\n    assert m.f == 'x'\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_assignment(m, 'f', 'y')\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'frozen_instance', 'loc': (), 'msg': 'Instance is frozen', 'input': 'y'}\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005171", "source": "def dataclass(\n        *,\n        init: Literal[False] = False,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool | None = None,\n        config: ConfigDict | type[object] | None = None,\n        validate_on_init: bool | None = None,\n    ) -> Callable[[type[_T]], type[PydanticDataclass]]:\n        ...", "target": "def test_slots() -> None:\n    @dataclasses.dataclass(slots=True)\n    class Model:\n        x: int\n    schema = core_schema.dataclass_schema(\n        Model,\n        core_schema.dataclass_args_schema(\n            'Model', [core_schema.dataclass_field(name='x', schema=core_schema.int_schema())]\n        ),\n        ['x'],\n        slots=True,\n    )\n    val = SchemaValidator(schema)\n    m: Model\n    m = val.validate_python({'x': 123})\n    assert m == Model(x=123)\n    with pytest.raises(ValidationError):\n        val.validate_python({'x': 'abc'})\n    val.validate_assignment(m, 'x', 456)\n    assert m.x == 456\n    with pytest.raises(ValidationError):\n        val.validate_assignment(m, 'x', 'abc')"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005172", "source": "def inherit_config(self_config: 'ConfigType', parent_config: 'ConfigType', **namespace: Any) -> 'ConfigType':\n    if not self_config:\n        base_classes: Tuple['ConfigType', ...] = (parent_config,)\n    elif self_config == parent_config:\n        base_classes = (self_config,)\n    else:\n        base_classes = self_config, parent_config\n    namespace['json_encoders'] = {\n        **getattr(parent_config, 'json_encoders', {}),\n        **getattr(self_config, 'json_encoders', {}),\n        **namespace.get('json_encoders', {}),\n    }\n    return type('Config', base_classes, namespace)", "target": "def test_hide_input_in_errors(config, input_str):\n    v = SchemaValidator(\n        cs.model_schema(\n            cls=MyModel, schema=cs.model_fields_schema(fields={'f': cs.model_field(schema=cs.str_schema())})\n        ),\n        config=config,\n    )\n    with pytest.raises(ValidationError, match=re.escape(f'Input should be a valid string [{input_str}]')):\n        assert v.validate_python({'f': 123})"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005173", "source": "def _is_field_cached_property(obj: 'Dataclass', k: str) -> bool:\n        return isinstance(getattr(type(obj), k, None), cached_property)", "target": "def test_dataclass_wrap_json():\n    schema = core_schema.no_info_wrap_validator_function(\n        lambda v, handler: handler(v),\n        core_schema.dataclass_schema(\n            FooDataclass,\n            core_schema.dataclass_args_schema(\n                'FooDataclass',\n                [\n                    core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                    core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n                ],\n            ),\n            ['a', 'b'],\n        ),\n    )\n    v = SchemaValidator(schema)\n    assert v.validate_json('{\"a\": \"hello\", \"b\": true}') == FooDataclass(a='hello', b=True)\n    assert v.validate_json('{\"a\": \"hello\", \"b\": true}', strict=True) == FooDataclass(a='hello', b=True)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005174", "source": "def rebuild_model_fields(\n    cls: type[BaseModel],\n    *,\n    config_wrapper: ConfigWrapper,\n    ns_resolver: NsResolver,\n    typevars_map: Mapping[TypeVar, Any],\n) -> dict[str, FieldInfo]:\n    rebuilt_fields: dict[str, FieldInfo] = {}\n    with ns_resolver.push(cls):\n        for f_name, field_info in cls.__pydantic_fields__.items():\n            if field_info._complete:\n                rebuilt_fields[f_name] = field_info\n            else:\n                new_field = _recreate_field_info(\n                    field_info, ns_resolver=ns_resolver, typevars_map=typevars_map, lenient=False\n                )\n                update_field_from_config(config_wrapper, f_name, new_field)\n                rebuilt_fields[f_name] = new_field\n    return rebuilt_fields", "target": "def test_only_validate_by_name(py_and_json) -> None:\n    v = py_and_json(\n        {\n            'type': 'model-fields',\n            'fields': {'field_a': {'validation_alias': 'FieldA', 'type': 'model-field', 'schema': {'type': 'int'}}},\n        },\n        config=CoreConfig(validate_by_name=True, validate_by_alias=False),\n    )\n    assert v.validate_test({'field_a': '123'}) == ({'field_a': 123}, None, {'field_a'})\n    with pytest.raises(ValidationError, match=r'field_a\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'FieldA': '123'})"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005175", "source": "def isoformat(o: Union[datetime.date, datetime.time]) -> str:\n    return o.isoformat()", "target": "def test_enum() -> None:\n    class MyEnum(Enum):\n        a = 'a'\n        b = 'b'\n    enum_schema = core_schema.lax_or_strict_schema(\n        core_schema.no_info_after_validator_function(MyEnum, core_schema.str_schema()),\n        core_schema.is_instance_schema(MyEnum),\n    )\n    v = core_schema.json_schema(enum_schema)\n    v = SchemaValidator(v)\n    assert v.validate_python('\"a\"') == MyEnum.a\n    assert v.validate_python('\"b\"') == MyEnum.b\n    with pytest.raises(ValidationError):\n        v.validate_python('\"c\"')"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005176", "source": "def flatten(schema: Definition) -> FlatIntermediateDefinition:\n    result: FlatIntermediateDefinition = {}\n    _flatten(key_prefix=(), sub_schema=schema, result=result)\n    for k, v in result.items():\n        assert isinstance(k, tuple)\n        assert all(isinstance(ki, str) for ki in k)\n        assert isinstance(v, (TimerArgs, GroupedBenchmark))\n    return result", "target": "def test_merged_lastfailed_content_with_overlap(self) -> None:\n        last_failed_source = {\n            \"tools/tests/test_foo.py::test_num1\": True,\n            \"tools/tests/test_foo.py::test_num2\": True,\n            \"tools/tests/test_bar.py::test_num1\": True,\n        }\n        last_failed_dest = {\n            \"tools/tests/test_foo.py::test_num1\": True,\n            \"tools/tests/test_car.py::test_num1\": True,\n            \"tools/tests/test_car.py::test_num2\": True,\n        }\n        last_failed_merged = {\n            \"tools/tests/test_foo.py::test_num1\": True,\n            \"tools/tests/test_foo.py::test_num2\": True,\n            \"tools/tests/test_bar.py::test_num1\": True,\n            \"tools/tests/test_car.py::test_num1\": True,\n            \"tools/tests/test_car.py::test_num2\": True,\n        }\n        merged = _merged_lastfailed_content(last_failed_source, last_failed_dest)\n        self.assertEqual(merged, last_failed_merged)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005177", "source": "def decimal_encoder(dec_value: Decimal) -> Union[int, float]:\n    if dec_value.as_tuple().exponent >= 0:\n        return int(dec_value)\n    else:\n        return float(dec_value)", "target": "def test_any_python(input_value, expected):\n    v = SchemaValidator(core_schema.json_schema())\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            v.validate_python(input_value)\n    else:\n        assert v.validate_python(input_value) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005178", "source": "def new_init(self: 'Dataclass', *args: Any, **kwargs: Any) -> None:\n            handle_extra_init(self, *args, **kwargs)\n            if self.__class__.__pydantic_run_validation__:\n                self.__pydantic_validate_values__()\n            if hasattr(self, '__post_init_post_parse__'):\n                initvars_and_values: Dict[str, Any] = {}\n                for i, f in enumerate(self.__class__.__dataclass_fields__.values()):\n                    if f._field_type is dataclasses._FIELD_INITVAR:\n                        try:\n                            initvars_and_values[f.name] = args[i]\n                        except IndexError:\n                            initvars_and_values[f.name] = kwargs.get(f.name, f.default)\n                self.__post_init_post_parse__(**initvars_and_values)", "target": "def test_dataclass_slots_field_before_validator():\n    @dataclasses.dataclass(slots=True)\n    class Foo:\n        a: int\n        b: str\n        @classmethod\n        def validate_b(cls, v: bytes, info: core_schema.ValidationInfo) -> bytes:\n            assert v == b'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return b'hello world!'\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(\n                    name='b',\n                    schema=core_schema.with_info_before_validator_function(Foo.validate_b, core_schema.str_schema()),\n                ),\n            ],\n        ),\n        ['a', 'b'],\n        slots=True,\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 1, 'b': b'hello'})\n    assert dataclasses.asdict(foo) == {'a': 1, 'b': 'hello world!'}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005179", "source": "def timedelta_isoformat(td: datetime.timedelta) -> str:\n    minutes, seconds = divmod(td.seconds, 60)\n    hours, minutes = divmod(minutes, 60)\n    return f'{\"-\" if td.days < 0 else \"\"}P{abs(td.days)}DT{hours:d}H{minutes:d}M{seconds:d}.{td.microseconds:06d}S'", "target": "def test_json_invalid():\n    v = SchemaValidator(core_schema.bool_schema())\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_json('\"foobar')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'json_invalid',\n            'loc': (),\n            'msg': 'Invalid JSON: EOF while parsing a string at line 1 column 7',\n            'input': '\"foobar',\n            'ctx': {'error': 'EOF while parsing a string at line 1 column 7'},\n        }\n    ]\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_json('[1,\\n2,\\n3,]')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'json_invalid',\n            'loc': (),\n            'msg': 'Invalid JSON: trailing comma at line 3 column 3',\n            'input': '[1,\\n2,\\n3,]',\n            'ctx': {'error': 'trailing comma at line 3 column 3'},\n        }\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005180", "source": "def is_union(tp: Optional[Type[Any]]) -> bool:\n        return tp is Union", "target": "def test_ser_function_wrap():\n    def f(\n        input: Any, serialize: core_schema.SerializerFunctionWrapHandler, info: core_schema.SerializationInfo, /\n    ) -> str:\n        return f'{serialize} {info}'\n    s = SchemaSerializer(\n        core_schema.any_schema(\n            serialization=core_schema.wrap_serializer_function_ser_schema(\n                f, info_arg=True, schema=core_schema.str_schema(), when_used='json'\n            )\n        )\n    )\n    assert s.to_python(123, mode='json') == (\n        'SerializationCallable(serializer=str) '\n        \"SerializationInfo(include=None, exclude=None, context=None, mode='json', by_alias=False, exclude_unset=False, \"\n        'exclude_defaults=False, exclude_none=False, exclude_computed_fields=False, round_trip=False, serialize_as_any=False)'\n    )"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005181", "source": "def cherry_pick_commits(self, from_branch: str, to_branch: str) -> None:\n        orig_branch = self.current_branch()\n        assert orig_branch is not None, \"Must be on a branch\"\n        self.checkout(to_branch)\n        from_commits, to_commits = self.compute_branch_diffs(from_branch, to_branch)\n        if len(from_commits) == 0:\n            print(\"Nothing to do\")\n            self.checkout(orig_branch)\n            return\n        for commit in reversed(from_commits):\n            print(f\"Cherry picking commit {commit}\")\n            self.cherry_pick(commit)\n        self.checkout(orig_branch)", "target": "def test_iterator(self, input_: str = \"abcdef\") -> None:\n        iter_ = PeekableIterator(input_)\n        for idx, c in enumerate(iter_):\n            self.assertEqual(c, input_[idx])"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005182", "source": "def is_valid_privateattr_name(name: str) -> bool:\n    return name.startswith('_') and not name.startswith('__')", "target": "def test_from_attributes_function(input_value, expected):\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={'a': core_schema.model_field(schema=core_schema.any_schema())}, from_attributes=True\n        )\n    )\n    model_dict, model_extra, fields_set = v.validate_python(input_value)\n    assert model_dict == expected\n    assert model_extra is None\n    assert fields_set == {'a'}"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005183", "source": "def commit_message(self, ref: str) -> str:\n        return self._run_git(\"log\", \"-1\", \"--format=%B\", ref)", "target": "def test_double_asterisks(self) -> None:\n        allowed_patterns = [\n            \"aten/src/ATen/native/**LinearAlgebra*\",\n        ]\n        patterns_re = patterns_to_regex(allowed_patterns)\n        fnames = [\n            \"aten/src/ATen/native/LinearAlgebra.cpp\",\n            \"aten/src/ATen/native/cpu/LinearAlgebraKernel.cpp\",\n        ]\n        for filename in fnames:\n            self.assertTrue(patterns_re.match(filename))"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005184", "source": "def rev_parse(self, name: str) -> str:\n        return self._run_git(\"rev-parse\", \"--verify\", name).strip()", "target": "def test_double_asterisks(self) -> None:\n        allowed_patterns = [\n            \"aten/src/ATen/native/**LinearAlgebra*\",\n        ]\n        patterns_re = patterns_to_regex(allowed_patterns)\n        fnames = [\n            \"aten/src/ATen/native/LinearAlgebra.cpp\",\n            \"aten/src/ATen/native/cpu/LinearAlgebraKernel.cpp\",\n        ]\n        for filename in fnames:\n            self.assertTrue(patterns_re.match(filename))"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005185", "source": "def is_valid_field_name(name: str) -> bool:\n    return not name.startswith('_')", "target": "def test_alias_extra_from_attributes():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            extra_behavior='allow',\n            from_attributes=True,\n            fields={\n                'field_a': core_schema.model_field(\n                    validation_alias=[['FieldA'], ['foo', 2]], schema=core_schema.int_schema()\n                )\n            },\n        )\n    )\n    assert v.validate_python({'FieldA': 1}) == ({'field_a': 1}, {}, {'field_a'})\n    assert v.validate_python(Cls(FieldA=1)) == ({'field_a': 1}, {}, {'field_a'})\n    assert v.validate_python(Cls(foo=[1, 2, 3])) == ({'field_a': 3}, {}, {'field_a'})\n    assert v.validate_python({'foo': [1, 2, 3]}) == ({'field_a': 3}, {}, {'field_a'})"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005186", "source": "def _general_metadata_cls() -> type[BaseMetadata]:\n    from annotated_types import BaseMetadata\n    class _PydanticGeneralMetadata(PydanticMetadata, BaseMetadata):\n        def __init__(self, metadata: Any):\n            self.__dict__ = metadata\n    return _PydanticGeneralMetadata", "target": "def test_validate_assignment_functions():\n    calls: list[Any] = []\n    def func_a(input_value, info):\n        calls.append(('func_a', input_value))\n        return input_value * 2\n    def func_b(input_value, info):\n        calls.append(('func_b', input_value))\n        return input_value / 2\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'field_a': core_schema.model_field(\n                    schema={\n                        'type': 'function-after',\n                        'function': {'type': 'with-info', 'function': func_a},\n                        'schema': core_schema.str_schema(),\n                    }\n                ),\n                'field_b': core_schema.model_field(\n                    schema={\n                        'type': 'function-after',\n                        'function': {'type': 'with-info', 'function': func_b},\n                        'schema': core_schema.int_schema(),\n                    }\n                ),\n            }\n        )\n    )\n    assert v.validate_python({'field_a': 'test', 'field_b': 12.0}) == (\n        {'field_a': 'testtest', 'field_b': 6},\n        None,\n        {'field_a', 'field_b'},\n    )\n    assert calls == [('func_a', 'test'), ('func_b', 12)]\n    calls.clear()\n    assert v.validate_assignment({'field_a': 'testtest', 'field_b': 6}, 'field_a', 'new-val') == (\n        {'field_a': 'new-valnew-val', 'field_b': 6},\n        None,\n        {'field_a'},\n    )\n    assert calls == [('func_a', 'new-val')]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005187", "source": "def pydantic_general_metadata(**metadata: Any) -> BaseMetadata:\n    return _general_metadata_cls()(metadata)", "target": "def test_allow_extra_fn_override_wrong():\n    v = SchemaValidator(schema=core_schema.model_fields_schema(fields={}))\n    with pytest.raises(ValueError, match='Invalid extra_behavior: `wrong`'):\n        v.validate_python({}, extra='wrong')"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005188", "source": "def arbitrary_type_validator(v: Any) -> T:\n        if isinstance(v, type_):\n            return v\n        raise errors.ArbitraryTypeError(expected_arbitrary_type=type_)", "target": "def test_validators_build(benchmark) -> None:\n    class Base1(BaseModel):\n        a: int\n        @field_validator('a', mode='after')\n        @classmethod\n        def val_a(cls, value: int) -> int: ...\n        @computed_field\n        def prop(self) -> int: ...\n    class Bare:\n        @computed_field\n        def prop_bare(self) -> int: ...\n    class Sub1(Base1):\n        @computed_field\n        def prop_2(self) -> int: ...\n        @computed_field\n        def prop_3(self) -> int: ...\n        @computed_field\n        def prop_4(self) -> int: ...\n    @benchmark\n    def bench() -> None:\n        class SubS(Sub1, Bare, defer_build=True):\n            @computed_field\n            def prop_5(self) -> int: ...\n            @computed_field\n            def prop_6(self) -> int: ...\n            @computed_field\n            def prop_7(self) -> int: ..."}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005189", "source": "def isoformat(o: Union[datetime.date, datetime.time]) -> str:\n    return o.isoformat()", "target": "def test_json_or_python():\n    def s1(v: int) -> int:\n        return v + 1\n    def s2(v: int) -> int:\n        return v + 2\n    s = SchemaSerializer(\n        core_schema.json_or_python_schema(\n            core_schema.int_schema(serialization=core_schema.plain_serializer_function_ser_schema(s1)),\n            core_schema.int_schema(serialization=core_schema.plain_serializer_function_ser_schema(s2)),\n        )\n    )\n    assert s.to_json(0) == b'1'\n    assert s.to_python(0) == 2"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005190", "source": "def dataclass(\n        *,\n        init: bool = True,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool = False,\n        config: Union[ConfigDict, Type[object], None] = None,\n        validate_on_init: Optional[bool] = None,\n        use_proxy: Optional[bool] = None,\n    ) -> Callable[[Type[_T]], 'DataclassClassOrWrapper']:\n        ...", "target": "def test_dataclass_args_init(input_value, extra_behavior, expected):\n    @dataclasses.dataclass\n    class Foo:\n        a: str\n        b: str\n        def __post_init__(self):\n            self.b = self.a.upper()\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.str_schema(), init=False),\n            ],\n            extra_behavior=extra_behavior,\n        ),\n        ['a', 'b'],\n        post_init=True,\n    )\n    v = SchemaValidator(schema)\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)) as exc_info:\n            v.validate_python(input_value)\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        assert dataclasses.asdict(v.validate_python(input_value)) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005191", "source": "def is_literal_type(type_: Type[Any]) -> bool:\n    return Literal is not None and get_origin(type_) in LITERAL_TYPES", "target": "def test_schema_typing() -> None:\n    schema: CoreSchema = {\n        'type': 'union',\n        'choices': [{'type': 'int'}, {'type': 'int', 'ge': 1}, {'type': 'float', 'lt': 1.0}],\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = {\n        'type': 'tagged-union',\n        'discriminator': 'type',\n        'choices': {\n            'apple': {\n                'type': 'typed-dict',\n                'fields': {'pips': {'type': 'typed-dict-field', 'schema': {'type': 'int'}}},\n            },\n            'banana': {\n                'type': 'typed-dict',\n                'fields': {'curvature': {'type': 'typed-dict-field', 'schema': {'type': 'float'}}},\n            },\n        },\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'int', 'ge': 1}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'float', 'lt': 1.0}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'str', 'pattern': r'http://.*'}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'bool', 'strict': False}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'literal', 'expected': [1, '1']}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'any'}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'none'}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'bytes'}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'list', 'items_schema': {'type': 'str'}, 'min_length': 3}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'set', 'items_schema': {'type': 'str'}, 'max_length': 3}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'tuple', 'items_schema': [{'type': 'str'}], 'variadic_item_index': 0, 'max_length': 3}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'tuple', 'items_schema': [{'type': 'str'}, {'type': 'int'}]}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'frozenset', 'items_schema': {'type': 'str'}, 'max_length': 3}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'dict', 'keys_schema': {'type': 'str'}, 'values_schema': {'type': 'any'}}\n    SchemaValidator(schema)\n    schema: CoreSchema = {\n        'type': 'typed-dict',\n        'fields': {'bar': {'type': 'typed-dict-field', 'schema': {'type': 'str'}}},\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = {\n        'type': 'model',\n        'cls': Foo,\n        'schema': {'type': 'model-fields', 'fields': {'bar': {'type': 'model-field', 'schema': {'type': 'str'}}}},\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = {\n        'type': 'typed-dict',\n        'fields': {\n            'a': {'type': 'typed-dict-field', 'schema': {'type': 'str'}},\n            'b': {'type': 'typed-dict-field', 'schema': {'type': 'str'}, 'validation_alias': 'foobar'},\n            'c': {\n                'type': 'typed-dict-field',\n                'schema': {'type': 'str'},\n                'validation_alias': [['foobar', 0, 'bar'], ['foo']],\n            },\n            'd': {\n                'type': 'typed-dict-field',\n                'schema': {'type': 'default', 'schema': {'type': 'str'}, 'default': 'spam'},\n            },\n        },\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = {\n        'type': 'function-wrap',\n        'function': {'type': 'with-info', 'function': wrap_validator, 'field_name': 'foobar'},\n        'schema': {'type': 'str'},\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = core_schema.with_info_plain_validator_function(validator)\n    SchemaValidator(schema)\n    schema: CoreSchema = {\n        'type': 'definitions',\n        'schema': {'type': 'definition-ref', 'schema_ref': 'Branch'},\n        'definitions': [\n            {\n                'type': 'typed-dict',\n                'fields': {\n                    'name': {'type': 'typed-dict-field', 'schema': {'type': 'str'}},\n                    'sub_branch': {\n                        'type': 'typed-dict-field',\n                        'schema': {\n                            'type': 'default',\n                            'schema': {\n                                'type': 'nullable',\n                                'schema': {'type': 'definition-ref', 'schema_ref': 'Branch'},\n                            },\n                            'default': None,\n                        },\n                    },\n                },\n                'ref': 'Branch',\n            }\n        ],\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'date', 'le': date.today()}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'time', 'lt': time(12, 13, 14)}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'datetime', 'ge': datetime.now()}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'is-instance', 'cls': Foo}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'callable'}\n    SchemaValidator(schema)\n    schema: CoreSchema = {\n        'type': 'arguments',\n        'arguments_schema': [\n            {'name': 'a', 'mode': 'positional_only', 'schema': {'type': 'int'}},\n            {'name': 'b', 'schema': {'type': 'str'}},\n            {'name': 'c', 'mode': 'keyword_only', 'schema': {'type': 'bool'}},\n        ],\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'call', 'arguments_schema': {'type': 'any'}, 'function': foo}\n    SchemaValidator(schema)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005192", "source": "def create_pydantic_model_from_dataclass(\n    dc_cls: Type['Dataclass'],\n    config: Type[Any] = BaseConfig,\n    dc_cls_doc: Optional[str] = None,\n) -> Type['BaseModel']:\n    field_definitions: Dict[str, Any] = {}\n    for field in dataclasses.fields(dc_cls):\n        default: Any = Undefined\n        default_factory: Optional['NoArgAnyCallable'] = None\n        field_info: FieldInfo\n        if field.default is not dataclasses.MISSING:\n            default = field.default\n        elif field.default_factory is not dataclasses.MISSING:\n            default_factory = field.default_factory\n        else:\n            default = Required\n        if isinstance(default, FieldInfo):\n            field_info = default\n            dc_cls.__pydantic_has_field_info_default__ = True\n        else:\n            field_info = Field(default=default, default_factory=default_factory, **field.metadata)\n        field_definitions[field.name] = (field.type, field_info)\n    validators = gather_all_validators(dc_cls)\n    model: Type['BaseModel'] = create_model(\n        dc_cls.__name__,\n        __config__=config,\n        __module__=dc_cls.__module__,\n        __validators__=validators,\n        __cls_kwargs__={'__resolve_forward_refs__': False},\n        **field_definitions,\n    )\n    model.__doc__ = dc_cls_doc if dc_cls_doc is not None else dc_cls.__doc__ or ''\n    return model", "target": "def test_only_allow_alias(py_and_json) -> None:\n    schema = core_schema.dataclass_schema(\n        BasicDataclass,\n        core_schema.dataclass_args_schema(\n            'BasicDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), validation_alias='FieldA'),\n            ],\n        ),\n        ['a'],\n        config=core_schema.CoreConfig(validate_by_name=False, validate_by_alias=True),\n    )\n    v = py_and_json(schema)\n    assert v.validate_test({'FieldA': 'hello'}) == BasicDataclass(a='hello')\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'a': 'hello'})"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005193", "source": "def get_origin(t: Type[Any]) -> Optional[Type[Any]]:\n        if type(t).__name__ in AnnotatedTypeNames:\n            return cast(Type[Any], Annotated)\n        return getattr(t, '__origin__', None)", "target": "def test_ser_function_plain():\n    def f(input: Any, info: core_schema.SerializationInfo, /) -> str:\n        return str(info)\n    s = SchemaSerializer(\n        core_schema.any_schema(\n            serialization=core_schema.plain_serializer_function_ser_schema(\n                f, info_arg=True, return_schema=core_schema.str_schema()\n            )\n        )\n    )\n    assert s.to_python(123) == (\n        \"SerializationInfo(include=None, exclude=None, context=None, mode='python', by_alias=False, exclude_unset=False, \"\n        'exclude_defaults=False, exclude_none=False, exclude_computed_fields=False, round_trip=False, serialize_as_any=False)'\n    )"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005194", "source": "def custom_pydantic_encoder(type_encoders: Dict[Any, Callable[[Type[Any]], Any]], obj: Any) -> Any:\n    for base in obj.__class__.__mro__[:-1]:\n        try:\n            encoder = type_encoders[base]\n        except KeyError:\n            continue\n        return encoder(obj)\n    else:\n        return pydantic_encoder(obj)", "target": "def test_dict():\n    v = SchemaValidator(\n        core_schema.dict_schema(keys_schema=core_schema.int_schema(), values_schema=core_schema.int_schema())\n    )\n    assert v.validate_json('{\"1\": 2, \"3\": 4}') == {1: 2, 3: 4}\n    assert json.loads('{\"1\": 1, \"1\": 2}') == {'1': 2}\n    assert v.validate_json('{\"1\": 1, \"1\": 2}') == {1: 2}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005195", "source": "def new_post_init(self: 'Dataclass', *args: Any, **kwargs: Any) -> None:\n            if config.post_init_call == 'before_validation':\n                post_init(self, *args, **kwargs)\n            if self.__class__.__pydantic_run_validation__:\n                self.__pydantic_validate_values__()\n                if hasattr(self, '__post_init_post_parse__'):\n                    self.__post_init_post_parse__(*args, **kwargs)\n            if config.post_init_call == 'after_validation':\n                post_init(self, *args, **kwargs)", "target": "def test_dataclass_field_wrap_validator2():\n    @dataclasses.dataclass\n    class Foo:\n        a: int\n        b: str\n        @classmethod\n        def validate_b(\n            cls, v: bytes, nxt: core_schema.ValidatorFunctionWrapHandler, info: core_schema.ValidationInfo\n        ) -> bytes:\n            assert v == b'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return nxt(b'hello world!')\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(\n                    name='b',\n                    schema=core_schema.with_info_wrap_validator_function(Foo.validate_b, core_schema.str_schema()),\n                ),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 1, 'b': b'hello'})\n    assert dataclasses.asdict(foo) == {'a': 1, 'b': 'hello world!'}"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005196", "source": "def fetch(self, ref: Optional[str] = None, branch: Optional[str] = None) -> None:\n        if branch is None and ref is None:\n            self._run_git(\"fetch\", self.remote)\n        elif branch is None:\n            self._run_git(\"fetch\", self.remote, ref)\n        else:\n            self._run_git(\"fetch\", self.remote, f\"{ref}:{branch}\")", "target": "def test_simple(self) -> None:\n        @retries_decorator()\n        def foo(x: int, y: int) -> int:\n            return x + y\n        self.assertEqual(foo(3, 4), 7)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005197", "source": "def is_typeddict_special(type_: Any) -> bool:\n    return _check_typeddict_special(type_) or _check_typeddict_special(get_origin(type_))", "target": "def test_schema_validator_wrong() -> None:\n    try:\n        SchemaValidator({'type': 'bad'})\n    except SchemaError:\n        pass\n    else:\n        raise AssertionError('SchemaValidator did not raise SchemaError')"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005198", "source": "def patch_id(self, ref: Union[str, list[str]]) -> list[tuple[str, str]]:\n        is_list = isinstance(ref, list)\n        if is_list:\n            if len(ref) == 0:\n                return []\n            ref = \" \".join(ref)\n        rc = _check_output(\n            [\"sh\", \"-c\", f\"git -C {self.repo_dir} show {ref}|git patch-id --stable\"]\n        ).strip()\n        return [cast(tuple[str, str], x.split(\" \", 1)) for x in rc.split(\"\\n\")]", "target": "def test_peek(self, input_: str = \"abcdef\") -> None:\n        iter_ = PeekableIterator(input_)\n        for idx, c in enumerate(iter_):\n            if idx + 1 < len(input_):\n                self.assertEqual(iter_.peek(), input_[idx + 1])\n            else:\n                self.assertTrue(iter_.peek() is None)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005199", "source": "def dataclass(\n        _cls: type[_T],\n        *,\n        init: Literal[False] = False,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool | None = None,\n        config: ConfigDict | type[object] | None = None,\n        validate_on_init: bool | None = None,\n    ) -> type[PydanticDataclass]: ...", "target": "def test_dataclass_exact_validation(revalidate_instances, input_value, expected):\n    schema = core_schema.dataclass_schema(\n        FooDataclass,\n        core_schema.dataclass_args_schema(\n            'FooDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n            ],\n        ),\n        ['a', 'b'],\n        revalidate_instances=revalidate_instances,\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python(input_value)\n    assert dataclasses.asdict(foo) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005200", "source": "def is_namedtuple(type_: Type[Any]) -> bool:\n    from pydantic.v1.utils import lenient_issubclass\n    return lenient_issubclass(type_, tuple) and hasattr(type_, '_fields')", "target": "def test_schema_typing_error() -> None:\n    _: CoreSchema = {'type': 'wrong'}"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005201", "source": "def check_for_functorch():\n    try:\n        import functorch\n        return True\n    except ImportError:\n        return False", "target": "def test_raises_for_missing_dir(self):\n        start = Path.cwd()\n        with tempfile.TemporaryDirectory() as td:\n            missing = Path(td) / \"does_not_exist\"\n            with self.assertRaises(FileNotFoundError):\n                with working_directory(str(missing)):\n                    pass\n        self.assertEqual(Path.cwd(), start)"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005202", "source": "def flatten(schema: Definition) -> FlatIntermediateDefinition:\n    result: FlatIntermediateDefinition = {}\n    _flatten(key_prefix=(), sub_schema=schema, result=result)\n    for k, v in result.items():\n        assert isinstance(k, tuple)\n        assert all(isinstance(ki, str) for ki in k)\n        assert isinstance(v, (TimerArgs, GroupedBenchmark))\n    return result", "target": "def test_peek(self, input_: str = \"abcdef\") -> None:\n        iter_ = PeekableIterator(input_)\n        for idx, c in enumerate(iter_):\n            if idx + 1 < len(input_):\n                self.assertEqual(iter_.peek(), input_[idx + 1])\n            else:\n                self.assertTrue(iter_.peek() is None)"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005203", "source": "def gh_owner_and_name(self) -> tuple[str, str]:\n        url = os.getenv(\"GIT_REMOTE_URL\", None)\n        if url is None:\n            url = self.remote_url()\n        rc = RE_GITHUB_URL_MATCH.match(url)\n        if rc is None:\n            raise RuntimeError(f\"Unexpected url format {url}\")\n        return cast(tuple[str, str], rc.groups())", "target": "def test_ghstack_branches_in_sync(self) -> None:\n        head_ref = \"gh/SS-JIA/206/head\"\n        self._skip_if_ref_does_not_exist(head_ref)\n        self.assertTrue(are_ghstack_branches_in_sync(self.repo, head_ref))"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005204", "source": "def _update_fields_from_docstrings(cls: type[Any], fields: dict[str, FieldInfo], use_inspect: bool = False) -> None:\n    fields_docs = extract_docstrings_from_cls(cls, use_inspect=use_inspect)\n    for ann_name, field_info in fields.items():\n        if field_info.description is None and ann_name in fields_docs:\n            field_info.description = fields_docs[ann_name]", "target": "def test_from_attributes_override_true():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={'a': core_schema.model_field(schema=core_schema.int_schema())}, from_attributes=False\n        )\n    )\n    with pytest.raises(ValidationError, match='Input should be a valid dictionary'):\n        v.validate_python(Cls(a=1))\n    assert v.validate_python(Cls(a=1), from_attributes=True) == ({'a': 1}, None, {'a'})\n    assert v.isinstance_python(Cls(a=1), from_attributes=True) is True\n    assert v.isinstance_python(Cls(a=1)) is False"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005205", "source": "def decimal_encoder(dec_value: Decimal) -> Union[int, float]:\n    if dec_value.as_tuple().exponent >= 0:\n        return int(dec_value)\n    else:\n        return float(dec_value)", "target": "def test_dict():\n    v = SchemaValidator(\n        core_schema.dict_schema(keys_schema=core_schema.int_schema(), values_schema=core_schema.int_schema())\n    )\n    assert v.validate_json('{\"1\": 2, \"3\": 4}') == {1: 2, 3: 4}\n    assert json.loads('{\"1\": 1, \"1\": 2}') == {'1': 2}\n    assert v.validate_json('{\"1\": 1, \"1\": 2}') == {1: 2}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005206", "source": "def is_pydantic_dataclass(class_: type[Any], /) -> TypeGuard[type[PydanticDataclass]]:\n    try:\n        return '__is_pydantic_dataclass__' in class_.__dict__ and dataclasses.is_dataclass(class_)\n    except AttributeError:\n        return False", "target": "def test_dataclass_slots_field_before_validator():\n    @dataclasses.dataclass(slots=True)\n    class Foo:\n        a: int\n        b: str\n        @classmethod\n        def validate_b(cls, v: bytes, info: core_schema.ValidationInfo) -> bytes:\n            assert v == b'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return b'hello world!'\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(\n                    name='b',\n                    schema=core_schema.with_info_before_validator_function(Foo.validate_b, core_schema.str_schema()),\n                ),\n            ],\n        ),\n        ['a', 'b'],\n        slots=True,\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 1, 'b': b'hello'})\n    assert dataclasses.asdict(foo) == {'a': 1, 'b': 'hello world!'}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005207", "source": "def dataclass(\n        *,\n        init: bool = True,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool = False,\n        config: Union[ConfigDict, Type[object], None] = None,\n        validate_on_init: Optional[bool] = None,\n        use_proxy: Optional[bool] = None,\n    ) -> Callable[[Type[_T]], 'DataclassClassOrWrapper']:\n        ...", "target": "def test_dataclass_slots_field_after_validator():\n    @dataclasses.dataclass(slots=True)\n    class Foo:\n        a: int\n        b: str\n        @classmethod\n        def validate_b(cls, v: str, info: core_schema.ValidationInfo) -> str:\n            assert v == 'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return 'hello world!'\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(\n                    name='b',\n                    schema=core_schema.with_info_after_validator_function(Foo.validate_b, core_schema.str_schema()),\n                ),\n            ],\n        ),\n        ['a', 'b'],\n        slots=True,\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 1, 'b': b'hello'})\n    assert dataclasses.asdict(foo) == {'a': 1, 'b': 'hello world!'}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005208", "source": "def _check_protected_namespaces(\n    protected_namespaces: tuple[str | Pattern[str], ...],\n    ann_name: str,\n    bases: tuple[type[Any], ...],\n    cls_name: str,\n) -> None:\n    BaseModel = import_cached_base_model()\n    for protected_namespace in protected_namespaces:\n        ns_violation = False\n        if isinstance(protected_namespace, Pattern):\n            ns_violation = protected_namespace.match(ann_name) is not None\n        elif isinstance(protected_namespace, str):\n            ns_violation = ann_name.startswith(protected_namespace)\n        if ns_violation:\n            for b in bases:\n                if hasattr(b, ann_name):\n                    if not (issubclass(b, BaseModel) and ann_name in getattr(b, '__pydantic_fields__', {})):\n                        raise ValueError(\n                            f'Field {ann_name!r} conflicts with member {getattr(b, ann_name)}'\n                            f' of protected namespace {protected_namespace!r}.'\n                        )\n            else:\n                valid_namespaces: list[str] = []\n                for pn in protected_namespaces:\n                    if isinstance(pn, Pattern):\n                        if not pn.match(ann_name):\n                            valid_namespaces.append(f're.compile({pn.pattern!r})')\n                    else:\n                        if not ann_name.startswith(pn):\n                            valid_namespaces.append(f\"'{pn}'\")\n                valid_namespaces_str = f'({\", \".join(valid_namespaces)}{\",)\" if len(valid_namespaces) == 1 else \")\"}'\n                warnings.warn(\n                    f'Field {ann_name!r} in {cls_name!r} conflicts with protected namespace {protected_namespace!r}.\\n\\n'\n                    f\"You may be able to solve this by setting the 'protected_namespaces' configuration to {valid_namespaces_str}.\",\n                    UserWarning,\n                    stacklevel=5,\n                )", "target": "def test_on_error_default_factory(self, py_and_json: PyAndJson):\n        v = py_and_json(\n            {\n                'type': 'model-fields',\n                'fields': {\n                    'x': {\n                        'type': 'model-field',\n                        'schema': {\n                            'type': 'default',\n                            'schema': {'type': 'str'},\n                            'on_error': 'default',\n                            'default_factory': lambda: 'pika',\n                        },\n                    }\n                },\n            }\n        )\n        assert v.validate_test({'x': 'foo'}) == ({'x': 'foo'}, None, {'x'})\n        assert v.validate_test({'x': ['foo']}) == ({'x': 'pika'}, None, {'x'})"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005209", "source": "def env_path_optional(\n    name: str,\n    default: Optional[Union[str, Path]] = None,\n    resolve: bool = True,\n) -> Optional[Path]:\n    val = get_env(name) or default\n    if not val:\n        return None\n    path = Path(val)\n    return path.resolve() if resolve else path", "target": "def test_env_bool_uses_default_when_unset(self):\n        with patch.dict(os.environ, {}, clear=True):\n            self.assertTrue(m.env_bool(\"FLAG\", default=True))\n            self.assertFalse(m.env_bool(\"FLAG\", default=False))"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005210", "source": "def dataclass(\n        _cls: type[_T],\n        *,\n        init: Literal[False] = False,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool | None = None,\n        config: ConfigDict | type[object] | None = None,\n        validate_on_init: bool | None = None,\n    ) -> type[PydanticDataclass]: ...", "target": "def test_dataclass_self_init_post_init():\n    calls = []\n    @dataclasses.dataclass(init=False)\n    class Foo:\n        a: str\n        b: bool\n        c: dataclasses.InitVar[int]\n        def __init__(self, *args, **kwargs):\n            v.validate_python(ArgsKwargs(args, kwargs), self_instance=self)\n        def __post_init__(self, c):\n            calls.append(c)\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), kw_only=False),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), kw_only=False),\n                core_schema.dataclass_field(name='c', schema=core_schema.int_schema(), init_only=True),\n            ],\n            collect_init_only=True,\n        ),\n        ['a', 'b', 'c'],\n        post_init=True,\n    )\n    v = SchemaValidator(schema)\n    foo = Foo(b'hello', 'True', c='123')\n    assert dataclasses.is_dataclass(foo)\n    assert dataclasses.asdict(foo) == {'a': 'hello', 'b': True}\n    assert calls == [123]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005211", "source": "def convert_generics(tp: Type[Any]) -> Type[Any]:\n        origin = get_origin(tp)\n        if not origin or not hasattr(tp, '__args__'):\n            return tp\n        args = get_args(tp)\n        if origin is Annotated:\n            return Annotated[(convert_generics(args[0]), *args[1:])]\n        converted = tuple(\n            ForwardRef(arg) if isinstance(arg, str) and isinstance(tp, TypingGenericAlias) else convert_generics(arg)\n            for arg in args\n        )\n        if converted == args:\n            return tp\n        elif isinstance(tp, TypingGenericAlias):\n            return TypingGenericAlias(origin, converted)\n        elif isinstance(tp, TypesUnionType):\n            return functools.reduce(operator.or_, converted)\n        else:\n            try:\n                setattr(tp, '__args__', converted)\n            except AttributeError:\n                pass\n            return tp", "target": "def test_wrong_function_signature() -> None:\n    def wrong_validator(value: Any) -> Any:\n        return value\n    v = SchemaValidator(core_schema.with_info_plain_validator_function(wrong_validator))\n    try:\n        v.validate_python(1)\n    except TypeError as exc:\n        assert 'takes 1 positional argument but 2 were given' in str(exc)\n    else:\n        raise AssertionError('v.validate_python(1) did not raise TypeError')"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005212", "source": "def pydantic_encoder(obj: Any) -> Any:\n    warnings.warn(\n        '`pydantic_encoder` is deprecated, use `pydantic_core.to_jsonable_python` instead.',\n        category=PydanticDeprecatedSince20,\n        stacklevel=2,\n    )\n    from dataclasses import asdict, is_dataclass\n    BaseModel = import_cached_base_model()\n    if isinstance(obj, BaseModel):\n        return obj.model_dump()\n    elif is_dataclass(obj):\n        return asdict(obj)\n    for base in obj.__class__.__mro__[:-1]:\n        try:\n            encoder = ENCODERS_BY_TYPE[base]\n        except KeyError:\n            continue\n        return encoder(obj)\n    else:\n        raise TypeError(f\"Object of type '{obj.__class__.__name__}' is not JSON serializable\")", "target": "def test_to_jsonable_python():\n    assert to_jsonable_python([1, 2]) == [1, 2]\n    assert to_jsonable_python({1, 2}) == IsList(1, 2, check_order=False)\n    assert to_jsonable_python([1, b'x']) == [1, 'x']\n    assert to_jsonable_python([0, 1, 2, 3, 4], exclude={1, 3}) == [0, 2, 4]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005213", "source": "def _pydantic_fields_complete(cls: type[PydanticDataclass]) -> bool:\n    return all(field_info._complete for field_info in cls.__pydantic_fields__.values())", "target": "def test_dataclass_validate_assignment():\n    schema = core_schema.dataclass_schema(\n        FooDataclass,\n        core_schema.dataclass_args_schema(\n            'FooDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), kw_only=False),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), kw_only=False),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 'hello', 'b': 'True'})\n    assert dataclasses.asdict(foo) == {'a': 'hello', 'b': True}\n    v.validate_assignment(foo, 'a', b'world')\n    assert dataclasses.asdict(foo) == {'a': 'world', 'b': True}\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_assignment(foo, 'a', 123)\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'string_type', 'loc': ('a',), 'msg': 'Input should be a valid string', 'input': 123}\n    ]\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_assignment(foo, 'c', '123')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'no_such_attribute',\n            'loc': ('c',),\n            'msg': \"Object has no attribute 'c'\",\n            'input': '123',\n            'ctx': {'attribute': 'c'},\n        }\n    ]\n    assert not hasattr(foo, 'c')\n    with pytest.raises(AttributeError, match=\"'str' object has no attribute 'a'\"):\n        v.validate_assignment('field_a', 'c', 123)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005214", "source": "def _dataclass_validate_assignment_setattr(self: 'Dataclass', name: str, value: Any) -> None:\n    if self.__pydantic_initialised__:\n        d = dict(self.__dict__)\n        d.pop(name, None)\n        known_field = self.__pydantic_model__.__fields__.get(name, None)\n        if known_field:\n            value, error_ = known_field.validate(value, d, loc=name, cls=self.__class__)\n            if error_:\n                raise ValidationError([error_], self.__class__)\n    object.__setattr__(self, name, value)", "target": "def test_dataclass_json_duplicate_keys():\n    @dataclasses.dataclass\n    class MyDataclass:\n        name: str\n        age: int\n    schema = core_schema.dataclass_schema(\n        MyDataclass,\n        core_schema.dataclass_args_schema(\n            'MyDataclass',\n            [\n                core_schema.dataclass_field(name='name', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='age', schema=core_schema.int_schema()),\n            ],\n        ),\n        ['name', 'age'],\n    )\n    v = SchemaValidator(schema)\n    json_with_duplicates = '{\"name\": \"Alice\", \"age\": 30, \"name\": \"Bob\", \"age\": 25}'\n    result = v.validate_json(json_with_duplicates)\n    assert result.name == 'Bob', \"Last value for 'name' should win\"\n    assert result.age == 25, \"Last value for 'age' should win\"\n    assert dataclasses.asdict(result) == {'name': 'Bob', 'age': 25}\n    json_multiple_duplicates = '{\"name\": \"First\", \"age\": 1, \"name\": \"Second\", \"name\": \"Third\", \"age\": 3}'\n    result2 = v.validate_json(json_multiple_duplicates)\n    assert result2.name == 'Third', 'Last value among multiple duplicates should win'\n    assert result2.age == 3\n    assert dataclasses.asdict(result2) == {'name': 'Third', 'age': 3}"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005215", "source": "def get_wheels(\n    output_dir: Path,\n    max_depth: Optional[int] = None,\n) -> list[str]:\n    root = Path(output_dir)\n    if not root.exists():\n        return []\n    items = []\n    for dirpath, _, filenames in os.walk(root):\n        depth = Path(dirpath).relative_to(root).parts\n        if max_depth is not None and len(depth) > max_depth:\n            continue\n        for fname in sorted(filenames):\n            if fname.endswith(\".whl\"):\n                pkg = fname.split(\"-\")[0]\n                relpath = str((Path(dirpath) / fname).relative_to(root))\n                items.append({\"pkg\": pkg, \"relpath\": relpath})\n    return items", "target": "def test_pr_with_missing_labels(\n        self, mocked_rn_labels: Any, mocked_gql: Any\n    ) -> None:\n        \"Test PR with no 'release notes:' label or 'topic: not user facing' label\"\n        pr = GitHubPR(\"pytorch\", \"pytorch\", 82169)\n        self.assertFalse(has_required_labels(pr))"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005216", "source": "def rebuild_model_fields(\n    cls: type[BaseModel],\n    *,\n    config_wrapper: ConfigWrapper,\n    ns_resolver: NsResolver,\n    typevars_map: Mapping[TypeVar, Any],\n) -> dict[str, FieldInfo]:\n    rebuilt_fields: dict[str, FieldInfo] = {}\n    with ns_resolver.push(cls):\n        for f_name, field_info in cls.__pydantic_fields__.items():\n            if field_info._complete:\n                rebuilt_fields[f_name] = field_info\n            else:\n                new_field = _recreate_field_info(\n                    field_info, ns_resolver=ns_resolver, typevars_map=typevars_map, lenient=False\n                )\n                update_field_from_config(config_wrapper, f_name, new_field)\n                rebuilt_fields[f_name] = new_field\n    return rebuilt_fields", "target": "def test_aliases_debug():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'field_a': core_schema.model_field(\n                    validation_alias=[['foo', 'bar', 'bat'], ['foo', 3]], schema=core_schema.int_schema()\n                )\n            }\n        )\n    )\n    print(repr(v))\n    assert repr(v).startswith('SchemaValidator(title=\"model-fields\", validator=ModelFields(')\n    assert 'PathChoices(' in repr(v)"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005217", "source": "def secs_to_us(time_s):\n    return time_s * 1e6", "target": "def test_iterator(self, input_: str = \"abcdef\") -> None:\n        iter_ = PeekableIterator(input_)\n        for idx, c in enumerate(iter_):\n            self.assertEqual(c, input_[idx])"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005218", "source": "def rebuild_dataclass_fields(\n    cls: type[PydanticDataclass],\n    *,\n    config_wrapper: ConfigWrapper,\n    ns_resolver: NsResolver,\n    typevars_map: Mapping[TypeVar, Any],\n) -> dict[str, FieldInfo]:\n    FieldInfo_ = import_cached_field_info()\n    rebuilt_fields: dict[str, FieldInfo] = {}\n    with ns_resolver.push(cls):\n        for f_name, field_info in cls.__pydantic_fields__.items():\n            if field_info._complete:\n                rebuilt_fields[f_name] = field_info\n            else:\n                existing_desc = field_info.description\n                ann = _typing_extra.eval_type(\n                    field_info._original_annotation,\n                    *ns_resolver.types_namespace,\n                )\n                ann = _generics.replace_types(ann, typevars_map)\n                new_field = FieldInfo_.from_annotated_attribute(\n                    ann,\n                    field_info._original_assignment,\n                    _source=AnnotationSource.DATACLASS,\n                )\n                new_field.description = new_field.description if new_field.description is not None else existing_desc\n                update_field_from_config(config_wrapper, f_name, new_field)\n                rebuilt_fields[f_name] = new_field\n    return rebuilt_fields", "target": "def test_on_error_raise_by_default(self, py_and_json: PyAndJson):\n        v = py_and_json({'type': 'model-fields', 'fields': {'x': {'type': 'model-field', 'schema': {'type': 'str'}}}})\n        assert v.validate_test({'x': 'foo'}) == ({'x': 'foo'}, None, {'x'})\n        with pytest.raises(ValidationError) as exc_info:\n            v.validate_test({'x': ['foo']})\n        assert exc_info.value.errors(include_url=False) == [\n            {'input': ['foo'], 'type': 'string_type', 'loc': ('x',), 'msg': 'Input should be a valid string'}\n        ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005219", "source": "def _validate_dataclass(cls: Type['DataclassT'], v: Any) -> 'DataclassT':\n    with set_validation(cls, True):\n        if isinstance(v, cls):\n            v.__pydantic_validate_values__()\n            return v\n        elif isinstance(v, (list, tuple)):\n            return cls(*v)\n        elif isinstance(v, dict):\n            return cls(**v)\n        else:\n            raise DataclassTypeError(class_name=cls.__name__)", "target": "def test_leak_dataclass(validator):\n    def fn():\n        @dataclasses.dataclass\n        class Dataclass:\n            a: int\n            @classmethod\n            def _validator(cls, v, info):\n                return v\n            @classmethod\n            def _wrap_validator(cls, v, validator, info):\n                return validator(v)\n        field_schema = core_schema.int_schema()\n        if validator == 'field':\n            field_schema = core_schema.with_info_before_validator_function(Dataclass._validator, field_schema)\n            field_schema = core_schema.with_info_wrap_validator_function(Dataclass._wrap_validator, field_schema)\n            field_schema = core_schema.with_info_after_validator_function(Dataclass._validator, field_schema)\n        dataclass_schema = core_schema.dataclass_schema(\n            Dataclass,\n            core_schema.dataclass_args_schema('Dataclass', [core_schema.dataclass_field('a', field_schema)]),\n            ['a'],\n        )\n        if validator == 'dataclass':\n            dataclass_schema = core_schema.with_info_before_validator_function(Dataclass._validator, dataclass_schema)\n            dataclass_schema = core_schema.with_info_wrap_validator_function(\n                Dataclass._wrap_validator, dataclass_schema\n            )\n            dataclass_schema = core_schema.with_info_after_validator_function(Dataclass._validator, dataclass_schema)\n        Dataclass.__pydantic_validator__ = SchemaValidator(dataclass_schema)\n        return Dataclass\n    klass = fn()\n    ref = weakref.ref(klass)\n    assert ref() is not None\n    del klass\n    assert_gc(lambda: ref() is None)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005220", "source": "def new_init(self: 'Dataclass', *args: Any, **kwargs: Any) -> None:\n            handle_extra_init(self, *args, **kwargs)\n            if self.__class__.__pydantic_run_validation__:\n                self.__pydantic_validate_values__()\n            if hasattr(self, '__post_init_post_parse__'):\n                initvars_and_values: Dict[str, Any] = {}\n                for i, f in enumerate(self.__class__.__dataclass_fields__.values()):\n                    if f._field_type is dataclasses._FIELD_INITVAR:\n                        try:\n                            initvars_and_values[f.name] = args[i]\n                        except IndexError:\n                            initvars_and_values[f.name] = kwargs.get(f.name, f.default)\n                self.__post_init_post_parse__(**initvars_and_values)", "target": "def test_only_validate_by_name(py_and_json) -> None:\n    schema = core_schema.dataclass_schema(\n        BasicDataclass,\n        core_schema.dataclass_args_schema(\n            'BasicDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), validation_alias='FieldA'),\n            ],\n        ),\n        ['a'],\n        config=core_schema.CoreConfig(validate_by_name=True, validate_by_alias=False),\n    )\n    v = py_and_json(schema)\n    assert v.validate_test({'a': 'hello'}) == BasicDataclass(a='hello')\n    with pytest.raises(ValidationError, match=r'a\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'FieldA': 'hello'})"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005221", "source": "def dataclass(\n        _cls: Type[_T],\n        *,\n        init: bool = True,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool = False,\n        config: Union[ConfigDict, Type[object], None] = None,\n        validate_on_init: Optional[bool] = None,\n        use_proxy: Optional[bool] = None,\n        kw_only: bool = ...,\n    ) -> 'DataclassClassOrWrapper':\n        ...", "target": "def test_dataclass_json():\n    schema = core_schema.dataclass_schema(\n        FooDataclass,\n        core_schema.dataclass_args_schema(\n            'FooDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    assert v.validate_json('{\"a\": \"hello\", \"b\": true}') == FooDataclass(a='hello', b=True)\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_json('[\"a\", \"b\"]')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'class_name': 'FooDataclass'},\n            'input': ['a', 'b'],\n            'loc': (),\n            'msg': 'Input should be an object',\n            'type': 'dataclass_type',\n        }\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005222", "source": "def dataclass(\n        *,\n        init: bool = True,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool = False,\n        config: Union[ConfigDict, Type[object], None] = None,\n        validate_on_init: Optional[bool] = None,\n        use_proxy: Optional[bool] = None,\n        kw_only: bool = ...,\n    ) -> Callable[[Type[_T]], 'DataclassClassOrWrapper']:\n        ...", "target": "def test_dataclass_field_before_validator():\n    @dataclasses.dataclass\n    class Foo:\n        a: int\n        b: str\n        @classmethod\n        def validate_b(cls, v: bytes, info: core_schema.ValidationInfo) -> bytes:\n            assert v == b'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return b'hello world!'\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(\n                    name='b',\n                    schema=core_schema.with_info_before_validator_function(Foo.validate_b, core_schema.str_schema()),\n                ),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 1, 'b': b'hello'})\n    assert dataclasses.asdict(foo) == {'a': 1, 'b': 'hello world!'}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005223", "source": "def custom_pydantic_encoder(type_encoders: dict[Any, Callable[[type[Any]], Any]], obj: Any) -> Any:\n    warnings.warn(\n        '`custom_pydantic_encoder` is deprecated, use `BaseModel.model_dump` instead.',\n        category=PydanticDeprecatedSince20,\n        stacklevel=2,\n    )\n    for base in obj.__class__.__mro__[:-1]:\n        try:\n            encoder = type_encoders[base]\n        except KeyError:\n            continue\n        return encoder(obj)\n    else:\n        return pydantic_encoder(obj)", "target": "def test_json_bytes_base64_no_padding():\n    v = SchemaValidator(core_schema.bytes_schema(), config=CoreConfig(val_json_bytes='base64'))\n    base_64_without_padding = 'bm8tcGFkZGluZw'\n    assert v.validate_json(json.dumps(base_64_without_padding)) == b'no-padding'"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005224", "source": "def _check_protected_namespaces(\n    protected_namespaces: tuple[str | Pattern[str], ...],\n    ann_name: str,\n    bases: tuple[type[Any], ...],\n    cls_name: str,\n) -> None:\n    BaseModel = import_cached_base_model()\n    for protected_namespace in protected_namespaces:\n        ns_violation = False\n        if isinstance(protected_namespace, Pattern):\n            ns_violation = protected_namespace.match(ann_name) is not None\n        elif isinstance(protected_namespace, str):\n            ns_violation = ann_name.startswith(protected_namespace)\n        if ns_violation:\n            for b in bases:\n                if hasattr(b, ann_name):\n                    if not (issubclass(b, BaseModel) and ann_name in getattr(b, '__pydantic_fields__', {})):\n                        raise ValueError(\n                            f'Field {ann_name!r} conflicts with member {getattr(b, ann_name)}'\n                            f' of protected namespace {protected_namespace!r}.'\n                        )\n            else:\n                valid_namespaces: list[str] = []\n                for pn in protected_namespaces:\n                    if isinstance(pn, Pattern):\n                        if not pn.match(ann_name):\n                            valid_namespaces.append(f're.compile({pn.pattern!r})')\n                    else:\n                        if not ann_name.startswith(pn):\n                            valid_namespaces.append(f\"'{pn}'\")\n                valid_namespaces_str = f'({\", \".join(valid_namespaces)}{\",)\" if len(valid_namespaces) == 1 else \")\"}'\n                warnings.warn(\n                    f'Field {ann_name!r} in {cls_name!r} conflicts with protected namespace {protected_namespace!r}.\\n\\n'\n                    f\"You may be able to solve this by setting the 'protected_namespaces' configuration to {valid_namespaces_str}.\",\n                    UserWarning,\n                    stacklevel=5,\n                )", "target": "def test_by_alias_and_name_config_interaction(\n    config_by_alias: Union[bool, None],\n    config_by_name: Union[bool, None],\n    runtime_by_alias: Union[bool, None],\n    runtime_by_name: Union[bool, None],\n) -> None:\n    if config_by_alias is False and config_by_name is False and runtime_by_alias is False and runtime_by_name is False:\n        pytest.skip(\"Can't have both by_alias and by_name as effectively False\")\n    class Model:\n        def __init__(self, my_field: int) -> None:\n            self.my_field = my_field\n    core_config = {\n        **({'validate_by_alias': config_by_alias} if config_by_alias is not None else {}),\n        **({'validate_by_name': config_by_name} if config_by_name is not None else {}),\n    }\n    schema = core_schema.model_schema(\n        Model,\n        core_schema.model_fields_schema(\n            {\n                'my_field': core_schema.model_field(core_schema.int_schema(), validation_alias='my_alias'),\n            }\n        ),\n        config=core_schema.CoreConfig(**core_config),\n    )\n    s = SchemaValidator(schema)\n    alias_allowed = next(x for x in (runtime_by_alias, config_by_alias, True) if x is not None)\n    name_allowed = next(x for x in (runtime_by_name, config_by_name, False) if x is not None)\n    if alias_allowed:\n        assert s.validate_python({'my_alias': 1}, by_alias=runtime_by_alias, by_name=runtime_by_name).my_field == 1\n    if name_allowed:\n        assert s.validate_python({'my_field': 1}, by_alias=runtime_by_alias, by_name=runtime_by_name).my_field == 1"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005225", "source": "def benchmark_module(config, module, use_throughput_benchmark=False):\n    if use_throughput_benchmark:\n        return benchmark_using_throughput_benchmark(config, module)\n    module.forward(config.num_warmup_iters)\n    print(f\"Running module for {config.num_iters} iterations\")\n    start = time.time()\n    module.forward(config.num_iters)\n    end = time.time()\n    time_elapsed_s = end - start\n    return secs_to_ms(time_elapsed_s) / config.num_iters / NUM_LOOP_ITERS", "target": "def test_sets_and_restores_new_var(self):\n        var = \"TEST_TMP_ENV_NEW\"\n        self.assertNotIn(var, os.environ)\n        with temp_environ({var: \"123\"}):\n            self.assertEqual(os.environ[var], \"123\")\n        self.assertNotIn(var, os.environ)"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005226", "source": "def parse_stmts(stmts: str) -> tuple[str, str]:\n    stmts = textwrap.dedent(stmts).strip()\n    lines: list[str] = stmts.splitlines(keepends=False)\n    assert len(lines) >= 3, f\"Invalid string:\\n{stmts}\"\n    column_header_pattern = r\"^Python\\s{35}\\| C\\+\\+(\\s*)$\"\n    signature_pattern = r\"^: f\\((.*)\\)( -> (.+))?\\s*$\"\n    separation_pattern = r\"^[-]{40} | [-]{40}$\"\n    code_pattern = r\"^(.{40}) \\|($| (.*)$)\"\n    column_match = re.search(column_header_pattern, lines[0])\n    if column_match is None:\n        raise ValueError(\n            f\"Column header `{lines[0]}` \"\n            f\"does not match pattern `{column_header_pattern}`\"\n        )\n    assert re.search(separation_pattern, lines[1])\n    py_lines: list[str] = []\n    cpp_lines: list[str] = []\n    for l in lines[2:]:\n        l_match = re.search(code_pattern, l)\n        if l_match is None:\n            raise ValueError(f\"Invalid line `{l}`\")\n        py_lines.append(l_match.groups()[0])\n        cpp_lines.append(l_match.groups()[2] or \"\")\n        l_from_stmts = f\"{py_lines[-1]:<40} | {cpp_lines[-1]:<40}\".rstrip()\n        assert l_from_stmts == l.rstrip(), f\"Failed to round trip `{l}`\"\n    return \"\\n\".join(py_lines), \"\\n\".join(cpp_lines)", "target": "def test_restores_even_on_exception(self):\n        var = \"TEST_TMP_ENV_EXCEPTION\"\n        self.assertNotIn(var, os.environ)\n        with self.assertRaises(RuntimeError):\n            with temp_environ({var: \"x\"}):\n                self.assertEqual(os.environ[var], \"x\")\n                raise RuntimeError(\"boom\")\n        self.assertNotIn(var, os.environ)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005227", "source": "def _add_pydantic_validation_attributes(\n    dc_cls: Type['Dataclass'],\n    config: Type[BaseConfig],\n    validate_on_init: bool,\n    dc_cls_doc: str,\n) -> None:\n    init = dc_cls.__init__\n    @wraps(init)\n    def handle_extra_init(self: 'Dataclass', *args: Any, **kwargs: Any) -> None:\n        if config.extra == Extra.ignore:\n            init(self, *args, **{k: v for k, v in kwargs.items() if k in self.__dataclass_fields__})\n        elif config.extra == Extra.allow:\n            for k, v in kwargs.items():\n                self.__dict__.setdefault(k, v)\n            init(self, *args, **{k: v for k, v in kwargs.items() if k in self.__dataclass_fields__})\n        else:\n            init(self, *args, **kwargs)\n    if hasattr(dc_cls, '__post_init__'):\n        try:\n            post_init = dc_cls.__post_init__.__wrapped__\n        except AttributeError:\n            post_init = dc_cls.__post_init__\n        @wraps(post_init)\n        def new_post_init(self: 'Dataclass', *args: Any, **kwargs: Any) -> None:\n            if config.post_init_call == 'before_validation':\n                post_init(self, *args, **kwargs)\n            if self.__class__.__pydantic_run_validation__:\n                self.__pydantic_validate_values__()\n                if hasattr(self, '__post_init_post_parse__'):\n                    self.__post_init_post_parse__(*args, **kwargs)\n            if config.post_init_call == 'after_validation':\n                post_init(self, *args, **kwargs)\n        setattr(dc_cls, '__init__', handle_extra_init)\n        setattr(dc_cls, '__post_init__', new_post_init)\n    else:\n        @wraps(init)\n        def new_init(self: 'Dataclass', *args: Any, **kwargs: Any) -> None:\n            handle_extra_init(self, *args, **kwargs)\n            if self.__class__.__pydantic_run_validation__:\n                self.__pydantic_validate_values__()\n            if hasattr(self, '__post_init_post_parse__'):\n                initvars_and_values: Dict[str, Any] = {}\n                for i, f in enumerate(self.__class__.__dataclass_fields__.values()):\n                    if f._field_type is dataclasses._FIELD_INITVAR:\n                        try:\n                            initvars_and_values[f.name] = args[i]\n                        except IndexError:\n                            initvars_and_values[f.name] = kwargs.get(f.name, f.default)\n                self.__post_init_post_parse__(**initvars_and_values)\n        setattr(dc_cls, '__init__', new_init)\n    setattr(dc_cls, '__pydantic_run_validation__', ClassAttribute('__pydantic_run_validation__', validate_on_init))\n    setattr(dc_cls, '__pydantic_initialised__', False)\n    setattr(dc_cls, '__pydantic_model__', create_pydantic_model_from_dataclass(dc_cls, config, dc_cls_doc))\n    setattr(dc_cls, '__pydantic_validate_values__', _dataclass_validate_values)\n    setattr(dc_cls, '__validate__', classmethod(_validate_dataclass))\n    setattr(dc_cls, '__get_validators__', classmethod(_get_validators))\n    if dc_cls.__pydantic_model__.__config__.validate_assignment and not dc_cls.__dataclass_params__.frozen:\n        setattr(dc_cls, '__setattr__', _dataclass_validate_assignment_setattr)", "target": "def test_slots_dataclass_subclass(revalidate_instances, input_value, expected):\n    schema = core_schema.dataclass_schema(\n        FooDataclassSlots,\n        core_schema.dataclass_args_schema(\n            'FooDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n            ],\n            extra_behavior='forbid',\n        ),\n        ['a', 'b'],\n        revalidate_instances=revalidate_instances,\n        slots=True,\n    )\n    v = SchemaValidator(schema)\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=expected.message) as exc_info:\n            print(v.validate_python(input_value))\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        dc = v.validate_python(input_value)\n        assert dataclasses.is_dataclass(dc)\n        assert dataclasses.asdict(dc) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005228", "source": "def get_merge_base(self, from_ref: str, to_ref: str) -> str:\n        return self._run_git(\"merge-base\", from_ref, to_ref).strip()", "target": "def test_is_iterable(self) -> None:\n        from collections.abc import Iterator\n        iter_ = PeekableIterator(\"\")\n        self.assertTrue(isinstance(iter_, Iterator))"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005229", "source": "def _check_finalvar(v: Optional[Type[Any]]) -> bool:\n    if v is None:\n        return False\n    return v.__class__ == Final.__class__ and (sys.version_info < (3, 8) or getattr(v, '_name', None) == 'Final')", "target": "def test_schema_typing_error() -> None:\n    _: CoreSchema = {'type': 'wrong'}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005230", "source": "def collect_model_fields(\n    cls: type[BaseModel],\n    config_wrapper: ConfigWrapper,\n    ns_resolver: NsResolver,\n    *,\n    typevars_map: Mapping[TypeVar, Any] | None = None,\n) -> tuple[dict[str, FieldInfo], set[str]]:\n    FieldInfo_ = import_cached_field_info()\n    BaseModel_ = import_cached_base_model()\n    bases = cls.__bases__\n    parent_fields_lookup: dict[str, FieldInfo] = {}\n    for base in reversed(bases):\n        if model_fields := getattr(base, '__pydantic_fields__', None):\n            parent_fields_lookup.update(model_fields)\n    type_hints = _typing_extra.get_model_type_hints(cls, ns_resolver=ns_resolver)\n    annotations = _typing_extra.safe_get_annotations(cls)\n    fields: dict[str, FieldInfo] = {}\n    class_vars: set[str] = set()\n    for ann_name, (ann_type, evaluated) in type_hints.items():\n        if ann_name == 'model_config':\n            continue\n        _check_protected_namespaces(\n            protected_namespaces=config_wrapper.protected_namespaces,\n            ann_name=ann_name,\n            bases=bases,\n            cls_name=cls.__name__,\n        )\n        if _typing_extra.is_classvar_annotation(ann_type):\n            class_vars.add(ann_name)\n            continue\n        assigned_value = getattr(cls, ann_name, PydanticUndefined)\n        if assigned_value is not PydanticUndefined and (\n            any(getattr(BaseModel_, depr_name, None) is assigned_value for depr_name in _deprecated_method_names)\n            or (\n                hasattr(assigned_value, '__func__')\n                and any(\n                    getattr(getattr(BaseModel_, depr_name, None), '__func__', None) is assigned_value.__func__\n                    for depr_name in _deprecated_classmethod_names\n                )\n            )\n        ):\n            assigned_value = PydanticUndefined\n        if not is_valid_field_name(ann_name):\n            continue\n        if cls.__pydantic_root_model__ and ann_name != 'root':\n            raise NameError(\n                f\"Unexpected field with name {ann_name!r}; only 'root' is allowed as a field of a `RootModel`\"\n            )\n        generic_origin = getattr(cls, '__pydantic_generic_metadata__', {}).get('origin')\n        for base in bases:\n            dataclass_fields = {\n                field.name for field in (dataclasses.fields(base) if dataclasses.is_dataclass(base) else ())\n            }\n            if hasattr(base, ann_name):\n                if base is generic_origin:\n                    continue\n                if ann_name in dataclass_fields:\n                    continue\n                if ann_name not in annotations:\n                    continue\n                warnings.warn(\n                    f'Field name \"{ann_name}\" in \"{cls.__qualname__}\" shadows an attribute in parent '\n                    f'\"{base.__qualname__}\"',\n                    UserWarning,\n                    stacklevel=4,\n                )\n        if assigned_value is PydanticUndefined:\n            if ann_name in annotations or ann_name not in parent_fields_lookup:\n                field_info = FieldInfo_.from_annotation(ann_type, _source=AnnotationSource.CLASS)\n                field_info._original_annotation = ann_type\n                if not evaluated:\n                    field_info._complete = False\n            else:\n                parent_field_info = parent_fields_lookup[ann_name]._copy()\n                if typevars_map:\n                    field_info = _recreate_field_info(\n                        parent_field_info, ns_resolver=ns_resolver, typevars_map=typevars_map, lenient=True\n                    )\n                else:\n                    field_info = parent_field_info\n        else:\n            if isinstance(assigned_value, FieldInfo_) and ismethoddescriptor(assigned_value.default):\n                default = assigned_value.default.__get__(None, cls)\n                assigned_value.default = default\n                assigned_value._attributes_set['default'] = default\n            field_info = FieldInfo_.from_annotated_attribute(ann_type, assigned_value, _source=AnnotationSource.CLASS)\n            field_info._original_assignment = assigned_value\n            field_info._original_annotation = ann_type\n            if not evaluated:\n                field_info._complete = False\n            elif 'final' in field_info._qualifiers and not field_info.is_required():\n                warnings.warn(\n                    f'Annotation {ann_name!r} is marked as final and has a default value. Pydantic treats {ann_name!r} as a '\n                    'class variable, but it will be considered as a normal field in V3 to be aligned with dataclasses. If you '\n                    f'still want {ann_name!r} to be considered as a class variable, annotate it as: `ClassVar[<type>] = <default>.`',\n                    category=PydanticDeprecatedSince211,\n                    stacklevel=4,\n                )\n                class_vars.add(ann_name)\n                continue\n            try:\n                delattr(cls, ann_name)\n            except AttributeError:\n                pass\n        decorators: DecoratorInfos = cls.__dict__['__pydantic_decorators__']\n        if ann_name in decorators.computed_fields:\n            raise TypeError(\n                f'Field {ann_name!r} of class {cls.__name__!r} overrides symbol of same name in a parent class. '\n                'This override with a computed_field is incompatible.'\n            )\n        fields[ann_name] = field_info\n        if field_info._complete:\n            update_field_from_config(config_wrapper, ann_name, field_info)\n    if config_wrapper.use_attribute_docstrings:\n        _update_fields_from_docstrings(cls, fields)\n    return fields, class_vars", "target": "def test_validate_assignment():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(fields={'field_a': core_schema.model_field(schema=core_schema.str_schema())})\n    )\n    assert v.validate_python({'field_a': 'test'}) == ({'field_a': 'test'}, None, {'field_a'})\n    data = {'field_a': 'test'}\n    assert v.validate_assignment(data, 'field_a', b'abc') == ({'field_a': 'abc'}, None, {'field_a'})\n    assert data == {'field_a': 'abc'}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005231", "source": "def dataclass(\n        *,\n        init: Literal[False] = False,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool = False,\n        config: ConfigDict | type[object] | None = None,\n        validate_on_init: bool | None = None,\n        kw_only: bool = ...,\n        slots: bool = ...,\n    ) -> Callable[[type[_T]], type[PydanticDataclass]]:\n        ...", "target": "def test_only_validate_by_name(py_and_json) -> None:\n    schema = core_schema.dataclass_schema(\n        BasicDataclass,\n        core_schema.dataclass_args_schema(\n            'BasicDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), validation_alias='FieldA'),\n            ],\n        ),\n        ['a'],\n        config=core_schema.CoreConfig(validate_by_name=True, validate_by_alias=False),\n    )\n    v = py_and_json(schema)\n    assert v.validate_test({'a': 'hello'}) == BasicDataclass(a='hello')\n    with pytest.raises(ValidationError, match=r'a\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'FieldA': 'hello'})"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005232", "source": "def dataclass(\n    _cls: type[_T] | None = None,\n    *,\n    init: Literal[False] = False,\n    repr: bool = True,\n    eq: bool = True,\n    order: bool = False,\n    unsafe_hash: bool = False,\n    frozen: bool | None = None,\n    config: ConfigDict | type[object] | None = None,\n    validate_on_init: bool | None = None,\n    kw_only: bool = False,\n    slots: bool = False,\n) -> Callable[[type[_T]], type[PydanticDataclass]] | type[PydanticDataclass]:\n    assert init is False, 'pydantic.dataclasses.dataclass only supports init=False'\n    assert validate_on_init is not False, 'validate_on_init=False is no longer supported'\n    if sys.version_info >= (3, 10):\n        kwargs = {'kw_only': kw_only, 'slots': slots}\n    else:\n        kwargs = {}\n    def create_dataclass(cls: type[Any]) -> type[PydanticDataclass]:\n        from ._internal._utils import is_model_class\n        if is_model_class(cls):\n            raise PydanticUserError(\n                f'Cannot create a Pydantic dataclass from {cls.__name__} as it is already a Pydantic model',\n                code='dataclass-on-model',\n            )\n        original_cls = cls\n        has_dataclass_base = any(dataclasses.is_dataclass(base) for base in cls.__bases__)\n        if not has_dataclass_base and config is not None and hasattr(cls, '__pydantic_config__'):\n            warn(\n                f'`config` is set via both the `dataclass` decorator and `__pydantic_config__` for dataclass {cls.__name__}. '\n                f'The `config` specification from `dataclass` decorator will take priority.',\n                category=UserWarning,\n                stacklevel=2,\n            )\n        config_dict = config if config is not None else getattr(cls, '__pydantic_config__', None)\n        config_wrapper = _config.ConfigWrapper(config_dict)\n        decorators = _decorators.DecoratorInfos.build(cls, replace_wrapped_methods=True)\n        decorators.update_from_config(config_wrapper)\n        original_doc = cls.__doc__\n        if _pydantic_dataclasses.is_stdlib_dataclass(cls):\n            original_doc = None\n            bases = (cls,)\n            if issubclass(cls, Generic):\n                generic_base = Generic[cls.__parameters__]\n                bases = bases + (generic_base,)\n            cls = types.new_class(cls.__name__, bases)\n        if frozen is not None:\n            frozen_ = frozen\n            if config_wrapper.frozen:\n                warn(\n                    f'`frozen` is set via both the `dataclass` decorator and `config` for dataclass {cls.__name__!r}.'\n                    'This is not recommended. The `frozen` specification on `dataclass` will take priority.',\n                    category=UserWarning,\n                    stacklevel=2,\n                )\n        else:\n            frozen_ = config_wrapper.frozen or False\n        cls_anns = _typing_extra.safe_get_annotations(cls)\n        for field_name in cls_anns:\n            field_value = getattr(cls, field_name, None)\n            if isinstance(field_value, FieldInfo):\n                setattr(cls, field_name, _pydantic_dataclasses.as_dataclass_field(field_value))\n        with _pydantic_dataclasses.patch_base_fields(cls):\n            cls = dataclasses.dataclass(\n                cls,\n                init=True,\n                repr=repr,\n                eq=eq,\n                order=order,\n                unsafe_hash=unsafe_hash,\n                frozen=frozen_,\n                **kwargs,\n            )\n        if config_wrapper.validate_assignment:\n            original_setattr = cls.__setattr__\n            @functools.wraps(cls.__setattr__)\n            def validated_setattr(instance: PydanticDataclass, name: str, value: Any, /) -> None:\n                if frozen_:\n                    return original_setattr(instance, name, value)\n                inst_cls = type(instance)\n                attr = getattr(inst_cls, name, None)\n                if isinstance(attr, property):\n                    attr.__set__(instance, value)\n                elif isinstance(attr, functools.cached_property):\n                    instance.__dict__.__setitem__(name, value)\n                else:\n                    inst_cls.__pydantic_validator__.validate_assignment(instance, name, value)\n            cls.__setattr__ = validated_setattr.__get__(None, cls)\n            if slots and not hasattr(cls, '__setstate__'):\n                def _dataclass_getstate(self: Any) -> list[Any]:\n                    return [getattr(self, f.name) for f in dataclasses.fields(self)]\n                def _dataclass_setstate(self: Any, state: list[Any]) -> None:\n                    for field, value in zip(dataclasses.fields(self), state):\n                        object.__setattr__(self, field.name, value)\n                cls.__getstate__ = _dataclass_getstate\n                cls.__setstate__ = _dataclass_setstate\n        cls.__is_pydantic_dataclass__ = True\n        cls.__pydantic_decorators__ = decorators\n        cls.__doc__ = original_doc\n        firstlineno = getattr(original_cls, '__firstlineno__', None)\n        cls.__module__ = original_cls.__module__\n        if sys.version_info >= (3, 13) and firstlineno is not None:\n            original_cls.__firstlineno__ = firstlineno\n            cls.__firstlineno__ = firstlineno\n        cls.__qualname__ = original_cls.__qualname__\n        cls.__pydantic_fields_complete__ = classmethod(_pydantic_fields_complete)\n        cls.__pydantic_complete__ = False\n        _pydantic_dataclasses.complete_dataclass(cls, config_wrapper, raise_errors=False)\n        return cls\n    return create_dataclass if _cls is None else create_dataclass(_cls)", "target": "def test_dataclass_post_init_args():\n    c_value = None\n    @dataclasses.dataclass\n    class Foo:\n        a: str\n        b: bool\n        c: dataclasses.InitVar[int]\n        def __post_init__(self, c: int):\n            nonlocal c_value\n            c_value = c\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n                core_schema.dataclass_field(name='c', schema=core_schema.int_schema(), init_only=True),\n            ],\n            collect_init_only=True,\n        ),\n        ['a', 'b'],\n        post_init=True,\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': b'hello', 'b': 'true', 'c': '42'})\n    assert foo.a == 'hello'\n    assert foo.b is True\n    assert not hasattr(foo, 'c')\n    assert c_value == 42"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005233", "source": "def powspace(start, stop, pow, step):\n    start = math.log(start, pow)\n    stop = math.log(stop, pow)\n    steps = int((stop - start + 1) // step)\n    ret = torch.pow(pow, torch.linspace(start, stop, steps))\n    ret = torch.unique(ret)\n    return list(map(int, ret))", "target": "def test_ghstack_branches_not_in_sync(self) -> None:\n        head_ref = \"gh/clee2000/1/head\"\n        self._skip_if_ref_does_not_exist(head_ref)\n        self.assertFalse(are_ghstack_branches_in_sync(self.repo, head_ref))"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005234", "source": "def force_create_dir(path: Union[str, Path]) -> Path:\n    remove_dir(path)\n    return ensure_dir_exists(path)", "target": "def test_copy_file_to_file(self):\n        src = self.tmp_path / \"src.txt\"\n        dst = self.tmp_path / \"out\" / \"dst.txt\"\n        src.write_text(\"hello\")\n        copy(src, dst)\n        self.assertEqual(dst.read_text(), \"hello\")"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005235", "source": "def get_args(tp: Type[Any]) -> Tuple[Any, ...]:\n        if type(tp).__name__ in AnnotatedTypeNames:\n            return tp.__args__ + tp.__metadata__\n        return _typing_get_args(tp) or getattr(tp, '__args__', ()) or _generic_get_args(tp)", "target": "def test_type_error():\n    try:\n        PydanticKnownError('foobar')\n    except KeyError as exc:\n        assert str(exc) == '\"Invalid error type: \\'foobar\\'\"'\n    else:\n        raise AssertionError(\"PydanticKnownError('foobar') did not raise KeyError\")\n    e = PydanticKnownError('recursion_loop')\n    assert isinstance(e, PydanticKnownError)"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005236", "source": "def remove_dir(path: Union[str, Path, None]) -> None:\n    if not path:\n        return\n    path_obj = get_path(path)\n    if path_obj.exists():\n        shutil.rmtree(path_obj)", "target": "def test_get_path_resolves(self):\n        rel_str = \"sub/f.txt\"\n        p = get_path(str(self.tmp_path / rel_str), resolve=True)\n        self.assertTrue(p.is_absolute())\n        self.assertTrue(str(p).endswith(rel_str))"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005237", "source": "def _validate_dataclass(cls: Type['DataclassT'], v: Any) -> 'DataclassT':\n    with set_validation(cls, True):\n        if isinstance(v, cls):\n            v.__pydantic_validate_values__()\n            return v\n        elif isinstance(v, (list, tuple)):\n            return cls(*v)\n        elif isinstance(v, dict):\n            return cls(**v)\n        else:\n            raise DataclassTypeError(class_name=cls.__name__)", "target": "def test_function_validator_wrapping_args_schema_before() -> None:\n    calls: list[Any] = []\n    def func(*args: Any) -> Any:\n        calls.append(args)\n        return args[0]\n    @dataclasses.dataclass\n    class Model:\n        number: int = 1\n    cs = core_schema.dataclass_schema(\n        Model,\n        core_schema.no_info_before_validator_function(\n            func,\n            core_schema.dataclass_args_schema(\n                'Model', [core_schema.dataclass_field('number', core_schema.int_schema())]\n            ),\n        ),\n        ['number'],\n    )\n    v = SchemaValidator(cs)\n    instance: Model = v.validate_python({'number': 1})\n    assert instance.number == 1\n    assert calls == [({'number': 1},)]\n    v.validate_assignment(instance, 'number', 2)\n    assert instance.number == 2\n    assert calls == [({'number': 1},), ({'number': 2},)]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005238", "source": "def collect_dataclass_fields(\n    cls: type[StandardDataclass],\n    *,\n    config_wrapper: ConfigWrapper,\n    ns_resolver: NsResolver | None = None,\n    typevars_map: dict[Any, Any] | None = None,\n) -> dict[str, FieldInfo]:\n    FieldInfo_ = import_cached_field_info()\n    fields: dict[str, FieldInfo] = {}\n    ns_resolver = ns_resolver or NsResolver()\n    dataclass_fields = cls.__dataclass_fields__\n    for base in reversed(cls.__mro__):\n        if not dataclasses.is_dataclass(base):\n            continue\n        with ns_resolver.push(base):\n            for ann_name, dataclass_field in dataclass_fields.items():\n                base_anns = _typing_extra.safe_get_annotations(base)\n                if ann_name not in base_anns:\n                    continue\n                globalns, localns = ns_resolver.types_namespace\n                ann_type, evaluated = _typing_extra.try_eval_type(dataclass_field.type, globalns, localns)\n                if _typing_extra.is_classvar_annotation(ann_type):\n                    continue\n                if (\n                    not dataclass_field.init\n                    and dataclass_field.default is dataclasses.MISSING\n                    and dataclass_field.default_factory is dataclasses.MISSING\n                ):\n                    continue\n                if isinstance(dataclass_field.default, FieldInfo_):\n                    if dataclass_field.default.init_var:\n                        if dataclass_field.default.init is False:\n                            raise PydanticUserError(\n                                f'Dataclass field {ann_name} has init=False and init_var=True, but these are mutually exclusive.',\n                                code='clashing-init-and-init-var',\n                            )\n                        continue\n                    field_info = FieldInfo_.from_annotated_attribute(\n                        ann_type, dataclass_field.default, _source=AnnotationSource.DATACLASS\n                    )\n                    field_info._original_assignment = dataclass_field.default\n                else:\n                    field_info = FieldInfo_.from_annotated_attribute(\n                        ann_type, dataclass_field, _source=AnnotationSource.DATACLASS\n                    )\n                    field_info._original_assignment = dataclass_field\n                if not evaluated:\n                    field_info._complete = False\n                    field_info._original_annotation = ann_type\n                fields[ann_name] = field_info\n                update_field_from_config(config_wrapper, ann_name, field_info)\n                if field_info.default is not PydanticUndefined and isinstance(\n                    getattr(cls, ann_name, field_info), FieldInfo_\n                ):\n                    setattr(cls, ann_name, field_info.default)\n    if typevars_map:\n        for field in fields.values():\n            field.apply_typevars_map(typevars_map)\n    if config_wrapper.use_attribute_docstrings:\n        _update_fields_from_docstrings(\n            cls,\n            fields,\n            use_inspect=not hasattr(cls, '__is_pydantic_dataclass__'),\n        )\n    return fields", "target": "def test_from_attributes_extra_forbid() -> None:\n    class Source:\n        a = 1\n        b = 2\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={'a': core_schema.model_field(schema=core_schema.int_schema())},\n            from_attributes=True,\n            extra_behavior='forbid',\n        )\n    )\n    assert v.validate_python(Source()) == ({'a': 1}, None, {'a'})"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005239", "source": "def peek(self) -> Optional[str]:\n        if self._idx + 1 >= len(self._val):\n            return None\n        return self._val[self._idx + 1]", "target": "def test_is_iterable(self) -> None:\n        from collections.abc import Iterator\n        iter_ = PeekableIterator(\"\")\n        self.assertTrue(isinstance(iter_, Iterator))"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005240", "source": "def _dataclass_getstate(self: Any) -> list[Any]:\n                    return [getattr(self, f.name) for f in dataclasses.fields(self)]", "target": "def test_slots_mixed():\n    @dataclasses.dataclass(slots=True)\n    class Model:\n        x: int\n        y: dataclasses.InitVar[str]\n        z: ClassVar[str] = 'z-classvar'\n    @dataclasses.dataclass\n    class SubModel(Model):\n        x2: int\n        y2: dataclasses.InitVar[str]\n        z2: ClassVar[str] = 'z2-classvar'\n    schema = core_schema.dataclass_schema(\n        SubModel,\n        core_schema.dataclass_args_schema(\n            'SubModel',\n            [\n                core_schema.dataclass_field(name='x', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(name='y', init_only=True, schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='x2', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(name='y2', init_only=True, schema=core_schema.str_schema()),\n            ],\n        ),\n        ['x', 'x2'],\n        slots=True,\n    )\n    dc = SubModel(x=1, y='a', x2=2, y2='b')\n    assert dataclasses.asdict(dc) == {'x': 1, 'x2': 2}\n    s = SchemaSerializer(schema)\n    assert s.to_python(dc) == {'x': 1, 'x2': 2}\n    assert s.to_json(dc) == b'{\"x\":1,\"x2\":2}'"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005241", "source": "def is_valid_field_name(name: str) -> bool:\n    return not name.startswith('_')", "target": "def test_validate_assignment_allow_extra_validate():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={'field_a': core_schema.model_field(schema=core_schema.str_schema())},\n            extras_schema=core_schema.int_schema(),\n            extra_behavior='allow',\n        )\n    )\n    assert v.validate_assignment({'field_a': 'test'}, 'other_field', '456') == (\n        {'field_a': 'test'},\n        {'other_field': 456},\n        {'other_field'},\n    )\n    with pytest.raises(ValidationError) as exc_info:\n        assert v.validate_assignment({'field_a': 'test'}, 'other_field', 'xyz')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('other_field',),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'xyz',\n        }\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005242", "source": "def isoformat(o: Union[datetime.date, datetime.time]) -> str:\n    return o.isoformat()", "target": "def test_dict_key_json():\n    s = SchemaSerializer(core_schema.dict_schema(core_schema.json_schema(), core_schema.any_schema()))\n    v = {(1, 2): 3, (4, 5): 9}\n    assert s.to_python(v) == v\n    assert s.to_python(v, round_trip=True) == {'[1,2]': 3, '[4,5]': 9}\n    assert s.to_python(v, mode='json') == {'1,2': 3, '4,5': 9}\n    assert s.to_python(v, mode='json', round_trip=True) == {'[1,2]': 3, '[4,5]': 9}\n    assert s.to_json(v) == b'{\"1,2\":3,\"4,5\":9}'\n    assert s.to_json(v, round_trip=True) == b'{\"[1,2]\":3,\"[4,5]\":9}'"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005243", "source": "def dataclass(\n        *,\n        init: Literal[False] = False,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool = False,\n        config: ConfigDict | type[object] | None = None,\n        validate_on_init: bool | None = None,\n        kw_only: bool = ...,\n        slots: bool = ...,\n    ) -> Callable[[type[_T]], type[PydanticDataclass]]:\n        ...", "target": "def test_dataclass_self_init_post_init():\n    calls = []\n    @dataclasses.dataclass(init=False)\n    class Foo:\n        a: str\n        b: bool\n        c: dataclasses.InitVar[int]\n        def __init__(self, *args, **kwargs):\n            v.validate_python(ArgsKwargs(args, kwargs), self_instance=self)\n        def __post_init__(self, c):\n            calls.append(c)\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), kw_only=False),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), kw_only=False),\n                core_schema.dataclass_field(name='c', schema=core_schema.int_schema(), init_only=True),\n            ],\n            collect_init_only=True,\n        ),\n        ['a', 'b', 'c'],\n        post_init=True,\n    )\n    v = SchemaValidator(schema)\n    foo = Foo(b'hello', 'True', c='123')\n    assert dataclasses.is_dataclass(foo)\n    assert dataclasses.asdict(foo) == {'a': 'hello', 'b': True}\n    assert calls == [123]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005244", "source": "def custom_pydantic_encoder(type_encoders: dict[Any, Callable[[type[Any]], Any]], obj: Any) -> Any:\n    warnings.warn(\n        '`custom_pydantic_encoder` is deprecated, use `BaseModel.model_dump` instead.',\n        category=PydanticDeprecatedSince20,\n        stacklevel=2,\n    )\n    for base in obj.__class__.__mro__[:-1]:\n        try:\n            encoder = type_encoders[base]\n        except KeyError:\n            continue\n        return encoder(obj)\n    else:\n        return pydantic_encoder(obj)", "target": "def test_json_int():\n    s = SchemaSerializer(core_schema.json_schema(core_schema.int_schema()))\n    assert s.to_python(1) == 1\n    assert s.to_python(1, round_trip=True) == '1'\n    assert s.to_python(1, mode='json') == 1\n    assert s.to_python(1, mode='json', round_trip=True) == '1'\n    assert s.to_json(1) == b'1'\n    assert s.to_json(1, round_trip=True) == b'\"1\"'"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005245", "source": "def update_field_from_config(config_wrapper: ConfigWrapper, field_name: str, field_info: FieldInfo) -> None:\n    field_title_generator = field_info.field_title_generator or config_wrapper.field_title_generator\n    if field_title_generator is not None:\n        _apply_field_title_generator_to_field_info(field_title_generator, field_name, field_info)\n    if config_wrapper.alias_generator is not None:\n        _apply_alias_generator_to_field_info(config_wrapper.alias_generator, field_name, field_info)", "target": "def test_from_attributes_override_false():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={'a': core_schema.model_field(schema=core_schema.int_schema())}, from_attributes=True\n        )\n    )\n    with pytest.raises(ValidationError, match='Input should be a valid dictionary'):\n        v.validate_python(Cls(a=1), from_attributes=False)\n    assert v.validate_python(Cls(a=1)) == ({'a': 1}, None, {'a'})\n    assert v.isinstance_python(Cls(a=1)) is True\n    assert v.isinstance_python(Cls(a=1), from_attributes=False) is False"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005246", "source": "def to_markdown_table(\n    res: TimingResultType, header: Optional[tuple[str, ...]] = None\n) -> str:\n    if header is None:\n        header = (\"model\", \"task\", \"mean\", \"var\")\n    out = \"\"\n    def write_line(*args):\n        nonlocal out\n        out += f\"| {' | '.join(str(a) for a in args)} |\\n\"\n    write_line(*header)\n    write_line(*[\"--\"] * len(header))\n    for model, tasks in res.items():\n        for task, line in tasks.items():\n            write_line(*(model, task) + line)\n    return out", "target": "def test_simple(self) -> None:\n        @retries_decorator()\n        def foo(x: int, y: int) -> int:\n            return x + y\n        self.assertEqual(foo(3, 4), 7)"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005247", "source": "def env_path(\n    name: str,\n    default: Optional[Union[str, Path]] = None,\n    resolve: bool = True,\n) -> Path:\n    path = env_path_optional(name, default, resolve)\n    if not path:\n        raise ValueError(f\"Missing path value for {name}\")\n    return path", "target": "def test_env_path_optional_unset_returns_none_when_env_var_is_empty(self):\n        with patch.dict(os.environ, {\"P\": \"\"}, clear=True):\n            self.assertIsNone(m.env_path_optional(\"P\"))"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005248", "source": "def wrap(cls: Type[Any]) -> 'DataclassClassOrWrapper':\n        should_use_proxy = (\n            use_proxy\n            if use_proxy is not None\n            else (\n                is_builtin_dataclass(cls)\n                and (cls.__bases__[0] is object or set(dir(cls)) == set(dir(cls.__bases__[0])))\n            )\n        )\n        if should_use_proxy:\n            dc_cls_doc = ''\n            dc_cls = DataclassProxy(cls)\n            default_validate_on_init = False\n        else:\n            dc_cls_doc = cls.__doc__ or ''\n            if sys.version_info >= (3, 10):\n                dc_cls = dataclasses.dataclass(\n                    cls,\n                    init=init,\n                    repr=repr,\n                    eq=eq,\n                    order=order,\n                    unsafe_hash=unsafe_hash,\n                    frozen=frozen,\n                    kw_only=kw_only,\n                )\n            else:\n                dc_cls = dataclasses.dataclass(\n                    cls, init=init, repr=repr, eq=eq, order=order, unsafe_hash=unsafe_hash, frozen=frozen\n                )\n            default_validate_on_init = True\n        should_validate_on_init = default_validate_on_init if validate_on_init is None else validate_on_init\n        _add_pydantic_validation_attributes(cls, the_config, should_validate_on_init, dc_cls_doc)\n        dc_cls.__pydantic_model__.__try_update_forward_refs__(**{cls.__name__: cls})\n        return dc_cls", "target": "def test_dataclass_initvar_not_required_on_union_ser() -> None:\n    @dataclasses.dataclass\n    class Foo:\n        x: int\n        init_var: dataclasses.InitVar[int] = 1\n    @dataclasses.dataclass\n    class Bar:\n        x: int\n    schema = core_schema.union_schema(\n        [\n            core_schema.dataclass_schema(\n                Foo,\n                core_schema.dataclass_args_schema(\n                    'Foo',\n                    [\n                        core_schema.dataclass_field(name='x', schema=core_schema.int_schema()),\n                        core_schema.dataclass_field(\n                            name='init_var',\n                            init_only=True,\n                            schema=core_schema.with_default_schema(core_schema.int_schema(), default=1),\n                        ),\n                    ],\n                ),\n                ['x'],\n                post_init=True,\n            ),\n            core_schema.dataclass_schema(\n                Bar,\n                core_schema.dataclass_args_schema(\n                    'Bar', [core_schema.dataclass_field(name='x', schema=core_schema.int_schema())]\n                ),\n                ['x'],\n            ),\n        ]\n    )\n    s = SchemaSerializer(schema)\n    assert s.to_python(Foo(x=1), warnings='error') == {'x': 1}\n    assert s.to_python(Foo(x=1, init_var=2), warnings='error') == {'x': 1}"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005249", "source": "def benchmark_module(config, module, use_throughput_benchmark=False):\n    if use_throughput_benchmark:\n        return benchmark_using_throughput_benchmark(config, module)\n    module.forward(config.num_warmup_iters)\n    print(f\"Running module for {config.num_iters} iterations\")\n    start = time.time()\n    module.forward(config.num_iters)\n    end = time.time()\n    time_elapsed_s = end - start\n    return secs_to_ms(time_elapsed_s) / config.num_iters / NUM_LOOP_ITERS", "target": "def test_merged_lastfailed_content_with_empty_dest(self) -> None:\n        last_failed_source = {\n            \"tools/tests/test_car.py::test_num1\": True,\n            \"tools/tests/test_car.py::test_num2\": True,\n        }\n        last_failed_dest = {\n            \"\": True,\n        }\n        last_failed_merged = {\n            \"tools/tests/test_car.py::test_num1\": True,\n            \"tools/tests/test_car.py::test_num2\": True,\n        }\n        merged = _merged_lastfailed_content(last_failed_source, last_failed_dest)\n        self.assertEqual(merged, last_failed_merged)"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005250", "source": "def env_str_field(\n    name: str,\n    default: str = \"\",\n) -> str:\n    return field(default_factory=lambda: get_env(name, default))", "target": "def test_get_env_not_exist_without_default(self):\n        with patch.dict(os.environ, {\"FOO\": \"bar\"}, clear=True):\n            self.assertEqual(m.get_env(\"TEST_NOT_EXIST\"), \"\")"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005251", "source": "def _set_nested_attr(obj: nn.Module, names: list[str], value: Tensor) -> None:\n    if len(names) == 1:\n        setattr(obj, names[0], value)\n    else:\n        _set_nested_attr(getattr(obj, names[0]), names[1:], value)", "target": "def test_iterator(self, input_: str = \"abcdef\") -> None:\n        iter_ = PeekableIterator(input_)\n        for idx, c in enumerate(iter_):\n            self.assertEqual(c, input_[idx])"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005252", "source": "def is_union(tp: Optional[Type[Any]]) -> bool:\n        return tp is Union or tp is types.UnionType", "target": "def test_correct_function_signature() -> None:\n    def my_validator(value: Any, info: Any) -> str:\n        return str(value)\n    v = SchemaValidator(core_schema.with_info_plain_validator_function(my_validator))\n    assert v.validate_python(1) == '1'"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005253", "source": "def as_dataclass_field(pydantic_field: FieldInfo) -> dataclasses.Field[Any]:\n    field_args: dict[str, Any] = {'default': pydantic_field}\n    if sys.version_info >= (3, 14) and pydantic_field.description is not None:\n        field_args['doc'] = pydantic_field.description\n    if sys.version_info >= (3, 10) and pydantic_field.kw_only:\n        field_args['kw_only'] = True\n    if pydantic_field.repr is not True:\n        field_args['repr'] = pydantic_field.repr\n    return dataclasses.field(**field_args)", "target": "def test_dataclass_args_init(input_value, extra_behavior, expected):\n    @dataclasses.dataclass\n    class Foo:\n        a: str\n        b: str\n        def __post_init__(self):\n            self.b = self.a.upper()\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.str_schema(), init=False),\n            ],\n            extra_behavior=extra_behavior,\n        ),\n        ['a', 'b'],\n        post_init=True,\n    )\n    v = SchemaValidator(schema)\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)) as exc_info:\n            v.validate_python(input_value)\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        assert dataclasses.asdict(v.validate_python(input_value)) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005254", "source": "def create_pydantic_model_from_dataclass(\n    dc_cls: Type['Dataclass'],\n    config: Type[Any] = BaseConfig,\n    dc_cls_doc: Optional[str] = None,\n) -> Type['BaseModel']:\n    field_definitions: Dict[str, Any] = {}\n    for field in dataclasses.fields(dc_cls):\n        default: Any = Undefined\n        default_factory: Optional['NoArgAnyCallable'] = None\n        field_info: FieldInfo\n        if field.default is not dataclasses.MISSING:\n            default = field.default\n        elif field.default_factory is not dataclasses.MISSING:\n            default_factory = field.default_factory\n        else:\n            default = Required\n        if isinstance(default, FieldInfo):\n            field_info = default\n            dc_cls.__pydantic_has_field_info_default__ = True\n        else:\n            field_info = Field(default=default, default_factory=default_factory, **field.metadata)\n        field_definitions[field.name] = (field.type, field_info)\n    validators = gather_all_validators(dc_cls)\n    model: Type['BaseModel'] = create_model(\n        dc_cls.__name__,\n        __config__=config,\n        __module__=dc_cls.__module__,\n        __validators__=validators,\n        __cls_kwargs__={'__resolve_forward_refs__': False},\n        **field_definitions,\n    )\n    model.__doc__ = dc_cls_doc if dc_cls_doc is not None else dc_cls.__doc__ or ''\n    return model", "target": "def test_dataclass_exact_validation(revalidate_instances, input_value, expected):\n    schema = core_schema.dataclass_schema(\n        FooDataclass,\n        core_schema.dataclass_args_schema(\n            'FooDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n            ],\n        ),\n        ['a', 'b'],\n        revalidate_instances=revalidate_instances,\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python(input_value)\n    assert dataclasses.asdict(foo) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005255", "source": "def tail(self) -> ConfigWrapper:\n        return self._config_wrapper_stack[-1]", "target": "def test_on_model_class():\n    v = SchemaValidator(\n        cs.model_schema(\n            cls=MyModel,\n            config=CoreConfig(str_max_length=5),\n            schema=cs.model_fields_schema(fields={'f': cs.model_field(schema=cs.str_schema())}),\n        )\n    )\n    assert 'max_length:Some(5)' in plain_repr(v)\n    assert v.isinstance_python({'f': 'test'}) is True\n    assert v.isinstance_python({'f': 'test long'}) is False"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005256", "source": "def generate_dataclass_help(cls) -> str:\n    if not is_dataclass(cls):\n        raise TypeError(f\"{cls} is not a dataclass\")\n    def get_value(f):\n        if f.default is not MISSING:\n            return f.default\n        if f.default_factory is not MISSING:\n            try:\n                return f.default_factory()\n            except Exception as e:\n                return f\"<error: {e}>\"\n        return \"<required>\"\n    lines = [f\"{f.name:<22} = {repr(get_value(f))}\" for f in fields(cls)]\n    return indent(\"\\n\".join(lines), \"    \")", "target": "def test_dataclass_fields_read_env_at_instantiation(self):\n        @dataclass\n        class Cfg:\n            flag: bool = m.env_bool_field(\"FLAG\", default=False)\n            out: Path = m.env_path_field(\"OUT\", default=\"ab\", resolve=True)\n            name: str = m.env_str_field(\"NAME\", default=\"anon\")\n        with patch.dict(\n            os.environ, {\"FLAG\": \"true\", \"OUT\": \"outdir\", \"NAME\": \"alice\"}, clear=True\n        ):\n            cfg1 = Cfg()\n            self.assertTrue(cfg1.flag)\n            self.assertIsInstance(cfg1.out, Path)\n            self.assertTrue(cfg1.out.is_absolute())\n            self.assertEqual(cfg1.name, \"alice\")\n            cfg1.name = \"bob\"\n            self.assertEqual(cfg1.name, \"bob\")\n        with patch.dict(os.environ, {\"FLAG\": \"false\", \"NAME\": \"\"}, clear=True):\n            cfg2 = Cfg()\n            self.assertFalse(cfg2.flag)\n            self.assertTrue(\"ab\" in str(cfg2.out))\n            self.assertIsInstance(cfg2.out, Path)\n            self.assertTrue(cfg2.out.is_absolute())\n            self.assertEqual(cfg2.name, \"anon\")"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005257", "source": "def _recreate_field_info(\n    field_info: FieldInfo,\n    ns_resolver: NsResolver,\n    typevars_map: Mapping[TypeVar, Any],\n    *,\n    lenient: bool,\n) -> FieldInfo:\n    FieldInfo_ = import_cached_field_info()\n    existing_desc = field_info.description\n    if lenient:\n        ann = _generics.replace_types(field_info._original_annotation, typevars_map)\n        ann, evaluated = _typing_extra.try_eval_type(\n            ann,\n            *ns_resolver.types_namespace,\n        )\n    else:\n        ann = _typing_extra.eval_type(\n            field_info._original_annotation,\n            *ns_resolver.types_namespace,\n        )\n        ann = _generics.replace_types(ann, typevars_map)\n        ann = _typing_extra.eval_type(\n            ann,\n            *ns_resolver.types_namespace,\n        )\n        evaluated = True\n    if (assign := field_info._original_assignment) is PydanticUndefined:\n        new_field = FieldInfo_.from_annotation(ann, _source=AnnotationSource.CLASS)\n    else:\n        new_field = FieldInfo_.from_annotated_attribute(ann, assign, _source=AnnotationSource.CLASS)\n        new_field._original_assignment = assign\n    new_field._original_annotation = ann\n    new_field.description = new_field.description if new_field.description is not None else existing_desc\n    if not evaluated:\n        new_field._complete = False\n    return new_field", "target": "def test_from_attributes_override_true():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={'a': core_schema.model_field(schema=core_schema.int_schema())}, from_attributes=False\n        )\n    )\n    with pytest.raises(ValidationError, match='Input should be a valid dictionary'):\n        v.validate_python(Cls(a=1))\n    assert v.validate_python(Cls(a=1), from_attributes=True) == ({'a': 1}, None, {'a'})\n    assert v.isinstance_python(Cls(a=1), from_attributes=True) is True\n    assert v.isinstance_python(Cls(a=1)) is False"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005258", "source": "def update_field_from_config(config_wrapper: ConfigWrapper, field_name: str, field_info: FieldInfo) -> None:\n    field_title_generator = field_info.field_title_generator or config_wrapper.field_title_generator\n    if field_title_generator is not None:\n        _apply_field_title_generator_to_field_info(field_title_generator, field_name, field_info)\n    if config_wrapper.alias_generator is not None:\n        _apply_alias_generator_to_field_info(config_wrapper.alias_generator, field_name, field_info)", "target": "def test_model_fields_deep():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'field_a': core_schema.model_field(schema=core_schema.str_schema()),\n                'field_b': core_schema.model_field(\n                    schema=core_schema.model_fields_schema(\n                        fields={\n                            'field_c': core_schema.model_field(schema=core_schema.str_schema()),\n                            'field_d': core_schema.model_field(\n                                schema=core_schema.model_fields_schema(\n                                    fields={\n                                        'field_e': core_schema.model_field(schema=core_schema.str_schema()),\n                                        'field_f': core_schema.model_field(schema=core_schema.int_schema()),\n                                    }\n                                )\n                            ),\n                        }\n                    )\n                ),\n            }\n        )\n    )\n    model_dict, model_extra, fields_set = v.validate_python(\n        {'field_a': '1', 'field_b': {'field_c': '2', 'field_d': {'field_e': '4', 'field_f': 4}}}\n    )\n    assert model_dict == {\n        'field_a': '1',\n        'field_b': (\n            {'field_c': '2', 'field_d': ({'field_e': '4', 'field_f': 4}, None, {'field_f', 'field_e'})},\n            None,\n            {'field_d', 'field_c'},\n        ),\n    }\n    assert model_extra is None\n    assert fields_set == {'field_a', 'field_b'}\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python({'field_a': '1', 'field_b': {'field_c': '2', 'field_d': {'field_e': '4', 'field_f': 'xx'}}})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('field_b', 'field_d', 'field_f'),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'xx',\n        }\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005259", "source": "def _get_validators(cls: 'DataclassClassOrWrapper') -> 'CallableGenerator':\n    yield cls.__validate__", "target": "def test_custom_dataclass_names():\n    schema = core_schema.dataclass_schema(\n        FooParentDataclass,\n        core_schema.dataclass_args_schema(\n            'FooParentDataclass',\n            [\n                core_schema.dataclass_field(\n                    name='foo',\n                    schema=core_schema.union_schema(\n                        [\n                            core_schema.dataclass_schema(\n                                FooDataclass,\n                                core_schema.dataclass_args_schema(\n                                    'FooDataclass[dataclass_args_schema]',\n                                    [\n                                        core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                                        core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n                                    ],\n                                ),\n                                ['a', 'b'],\n                                cls_name='FooDataclass[cls_name]',\n                            ),\n                            core_schema.none_schema(),\n                        ]\n                    ),\n                )\n            ],\n        ),\n        ['foo'],\n    )\n    v = SchemaValidator(schema)\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python({'foo': 123})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'class_name': 'FooDataclass[dataclass_args_schema]'},\n            'input': 123,\n            'loc': ('foo', 'FooDataclass[cls_name]'),\n            'msg': 'Input should be a dictionary or an instance of FooDataclass[dataclass_args_schema]',\n            'type': 'dataclass_type',\n        },\n        {'input': 123, 'loc': ('foo', 'none'), 'msg': 'Input should be None', 'type': 'none_required'},\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005260", "source": "def new_type_supertype(type_: Type[Any]) -> Type[Any]:\n    while hasattr(type_, '__supertype__'):\n        type_ = type_.__supertype__\n    return type_", "target": "def test_schema_typing() -> None:\n    schema: CoreSchema = {\n        'type': 'union',\n        'choices': [{'type': 'int'}, {'type': 'int', 'ge': 1}, {'type': 'float', 'lt': 1.0}],\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = {\n        'type': 'tagged-union',\n        'discriminator': 'type',\n        'choices': {\n            'apple': {\n                'type': 'typed-dict',\n                'fields': {'pips': {'type': 'typed-dict-field', 'schema': {'type': 'int'}}},\n            },\n            'banana': {\n                'type': 'typed-dict',\n                'fields': {'curvature': {'type': 'typed-dict-field', 'schema': {'type': 'float'}}},\n            },\n        },\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'int', 'ge': 1}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'float', 'lt': 1.0}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'str', 'pattern': r'http://.*'}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'bool', 'strict': False}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'literal', 'expected': [1, '1']}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'any'}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'none'}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'bytes'}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'list', 'items_schema': {'type': 'str'}, 'min_length': 3}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'set', 'items_schema': {'type': 'str'}, 'max_length': 3}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'tuple', 'items_schema': [{'type': 'str'}], 'variadic_item_index': 0, 'max_length': 3}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'tuple', 'items_schema': [{'type': 'str'}, {'type': 'int'}]}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'frozenset', 'items_schema': {'type': 'str'}, 'max_length': 3}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'dict', 'keys_schema': {'type': 'str'}, 'values_schema': {'type': 'any'}}\n    SchemaValidator(schema)\n    schema: CoreSchema = {\n        'type': 'typed-dict',\n        'fields': {'bar': {'type': 'typed-dict-field', 'schema': {'type': 'str'}}},\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = {\n        'type': 'model',\n        'cls': Foo,\n        'schema': {'type': 'model-fields', 'fields': {'bar': {'type': 'model-field', 'schema': {'type': 'str'}}}},\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = {\n        'type': 'typed-dict',\n        'fields': {\n            'a': {'type': 'typed-dict-field', 'schema': {'type': 'str'}},\n            'b': {'type': 'typed-dict-field', 'schema': {'type': 'str'}, 'validation_alias': 'foobar'},\n            'c': {\n                'type': 'typed-dict-field',\n                'schema': {'type': 'str'},\n                'validation_alias': [['foobar', 0, 'bar'], ['foo']],\n            },\n            'd': {\n                'type': 'typed-dict-field',\n                'schema': {'type': 'default', 'schema': {'type': 'str'}, 'default': 'spam'},\n            },\n        },\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = {\n        'type': 'function-wrap',\n        'function': {'type': 'with-info', 'function': wrap_validator, 'field_name': 'foobar'},\n        'schema': {'type': 'str'},\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = core_schema.with_info_plain_validator_function(validator)\n    SchemaValidator(schema)\n    schema: CoreSchema = {\n        'type': 'definitions',\n        'schema': {'type': 'definition-ref', 'schema_ref': 'Branch'},\n        'definitions': [\n            {\n                'type': 'typed-dict',\n                'fields': {\n                    'name': {'type': 'typed-dict-field', 'schema': {'type': 'str'}},\n                    'sub_branch': {\n                        'type': 'typed-dict-field',\n                        'schema': {\n                            'type': 'default',\n                            'schema': {\n                                'type': 'nullable',\n                                'schema': {'type': 'definition-ref', 'schema_ref': 'Branch'},\n                            },\n                            'default': None,\n                        },\n                    },\n                },\n                'ref': 'Branch',\n            }\n        ],\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'date', 'le': date.today()}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'time', 'lt': time(12, 13, 14)}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'datetime', 'ge': datetime.now()}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'is-instance', 'cls': Foo}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'callable'}\n    SchemaValidator(schema)\n    schema: CoreSchema = {\n        'type': 'arguments',\n        'arguments_schema': [\n            {'name': 'a', 'mode': 'positional_only', 'schema': {'type': 'int'}},\n            {'name': 'b', 'schema': {'type': 'str'}},\n            {'name': 'c', 'mode': 'keyword_only', 'schema': {'type': 'bool'}},\n        ],\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'call', 'arguments_schema': {'type': 'any'}, 'function': foo}\n    SchemaValidator(schema)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005261", "source": "def dataclass(\n        *,\n        init: bool = True,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool = False,\n        config: Union[ConfigDict, Type[object], None] = None,\n        validate_on_init: Optional[bool] = None,\n        use_proxy: Optional[bool] = None,\n        kw_only: bool = ...,\n    ) -> Callable[[Type[_T]], 'DataclassClassOrWrapper']:\n        ...", "target": "def test_dataclass_args_init_with_default(input_value, extra_behavior, expected):\n    @dataclasses.dataclass\n    class Foo:\n        a: str\n        b: str\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(\n                    name='b',\n                    schema=core_schema.with_default_schema(schema=core_schema.str_schema(), default='HELLO'),\n                    init=False,\n                ),\n            ],\n            extra_behavior=extra_behavior,\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)) as exc_info:\n            v.validate_python(input_value)\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        assert dataclasses.asdict(v.validate_python(input_value)) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005262", "source": "def validated_setattr(instance: PydanticDataclass, name: str, value: Any, /) -> None:\n                if frozen_:\n                    return original_setattr(instance, name, value)\n                inst_cls = type(instance)\n                attr = getattr(inst_cls, name, None)\n                if isinstance(attr, property):\n                    attr.__set__(instance, value)\n                elif isinstance(attr, functools.cached_property):\n                    instance.__dict__.__setitem__(name, value)\n                else:\n                    inst_cls.__pydantic_validator__.validate_assignment(instance, name, value)", "target": "def test_dataclass_post_init_args_multiple():\n    dc_args = None\n    @dataclasses.dataclass\n    class Foo:\n        a: str\n        b: dataclasses.InitVar[bool]\n        c: dataclasses.InitVar[int]\n        def __post_init__(self, *args):\n            nonlocal dc_args\n            dc_args = args\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), init_only=True),\n                core_schema.dataclass_field(name='c', schema=core_schema.int_schema(), init_only=True),\n            ],\n            collect_init_only=True,\n        ),\n        ['a', 'b'],\n        post_init=True,\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': b'hello', 'b': 'true', 'c': '42'})\n    assert dataclasses.asdict(foo) == {'a': 'hello'}\n    assert dc_args == (True, 42)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005263", "source": "def set_dataclass_fields(\n    cls: type[StandardDataclass],\n    config_wrapper: _config.ConfigWrapper,\n    ns_resolver: NsResolver | None = None,\n) -> None:\n    typevars_map = get_standard_typevars_map(cls)\n    fields = collect_dataclass_fields(\n        cls, ns_resolver=ns_resolver, typevars_map=typevars_map, config_wrapper=config_wrapper\n    )\n    cls.__pydantic_fields__ = fields", "target": "def test_extra_custom_serializer():\n    @dataclasses.dataclass\n    class Model:\n        pass\n    schema = core_schema.dataclass_schema(\n        Model,\n        core_schema.dataclass_args_schema(\n            'Model',\n            [],\n            extra_behavior='allow',\n        ),\n        [],\n    )\n    s = SchemaSerializer(schema)\n    v = SchemaValidator(schema)\n    m = v.validate_python({'extra': 'extra'})\n    assert s.to_python(m) == {'extra': 'extra bam!'}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005264", "source": "def type_var_default_factory() -> None:\n                raise RuntimeError(\n                    'Generic defaultdict cannot be used without a concrete value type or an'\n                    ' explicit default factory, ' + instructions\n                )", "target": "def test_validators_build(benchmark) -> None:\n    class Base1(BaseModel):\n        a: int\n        @field_validator('a', mode='after')\n        @classmethod\n        def val_a(cls, value: int) -> int: ...\n        @computed_field\n        def prop(self) -> int: ...\n    class Bare:\n        @computed_field\n        def prop_bare(self) -> int: ...\n    class Sub1(Base1):\n        @computed_field\n        def prop_2(self) -> int: ...\n        @computed_field\n        def prop_3(self) -> int: ...\n        @computed_field\n        def prop_4(self) -> int: ...\n    @benchmark\n    def bench() -> None:\n        class SubS(Sub1, Bare, defer_build=True):\n            @computed_field\n            def prop_5(self) -> int: ...\n            @computed_field\n            def prop_6(self) -> int: ...\n            @computed_field\n            def prop_7(self) -> int: ..."}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005265", "source": "def revert(self, ref: str) -> None:\n        self._run_git(\"revert\", \"--no-edit\", ref)", "target": "def test_is_iterable(self) -> None:\n        from collections.abc import Iterator\n        iter_ = PeekableIterator(\"\")\n        self.assertTrue(isinstance(iter_, Iterator))"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005266", "source": "def extract_weights(mod: nn.Module) -> tuple[tuple[Tensor, ...], list[str]]:\n    orig_params = tuple(mod.parameters())\n    names = []\n    for name, p in list(mod.named_parameters()):\n        _del_nested_attr(mod, name.split(\".\"))\n        names.append(name)\n    params = tuple(p.detach().requires_grad_() for p in orig_params)\n    return params, names", "target": "def test_is_iterable(self) -> None:\n        from collections.abc import Iterator\n        iter_ = PeekableIterator(\"\")\n        self.assertTrue(isinstance(iter_, Iterator))"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005267", "source": "def dataclass(\n        _cls: Type[_T],\n        *,\n        init: bool = True,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool = False,\n        config: Union[ConfigDict, Type[object], None] = None,\n        validate_on_init: Optional[bool] = None,\n        use_proxy: Optional[bool] = None,\n        kw_only: bool = ...,\n    ) -> 'DataclassClassOrWrapper':\n        ...", "target": "def test_dataclass_wrap_json():\n    schema = core_schema.no_info_wrap_validator_function(\n        lambda v, handler: handler(v),\n        core_schema.dataclass_schema(\n            FooDataclass,\n            core_schema.dataclass_args_schema(\n                'FooDataclass',\n                [\n                    core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                    core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n                ],\n            ),\n            ['a', 'b'],\n        ),\n    )\n    v = SchemaValidator(schema)\n    assert v.validate_json('{\"a\": \"hello\", \"b\": true}') == FooDataclass(a='hello', b=True)\n    assert v.validate_json('{\"a\": \"hello\", \"b\": true}', strict=True) == FooDataclass(a='hello', b=True)"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005268", "source": "def env_path_optional(\n    name: str,\n    default: Optional[Union[str, Path]] = None,\n    resolve: bool = True,\n) -> Optional[Path]:\n    val = get_env(name) or default\n    if not val:\n        return None\n    path = Path(val)\n    return path.resolve() if resolve else path", "target": "def test_env_path_optional_unset_returns_none_when_env_var_is_empty(self):\n        with patch.dict(os.environ, {\"P\": \"\"}, clear=True):\n            self.assertIsNone(m.env_path_optional(\"P\"))"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005269", "source": "def _call_initvar(*args: Any, **kwargs: Any) -> NoReturn:\n        raise TypeError(\"'InitVar' object is not callable\")", "target": "def test_function_validator_wrapping_args_schema_after() -> None:\n    calls: list[Any] = []\n    def func(*args: Any) -> Any:\n        calls.append(args)\n        return args[0]\n    @dataclasses.dataclass\n    class Model:\n        number: int = 1\n    cs = core_schema.dataclass_schema(\n        Model,\n        core_schema.no_info_after_validator_function(\n            func,\n            core_schema.dataclass_args_schema(\n                'Model', [core_schema.dataclass_field('number', core_schema.int_schema())]\n            ),\n        ),\n        ['number'],\n    )\n    v = SchemaValidator(cs)\n    instance: Model = v.validate_python({'number': 1})\n    assert instance.number == 1\n    assert calls == [(({'number': 1}, None),)]\n    v.validate_assignment(instance, 'number', 2)\n    assert instance.number == 2\n    assert calls == [(({'number': 1}, None),), (({'number': 2}, None),)]"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005270", "source": "def get_git_remote_name() -> str:\n    return os.getenv(\"GIT_REMOTE_NAME\", \"origin\")", "target": "def test_iterator(self, input_: str = \"abcdef\") -> None:\n        iter_ = PeekableIterator(input_)\n        for idx, c in enumerate(iter_):\n            self.assertEqual(c, input_[idx])"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005271", "source": "def _recreate_field_info(\n    field_info: FieldInfo,\n    ns_resolver: NsResolver,\n    typevars_map: Mapping[TypeVar, Any],\n    *,\n    lenient: bool,\n) -> FieldInfo:\n    FieldInfo_ = import_cached_field_info()\n    existing_desc = field_info.description\n    if lenient:\n        ann = _generics.replace_types(field_info._original_annotation, typevars_map)\n        ann, evaluated = _typing_extra.try_eval_type(\n            ann,\n            *ns_resolver.types_namespace,\n        )\n    else:\n        ann = _typing_extra.eval_type(\n            field_info._original_annotation,\n            *ns_resolver.types_namespace,\n        )\n        ann = _generics.replace_types(ann, typevars_map)\n        ann = _typing_extra.eval_type(\n            ann,\n            *ns_resolver.types_namespace,\n        )\n        evaluated = True\n    if (assign := field_info._original_assignment) is PydanticUndefined:\n        new_field = FieldInfo_.from_annotation(ann, _source=AnnotationSource.CLASS)\n    else:\n        new_field = FieldInfo_.from_annotated_attribute(ann, assign, _source=AnnotationSource.CLASS)\n        new_field._original_assignment = assign\n    new_field._original_annotation = ann\n    new_field.description = new_field.description if new_field.description is not None else existing_desc\n    if not evaluated:\n        new_field._complete = False\n    return new_field", "target": "def test_from_attributes_override_false():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={'a': core_schema.model_field(schema=core_schema.int_schema())}, from_attributes=True\n        )\n    )\n    with pytest.raises(ValidationError, match='Input should be a valid dictionary'):\n        v.validate_python(Cls(a=1), from_attributes=False)\n    assert v.validate_python(Cls(a=1)) == ({'a': 1}, None, {'a'})\n    assert v.isinstance_python(Cls(a=1)) is True\n    assert v.isinstance_python(Cls(a=1), from_attributes=False) is False"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005272", "source": "def update_field_from_config(config_wrapper: ConfigWrapper, field_name: str, field_info: FieldInfo) -> None:\n    field_title_generator = field_info.field_title_generator or config_wrapper.field_title_generator\n    if field_title_generator is not None:\n        _apply_field_title_generator_to_field_info(field_title_generator, field_name, field_info)\n    if config_wrapper.alias_generator is not None:\n        _apply_alias_generator_to_field_info(config_wrapper.alias_generator, field_name, field_info)", "target": "def test_validate_assignment_allow_extra_validate():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={'field_a': core_schema.model_field(schema=core_schema.str_schema())},\n            extras_schema=core_schema.int_schema(),\n            extra_behavior='allow',\n        )\n    )\n    assert v.validate_assignment({'field_a': 'test'}, 'other_field', '456') == (\n        {'field_a': 'test'},\n        {'other_field': 456},\n        {'other_field'},\n    )\n    with pytest.raises(ValidationError) as exc_info:\n        assert v.validate_assignment({'field_a': 'test'}, 'other_field', 'xyz')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('other_field',),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'xyz',\n        }\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005273", "source": "def force_create_dir(path: Union[str, Path]) -> Path:\n    remove_dir(path)\n    return ensure_dir_exists(path)", "target": "def test_get_path_returns_path_for_str(self):\n        rel_str = \"sub/f.txt\"\n        os.chdir(self.tmp_path)\n        p = get_path(rel_str, resolve=False)\n        self.assertIsInstance(p, Path)\n        self.assertFalse(p.is_absolute())\n        self.assertEqual(str(p), rel_str)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005274", "source": "def custom_pydantic_encoder(type_encoders: dict[Any, Callable[[type[Any]], Any]], obj: Any) -> Any:\n    warnings.warn(\n        '`custom_pydantic_encoder` is deprecated, use `BaseModel.model_dump` instead.',\n        category=PydanticDeprecatedSince20,\n        stacklevel=2,\n    )\n    for base in obj.__class__.__mro__[:-1]:\n        try:\n            encoder = type_encoders[base]\n        except KeyError:\n            continue\n        return encoder(obj)\n    else:\n        return pydantic_encoder(obj)", "target": "def test_to_jsonable_python_fallback():\n    with pytest.raises(PydanticSerializationError, match=r'Unable to serialize unknown type: <.+\\.Foobar'):\n        to_jsonable_python(Foobar())\n    assert to_jsonable_python(Foobar(), serialize_unknown=True) == 'Foobar.__str__'\n    assert to_jsonable_python(Foobar(), serialize_unknown=True, fallback=fallback_func) == 'fallback:Foobar'\n    assert to_jsonable_python(Foobar(), fallback=fallback_func) == 'fallback:Foobar'"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005275", "source": "def rebuild_model_fields(\n    cls: type[BaseModel],\n    *,\n    config_wrapper: ConfigWrapper,\n    ns_resolver: NsResolver,\n    typevars_map: Mapping[TypeVar, Any],\n) -> dict[str, FieldInfo]:\n    rebuilt_fields: dict[str, FieldInfo] = {}\n    with ns_resolver.push(cls):\n        for f_name, field_info in cls.__pydantic_fields__.items():\n            if field_info._complete:\n                rebuilt_fields[f_name] = field_info\n            else:\n                new_field = _recreate_field_info(\n                    field_info, ns_resolver=ns_resolver, typevars_map=typevars_map, lenient=False\n                )\n                update_field_from_config(config_wrapper, f_name, new_field)\n                rebuilt_fields[f_name] = new_field\n    return rebuilt_fields", "target": "def test_validate_assignment_ignore_extra():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(fields={'field_a': core_schema.model_field(schema=core_schema.str_schema())})\n    )\n    assert v.validate_python({'field_a': 'test'}) == ({'field_a': 'test'}, None, {'field_a'})\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_assignment({'field_a': 'test'}, 'other_field', 456)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'no_such_attribute',\n            'loc': ('other_field',),\n            'msg': \"Object has no attribute 'other_field'\",\n            'input': 456,\n            'ctx': {'attribute': 'other_field'},\n        }\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005276", "source": "def env_bool_field(\n    name: str,\n    default: bool = False,\n):\n    return field(default_factory=lambda: env_bool(name, default))", "target": "def test_env_bool_uses_str2bool_when_set(self):\n        def fake_str2bool(s: str) -> bool:\n            return s.lower() in {\"1\", \"true\", \"yes\", \"on\", \"y\"}\n        with (\n            patch.dict(os.environ, {\"FLAG\": \"yEs\"}, clear=True),\n            patch.object(m, \"str2bool\", fake_str2bool),\n        ):\n            self.assertTrue(m.env_bool(\"FLAG\", default=False))"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005277", "source": "def from_markdown_table(data: str) -> TimingResultType:\n    out = data.strip().split(\"\\n\")\n    out = out[2:]\n    res: TimingResultType\n    res = defaultdict(defaultdict)\n    for line in out:\n        model, task, mean, var = (f.strip() for f in line.strip().split(\"|\") if f)\n        res[model][task] = (float(mean), float(var))\n    return res", "target": "def test_fails(self) -> None:\n        @retries_decorator(rc=0)\n        def foo(x: int, y: int) -> int:\n            return x + y\n        self.assertEqual(foo(\"a\", 4), 0)"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005278", "source": "def are_ghstack_branches_in_sync(\n    repo: GitRepo, head_ref: str, base_ref: Optional[str] = None\n) -> bool:\n    orig_ref = re.sub(r\"/head$\", \"/orig\", head_ref)\n    if base_ref is None:\n        base_ref = re.sub(r\"/head$\", \"/base\", head_ref)\n    orig_diff_sha = _shasum(repo.diff(f\"{repo.remote}/{orig_ref}\"))\n    head_diff_sha = _shasum(\n        repo.diff(\n            base_ref if is_commit_hash(base_ref) else f\"{repo.remote}/{base_ref}\",\n            f\"{repo.remote}/{head_ref}\",\n        )\n    )\n    return orig_diff_sha == head_diff_sha", "target": "def test_double_asterisks(self) -> None:\n        allowed_patterns = [\n            \"aten/src/ATen/native/**LinearAlgebra*\",\n        ]\n        patterns_re = patterns_to_regex(allowed_patterns)\n        fnames = [\n            \"aten/src/ATen/native/LinearAlgebra.cpp\",\n            \"aten/src/ATen/native/cpu/LinearAlgebraKernel.cpp\",\n        ]\n        for filename in fnames:\n            self.assertTrue(patterns_re.match(filename))"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005279", "source": "def dataclass(\n        *,\n        init: bool = True,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool = False,\n        config: Union[ConfigDict, Type[object], None] = None,\n        validate_on_init: Optional[bool] = None,\n        use_proxy: Optional[bool] = None,\n        kw_only: bool = ...,\n    ) -> Callable[[Type[_T]], 'DataclassClassOrWrapper']:\n        ...", "target": "def test_properties():\n    @dataclasses.dataclass\n    class FooProp:\n        a: str\n        b: bytes\n        @property\n        def c(self) -> str:\n            return f'{self.a} {self.b.decode()}'\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'FooProp',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bytes_schema()),\n            ],\n            computed_fields=[core_schema.computed_field('c', core_schema.str_schema())],\n        ),\n        ['a', 'b'],\n    )\n    s = SchemaSerializer(schema)\n    assert s.to_python(FooProp(a='hello', b=b'more')) == IsStrictDict(a='hello', b=b'more', c='hello more')\n    assert s.to_python(FooProp(a='hello', b=b'more'), mode='json') == IsStrictDict(a='hello', b='more', c='hello more')\n    j = s.to_json(FooProp(a='hello', b=b'more'))\n    if on_pypy:\n        assert json.loads(j) == {'a': 'hello', 'b': 'more', 'c': 'hello more'}\n    else:\n        assert j == b'{\"a\":\"hello\",\"b\":\"more\",\"c\":\"hello more\"}'\n    assert s.to_python(FooProp(a='hello', b=b'more'), exclude={'b'}) == IsStrictDict(a='hello', c='hello more')\n    assert s.to_json(FooProp(a='hello', b=b'more'), include={'a'}) == b'{\"a\":\"hello\"}'"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005280", "source": "def working_directory(path: str):\n    if not path:\n        yield\n        return\n    prev_cwd = os.getcwd()\n    try:\n        os.chdir(path)\n        yield\n    finally:\n        os.chdir(prev_cwd)", "target": "def test_ghstack_branches_not_in_sync(self) -> None:\n        head_ref = \"gh/clee2000/1/head\"\n        self._skip_if_ref_does_not_exist(head_ref)\n        self.assertFalse(are_ghstack_branches_in_sync(self.repo, head_ref))"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005281", "source": "def patch_base_fields(cls: type[Any]) -> Generator[None]:\n    original_fields_list: list[tuple[DcFields, DcFields]] = []\n    for base in cls.__mro__[1:]:\n        dc_fields: dict[str, dataclasses.Field[Any]] = base.__dict__.get('__dataclass_fields__', {})\n        dc_fields_with_pydantic_field_defaults = {\n            field_name: field\n            for field_name, field in dc_fields.items()\n            if isinstance(field.default, FieldInfo)\n            and (field.default.description is not None or field.default.kw_only or field.default.repr is not True)\n        }\n        if dc_fields_with_pydantic_field_defaults:\n            original_fields_list.append((dc_fields, dc_fields_with_pydantic_field_defaults))\n            for field_name, field in dc_fields_with_pydantic_field_defaults.items():\n                default = cast(FieldInfo, field.default)\n                new_dc_field = copy.copy(field)\n                if sys.version_info >= (3, 10) and default.kw_only:\n                    new_dc_field.kw_only = True\n                if default.repr is not True:\n                    new_dc_field.repr = default.repr\n                dc_fields[field_name] = new_dc_field\n    try:\n        yield\n    finally:\n        for fields, original_fields in original_fields_list:\n            for field_name, original_field in original_fields.items():\n                fields[field_name] = original_field", "target": "def test_serialization_exclude():\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(\n                    name='a', schema=core_schema.str_schema(), serialization_exclude_if=lambda x: x == 'bye'\n                ),\n                core_schema.dataclass_field(name='b', schema=core_schema.bytes_schema(), serialization_exclude=True),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    s = SchemaSerializer(schema)\n    assert s.to_python(Foo(a='hello', b=b'more')) == {'a': 'hello'}\n    assert s.to_python(Foo(a='hello', b=b'more'), mode='json') == {'a': 'hello'}\n    assert s.to_python(Foo(a='bye', b=b'more'), mode='json') == {}\n    j = s.to_json(Foo(a='hello', b=b'more'))\n    if on_pypy:\n        assert json.loads(j) == {'a': 'hello'}\n    else:\n        assert j == b'{\"a\":\"hello\"}'\n    j = s.to_json(Foo(a='bye', b=b'more'))\n    if on_pypy:\n        assert json.loads(j) == {}\n    else:\n        assert j == b'{}'"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005282", "source": "def env_path_optional(\n    name: str,\n    default: Optional[Union[str, Path]] = None,\n    resolve: bool = True,\n) -> Optional[Path]:\n    val = get_env(name) or default\n    if not val:\n        return None\n    path = Path(val)\n    return path.resolve() if resolve else path", "target": "def test_get_env_unset_returns_default(self):\n        with patch.dict(os.environ, {}, clear=True):\n            self.assertEqual(m.get_env(\"FOO\", \"default\"), \"default\")"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005283", "source": "def collect_model_fields(\n    cls: type[BaseModel],\n    config_wrapper: ConfigWrapper,\n    ns_resolver: NsResolver,\n    *,\n    typevars_map: Mapping[TypeVar, Any] | None = None,\n) -> tuple[dict[str, FieldInfo], set[str]]:\n    FieldInfo_ = import_cached_field_info()\n    BaseModel_ = import_cached_base_model()\n    bases = cls.__bases__\n    parent_fields_lookup: dict[str, FieldInfo] = {}\n    for base in reversed(bases):\n        if model_fields := getattr(base, '__pydantic_fields__', None):\n            parent_fields_lookup.update(model_fields)\n    type_hints = _typing_extra.get_model_type_hints(cls, ns_resolver=ns_resolver)\n    annotations = _typing_extra.safe_get_annotations(cls)\n    fields: dict[str, FieldInfo] = {}\n    class_vars: set[str] = set()\n    for ann_name, (ann_type, evaluated) in type_hints.items():\n        if ann_name == 'model_config':\n            continue\n        _check_protected_namespaces(\n            protected_namespaces=config_wrapper.protected_namespaces,\n            ann_name=ann_name,\n            bases=bases,\n            cls_name=cls.__name__,\n        )\n        if _typing_extra.is_classvar_annotation(ann_type):\n            class_vars.add(ann_name)\n            continue\n        assigned_value = getattr(cls, ann_name, PydanticUndefined)\n        if assigned_value is not PydanticUndefined and (\n            any(getattr(BaseModel_, depr_name, None) is assigned_value for depr_name in _deprecated_method_names)\n            or (\n                hasattr(assigned_value, '__func__')\n                and any(\n                    getattr(getattr(BaseModel_, depr_name, None), '__func__', None) is assigned_value.__func__\n                    for depr_name in _deprecated_classmethod_names\n                )\n            )\n        ):\n            assigned_value = PydanticUndefined\n        if not is_valid_field_name(ann_name):\n            continue\n        if cls.__pydantic_root_model__ and ann_name != 'root':\n            raise NameError(\n                f\"Unexpected field with name {ann_name!r}; only 'root' is allowed as a field of a `RootModel`\"\n            )\n        generic_origin = getattr(cls, '__pydantic_generic_metadata__', {}).get('origin')\n        for base in bases:\n            dataclass_fields = {\n                field.name for field in (dataclasses.fields(base) if dataclasses.is_dataclass(base) else ())\n            }\n            if hasattr(base, ann_name):\n                if base is generic_origin:\n                    continue\n                if ann_name in dataclass_fields:\n                    continue\n                if ann_name not in annotations:\n                    continue\n                warnings.warn(\n                    f'Field name \"{ann_name}\" in \"{cls.__qualname__}\" shadows an attribute in parent '\n                    f'\"{base.__qualname__}\"',\n                    UserWarning,\n                    stacklevel=4,\n                )\n        if assigned_value is PydanticUndefined:\n            if ann_name in annotations or ann_name not in parent_fields_lookup:\n                field_info = FieldInfo_.from_annotation(ann_type, _source=AnnotationSource.CLASS)\n                field_info._original_annotation = ann_type\n                if not evaluated:\n                    field_info._complete = False\n            else:\n                parent_field_info = parent_fields_lookup[ann_name]._copy()\n                if typevars_map:\n                    field_info = _recreate_field_info(\n                        parent_field_info, ns_resolver=ns_resolver, typevars_map=typevars_map, lenient=True\n                    )\n                else:\n                    field_info = parent_field_info\n        else:\n            if isinstance(assigned_value, FieldInfo_) and ismethoddescriptor(assigned_value.default):\n                default = assigned_value.default.__get__(None, cls)\n                assigned_value.default = default\n                assigned_value._attributes_set['default'] = default\n            field_info = FieldInfo_.from_annotated_attribute(ann_type, assigned_value, _source=AnnotationSource.CLASS)\n            field_info._original_assignment = assigned_value\n            field_info._original_annotation = ann_type\n            if not evaluated:\n                field_info._complete = False\n            elif 'final' in field_info._qualifiers and not field_info.is_required():\n                warnings.warn(\n                    f'Annotation {ann_name!r} is marked as final and has a default value. Pydantic treats {ann_name!r} as a '\n                    'class variable, but it will be considered as a normal field in V3 to be aligned with dataclasses. If you '\n                    f'still want {ann_name!r} to be considered as a class variable, annotate it as: `ClassVar[<type>] = <default>.`',\n                    category=PydanticDeprecatedSince211,\n                    stacklevel=4,\n                )\n                class_vars.add(ann_name)\n                continue\n            try:\n                delattr(cls, ann_name)\n            except AttributeError:\n                pass\n        decorators: DecoratorInfos = cls.__dict__['__pydantic_decorators__']\n        if ann_name in decorators.computed_fields:\n            raise TypeError(\n                f'Field {ann_name!r} of class {cls.__name__!r} overrides symbol of same name in a parent class. '\n                'This override with a computed_field is incompatible.'\n            )\n        fields[ann_name] = field_info\n        if field_info._complete:\n            update_field_from_config(config_wrapper, ann_name, field_info)\n    if config_wrapper.use_attribute_docstrings:\n        _update_fields_from_docstrings(cls, fields)\n    return fields, class_vars", "target": "def test_extra_behavior_ignore(\n    config: Union[core_schema.CoreConfig, None],\n    schema_extra_behavior_kw: dict[str, Any],\n    validate_fn_extra_kw: Union[ExtraBehavior, None],\n):\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            {'f': core_schema.model_field(core_schema.str_schema())}, **schema_extra_behavior_kw\n        ),\n        config=config,\n    )\n    m, model_extra, fields_set = v.validate_python({'f': 'x', 'extra_field': 123}, extra=validate_fn_extra_kw)\n    assert m == {'f': 'x'}\n    assert model_extra is None\n    assert fields_set == {'f'}\n    v.validate_assignment(m, 'f', 'y', extra=validate_fn_extra_kw)\n    assert m['f'] == 'y'\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_assignment(m, 'not_f', 'xyz', extra=validate_fn_extra_kw)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'no_such_attribute',\n            'loc': ('not_f',),\n            'msg': \"Object has no attribute 'not_f'\",\n            'input': 'xyz',\n            'ctx': {'attribute': 'not_f'},\n        }\n    ]\n    assert 'not_f' not in m"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005284", "source": "def dataclass(\n    _cls: type[_T] | None = None,\n    *,\n    init: Literal[False] = False,\n    repr: bool = True,\n    eq: bool = True,\n    order: bool = False,\n    unsafe_hash: bool = False,\n    frozen: bool | None = None,\n    config: ConfigDict | type[object] | None = None,\n    validate_on_init: bool | None = None,\n    kw_only: bool = False,\n    slots: bool = False,\n) -> Callable[[type[_T]], type[PydanticDataclass]] | type[PydanticDataclass]:\n    assert init is False, 'pydantic.dataclasses.dataclass only supports init=False'\n    assert validate_on_init is not False, 'validate_on_init=False is no longer supported'\n    if sys.version_info >= (3, 10):\n        kwargs = {'kw_only': kw_only, 'slots': slots}\n    else:\n        kwargs = {}\n    def create_dataclass(cls: type[Any]) -> type[PydanticDataclass]:\n        from ._internal._utils import is_model_class\n        if is_model_class(cls):\n            raise PydanticUserError(\n                f'Cannot create a Pydantic dataclass from {cls.__name__} as it is already a Pydantic model',\n                code='dataclass-on-model',\n            )\n        original_cls = cls\n        has_dataclass_base = any(dataclasses.is_dataclass(base) for base in cls.__bases__)\n        if not has_dataclass_base and config is not None and hasattr(cls, '__pydantic_config__'):\n            warn(\n                f'`config` is set via both the `dataclass` decorator and `__pydantic_config__` for dataclass {cls.__name__}. '\n                f'The `config` specification from `dataclass` decorator will take priority.',\n                category=UserWarning,\n                stacklevel=2,\n            )\n        config_dict = config if config is not None else getattr(cls, '__pydantic_config__', None)\n        config_wrapper = _config.ConfigWrapper(config_dict)\n        decorators = _decorators.DecoratorInfos.build(cls, replace_wrapped_methods=True)\n        decorators.update_from_config(config_wrapper)\n        original_doc = cls.__doc__\n        if _pydantic_dataclasses.is_stdlib_dataclass(cls):\n            original_doc = None\n            bases = (cls,)\n            if issubclass(cls, Generic):\n                generic_base = Generic[cls.__parameters__]\n                bases = bases + (generic_base,)\n            cls = types.new_class(cls.__name__, bases)\n        if frozen is not None:\n            frozen_ = frozen\n            if config_wrapper.frozen:\n                warn(\n                    f'`frozen` is set via both the `dataclass` decorator and `config` for dataclass {cls.__name__!r}.'\n                    'This is not recommended. The `frozen` specification on `dataclass` will take priority.',\n                    category=UserWarning,\n                    stacklevel=2,\n                )\n        else:\n            frozen_ = config_wrapper.frozen or False\n        cls_anns = _typing_extra.safe_get_annotations(cls)\n        for field_name in cls_anns:\n            field_value = getattr(cls, field_name, None)\n            if isinstance(field_value, FieldInfo):\n                setattr(cls, field_name, _pydantic_dataclasses.as_dataclass_field(field_value))\n        with _pydantic_dataclasses.patch_base_fields(cls):\n            cls = dataclasses.dataclass(\n                cls,\n                init=True,\n                repr=repr,\n                eq=eq,\n                order=order,\n                unsafe_hash=unsafe_hash,\n                frozen=frozen_,\n                **kwargs,\n            )\n        if config_wrapper.validate_assignment:\n            original_setattr = cls.__setattr__\n            @functools.wraps(cls.__setattr__)\n            def validated_setattr(instance: PydanticDataclass, name: str, value: Any, /) -> None:\n                if frozen_:\n                    return original_setattr(instance, name, value)\n                inst_cls = type(instance)\n                attr = getattr(inst_cls, name, None)\n                if isinstance(attr, property):\n                    attr.__set__(instance, value)\n                elif isinstance(attr, functools.cached_property):\n                    instance.__dict__.__setitem__(name, value)\n                else:\n                    inst_cls.__pydantic_validator__.validate_assignment(instance, name, value)\n            cls.__setattr__ = validated_setattr.__get__(None, cls)\n            if slots and not hasattr(cls, '__setstate__'):\n                def _dataclass_getstate(self: Any) -> list[Any]:\n                    return [getattr(self, f.name) for f in dataclasses.fields(self)]\n                def _dataclass_setstate(self: Any, state: list[Any]) -> None:\n                    for field, value in zip(dataclasses.fields(self), state):\n                        object.__setattr__(self, field.name, value)\n                cls.__getstate__ = _dataclass_getstate\n                cls.__setstate__ = _dataclass_setstate\n        cls.__is_pydantic_dataclass__ = True\n        cls.__pydantic_decorators__ = decorators\n        cls.__doc__ = original_doc\n        firstlineno = getattr(original_cls, '__firstlineno__', None)\n        cls.__module__ = original_cls.__module__\n        if sys.version_info >= (3, 13) and firstlineno is not None:\n            original_cls.__firstlineno__ = firstlineno\n            cls.__firstlineno__ = firstlineno\n        cls.__qualname__ = original_cls.__qualname__\n        cls.__pydantic_fields_complete__ = classmethod(_pydantic_fields_complete)\n        cls.__pydantic_complete__ = False\n        _pydantic_dataclasses.complete_dataclass(cls, config_wrapper, raise_errors=False)\n        return cls\n    return create_dataclass if _cls is None else create_dataclass(_cls)", "target": "def test_extra_behavior_ignore(\n    config: Union[core_schema.CoreConfig, None],\n    schema_extra_behavior_kw: dict[str, Any],\n    validate_fn_extra_kw: Union[ExtraBehavior, None],\n):\n    @dataclasses.dataclass\n    class MyModel:\n        f: str\n    v = SchemaValidator(\n        core_schema.dataclass_schema(\n            MyModel,\n            core_schema.dataclass_args_schema(\n                'MyModel', [core_schema.dataclass_field('f', core_schema.str_schema())], **schema_extra_behavior_kw\n            ),\n            ['f'],\n        ),\n        config=config,\n    )\n    m: MyModel = v.validate_python({'f': 'x', 'extra_field': 123}, extra=validate_fn_extra_kw)\n    assert m.f == 'x'\n    assert not hasattr(m, 'extra_field')\n    v.validate_assignment(m, 'f', 'y', extra=validate_fn_extra_kw)\n    assert m.f == 'y'\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_assignment(m, 'not_f', 'xyz', extra=validate_fn_extra_kw)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'no_such_attribute',\n            'loc': ('not_f',),\n            'msg': \"Object has no attribute 'not_f'\",\n            'input': 'xyz',\n            'ctx': {'attribute': 'not_f'},\n        }\n    ]\n    assert not hasattr(m, 'not_f')"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005285", "source": "def _apply_alias_generator_to_field_info(\n    alias_generator: Callable[[str], str] | AliasGenerator, field_name: str, field_info: FieldInfo\n):\n    if (\n        field_info.alias_priority is None\n        or field_info.alias_priority <= 1\n        or field_info.alias is None\n        or field_info.validation_alias is None\n        or field_info.serialization_alias is None\n    ):\n        alias, validation_alias, serialization_alias = None, None, None\n        if isinstance(alias_generator, AliasGenerator):\n            alias, validation_alias, serialization_alias = alias_generator.generate_aliases(field_name)\n        elif callable(alias_generator):\n            alias = alias_generator(field_name)\n            if not isinstance(alias, str):\n                raise TypeError(f'alias_generator {alias_generator} must return str, not {alias.__class__}')\n        if field_info.alias_priority is None or field_info.alias_priority <= 1:\n            field_info.alias_priority = 1\n        if field_info.alias_priority == 1:\n            field_info.serialization_alias = get_first_not_none(serialization_alias, alias)\n            field_info.validation_alias = get_first_not_none(validation_alias, alias)\n            field_info.alias = alias\n        if field_info.alias is None:\n            field_info.alias = alias\n        if field_info.serialization_alias is None:\n            field_info.serialization_alias = get_first_not_none(serialization_alias, alias)\n        if field_info.validation_alias is None:\n            field_info.validation_alias = get_first_not_none(validation_alias, alias)", "target": "def test_from_attributes_path_error():\n    class PropertyError:\n        @property\n        def foo(self):\n            raise RuntimeError('intentional error')\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'my_field': core_schema.model_field(\n                    validation_alias=[['foo', 'bar', 'bat'], ['foo', 3], ['spam']], schema=core_schema.int_schema()\n                )\n            },\n            from_attributes=True,\n        )\n    )\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python(PropertyError())\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'get_attribute_error',\n            'loc': ('my_field',),\n            'msg': 'Error extracting attribute: RuntimeError: intentional error',\n            'input': HasRepr(IsStr(regex='.+PropertyError object at.+')),\n            'ctx': {'error': 'RuntimeError: intentional error'},\n        }\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005286", "source": "def _del_nested_attr(obj: nn.Module, names: list[str]) -> None:\n    if len(names) == 1:\n        delattr(obj, names[0])\n    else:\n        _del_nested_attr(getattr(obj, names[0]), names[1:])", "target": "def test_gh_get_labels_raises_with_no_pages(\n        self,\n        mock_request_for_labels: Any,\n        get_last_page_num_from_header: Any,\n    ) -> None:\n        with self.assertRaises(AssertionError) as err:\n            gh_get_labels(\"foo\", \"bar\")\n        self.assertIn(\"number of pages of labels\", str(err.exception))"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005287", "source": "def takes_validated_data_argument(\n    default_factory: Callable[[], Any] | Callable[[dict[str, Any]], Any],\n) -> TypeIs[Callable[[dict[str, Any]], Any]]:\n    try:\n        sig = signature(default_factory)\n    except (ValueError, TypeError):\n        return False\n    parameters = list(sig.parameters.values())\n    return len(parameters) == 1 and can_be_positional(parameters[0]) and parameters[0].default is Parameter.empty", "target": "def test_extra_behavior_ignore(\n    config: Union[core_schema.CoreConfig, None],\n    schema_extra_behavior_kw: dict[str, Any],\n    validate_fn_extra_kw: Union[ExtraBehavior, None],\n):\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            {'f': core_schema.model_field(core_schema.str_schema())}, **schema_extra_behavior_kw\n        ),\n        config=config,\n    )\n    m, model_extra, fields_set = v.validate_python({'f': 'x', 'extra_field': 123}, extra=validate_fn_extra_kw)\n    assert m == {'f': 'x'}\n    assert model_extra is None\n    assert fields_set == {'f'}\n    v.validate_assignment(m, 'f', 'y', extra=validate_fn_extra_kw)\n    assert m['f'] == 'y'\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_assignment(m, 'not_f', 'xyz', extra=validate_fn_extra_kw)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'no_such_attribute',\n            'loc': ('not_f',),\n            'msg': \"Object has no attribute 'not_f'\",\n            'input': 'xyz',\n            'ctx': {'attribute': 'not_f'},\n        }\n    ]\n    assert 'not_f' not in m"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005288", "source": "def pydantic_general_metadata(**metadata: Any) -> BaseMetadata:\n    return _general_metadata_cls()(metadata)", "target": "def test_missing_error(pydantic_version):\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'field_a': core_schema.model_field(schema=core_schema.str_schema()),\n                'field_b': core_schema.model_field(schema=core_schema.int_schema()),\n            }\n        )\n    )\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python({'field_a': b'abc'})\n    assert (\n        str(exc_info.value)\n        ==\n        + (\n            f'\\n    For further information visit https://errors.pydantic.dev/{pydantic_version}/v/missing'\n            if os.environ.get('PYDANTIC_ERRORS_INCLUDE_URL', '1') != 'false'\n            else ''\n        )\n    )"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005289", "source": "def is_builtin_dataclass(_cls: Type[Any]) -> bool:\n    return (\n        dataclasses.is_dataclass(_cls)\n        and not hasattr(_cls, '__pydantic_model__')\n        and set(_cls.__dataclass_fields__).issuperset(set(getattr(_cls, '__annotations__', {})))\n    )", "target": "def test_dataclass():\n    schema = core_schema.dataclass_schema(\n        FooDataclass,\n        core_schema.dataclass_args_schema(\n            'FooDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 'hello', 'b': True})\n    assert dataclasses.is_dataclass(foo)\n    assert foo.a == 'hello'\n    assert foo.b is True\n    assert dataclasses.asdict(v.validate_python(FooDataclass(a='hello', b=True))) == {'a': 'hello', 'b': True}\n    with pytest.raises(ValidationError, match='Input should be an instance of FooDataclass') as exc_info:\n        v.validate_python({'a': 'hello', 'b': True}, strict=True)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'dataclass_exact_type',\n            'loc': (),\n            'msg': 'Input should be an instance of FooDataclass',\n            'input': {'a': 'hello', 'b': True},\n            'ctx': {'class_name': 'FooDataclass'},\n        }\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005290", "source": "def get_env(name: str, default: str = \"\") -> str:\n    return os.environ.get(name) or default", "target": "def test_env_bool_uses_default_when_unset(self):\n        with patch.dict(os.environ, {}, clear=True):\n            self.assertTrue(m.env_bool(\"FLAG\", default=True))\n            self.assertFalse(m.env_bool(\"FLAG\", default=False))"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005291", "source": "def _set_nested_attr(obj: nn.Module, names: list[str], value: Tensor) -> None:\n    if len(names) == 1:\n        setattr(obj, names[0], value)\n    else:\n        _set_nested_attr(getattr(obj, names[0]), names[1:], value)", "target": "def test_merged_lastfailed_content_without_overlap(self) -> None:\n        last_failed_source = {\n            \"tools/tests/test_foo.py::test_num1\": True,\n            \"tools/tests/test_foo.py::test_num2\": True,\n            \"tools/tests/test_bar.py::test_num1\": True,\n        }\n        last_failed_dest = {\n            \"tools/tests/test_car.py::test_num1\": True,\n            \"tools/tests/test_car.py::test_num2\": True,\n        }\n        last_failed_merged = {\n            \"tools/tests/test_foo.py::test_num1\": True,\n            \"tools/tests/test_foo.py::test_num2\": True,\n            \"tools/tests/test_bar.py::test_num1\": True,\n            \"tools/tests/test_car.py::test_num1\": True,\n            \"tools/tests/test_car.py::test_num2\": True,\n        }\n        merged = _merged_lastfailed_content(last_failed_source, last_failed_dest)\n        self.assertEqual(merged, last_failed_merged)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005292", "source": "def _validate_dataclass(cls: Type['DataclassT'], v: Any) -> 'DataclassT':\n    with set_validation(cls, True):\n        if isinstance(v, cls):\n            v.__pydantic_validate_values__()\n            return v\n        elif isinstance(v, (list, tuple)):\n            return cls(*v)\n        elif isinstance(v, dict):\n            return cls(**v)\n        else:\n            raise DataclassTypeError(class_name=cls.__name__)", "target": "def test_properties():\n    @dataclasses.dataclass\n    class FooProp:\n        a: str\n        b: bytes\n        @property\n        def c(self) -> str:\n            return f'{self.a} {self.b.decode()}'\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'FooProp',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bytes_schema()),\n            ],\n            computed_fields=[core_schema.computed_field('c', core_schema.str_schema())],\n        ),\n        ['a', 'b'],\n    )\n    s = SchemaSerializer(schema)\n    assert s.to_python(FooProp(a='hello', b=b'more')) == IsStrictDict(a='hello', b=b'more', c='hello more')\n    assert s.to_python(FooProp(a='hello', b=b'more'), mode='json') == IsStrictDict(a='hello', b='more', c='hello more')\n    j = s.to_json(FooProp(a='hello', b=b'more'))\n    if on_pypy:\n        assert json.loads(j) == {'a': 'hello', 'b': 'more', 'c': 'hello more'}\n    else:\n        assert j == b'{\"a\":\"hello\",\"b\":\"more\",\"c\":\"hello more\"}'\n    assert s.to_python(FooProp(a='hello', b=b'more'), exclude={'b'}) == IsStrictDict(a='hello', c='hello more')\n    assert s.to_json(FooProp(a='hello', b=b'more'), include={'a'}) == b'{\"a\":\"hello\"}'"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005293", "source": "def evaluate_forwardref(type_: ForwardRef, globalns: Any, localns: Any) -> Any:\n        return cast(Any, type_)._evaluate(globalns, localns, recursive_guard=set())", "target": "def test_wrong_function_signature() -> None:\n    def wrong_validator(value: Any) -> Any:\n        return value\n    v = SchemaValidator(core_schema.with_info_plain_validator_function(wrong_validator))\n    try:\n        v.validate_python(1)\n    except TypeError as exc:\n        assert 'takes 1 positional argument but 2 were given' in str(exc)\n    else:\n        raise AssertionError('v.validate_python(1) did not raise TypeError')"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005294", "source": "def find_validators(\n    type_: Type[Any], config: Type['BaseConfig']\n) -> Generator[AnyCallable, None, None]:\n    from pydantic.v1.dataclasses import is_builtin_dataclass, make_dataclass_validator\n    if type_ is Any or type_ is object:\n        return\n    type_type = type_.__class__\n    if type_type == ForwardRef or type_type == TypeVar:\n        return\n    if is_none_type(type_):\n        yield none_validator\n        return\n    if type_ is Pattern or type_ is re.Pattern:\n        yield pattern_validator\n        return\n    if type_ is Hashable or type_ is CollectionsHashable:\n        yield hashable_validator\n        return\n    if is_callable_type(type_):\n        yield callable_validator\n        return\n    if is_literal_type(type_):\n        yield make_literal_validator(type_)\n        return\n    if is_builtin_dataclass(type_):\n        yield from make_dataclass_validator(type_, config)\n        return\n    if type_ is Enum:\n        yield enum_validator\n        return\n    if type_ is IntEnum:\n        yield int_enum_validator\n        return\n    if is_namedtuple(type_):\n        yield tuple_validator\n        yield make_namedtuple_validator(type_, config)\n        return\n    if is_typeddict(type_):\n        yield make_typeddict_validator(type_, config)\n        return\n    class_ = get_class(type_)\n    if class_ is not None:\n        if class_ is not Any and isinstance(class_, type):\n            yield make_class_validator(class_)\n        else:\n            yield any_class_validator\n        return\n    for val_type, validators in _VALIDATORS:\n        try:\n            if issubclass(type_, val_type):\n                for v in validators:\n                    if isinstance(v, IfConfig):\n                        if v.check(config):\n                            yield v.validator\n                    else:\n                        yield v\n                return\n        except TypeError:\n            raise RuntimeError(f'error checking inheritance of {type_!r} (type: {display_as_type(type_)})')\n    if config.arbitrary_types_allowed:\n        yield make_arbitrary_type_validator(type_)\n    else:\n        if hasattr(type_, '__pydantic_core_schema__'):\n            warn(f'Mixing V1 and V2 models is not supported. `{type_.__name__}` is a V2 model.', UserWarning)\n        raise RuntimeError(f'no validator found for {type_}, see `arbitrary_types_allowed` in Config')", "target": "def test_validators_build(benchmark) -> None:\n    class Base1(BaseModel):\n        a: int\n        @field_validator('a', mode='after')\n        @classmethod\n        def val_a(cls, value: int) -> int: ...\n        @computed_field\n        def prop(self) -> int: ...\n    class Bare:\n        @computed_field\n        def prop_bare(self) -> int: ...\n    class Sub1(Base1):\n        @computed_field\n        def prop_2(self) -> int: ...\n        @computed_field\n        def prop_3(self) -> int: ...\n        @computed_field\n        def prop_4(self) -> int: ...\n    @benchmark\n    def bench() -> None:\n        class SubS(Sub1, Bare, defer_build=True):\n            @computed_field\n            def prop_5(self) -> int: ...\n            @computed_field\n            def prop_6(self) -> int: ...\n            @computed_field\n            def prop_7(self) -> int: ..."}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005295", "source": "def takes_validated_data_argument(\n    default_factory: Callable[[], Any] | Callable[[dict[str, Any]], Any],\n) -> TypeIs[Callable[[dict[str, Any]], Any]]:\n    try:\n        sig = signature(default_factory)\n    except (ValueError, TypeError):\n        return False\n    parameters = list(sig.parameters.values())\n    return len(parameters) == 1 and can_be_positional(parameters[0]) and parameters[0].default is Parameter.empty", "target": "def test_config(config: CoreConfig, input_value, expected):\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'a': core_schema.model_field(schema=core_schema.int_schema()),\n                'b': core_schema.model_field(\n                    schema=core_schema.with_default_schema(schema=core_schema.float_schema(), default=4.2)\n                ),\n            }\n        ),\n        config=config,\n    )\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            val = v.validate_python(input_value)\n            print(f'UNEXPECTED OUTPUT: {val!r}')\n    else:\n        result = v.validate_python(input_value)\n        assert result == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005296", "source": "def update_field_from_config(config_wrapper: ConfigWrapper, field_name: str, field_info: FieldInfo) -> None:\n    field_title_generator = field_info.field_title_generator or config_wrapper.field_title_generator\n    if field_title_generator is not None:\n        _apply_field_title_generator_to_field_info(field_title_generator, field_name, field_info)\n    if config_wrapper.alias_generator is not None:\n        _apply_alias_generator_to_field_info(config_wrapper.alias_generator, field_name, field_info)", "target": "def test_aliases_path_multiple(py_and_json: PyAndJson, input_value, expected):\n    v = py_and_json(\n        {\n            'type': 'model-fields',\n            'fields': {\n                'field_a': {\n                    'validation_alias': [['foo', 'bar', 'bat'], ['foo', 3], ['spam']],\n                    'type': 'model-field',\n                    'schema': {'type': 'int'},\n                }\n            },\n        },\n        {'loc_by_alias': False},\n    )\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=expected.message):\n            val = v.validate_test(input_value)\n            print(f'UNEXPECTED OUTPUT: {val!r}')\n    else:\n        output = v.validate_test(input_value)\n        assert output == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005297", "source": "def prepare_config(config: Type[BaseConfig], cls_name: str) -> None:\n    if not isinstance(config.extra, Extra):\n        try:\n            config.extra = Extra(config.extra)\n        except ValueError:\n            raise ValueError(f'\"{cls_name}\": {config.extra} is not a valid value for \"extra\"')", "target": "def test_on_field():\n    v = SchemaValidator(cs.str_schema(min_length=2, max_length=5))\n    r = plain_repr(v)\n    assert 'min_length:Some(2)' in r\n    assert 'max_length:Some(5)' in r\n    assert v.isinstance_python('test') is True\n    assert v.isinstance_python('test long') is False"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005298", "source": "def _pydantic_fields_complete(cls: type[PydanticDataclass]) -> bool:\n    return all(field_info._complete for field_info in cls.__pydantic_fields__.values())", "target": "def test_only_allow_alias(py_and_json) -> None:\n    schema = core_schema.dataclass_schema(\n        BasicDataclass,\n        core_schema.dataclass_args_schema(\n            'BasicDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), validation_alias='FieldA'),\n            ],\n        ),\n        ['a'],\n        config=core_schema.CoreConfig(validate_by_name=False, validate_by_alias=True),\n    )\n    v = py_and_json(schema)\n    assert v.validate_test({'FieldA': 'hello'}) == BasicDataclass(a='hello')\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'a': 'hello'})"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005299", "source": "def new_post_init(self: 'Dataclass', *args: Any, **kwargs: Any) -> None:\n            if config.post_init_call == 'before_validation':\n                post_init(self, *args, **kwargs)\n            if self.__class__.__pydantic_run_validation__:\n                self.__pydantic_validate_values__()\n                if hasattr(self, '__post_init_post_parse__'):\n                    self.__post_init_post_parse__(*args, **kwargs)\n            if config.post_init_call == 'after_validation':\n                post_init(self, *args, **kwargs)", "target": "def test_by_alias_and_name_config_interaction(config, runtime, expected) -> None:\n    @dataclasses.dataclass\n    class Foo:\n        my_field: str\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(\n                    name='my_field', schema=core_schema.str_schema(), serialization_alias='my_alias'\n                ),\n            ],\n        ),\n        ['my_field'],\n        config=core_schema.CoreConfig(serialize_by_alias=config or False),\n    )\n    s = SchemaSerializer(schema)\n    assert s.to_python(Foo(my_field='hello'), by_alias=runtime) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005300", "source": "def create_dataclass(cls: type[Any]) -> type[PydanticDataclass]:\n        from ._internal._utils import is_model_class\n        if is_model_class(cls):\n            raise PydanticUserError(\n                f'Cannot create a Pydantic dataclass from {cls.__name__} as it is already a Pydantic model',\n                code='dataclass-on-model',\n            )\n        original_cls = cls\n        has_dataclass_base = any(dataclasses.is_dataclass(base) for base in cls.__bases__)\n        if not has_dataclass_base and config is not None and hasattr(cls, '__pydantic_config__'):\n            warn(\n                f'`config` is set via both the `dataclass` decorator and `__pydantic_config__` for dataclass {cls.__name__}. '\n                f'The `config` specification from `dataclass` decorator will take priority.',\n                category=UserWarning,\n                stacklevel=2,\n            )\n        config_dict = config if config is not None else getattr(cls, '__pydantic_config__', None)\n        config_wrapper = _config.ConfigWrapper(config_dict)\n        decorators = _decorators.DecoratorInfos.build(cls, replace_wrapped_methods=True)\n        decorators.update_from_config(config_wrapper)\n        original_doc = cls.__doc__\n        if _pydantic_dataclasses.is_stdlib_dataclass(cls):\n            original_doc = None\n            bases = (cls,)\n            if issubclass(cls, Generic):\n                generic_base = Generic[cls.__parameters__]\n                bases = bases + (generic_base,)\n            cls = types.new_class(cls.__name__, bases)\n        if frozen is not None:\n            frozen_ = frozen\n            if config_wrapper.frozen:\n                warn(\n                    f'`frozen` is set via both the `dataclass` decorator and `config` for dataclass {cls.__name__!r}.'\n                    'This is not recommended. The `frozen` specification on `dataclass` will take priority.',\n                    category=UserWarning,\n                    stacklevel=2,\n                )\n        else:\n            frozen_ = config_wrapper.frozen or False\n        cls_anns = _typing_extra.safe_get_annotations(cls)\n        for field_name in cls_anns:\n            field_value = getattr(cls, field_name, None)\n            if isinstance(field_value, FieldInfo):\n                setattr(cls, field_name, _pydantic_dataclasses.as_dataclass_field(field_value))\n        with _pydantic_dataclasses.patch_base_fields(cls):\n            cls = dataclasses.dataclass(\n                cls,\n                init=True,\n                repr=repr,\n                eq=eq,\n                order=order,\n                unsafe_hash=unsafe_hash,\n                frozen=frozen_,\n                **kwargs,\n            )\n        if config_wrapper.validate_assignment:\n            original_setattr = cls.__setattr__\n            @functools.wraps(cls.__setattr__)\n            def validated_setattr(instance: PydanticDataclass, name: str, value: Any, /) -> None:\n                if frozen_:\n                    return original_setattr(instance, name, value)\n                inst_cls = type(instance)\n                attr = getattr(inst_cls, name, None)\n                if isinstance(attr, property):\n                    attr.__set__(instance, value)\n                elif isinstance(attr, functools.cached_property):\n                    instance.__dict__.__setitem__(name, value)\n                else:\n                    inst_cls.__pydantic_validator__.validate_assignment(instance, name, value)\n            cls.__setattr__ = validated_setattr.__get__(None, cls)\n            if slots and not hasattr(cls, '__setstate__'):\n                def _dataclass_getstate(self: Any) -> list[Any]:\n                    return [getattr(self, f.name) for f in dataclasses.fields(self)]\n                def _dataclass_setstate(self: Any, state: list[Any]) -> None:\n                    for field, value in zip(dataclasses.fields(self), state):\n                        object.__setattr__(self, field.name, value)\n                cls.__getstate__ = _dataclass_getstate\n                cls.__setstate__ = _dataclass_setstate\n        cls.__is_pydantic_dataclass__ = True\n        cls.__pydantic_decorators__ = decorators\n        cls.__doc__ = original_doc\n        firstlineno = getattr(original_cls, '__firstlineno__', None)\n        cls.__module__ = original_cls.__module__\n        if sys.version_info >= (3, 13) and firstlineno is not None:\n            original_cls.__firstlineno__ = firstlineno\n            cls.__firstlineno__ = firstlineno\n        cls.__qualname__ = original_cls.__qualname__\n        cls.__pydantic_fields_complete__ = classmethod(_pydantic_fields_complete)\n        cls.__pydantic_complete__ = False\n        _pydantic_dataclasses.complete_dataclass(cls, config_wrapper, raise_errors=False)\n        return cls", "target": "def test_extra_behavior_ignore(\n    config: Union[core_schema.CoreConfig, None],\n    schema_extra_behavior_kw: dict[str, Any],\n    validate_fn_extra_kw: Union[ExtraBehavior, None],\n):\n    @dataclasses.dataclass\n    class MyModel:\n        f: str\n    v = SchemaValidator(\n        core_schema.dataclass_schema(\n            MyModel,\n            core_schema.dataclass_args_schema(\n                'MyModel', [core_schema.dataclass_field('f', core_schema.str_schema())], **schema_extra_behavior_kw\n            ),\n            ['f'],\n        ),\n        config=config,\n    )\n    m: MyModel = v.validate_python({'f': 'x', 'extra_field': 123}, extra=validate_fn_extra_kw)\n    assert m.f == 'x'\n    assert not hasattr(m, 'extra_field')\n    v.validate_assignment(m, 'f', 'y', extra=validate_fn_extra_kw)\n    assert m.f == 'y'\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_assignment(m, 'not_f', 'xyz', extra=validate_fn_extra_kw)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'no_such_attribute',\n            'loc': ('not_f',),\n            'msg': \"Object has no attribute 'not_f'\",\n            'input': 'xyz',\n            'ctx': {'attribute': 'not_f'},\n        }\n    ]\n    assert not hasattr(m, 'not_f')"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005301", "source": "def patch_base_fields(cls: type[Any]) -> Generator[None]:\n    original_fields_list: list[tuple[DcFields, DcFields]] = []\n    for base in cls.__mro__[1:]:\n        dc_fields: dict[str, dataclasses.Field[Any]] = base.__dict__.get('__dataclass_fields__', {})\n        dc_fields_with_pydantic_field_defaults = {\n            field_name: field\n            for field_name, field in dc_fields.items()\n            if isinstance(field.default, FieldInfo)\n            and (field.default.description is not None or field.default.kw_only or field.default.repr is not True)\n        }\n        if dc_fields_with_pydantic_field_defaults:\n            original_fields_list.append((dc_fields, dc_fields_with_pydantic_field_defaults))\n            for field_name, field in dc_fields_with_pydantic_field_defaults.items():\n                default = cast(FieldInfo, field.default)\n                new_dc_field = copy.copy(field)\n                if sys.version_info >= (3, 10) and default.kw_only:\n                    new_dc_field.kw_only = True\n                if default.repr is not True:\n                    new_dc_field.repr = default.repr\n                dc_fields[field_name] = new_dc_field\n    try:\n        yield\n    finally:\n        for fields, original_fields in original_fields_list:\n            for field_name, original_field in original_fields.items():\n                fields[field_name] = original_field", "target": "def test_dataclass_subclass_subclass_revalidate():\n    v = SchemaValidator(\n        core_schema.dataclass_schema(\n            FooDataclass,\n            core_schema.dataclass_args_schema(\n                'FooDataclass',\n                [\n                    core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                    core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n                ],\n            ),\n            ['a', 'b'],\n            revalidate_instances='subclass-instances',\n            strict=True,\n        )\n    )\n    foo = FooDataclass(a='hello', b=True)\n    assert v.validate_python(foo) is foo\n    sub_foo = FooDataclassSame(a='hello', b='True')\n    sub_foo2 = v.validate_python(sub_foo)\n    assert sub_foo2 is not sub_foo\n    assert type(sub_foo2) is FooDataclass\n    assert dataclasses.asdict(sub_foo2) == dict(a='hello', b=True)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005302", "source": "def _apply_alias_generator_to_field_info(\n    alias_generator: Callable[[str], str] | AliasGenerator, field_name: str, field_info: FieldInfo\n):\n    if (\n        field_info.alias_priority is None\n        or field_info.alias_priority <= 1\n        or field_info.alias is None\n        or field_info.validation_alias is None\n        or field_info.serialization_alias is None\n    ):\n        alias, validation_alias, serialization_alias = None, None, None\n        if isinstance(alias_generator, AliasGenerator):\n            alias, validation_alias, serialization_alias = alias_generator.generate_aliases(field_name)\n        elif callable(alias_generator):\n            alias = alias_generator(field_name)\n            if not isinstance(alias, str):\n                raise TypeError(f'alias_generator {alias_generator} must return str, not {alias.__class__}')\n        if field_info.alias_priority is None or field_info.alias_priority <= 1:\n            field_info.alias_priority = 1\n        if field_info.alias_priority == 1:\n            field_info.serialization_alias = get_first_not_none(serialization_alias, alias)\n            field_info.validation_alias = get_first_not_none(validation_alias, alias)\n            field_info.alias = alias\n        if field_info.alias is None:\n            field_info.alias = alias\n        if field_info.serialization_alias is None:\n            field_info.serialization_alias = get_first_not_none(serialization_alias, alias)\n        if field_info.validation_alias is None:\n            field_info.validation_alias = get_first_not_none(validation_alias, alias)", "target": "def test_allow_extra_invalid():\n    with pytest.raises(SchemaError, match='extras_schema can only be used if extra_behavior=allow'):\n        SchemaValidator(\n            schema=core_schema.model_fields_schema(\n                fields={}, extras_schema=core_schema.int_schema(), extra_behavior='ignore'\n            )\n        )\n    with pytest.raises(SchemaError, match='extras_keys_schema can only be used if extra_behavior=allow'):\n        SchemaValidator(\n            schema=core_schema.model_fields_schema(\n                fields={}, extras_keys_schema=core_schema.int_schema(), extra_behavior='ignore'\n            )\n        )"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005303", "source": "def patch_base_fields(cls: type[Any]) -> Generator[None]:\n    original_fields_list: list[tuple[DcFields, DcFields]] = []\n    for base in cls.__mro__[1:]:\n        dc_fields: dict[str, dataclasses.Field[Any]] = base.__dict__.get('__dataclass_fields__', {})\n        dc_fields_with_pydantic_field_defaults = {\n            field_name: field\n            for field_name, field in dc_fields.items()\n            if isinstance(field.default, FieldInfo)\n            and (field.default.description is not None or field.default.kw_only or field.default.repr is not True)\n        }\n        if dc_fields_with_pydantic_field_defaults:\n            original_fields_list.append((dc_fields, dc_fields_with_pydantic_field_defaults))\n            for field_name, field in dc_fields_with_pydantic_field_defaults.items():\n                default = cast(FieldInfo, field.default)\n                new_dc_field = copy.copy(field)\n                if sys.version_info >= (3, 10) and default.kw_only:\n                    new_dc_field.kw_only = True\n                if default.repr is not True:\n                    new_dc_field.repr = default.repr\n                dc_fields[field_name] = new_dc_field\n    try:\n        yield\n    finally:\n        for fields, original_fields in original_fields_list:\n            for field_name, original_field in original_fields.items():\n                fields[field_name] = original_field", "target": "def test_dataclass_self_init_alias_field_name():\n    @dataclasses.dataclass(init=False)\n    class Foo:\n        a: str\n        b: bool\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), validation_alias='aAlias'),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), validation_alias=['bAlias', 0]),\n            ],\n        ),\n        ['a', 'b'],\n        config={'loc_by_alias': False},\n    )\n    v = SchemaValidator(schema)\n    def __init__(self, *args, **kwargs):\n        v.validate_python(ArgsKwargs(args, kwargs), self_instance=self)\n    Foo.__init__ = __init__\n    foo = Foo(aAlias=b'hello', bAlias=['True'])\n    assert dataclasses.asdict(foo) == {'a': 'hello', 'b': True}\n    with pytest.raises(ValidationError) as exc_info:\n        Foo(aAlias=b'hello', bAlias=['wrong'])\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'bool_parsing',\n            'loc': ('b',),\n            'msg': 'Input should be a valid boolean, unable to interpret input',\n            'input': 'wrong',\n        }\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005304", "source": "def _is_field_cached_property(obj: 'Dataclass', k: str) -> bool:\n        return isinstance(getattr(type(obj), k, None), cached_property)", "target": "def test_by_alias_and_name_config_interaction(config, runtime, expected) -> None:\n    @dataclasses.dataclass\n    class Foo:\n        my_field: str\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(\n                    name='my_field', schema=core_schema.str_schema(), serialization_alias='my_alias'\n                ),\n            ],\n        ),\n        ['my_field'],\n        config=core_schema.CoreConfig(serialize_by_alias=config or False),\n    )\n    s = SchemaSerializer(schema)\n    assert s.to_python(Foo(my_field='hello'), by_alias=runtime) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005305", "source": "def _update_fields_from_docstrings(cls: type[Any], fields: dict[str, FieldInfo], use_inspect: bool = False) -> None:\n    fields_docs = extract_docstrings_from_cls(cls, use_inspect=use_inspect)\n    for ann_name, field_info in fields.items():\n        if field_info.description is None and ann_name in fields_docs:\n            field_info.description = fields_docs[ann_name]", "target": "def test_validate_assignment_functions():\n    calls: list[Any] = []\n    def func_a(input_value, info):\n        calls.append(('func_a', input_value))\n        return input_value * 2\n    def func_b(input_value, info):\n        calls.append(('func_b', input_value))\n        return input_value / 2\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'field_a': core_schema.model_field(\n                    schema={\n                        'type': 'function-after',\n                        'function': {'type': 'with-info', 'function': func_a},\n                        'schema': core_schema.str_schema(),\n                    }\n                ),\n                'field_b': core_schema.model_field(\n                    schema={\n                        'type': 'function-after',\n                        'function': {'type': 'with-info', 'function': func_b},\n                        'schema': core_schema.int_schema(),\n                    }\n                ),\n            }\n        )\n    )\n    assert v.validate_python({'field_a': 'test', 'field_b': 12.0}) == (\n        {'field_a': 'testtest', 'field_b': 6},\n        None,\n        {'field_a', 'field_b'},\n    )\n    assert calls == [('func_a', 'test'), ('func_b', 12)]\n    calls.clear()\n    assert v.validate_assignment({'field_a': 'testtest', 'field_b': 6}, 'field_a', 'new-val') == (\n        {'field_a': 'new-valnew-val', 'field_b': 6},\n        None,\n        {'field_a'},\n    )\n    assert calls == [('func_a', 'new-val')]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005306", "source": "def dataclass(\n        _cls: type[_T],\n        *,\n        init: Literal[False] = False,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool | None = None,\n        config: ConfigDict | type[object] | None = None,\n        validate_on_init: bool | None = None,\n    ) -> type[PydanticDataclass]: ...", "target": "def test_dataclass_subclass_subclass_revalidate():\n    v = SchemaValidator(\n        core_schema.dataclass_schema(\n            FooDataclass,\n            core_schema.dataclass_args_schema(\n                'FooDataclass',\n                [\n                    core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                    core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n                ],\n            ),\n            ['a', 'b'],\n            revalidate_instances='subclass-instances',\n            strict=True,\n        )\n    )\n    foo = FooDataclass(a='hello', b=True)\n    assert v.validate_python(foo) is foo\n    sub_foo = FooDataclassSame(a='hello', b='True')\n    sub_foo2 = v.validate_python(sub_foo)\n    assert sub_foo2 is not sub_foo\n    assert type(sub_foo2) is FooDataclass\n    assert dataclasses.asdict(sub_foo2) == dict(a='hello', b=True)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005307", "source": "def create_dataclass(cls: type[Any]) -> type[PydanticDataclass]:\n        from ._internal._utils import is_model_class\n        if is_model_class(cls):\n            raise PydanticUserError(\n                f'Cannot create a Pydantic dataclass from {cls.__name__} as it is already a Pydantic model',\n                code='dataclass-on-model',\n            )\n        original_cls = cls\n        has_dataclass_base = any(dataclasses.is_dataclass(base) for base in cls.__bases__)\n        if not has_dataclass_base and config is not None and hasattr(cls, '__pydantic_config__'):\n            warn(\n                f'`config` is set via both the `dataclass` decorator and `__pydantic_config__` for dataclass {cls.__name__}. '\n                f'The `config` specification from `dataclass` decorator will take priority.',\n                category=UserWarning,\n                stacklevel=2,\n            )\n        config_dict = config if config is not None else getattr(cls, '__pydantic_config__', None)\n        config_wrapper = _config.ConfigWrapper(config_dict)\n        decorators = _decorators.DecoratorInfos.build(cls, replace_wrapped_methods=True)\n        decorators.update_from_config(config_wrapper)\n        original_doc = cls.__doc__\n        if _pydantic_dataclasses.is_stdlib_dataclass(cls):\n            original_doc = None\n            bases = (cls,)\n            if issubclass(cls, Generic):\n                generic_base = Generic[cls.__parameters__]\n                bases = bases + (generic_base,)\n            cls = types.new_class(cls.__name__, bases)\n        if frozen is not None:\n            frozen_ = frozen\n            if config_wrapper.frozen:\n                warn(\n                    f'`frozen` is set via both the `dataclass` decorator and `config` for dataclass {cls.__name__!r}.'\n                    'This is not recommended. The `frozen` specification on `dataclass` will take priority.',\n                    category=UserWarning,\n                    stacklevel=2,\n                )\n        else:\n            frozen_ = config_wrapper.frozen or False\n        cls_anns = _typing_extra.safe_get_annotations(cls)\n        for field_name in cls_anns:\n            field_value = getattr(cls, field_name, None)\n            if isinstance(field_value, FieldInfo):\n                setattr(cls, field_name, _pydantic_dataclasses.as_dataclass_field(field_value))\n        with _pydantic_dataclasses.patch_base_fields(cls):\n            cls = dataclasses.dataclass(\n                cls,\n                init=True,\n                repr=repr,\n                eq=eq,\n                order=order,\n                unsafe_hash=unsafe_hash,\n                frozen=frozen_,\n                **kwargs,\n            )\n        if config_wrapper.validate_assignment:\n            original_setattr = cls.__setattr__\n            @functools.wraps(cls.__setattr__)\n            def validated_setattr(instance: PydanticDataclass, name: str, value: Any, /) -> None:\n                if frozen_:\n                    return original_setattr(instance, name, value)\n                inst_cls = type(instance)\n                attr = getattr(inst_cls, name, None)\n                if isinstance(attr, property):\n                    attr.__set__(instance, value)\n                elif isinstance(attr, functools.cached_property):\n                    instance.__dict__.__setitem__(name, value)\n                else:\n                    inst_cls.__pydantic_validator__.validate_assignment(instance, name, value)\n            cls.__setattr__ = validated_setattr.__get__(None, cls)\n            if slots and not hasattr(cls, '__setstate__'):\n                def _dataclass_getstate(self: Any) -> list[Any]:\n                    return [getattr(self, f.name) for f in dataclasses.fields(self)]\n                def _dataclass_setstate(self: Any, state: list[Any]) -> None:\n                    for field, value in zip(dataclasses.fields(self), state):\n                        object.__setattr__(self, field.name, value)\n                cls.__getstate__ = _dataclass_getstate\n                cls.__setstate__ = _dataclass_setstate\n        cls.__is_pydantic_dataclass__ = True\n        cls.__pydantic_decorators__ = decorators\n        cls.__doc__ = original_doc\n        firstlineno = getattr(original_cls, '__firstlineno__', None)\n        cls.__module__ = original_cls.__module__\n        if sys.version_info >= (3, 13) and firstlineno is not None:\n            original_cls.__firstlineno__ = firstlineno\n            cls.__firstlineno__ = firstlineno\n        cls.__qualname__ = original_cls.__qualname__\n        cls.__pydantic_fields_complete__ = classmethod(_pydantic_fields_complete)\n        cls.__pydantic_complete__ = False\n        _pydantic_dataclasses.complete_dataclass(cls, config_wrapper, raise_errors=False)\n        return cls", "target": "def test_dataclass_args_init_only(py_and_json: PyAndJson, input_value, expected):\n    schema = core_schema.dataclass_args_schema(\n        'MyDataclass',\n        [\n            core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), kw_only=False),\n            core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), kw_only=False, init_only=True),\n        ],\n        collect_init_only=True,\n    )\n    v = py_and_json(schema)\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)) as exc_info:\n            v.validate_test(input_value)\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        assert v.validate_test(input_value) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005308", "source": "def dataclass(\n        *,\n        init: bool = True,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool = False,\n        config: Union[ConfigDict, Type[object], None] = None,\n        validate_on_init: Optional[bool] = None,\n        use_proxy: Optional[bool] = None,\n        kw_only: bool = ...,\n    ) -> Callable[[Type[_T]], 'DataclassClassOrWrapper']:\n        ...", "target": "def test_dataclass_post_init_args_multiple():\n    dc_args = None\n    @dataclasses.dataclass\n    class Foo:\n        a: str\n        b: dataclasses.InitVar[bool]\n        c: dataclasses.InitVar[int]\n        def __post_init__(self, *args):\n            nonlocal dc_args\n            dc_args = args\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), init_only=True),\n                core_schema.dataclass_field(name='c', schema=core_schema.int_schema(), init_only=True),\n            ],\n            collect_init_only=True,\n        ),\n        ['a', 'b'],\n        post_init=True,\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': b'hello', 'b': 'true', 'c': '42'})\n    assert dataclasses.asdict(foo) == {'a': 'hello'}\n    assert dc_args == (True, 42)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005309", "source": "def wrap(cls: Type[Any]) -> 'DataclassClassOrWrapper':\n        should_use_proxy = (\n            use_proxy\n            if use_proxy is not None\n            else (\n                is_builtin_dataclass(cls)\n                and (cls.__bases__[0] is object or set(dir(cls)) == set(dir(cls.__bases__[0])))\n            )\n        )\n        if should_use_proxy:\n            dc_cls_doc = ''\n            dc_cls = DataclassProxy(cls)\n            default_validate_on_init = False\n        else:\n            dc_cls_doc = cls.__doc__ or ''\n            if sys.version_info >= (3, 10):\n                dc_cls = dataclasses.dataclass(\n                    cls,\n                    init=init,\n                    repr=repr,\n                    eq=eq,\n                    order=order,\n                    unsafe_hash=unsafe_hash,\n                    frozen=frozen,\n                    kw_only=kw_only,\n                )\n            else:\n                dc_cls = dataclasses.dataclass(\n                    cls, init=init, repr=repr, eq=eq, order=order, unsafe_hash=unsafe_hash, frozen=frozen\n                )\n            default_validate_on_init = True\n        should_validate_on_init = default_validate_on_init if validate_on_init is None else validate_on_init\n        _add_pydantic_validation_attributes(cls, the_config, should_validate_on_init, dc_cls_doc)\n        dc_cls.__pydantic_model__.__try_update_forward_refs__(**{cls.__name__: cls})\n        return dc_cls", "target": "def test_dataclass_args(py_and_json: PyAndJson, input_value, expected):\n    schema = core_schema.dataclass_args_schema(\n        'MyDataclass',\n        [\n            core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), kw_only=False),\n            core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), kw_only=False),\n        ],\n    )\n    v = py_and_json(schema)\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)) as exc_info:\n            v.validate_test(input_value)\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        assert v.validate_test(input_value) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005310", "source": "def _apply_alias_generator_to_field_info(\n    alias_generator: Callable[[str], str] | AliasGenerator, field_name: str, field_info: FieldInfo\n):\n    if (\n        field_info.alias_priority is None\n        or field_info.alias_priority <= 1\n        or field_info.alias is None\n        or field_info.validation_alias is None\n        or field_info.serialization_alias is None\n    ):\n        alias, validation_alias, serialization_alias = None, None, None\n        if isinstance(alias_generator, AliasGenerator):\n            alias, validation_alias, serialization_alias = alias_generator.generate_aliases(field_name)\n        elif callable(alias_generator):\n            alias = alias_generator(field_name)\n            if not isinstance(alias, str):\n                raise TypeError(f'alias_generator {alias_generator} must return str, not {alias.__class__}')\n        if field_info.alias_priority is None or field_info.alias_priority <= 1:\n            field_info.alias_priority = 1\n        if field_info.alias_priority == 1:\n            field_info.serialization_alias = get_first_not_none(serialization_alias, alias)\n            field_info.validation_alias = get_first_not_none(validation_alias, alias)\n            field_info.alias = alias\n        if field_info.alias is None:\n            field_info.alias = alias\n        if field_info.serialization_alias is None:\n            field_info.serialization_alias = get_first_not_none(serialization_alias, alias)\n        if field_info.validation_alias is None:\n            field_info.validation_alias = get_first_not_none(validation_alias, alias)", "target": "def test_with_default_factory():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'x': core_schema.model_field(\n                    schema=core_schema.with_default_schema(\n                        schema=core_schema.str_schema(), default_factory=lambda: 'pikachu'\n                    )\n                )\n            }\n        )\n    )\n    assert v.validate_python({}) == ({'x': 'pikachu'}, None, set())\n    assert v.validate_python({'x': 'bulbi'}) == ({'x': 'bulbi'}, None, {'x'})"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005311", "source": "def new_post_init(self: 'Dataclass', *args: Any, **kwargs: Any) -> None:\n            if config.post_init_call == 'before_validation':\n                post_init(self, *args, **kwargs)\n            if self.__class__.__pydantic_run_validation__:\n                self.__pydantic_validate_values__()\n                if hasattr(self, '__post_init_post_parse__'):\n                    self.__post_init_post_parse__(*args, **kwargs)\n            if config.post_init_call == 'after_validation':\n                post_init(self, *args, **kwargs)", "target": "def test_dataclass():\n    schema = core_schema.dataclass_schema(\n        FooDataclass,\n        core_schema.dataclass_args_schema(\n            'FooDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 'hello', 'b': True})\n    assert dataclasses.is_dataclass(foo)\n    assert foo.a == 'hello'\n    assert foo.b is True\n    assert dataclasses.asdict(v.validate_python(FooDataclass(a='hello', b=True))) == {'a': 'hello', 'b': True}\n    with pytest.raises(ValidationError, match='Input should be an instance of FooDataclass') as exc_info:\n        v.validate_python({'a': 'hello', 'b': True}, strict=True)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'dataclass_exact_type',\n            'loc': (),\n            'msg': 'Input should be an instance of FooDataclass',\n            'input': {'a': 'hello', 'b': True},\n            'ctx': {'class_name': 'FooDataclass'},\n        }\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005312", "source": "def rebuild_dataclass_fields(\n    cls: type[PydanticDataclass],\n    *,\n    config_wrapper: ConfigWrapper,\n    ns_resolver: NsResolver,\n    typevars_map: Mapping[TypeVar, Any],\n) -> dict[str, FieldInfo]:\n    FieldInfo_ = import_cached_field_info()\n    rebuilt_fields: dict[str, FieldInfo] = {}\n    with ns_resolver.push(cls):\n        for f_name, field_info in cls.__pydantic_fields__.items():\n            if field_info._complete:\n                rebuilt_fields[f_name] = field_info\n            else:\n                existing_desc = field_info.description\n                ann = _typing_extra.eval_type(\n                    field_info._original_annotation,\n                    *ns_resolver.types_namespace,\n                )\n                ann = _generics.replace_types(ann, typevars_map)\n                new_field = FieldInfo_.from_annotated_attribute(\n                    ann,\n                    field_info._original_assignment,\n                    _source=AnnotationSource.DATACLASS,\n                )\n                new_field.description = new_field.description if new_field.description is not None else existing_desc\n                update_field_from_config(config_wrapper, f_name, new_field)\n                rebuilt_fields[f_name] = new_field\n    return rebuilt_fields", "target": "def test_on_error_raise_explicit(self, py_and_json: PyAndJson):\n        v = py_and_json(\n            {\n                'type': 'model-fields',\n                'fields': {\n                    'x': {\n                        'type': 'model-field',\n                        'schema': {'type': 'default', 'schema': {'type': 'str'}, 'on_error': 'raise'},\n                    }\n                },\n            }\n        )\n        assert v.validate_test({'x': 'foo'}) == ({'x': 'foo'}, None, {'x'})\n        with pytest.raises(ValidationError) as exc_info:\n            v.validate_test({'x': ['foo']})\n        assert exc_info.value.errors(include_url=False) == [\n            {'input': ['foo'], 'type': 'string_type', 'loc': ('x',), 'msg': 'Input should be a valid string'}\n        ]"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005313", "source": "def env_bool(\n    name: str,\n    default: bool = False,\n) -> bool:\n    val = get_env(name)\n    if not val:\n        return default\n    return str2bool(val)", "target": "def test_env_path_optional_unset_returns_none_when_env_var_is_empty(self):\n        with patch.dict(os.environ, {\"P\": \"\"}, clear=True):\n            self.assertIsNone(m.env_path_optional(\"P\"))"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005314", "source": "def _get_client() -> docker.DockerClient:\n    global _docker_client\n    if _docker_client is None:\n        _docker_client = docker.from_env()\n    return _docker_client", "target": "def test_local_image_exists_true(self):\n        mock_client = MagicMock()\n        mock_client.images.get.return_value = object()\n        ok = local_image_exists(\"repo:tag\", client=mock_client)\n        self.assertTrue(ok)"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005315", "source": "def create_branch_and_checkout(self, branch: str) -> None:\n        self._run_git(\"checkout\", \"-b\", branch)", "target": "def test_ghstack_branches_not_in_sync(self) -> None:\n        head_ref = \"gh/clee2000/1/head\"\n        self._skip_if_ref_does_not_exist(head_ref)\n        self.assertFalse(are_ghstack_branches_in_sync(self.repo, head_ref))"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005316", "source": "def _update_fields_from_docstrings(cls: type[Any], fields: dict[str, FieldInfo], use_inspect: bool = False) -> None:\n    fields_docs = extract_docstrings_from_cls(cls, use_inspect=use_inspect)\n    for ann_name, field_info in fields.items():\n        if field_info.description is None and ann_name in fields_docs:\n            field_info.description = fields_docs[ann_name]", "target": "def test_extra_behavior_forbid(\n    config: Union[core_schema.CoreConfig, None],\n    schema_extra_behavior_kw: dict[str, Any],\n    validate_fn_extra_kw: Union[ExtraBehavior, None],\n):\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            {'f': core_schema.model_field(core_schema.str_schema())}, **schema_extra_behavior_kw\n        ),\n        config=config,\n    )\n    m, model_extra, fields_set = v.validate_python({'f': 'x'}, extra=validate_fn_extra_kw)\n    assert m == {'f': 'x'}\n    assert fields_set == {'f'}\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python({'f': 'x', 'extra_field': 123}, extra=validate_fn_extra_kw)\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'extra_forbidden', 'loc': ('extra_field',), 'msg': 'Extra inputs are not permitted', 'input': 123}\n    ]\n    v.validate_assignment(m, 'f', 'y', extra=validate_fn_extra_kw)\n    assert m['f'] == 'y'\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_assignment(m, 'not_f', 'xyz', extra=validate_fn_extra_kw)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'no_such_attribute',\n            'loc': ('not_f',),\n            'msg': \"Object has no attribute 'not_f'\",\n            'input': 'xyz',\n            'ctx': {'attribute': 'not_f'},\n        }\n    ]\n    assert 'not_f' not in m"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005317", "source": "def fuzzy_list_to_dict(items: list[tuple[str, str]]) -> dict[str, list[str]]:\n    rc: dict[str, list[str]] = defaultdict(list)\n    for key, val in items:\n        rc[key].append(val)\n    return dict(rc)", "target": "def test_fails(self) -> None:\n        @retries_decorator(rc=0)\n        def foo(x: int, y: int) -> int:\n            return x + y\n        self.assertEqual(foo(\"a\", 4), 0)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005318", "source": "def _recreate_field_info(\n    field_info: FieldInfo,\n    ns_resolver: NsResolver,\n    typevars_map: Mapping[TypeVar, Any],\n    *,\n    lenient: bool,\n) -> FieldInfo:\n    FieldInfo_ = import_cached_field_info()\n    existing_desc = field_info.description\n    if lenient:\n        ann = _generics.replace_types(field_info._original_annotation, typevars_map)\n        ann, evaluated = _typing_extra.try_eval_type(\n            ann,\n            *ns_resolver.types_namespace,\n        )\n    else:\n        ann = _typing_extra.eval_type(\n            field_info._original_annotation,\n            *ns_resolver.types_namespace,\n        )\n        ann = _generics.replace_types(ann, typevars_map)\n        ann = _typing_extra.eval_type(\n            ann,\n            *ns_resolver.types_namespace,\n        )\n        evaluated = True\n    if (assign := field_info._original_assignment) is PydanticUndefined:\n        new_field = FieldInfo_.from_annotation(ann, _source=AnnotationSource.CLASS)\n    else:\n        new_field = FieldInfo_.from_annotated_attribute(ann, assign, _source=AnnotationSource.CLASS)\n        new_field._original_assignment = assign\n    new_field._original_annotation = ann\n    new_field.description = new_field.description if new_field.description is not None else existing_desc\n    if not evaluated:\n        new_field._complete = False\n    return new_field", "target": "def test_allow_extra_wrong():\n    with pytest.raises(SchemaError, match='Invalid extra_behavior: `wrong`'):\n        SchemaValidator(\n            schema=core_schema.model_fields_schema(fields={}), config=CoreConfig(extra_fields_behavior='wrong')\n        )"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005319", "source": "def dataclass(\n        *,\n        init: bool = True,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool = False,\n        config: Union[ConfigDict, Type[object], None] = None,\n        validate_on_init: Optional[bool] = None,\n        use_proxy: Optional[bool] = None,\n    ) -> Callable[[Type[_T]], 'DataclassClassOrWrapper']:\n        ...", "target": "def test_slots_mixed():\n    @dataclasses.dataclass(slots=True)\n    class Model:\n        x: int\n        y: dataclasses.InitVar[str]\n        z: ClassVar[str] = 'z-classvar'\n    @dataclasses.dataclass\n    class SubModel(Model):\n        x2: int\n        y2: dataclasses.InitVar[str]\n        z2: ClassVar[str] = 'z2-classvar'\n    schema = core_schema.dataclass_schema(\n        SubModel,\n        core_schema.dataclass_args_schema(\n            'SubModel',\n            [\n                core_schema.dataclass_field(name='x', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(name='y', init_only=True, schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='x2', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(name='y2', init_only=True, schema=core_schema.str_schema()),\n            ],\n        ),\n        ['x'],\n        slots=True,\n    )\n    v = SchemaValidator(schema)\n    dc = v.validate_python({'x': 1, 'y': 'a', 'x2': 2, 'y2': 'b'})\n    assert dc.x == 1\n    assert dc.x2 == 2\n    assert dataclasses.asdict(dc) == {'x': 1, 'x2': 2}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005320", "source": "def create_dataclass(cls: type[Any]) -> type[PydanticDataclass]:\n        from ._internal._utils import is_model_class\n        if is_model_class(cls):\n            raise PydanticUserError(\n                f'Cannot create a Pydantic dataclass from {cls.__name__} as it is already a Pydantic model',\n                code='dataclass-on-model',\n            )\n        original_cls = cls\n        has_dataclass_base = any(dataclasses.is_dataclass(base) for base in cls.__bases__)\n        if not has_dataclass_base and config is not None and hasattr(cls, '__pydantic_config__'):\n            warn(\n                f'`config` is set via both the `dataclass` decorator and `__pydantic_config__` for dataclass {cls.__name__}. '\n                f'The `config` specification from `dataclass` decorator will take priority.',\n                category=UserWarning,\n                stacklevel=2,\n            )\n        config_dict = config if config is not None else getattr(cls, '__pydantic_config__', None)\n        config_wrapper = _config.ConfigWrapper(config_dict)\n        decorators = _decorators.DecoratorInfos.build(cls, replace_wrapped_methods=True)\n        decorators.update_from_config(config_wrapper)\n        original_doc = cls.__doc__\n        if _pydantic_dataclasses.is_stdlib_dataclass(cls):\n            original_doc = None\n            bases = (cls,)\n            if issubclass(cls, Generic):\n                generic_base = Generic[cls.__parameters__]\n                bases = bases + (generic_base,)\n            cls = types.new_class(cls.__name__, bases)\n        if frozen is not None:\n            frozen_ = frozen\n            if config_wrapper.frozen:\n                warn(\n                    f'`frozen` is set via both the `dataclass` decorator and `config` for dataclass {cls.__name__!r}.'\n                    'This is not recommended. The `frozen` specification on `dataclass` will take priority.',\n                    category=UserWarning,\n                    stacklevel=2,\n                )\n        else:\n            frozen_ = config_wrapper.frozen or False\n        cls_anns = _typing_extra.safe_get_annotations(cls)\n        for field_name in cls_anns:\n            field_value = getattr(cls, field_name, None)\n            if isinstance(field_value, FieldInfo):\n                setattr(cls, field_name, _pydantic_dataclasses.as_dataclass_field(field_value))\n        with _pydantic_dataclasses.patch_base_fields(cls):\n            cls = dataclasses.dataclass(\n                cls,\n                init=True,\n                repr=repr,\n                eq=eq,\n                order=order,\n                unsafe_hash=unsafe_hash,\n                frozen=frozen_,\n                **kwargs,\n            )\n        if config_wrapper.validate_assignment:\n            original_setattr = cls.__setattr__\n            @functools.wraps(cls.__setattr__)\n            def validated_setattr(instance: PydanticDataclass, name: str, value: Any, /) -> None:\n                if frozen_:\n                    return original_setattr(instance, name, value)\n                inst_cls = type(instance)\n                attr = getattr(inst_cls, name, None)\n                if isinstance(attr, property):\n                    attr.__set__(instance, value)\n                elif isinstance(attr, functools.cached_property):\n                    instance.__dict__.__setitem__(name, value)\n                else:\n                    inst_cls.__pydantic_validator__.validate_assignment(instance, name, value)\n            cls.__setattr__ = validated_setattr.__get__(None, cls)\n            if slots and not hasattr(cls, '__setstate__'):\n                def _dataclass_getstate(self: Any) -> list[Any]:\n                    return [getattr(self, f.name) for f in dataclasses.fields(self)]\n                def _dataclass_setstate(self: Any, state: list[Any]) -> None:\n                    for field, value in zip(dataclasses.fields(self), state):\n                        object.__setattr__(self, field.name, value)\n                cls.__getstate__ = _dataclass_getstate\n                cls.__setstate__ = _dataclass_setstate\n        cls.__is_pydantic_dataclass__ = True\n        cls.__pydantic_decorators__ = decorators\n        cls.__doc__ = original_doc\n        firstlineno = getattr(original_cls, '__firstlineno__', None)\n        cls.__module__ = original_cls.__module__\n        if sys.version_info >= (3, 13) and firstlineno is not None:\n            original_cls.__firstlineno__ = firstlineno\n            cls.__firstlineno__ = firstlineno\n        cls.__qualname__ = original_cls.__qualname__\n        cls.__pydantic_fields_complete__ = classmethod(_pydantic_fields_complete)\n        cls.__pydantic_complete__ = False\n        _pydantic_dataclasses.complete_dataclass(cls, config_wrapper, raise_errors=False)\n        return cls", "target": "def test_slots_mixed():\n    @dataclasses.dataclass(slots=True)\n    class Model:\n        x: int\n        y: dataclasses.InitVar[str]\n        z: ClassVar[str] = 'z-classvar'\n    @dataclasses.dataclass\n    class SubModel(Model):\n        x2: int\n        y2: dataclasses.InitVar[str]\n        z2: ClassVar[str] = 'z2-classvar'\n    schema = core_schema.dataclass_schema(\n        SubModel,\n        core_schema.dataclass_args_schema(\n            'SubModel',\n            [\n                core_schema.dataclass_field(name='x', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(name='y', init_only=True, schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='x2', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(name='y2', init_only=True, schema=core_schema.str_schema()),\n            ],\n        ),\n        ['x', 'x2'],\n        slots=True,\n    )\n    dc = SubModel(x=1, y='a', x2=2, y2='b')\n    assert dataclasses.asdict(dc) == {'x': 1, 'x2': 2}\n    s = SchemaSerializer(schema)\n    assert s.to_python(dc) == {'x': 1, 'x2': 2}\n    assert s.to_json(dc) == b'{\"x\":1,\"x2\":2}'"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005321", "source": "def _is_field_cached_property(obj: 'Dataclass', k: str) -> bool:\n        return False", "target": "def test_dataclass_field_before_validator():\n    @dataclasses.dataclass\n    class Foo:\n        a: int\n        b: str\n        @classmethod\n        def validate_b(cls, v: bytes, info: core_schema.ValidationInfo) -> bytes:\n            assert v == b'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return b'hello world!'\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(\n                    name='b',\n                    schema=core_schema.with_info_before_validator_function(Foo.validate_b, core_schema.str_schema()),\n                ),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 1, 'b': b'hello'})\n    assert dataclasses.asdict(foo) == {'a': 1, 'b': 'hello world!'}"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005322", "source": "def secs_to_us(time_s):\n    return time_s * 1e6", "target": "def test_ghstack_branches_in_sync(self) -> None:\n        head_ref = \"gh/SS-JIA/206/head\"\n        self._skip_if_ref_does_not_exist(head_ref)\n        self.assertTrue(are_ghstack_branches_in_sync(self.repo, head_ref))"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005323", "source": "def patch_id(self, ref: Union[str, list[str]]) -> list[tuple[str, str]]:\n        is_list = isinstance(ref, list)\n        if is_list:\n            if len(ref) == 0:\n                return []\n            ref = \" \".join(ref)\n        rc = _check_output(\n            [\"sh\", \"-c\", f\"git -C {self.repo_dir} show {ref}|git patch-id --stable\"]\n        ).strip()\n        return [cast(tuple[str, str], x.split(\" \", 1)) for x in rc.split(\"\\n\")]", "target": "def test_double_asterisks(self) -> None:\n        allowed_patterns = [\n            \"aten/src/ATen/native/**LinearAlgebra*\",\n        ]\n        patterns_re = patterns_to_regex(allowed_patterns)\n        fnames = [\n            \"aten/src/ATen/native/LinearAlgebra.cpp\",\n            \"aten/src/ATen/native/cpu/LinearAlgebraKernel.cpp\",\n        ]\n        for filename in fnames:\n            self.assertTrue(patterns_re.match(filename))"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005324", "source": "def is_commit_hash(ref: str) -> bool:\n    \"True if ref is hexadecimal number, else false\"\n    try:\n        int(ref, 16)\n    except ValueError:\n        return False\n    return True", "target": "def test_ghstack_branches_in_sync(self) -> None:\n        head_ref = \"gh/SS-JIA/206/head\"\n        self._skip_if_ref_does_not_exist(head_ref)\n        self.assertTrue(are_ghstack_branches_in_sync(self.repo, head_ref))"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005325", "source": "def timedelta_isoformat(td: datetime.timedelta) -> str:\n    minutes, seconds = divmod(td.seconds, 60)\n    hours, minutes = divmod(minutes, 60)\n    return f'{\"-\" if td.days < 0 else \"\"}P{abs(td.days)}DT{hours:d}H{minutes:d}M{seconds:d}.{td.microseconds:06d}S'", "target": "def test_cycle_same():\n    def fallback_func_passthrough(obj):\n        return obj\n    f = Foobar()\n    with pytest.raises(ValueError, match=r'Circular reference detected \\(id repeated\\)'):\n        to_jsonable_python(f, fallback=fallback_func_passthrough)\n    with pytest.raises(ValueError, match=r'Circular reference detected \\(id repeated\\)'):\n        to_json(f, fallback=fallback_func_passthrough)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005326", "source": "def decimal_encoder(dec_value: Decimal) -> Union[int, float]:\n    if dec_value.as_tuple().exponent >= 0:\n        return int(dec_value)\n    else:\n        return float(dec_value)", "target": "def test_float(input_value, expected):\n    v = SchemaValidator(core_schema.float_schema())\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            v.validate_json(input_value)\n    else:\n        assert v.validate_json(input_value) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005327", "source": "def evaluate_forwardref(type_: ForwardRef, globalns: Any, localns: Any) -> Any:\n        return cast(Any, type_)._evaluate(globalns, localns, recursive_guard=set())", "target": "def test_error_details() -> None:\n    def act_on_error_details(_: ErrorDetails) -> None:\n        pass\n    v = SchemaValidator({'type': 'int'})\n    try:\n        v.validate_python('not an int')\n    except ValidationError as err:\n        for details in err.errors(include_url=False):\n            act_on_error_details(details)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005328", "source": "def is_valid_privateattr_name(name: str) -> bool:\n    return name.startswith('_') and not name.startswith('__')", "target": "def test_from_attributes_extra():\n    def another_function(x):\n        return x\n    class Foobar:\n        def __init__(self):\n            self.a = 1\n            self.b = 2\n            self._private_attribute = 4\n        @property\n        def c(self):\n            return 'ham'\n        @property\n        def _private_property(self):\n            return 'wrong'\n        @property\n        def property_error(self):\n            raise RuntimeError('xxx')\n        def bound_method(self):\n            return f'wrong {self.a}'\n        @staticmethod\n        def static_method():\n            return 'wrong'\n        function_attribute = another_function\n        @classmethod\n        def class_method(cls):\n            return 'wrong'\n    @dataclass\n    class MyDataclass:\n        a: int = 1\n        b: int = 2\n        c: str = 'ham'\n        _d: int = 4\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={'a': core_schema.model_field(schema=core_schema.int_schema())},\n            from_attributes=True,\n            extra_behavior='allow',\n        )\n    )\n    assert v.validate_python(Foobar()) == ({'a': 1}, {}, {'a'})\n    assert v.validate_python(MyDataclass()) == ({'a': 1}, {}, {'a'})\n    assert v.validate_python(Cls(a=1, b=2, c='ham')) == ({'a': 1}, {}, {'a'})\n    assert v.validate_python(Cls(a=1, b=datetime(2000, 1, 1))) == ({'a': 1}, {}, {'a'})\n    assert v.validate_python(Cls(a=1, b=datetime.now, c=lambda: 42)) == ({'a': 1}, {}, {'a'})"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005329", "source": "def dataclass(\n        *,\n        init: bool = True,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool = False,\n        config: Union[ConfigDict, Type[object], None] = None,\n        validate_on_init: Optional[bool] = None,\n        use_proxy: Optional[bool] = None,\n    ) -> Callable[[Type[_T]], 'DataclassClassOrWrapper']:\n        ...", "target": "def test_dataclass_args_init_only(py_and_json: PyAndJson, input_value, expected):\n    schema = core_schema.dataclass_args_schema(\n        'MyDataclass',\n        [\n            core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), kw_only=False),\n            core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), kw_only=False, init_only=True),\n        ],\n        collect_init_only=True,\n    )\n    v = py_and_json(schema)\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)) as exc_info:\n            v.validate_test(input_value)\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        assert v.validate_test(input_value) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005330", "source": "def is_builtin_dataclass(_cls: Type[Any]) -> bool:\n    return (\n        dataclasses.is_dataclass(_cls)\n        and not hasattr(_cls, '__pydantic_model__')\n        and set(_cls.__dataclass_fields__).issuperset(set(getattr(_cls, '__annotations__', {})))\n    )", "target": "def test_dataclass_post_init_args():\n    c_value = None\n    @dataclasses.dataclass\n    class Foo:\n        a: str\n        b: bool\n        c: dataclasses.InitVar[int]\n        def __post_init__(self, c: int):\n            nonlocal c_value\n            c_value = c\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n                core_schema.dataclass_field(name='c', schema=core_schema.int_schema(), init_only=True),\n            ],\n            collect_init_only=True,\n        ),\n        ['a', 'b'],\n        post_init=True,\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': b'hello', 'b': 'true', 'c': '42'})\n    assert foo.a == 'hello'\n    assert foo.b is True\n    assert not hasattr(foo, 'c')\n    assert c_value == 42"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005331", "source": "def update_field_from_config(config_wrapper: ConfigWrapper, field_name: str, field_info: FieldInfo) -> None:\n    field_title_generator = field_info.field_title_generator or config_wrapper.field_title_generator\n    if field_title_generator is not None:\n        _apply_field_title_generator_to_field_info(field_title_generator, field_name, field_info)\n    if config_wrapper.alias_generator is not None:\n        _apply_alias_generator_to_field_info(config_wrapper.alias_generator, field_name, field_info)", "target": "def test_allow_extra_invalid():\n    with pytest.raises(SchemaError, match='extras_schema can only be used if extra_behavior=allow'):\n        SchemaValidator(\n            schema=core_schema.model_fields_schema(\n                fields={}, extras_schema=core_schema.int_schema(), extra_behavior='ignore'\n            )\n        )\n    with pytest.raises(SchemaError, match='extras_keys_schema can only be used if extra_behavior=allow'):\n        SchemaValidator(\n            schema=core_schema.model_fields_schema(\n                fields={}, extras_keys_schema=core_schema.int_schema(), extra_behavior='ignore'\n            )\n        )"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005332", "source": "def _update_fields_from_docstrings(cls: type[Any], fields: dict[str, FieldInfo], use_inspect: bool = False) -> None:\n    fields_docs = extract_docstrings_from_cls(cls, use_inspect=use_inspect)\n    for ann_name, field_info in fields.items():\n        if field_info.description is None and ann_name in fields_docs:\n            field_info.description = fields_docs[ann_name]", "target": "def test_model_fields_deep():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'field_a': core_schema.model_field(schema=core_schema.str_schema()),\n                'field_b': core_schema.model_field(\n                    schema=core_schema.model_fields_schema(\n                        fields={\n                            'field_c': core_schema.model_field(schema=core_schema.str_schema()),\n                            'field_d': core_schema.model_field(\n                                schema=core_schema.model_fields_schema(\n                                    fields={\n                                        'field_e': core_schema.model_field(schema=core_schema.str_schema()),\n                                        'field_f': core_schema.model_field(schema=core_schema.int_schema()),\n                                    }\n                                )\n                            ),\n                        }\n                    )\n                ),\n            }\n        )\n    )\n    model_dict, model_extra, fields_set = v.validate_python(\n        {'field_a': '1', 'field_b': {'field_c': '2', 'field_d': {'field_e': '4', 'field_f': 4}}}\n    )\n    assert model_dict == {\n        'field_a': '1',\n        'field_b': (\n            {'field_c': '2', 'field_d': ({'field_e': '4', 'field_f': 4}, None, {'field_f', 'field_e'})},\n            None,\n            {'field_d', 'field_c'},\n        ),\n    }\n    assert model_extra is None\n    assert fields_set == {'field_a', 'field_b'}\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python({'field_a': '1', 'field_b': {'field_c': '2', 'field_d': {'field_e': '4', 'field_f': 'xx'}}})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('field_b', 'field_d', 'field_f'),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'xx',\n        }\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005333", "source": "def decimal_encoder(dec_value: Decimal) -> Union[int, float]:\n    if dec_value.as_tuple().exponent >= 0:\n        return int(dec_value)\n    else:\n        return float(dec_value)", "target": "def test_any(py_and_json: PyAndJson, input_value, expected):\n    v = py_and_json(core_schema.json_schema())\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)) as exc_info:\n            v.validate_test(input_value)\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        assert v.validate_test(input_value) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005334", "source": "def secs_to_ms(time_s):\n    return time_s * 1e3", "target": "def test_restores_on_exception(self):\n        start = Path.cwd()\n        with tempfile.TemporaryDirectory() as td:\n            target = Path(td) / \"wd_exc\"\n            target.mkdir()\n            with self.assertRaises(ValueError):\n                with working_directory(str(target)):\n                    self.assertEqual(Path.cwd().resolve(), target.resolve())\n                    raise ValueError(\"boom\")\n        self.assertEqual(Path.cwd().resolve(), start.resolve())"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005335", "source": "def _apply_alias_generator_to_field_info(\n    alias_generator: Callable[[str], str] | AliasGenerator, field_name: str, field_info: FieldInfo\n):\n    if (\n        field_info.alias_priority is None\n        or field_info.alias_priority <= 1\n        or field_info.alias is None\n        or field_info.validation_alias is None\n        or field_info.serialization_alias is None\n    ):\n        alias, validation_alias, serialization_alias = None, None, None\n        if isinstance(alias_generator, AliasGenerator):\n            alias, validation_alias, serialization_alias = alias_generator.generate_aliases(field_name)\n        elif callable(alias_generator):\n            alias = alias_generator(field_name)\n            if not isinstance(alias, str):\n                raise TypeError(f'alias_generator {alias_generator} must return str, not {alias.__class__}')\n        if field_info.alias_priority is None or field_info.alias_priority <= 1:\n            field_info.alias_priority = 1\n        if field_info.alias_priority == 1:\n            field_info.serialization_alias = get_first_not_none(serialization_alias, alias)\n            field_info.validation_alias = get_first_not_none(validation_alias, alias)\n            field_info.alias = alias\n        if field_info.alias is None:\n            field_info.alias = alias\n        if field_info.serialization_alias is None:\n            field_info.serialization_alias = get_first_not_none(serialization_alias, alias)\n        if field_info.validation_alias is None:\n            field_info.validation_alias = get_first_not_none(validation_alias, alias)", "target": "def test_with_default():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'field_a': core_schema.model_field(schema=core_schema.str_schema()),\n                'field_b': core_schema.model_field(\n                    schema=core_schema.with_default_schema(schema=core_schema.int_schema(), default=666)\n                ),\n            }\n        )\n    )\n    assert v.validate_python({'field_a': b'abc'}) == ({'field_a': 'abc', 'field_b': 666}, None, {'field_a'})\n    assert v.validate_python({'field_a': b'abc', 'field_b': 1}) == (\n        {'field_a': 'abc', 'field_b': 1},\n        None,\n        {'field_b', 'field_a'},\n    )"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005336", "source": "def set_validation(cls: Type['DataclassT'], value: bool) -> Generator[Type['DataclassT'], None, None]:\n    original_run_validation = cls.__pydantic_run_validation__\n    try:\n        cls.__pydantic_run_validation__ = value\n        yield cls\n    finally:\n        cls.__pydantic_run_validation__ = original_run_validation", "target": "def test_dataclass_self_init_alias():\n    @dataclasses.dataclass(init=False)\n    class Foo:\n        a: str\n        b: bool\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), validation_alias='aAlias'),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), validation_alias=['bAlias', 0]),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    def __init__(self, *args, **kwargs):\n        v.validate_python(ArgsKwargs(args, kwargs), self_instance=self)\n    Foo.__init__ = __init__\n    foo = Foo(aAlias=b'hello', bAlias=['True'])\n    assert dataclasses.is_dataclass(foo)\n    assert dataclasses.asdict(foo) == {'a': 'hello', 'b': True}\n    with pytest.raises(ValidationError) as exc_info:\n        Foo(aAlias=b'hello', bAlias=['wrong'])\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'bool_parsing',\n            'loc': ('bAlias', 0),\n            'msg': 'Input should be a valid boolean, unable to interpret input',\n            'input': 'wrong',\n        }\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005337", "source": "def _dataclass_validate_values(self: 'Dataclass') -> None:\n    if getattr(self, '__pydantic_initialised__'):\n        return\n    if getattr(self, '__pydantic_has_field_info_default__', False):\n        input_data = {\n            k: v\n            for k, v in self.__dict__.items()\n            if not (isinstance(v, FieldInfo) or _is_field_cached_property(self, k))\n        }\n    else:\n        input_data = {k: v for k, v in self.__dict__.items() if not _is_field_cached_property(self, k)}\n    d, _, validation_error = validate_model(self.__pydantic_model__, input_data, cls=self.__class__)\n    if validation_error:\n        raise validation_error\n    self.__dict__.update(d)\n    object.__setattr__(self, '__pydantic_initialised__', True)", "target": "def test_dataclass_validate_assignment():\n    schema = core_schema.dataclass_schema(\n        FooDataclass,\n        core_schema.dataclass_args_schema(\n            'FooDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), kw_only=False),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), kw_only=False),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 'hello', 'b': 'True'})\n    assert dataclasses.asdict(foo) == {'a': 'hello', 'b': True}\n    v.validate_assignment(foo, 'a', b'world')\n    assert dataclasses.asdict(foo) == {'a': 'world', 'b': True}\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_assignment(foo, 'a', 123)\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'string_type', 'loc': ('a',), 'msg': 'Input should be a valid string', 'input': 123}\n    ]\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_assignment(foo, 'c', '123')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'no_such_attribute',\n            'loc': ('c',),\n            'msg': \"Object has no attribute 'c'\",\n            'input': '123',\n            'ctx': {'attribute': 'c'},\n        }\n    ]\n    assert not hasattr(foo, 'c')\n    with pytest.raises(AttributeError, match=\"'str' object has no attribute 'a'\"):\n        v.validate_assignment('field_a', 'c', 123)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005338", "source": "def handle_extra_init(self: 'Dataclass', *args: Any, **kwargs: Any) -> None:\n        if config.extra == Extra.ignore:\n            init(self, *args, **{k: v for k, v in kwargs.items() if k in self.__dataclass_fields__})\n        elif config.extra == Extra.allow:\n            for k, v in kwargs.items():\n                self.__dict__.setdefault(k, v)\n            init(self, *args, **{k: v for k, v in kwargs.items() if k in self.__dataclass_fields__})\n        else:\n            init(self, *args, **kwargs)", "target": "def test_dataclass_field_wrap_validator1():\n    @dataclasses.dataclass\n    class Foo:\n        a: int\n        b: str\n        @classmethod\n        def validate_b(\n            cls, v: bytes, nxt: core_schema.ValidatorFunctionWrapHandler, info: core_schema.ValidationInfo\n        ) -> str:\n            assert v == b'hello'\n            v = nxt(v)\n            assert v == 'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return 'hello world!'\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(\n                    name='b',\n                    schema=core_schema.with_info_wrap_validator_function(Foo.validate_b, core_schema.str_schema()),\n                ),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 1, 'b': b'hello'})\n    assert dataclasses.asdict(foo) == {'a': 1, 'b': 'hello world!'}"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005339", "source": "def run_command(\n    cmd: str,\n    use_shell: bool = False,\n    log_cmd: bool = True,\n    cwd: Optional[str] = None,\n    env: Optional[dict] = None,\n    check: bool = True,\n) -> int:\n    if use_shell:\n        args = cmd\n        log_prefix = \"[shell]\"\n        executable = \"/bin/bash\"\n    else:\n        args = shlex.split(cmd)\n        log_prefix = \"[cmd]\"\n        executable = None\n    if log_cmd:\n        display_cmd = cmd if use_shell else \" \".join(args)\n        logger.info(\"%s %s\", log_prefix, display_cmd)\n    run_env = {**os.environ, **(env or {})}\n    proc = subprocess.run(\n        args,\n        shell=use_shell,\n        executable=executable,\n        stdout=sys.stdout,\n        stderr=sys.stderr,\n        cwd=cwd,\n        env=run_env,\n        check=False,\n    )\n    if check and proc.returncode != 0:\n        logger.error(\n            \"%s Command failed (exit %s): %s\", log_prefix, proc.returncode, cmd\n        )\n        raise subprocess.CalledProcessError(\n            proc.returncode, args if not use_shell else cmd\n        )\n    return proc.returncode", "target": "def test_ghstack_branches_in_sync(self) -> None:\n        head_ref = \"gh/SS-JIA/206/head\"\n        self._skip_if_ref_does_not_exist(head_ref)\n        self.assertTrue(are_ghstack_branches_in_sync(self.repo, head_ref))"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005340", "source": "def flatten(schema: Definition) -> FlatIntermediateDefinition:\n    result: FlatIntermediateDefinition = {}\n    _flatten(key_prefix=(), sub_schema=schema, result=result)\n    for k, v in result.items():\n        assert isinstance(k, tuple)\n        assert all(isinstance(ki, str) for ki in k)\n        assert isinstance(v, (TimerArgs, GroupedBenchmark))\n    return result", "target": "def test_pr_with_release_notes_label(\n        self, mocked_rn_labels: Any, mocked_gql: Any\n    ) -> None:\n        \"Test PR with 'release notes: nn' label\"\n        pr = GitHubPR(\"pytorch\", \"pytorch\", 71759)\n        self.assertTrue(has_required_labels(pr))"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005341", "source": "def dataclass(\n        _cls: Type[_T],\n        *,\n        init: bool = True,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool = False,\n        config: Union[ConfigDict, Type[object], None] = None,\n        validate_on_init: Optional[bool] = None,\n        use_proxy: Optional[bool] = None,\n    ) -> 'DataclassClassOrWrapper':\n        ...", "target": "def test_slots() -> None:\n    @dataclasses.dataclass(slots=True)\n    class Model:\n        x: int\n    schema = core_schema.dataclass_schema(\n        Model,\n        core_schema.dataclass_args_schema(\n            'Model', [core_schema.dataclass_field(name='x', schema=core_schema.int_schema())]\n        ),\n        ['x'],\n        slots=True,\n    )\n    val = SchemaValidator(schema)\n    m: Model\n    m = val.validate_python({'x': 123})\n    assert m == Model(x=123)\n    with pytest.raises(ValidationError):\n        val.validate_python({'x': 'abc'})\n    val.validate_assignment(m, 'x', 456)\n    assert m.x == 456\n    with pytest.raises(ValidationError):\n        val.validate_assignment(m, 'x', 'abc')"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005342", "source": "def custom_pydantic_encoder(type_encoders: Dict[Any, Callable[[Type[Any]], Any]], obj: Any) -> Any:\n    for base in obj.__class__.__mro__[:-1]:\n        try:\n            encoder = type_encoders[base]\n        except KeyError:\n            continue\n        return encoder(obj)\n    else:\n        return pydantic_encoder(obj)", "target": "def test_to_jsonable_python_fallback():\n    with pytest.raises(PydanticSerializationError, match=r'Unable to serialize unknown type: <.+\\.Foobar'):\n        to_jsonable_python(Foobar())\n    assert to_jsonable_python(Foobar(), serialize_unknown=True) == 'Foobar.__str__'\n    assert to_jsonable_python(Foobar(), serialize_unknown=True, fallback=fallback_func) == 'fallback:Foobar'\n    assert to_jsonable_python(Foobar(), fallback=fallback_func) == 'fallback:Foobar'"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005343", "source": "def _dataclass_getstate(self: Any) -> list[Any]:\n                    return [getattr(self, f.name) for f in dataclasses.fields(self)]", "target": "def test_dataclass_field_after_validator():\n    @dataclasses.dataclass\n    class Foo:\n        a: int\n        b: str\n        @classmethod\n        def validate_b(cls, v: str, info: core_schema.ValidationInfo) -> str:\n            assert v == 'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return 'hello world!'\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(\n                    name='b',\n                    schema=core_schema.with_info_after_validator_function(Foo.validate_b, core_schema.str_schema()),\n                ),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 1, 'b': b'hello'})\n    assert dataclasses.asdict(foo) == {'a': 1, 'b': 'hello world!'}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005344", "source": "def _is_field_cached_property(obj: 'Dataclass', k: str) -> bool:\n        return isinstance(getattr(type(obj), k, None), cached_property)", "target": "def test_dataclass():\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bytes_schema()),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    s = SchemaSerializer(schema)\n    assert s.to_python(Foo(a='hello', b=b'more')) == IsStrictDict(a='hello', b=b'more')\n    assert s.to_python(Foo(a='hello', b=b'more'), mode='json') == IsStrictDict(a='hello', b='more')\n    j = s.to_json(Foo(a='hello', b=b'more'))\n    if on_pypy:\n        assert json.loads(j) == {'a': 'hello', 'b': 'more'}\n    else:\n        assert j == b'{\"a\":\"hello\",\"b\":\"more\"}'\n    assert s.to_python(Foo(a='hello', b=b'more'), exclude={'b'}) == IsStrictDict(a='hello')\n    assert s.to_json(Foo(a='hello', b=b'more'), include={'a'}) == b'{\"a\":\"hello\"}'"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005345", "source": "def is_new_type(type_: Type[Any]) -> bool:\n    return isinstance(type_, test_type.__class__) and hasattr(type_, '__supertype__')", "target": "def test_ser_function_plain():\n    def f(input: Any, info: core_schema.SerializationInfo, /) -> str:\n        return str(info)\n    s = SchemaSerializer(\n        core_schema.any_schema(\n            serialization=core_schema.plain_serializer_function_ser_schema(\n                f, info_arg=True, return_schema=core_schema.str_schema()\n            )\n        )\n    )\n    assert s.to_python(123) == (\n        \"SerializationInfo(include=None, exclude=None, context=None, mode='python', by_alias=False, exclude_unset=False, \"\n        'exclude_defaults=False, exclude_none=False, exclude_computed_fields=False, round_trip=False, serialize_as_any=False)'\n    )"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005346", "source": "def convert_generics(tp: Type[Any]) -> Type[Any]:\n        return tp", "target": "def test_schema_validator_wrong() -> None:\n    try:\n        SchemaValidator({'type': 'bad'})\n    except SchemaError:\n        pass\n    else:\n        raise AssertionError('SchemaValidator did not raise SchemaError')"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005347", "source": "def pydantic_general_metadata(**metadata: Any) -> BaseMetadata:\n    return _general_metadata_cls()(metadata)", "target": "def test_alias_extra_forbid(py_and_json: PyAndJson):\n    v = py_and_json(\n        {\n            'type': 'model-fields',\n            'extra_behavior': 'forbid',\n            'fields': {'field_a': {'type': 'model-field', 'validation_alias': 'FieldA', 'schema': {'type': 'int'}}},\n        }\n    )\n    assert v.validate_test({'FieldA': 1}) == ({'field_a': 1}, None, {'field_a'})"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005348", "source": "def decimal_encoder(dec_value: Decimal) -> Union[int, float]:\n    if dec_value.as_tuple().exponent >= 0:\n        return int(dec_value)\n    else:\n        return float(dec_value)", "target": "def test_json_key_fallback():\n    x = {FoobarHash(): 1}\n    assert to_jsonable_python(x, serialize_unknown=True) == {'Foobar.__str__': 1}\n    assert to_jsonable_python(x, fallback=fallback_func) == {'fallback:FoobarHash': 1}\n    assert to_json(x, serialize_unknown=True) == b'{\"Foobar.__str__\":1}'\n    assert to_json(x, fallback=fallback_func) == b'{\"fallback:FoobarHash\":1}'"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005349", "source": "def _pydantic_fields_complete(cls: type[PydanticDataclass]) -> bool:\n    return all(field_info._complete for field_info in cls.__pydantic_fields__.values())", "target": "def test_dataclass_json():\n    schema = core_schema.dataclass_schema(\n        FooDataclass,\n        core_schema.dataclass_args_schema(\n            'FooDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    assert v.validate_json('{\"a\": \"hello\", \"b\": true}') == FooDataclass(a='hello', b=True)\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_json('[\"a\", \"b\"]')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'class_name': 'FooDataclass'},\n            'input': ['a', 'b'],\n            'loc': (),\n            'msg': 'Input should be an object',\n            'type': 'dataclass_type',\n        }\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005350", "source": "def is_typeddict_special(type_: Any) -> bool:\n    return _check_typeddict_special(type_) or _check_typeddict_special(get_origin(type_))", "target": "def test_schema_typing_error() -> None:\n    _: CoreSchema = {'type': 'wrong'}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005351", "source": "def collect_dataclass_fields(\n    cls: type[StandardDataclass],\n    *,\n    config_wrapper: ConfigWrapper,\n    ns_resolver: NsResolver | None = None,\n    typevars_map: dict[Any, Any] | None = None,\n) -> dict[str, FieldInfo]:\n    FieldInfo_ = import_cached_field_info()\n    fields: dict[str, FieldInfo] = {}\n    ns_resolver = ns_resolver or NsResolver()\n    dataclass_fields = cls.__dataclass_fields__\n    for base in reversed(cls.__mro__):\n        if not dataclasses.is_dataclass(base):\n            continue\n        with ns_resolver.push(base):\n            for ann_name, dataclass_field in dataclass_fields.items():\n                base_anns = _typing_extra.safe_get_annotations(base)\n                if ann_name not in base_anns:\n                    continue\n                globalns, localns = ns_resolver.types_namespace\n                ann_type, evaluated = _typing_extra.try_eval_type(dataclass_field.type, globalns, localns)\n                if _typing_extra.is_classvar_annotation(ann_type):\n                    continue\n                if (\n                    not dataclass_field.init\n                    and dataclass_field.default is dataclasses.MISSING\n                    and dataclass_field.default_factory is dataclasses.MISSING\n                ):\n                    continue\n                if isinstance(dataclass_field.default, FieldInfo_):\n                    if dataclass_field.default.init_var:\n                        if dataclass_field.default.init is False:\n                            raise PydanticUserError(\n                                f'Dataclass field {ann_name} has init=False and init_var=True, but these are mutually exclusive.',\n                                code='clashing-init-and-init-var',\n                            )\n                        continue\n                    field_info = FieldInfo_.from_annotated_attribute(\n                        ann_type, dataclass_field.default, _source=AnnotationSource.DATACLASS\n                    )\n                    field_info._original_assignment = dataclass_field.default\n                else:\n                    field_info = FieldInfo_.from_annotated_attribute(\n                        ann_type, dataclass_field, _source=AnnotationSource.DATACLASS\n                    )\n                    field_info._original_assignment = dataclass_field\n                if not evaluated:\n                    field_info._complete = False\n                    field_info._original_annotation = ann_type\n                fields[ann_name] = field_info\n                update_field_from_config(config_wrapper, ann_name, field_info)\n                if field_info.default is not PydanticUndefined and isinstance(\n                    getattr(cls, ann_name, field_info), FieldInfo_\n                ):\n                    setattr(cls, ann_name, field_info.default)\n    if typevars_map:\n        for field in fields.values():\n            field.apply_typevars_map(typevars_map)\n    if config_wrapper.use_attribute_docstrings:\n        _update_fields_from_docstrings(\n            cls,\n            fields,\n            use_inspect=not hasattr(cls, '__is_pydantic_dataclass__'),\n        )\n    return fields", "target": "def test_on_error_bad_default(self):\n        with pytest.raises(SchemaError, match=\"'on_error = default' requires a `default` or `default_factory`\"):\n            SchemaValidator(\n                schema=core_schema.model_fields_schema(\n                    fields={\n                        'x': core_schema.model_field(\n                            schema=core_schema.with_default_schema(schema=core_schema.str_schema(), on_error='default')\n                        )\n                    }\n                )\n            )"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005352", "source": "def run_mixtral_8x7b_int8(device: str = \"cuda\"):\n    model = GPTModelConfig(\n        \"Mixtral-8x7B-v0.1\",\n        MixtralMoE,\n        \"int8\",\n        MixtralMoEWeightOnlyInt8QuantHandler,\n        175,\n        1130,\n        133,\n    )\n    token_per_sec, memory_bandwidth, compilation_time = run_experiment(\n        model, device=device\n    )\n    return [\n        Experiment(\n            model.name,\n            \"token_per_sec\",\n            model.token_per_sec,\n            f\"{token_per_sec:.02f}\",\n            model.mode,\n            device,\n            get_arch_name(),\n            True,\n        ),\n        Experiment(\n            model.name,\n            \"memory_bandwidth(GB/s)\",\n            model.memory_bandwidth,\n            f\"{memory_bandwidth:.02f}\",\n            model.mode,\n            device,\n            get_arch_name(),\n            True,\n        ),\n        Experiment(\n            model.name,\n            \"compilation_time(s)\",\n            model.compilation_time,\n            f\"{compilation_time:.02f}\",\n            model.mode,\n            device,\n            get_arch_name(),\n            True,\n        ),\n    ]", "target": "def testNonContiguous(self):\n        x = torch.tensor([100, 200, 300])[::2]\n        assert not x.is_contiguous()\n        assert x[0] == 100\n        assert x[1] == 300\n        return x"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005353", "source": "def branches_containing_ref(\n        self, ref: str, *, include_remote: bool = True\n    ) -> list[str]:\n        rc = (\n            self._run_git(\"branch\", \"--remote\", \"--contains\", ref)\n            if include_remote\n            else self._run_git(\"branch\", \"--contains\", ref)\n        )\n        return [x.strip() for x in rc.split(\"\\n\") if x.strip()] if len(rc) > 0 else []", "target": "def test_simple(self) -> None:\n        @retries_decorator()\n        def foo(x: int, y: int) -> int:\n            return x + y\n        self.assertEqual(foo(3, 4), 7)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005354", "source": "def _pydantic_fields_complete(cls: type[PydanticDataclass]) -> bool:\n    return all(field_info._complete for field_info in cls.__pydantic_fields__.values())", "target": "def test_function_validator_wrapping_args_schema_before() -> None:\n    calls: list[Any] = []\n    def func(*args: Any) -> Any:\n        calls.append(args)\n        return args[0]\n    @dataclasses.dataclass\n    class Model:\n        number: int = 1\n    cs = core_schema.dataclass_schema(\n        Model,\n        core_schema.no_info_before_validator_function(\n            func,\n            core_schema.dataclass_args_schema(\n                'Model', [core_schema.dataclass_field('number', core_schema.int_schema())]\n            ),\n        ),\n        ['number'],\n    )\n    v = SchemaValidator(cs)\n    instance: Model = v.validate_python({'number': 1})\n    assert instance.number == 1\n    assert calls == [({'number': 1},)]\n    v.validate_assignment(instance, 'number', 2)\n    assert instance.number == 2\n    assert calls == [({'number': 1},), ({'number': 2},)]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005355", "source": "def isoformat(o: Union[datetime.date, datetime.time]) -> str:\n    return o.isoformat()", "target": "def test_enum() -> None:\n    class MyEnum(Enum):\n        a = 'a'\n        b = 'b'\n    enum_schema = core_schema.lax_or_strict_schema(\n        core_schema.no_info_after_validator_function(MyEnum, core_schema.str_schema()),\n        core_schema.is_instance_schema(MyEnum),\n    )\n    v = core_schema.json_schema(enum_schema)\n    v = SchemaValidator(v)\n    assert v.validate_python('\"a\"') == MyEnum.a\n    assert v.validate_python('\"b\"') == MyEnum.b\n    with pytest.raises(ValidationError):\n        v.validate_python('\"c\"')"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005356", "source": "def remote_url(self) -> str:\n        return self._run_git(\"remote\", \"get-url\", self.remote)", "target": "def test_is_iterable(self) -> None:\n        from collections.abc import Iterator\n        iter_ = PeekableIterator(\"\")\n        self.assertTrue(isinstance(iter_, Iterator))"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005357", "source": "def ms_to_us(time_ms):\n    return time_ms * 1e3", "target": "def test_merged_lastfailed_content_without_overlap(self) -> None:\n        last_failed_source = {\n            \"tools/tests/test_foo.py::test_num1\": True,\n            \"tools/tests/test_foo.py::test_num2\": True,\n            \"tools/tests/test_bar.py::test_num1\": True,\n        }\n        last_failed_dest = {\n            \"tools/tests/test_car.py::test_num1\": True,\n            \"tools/tests/test_car.py::test_num2\": True,\n        }\n        last_failed_merged = {\n            \"tools/tests/test_foo.py::test_num1\": True,\n            \"tools/tests/test_foo.py::test_num2\": True,\n            \"tools/tests/test_bar.py::test_num1\": True,\n            \"tools/tests/test_car.py::test_num1\": True,\n            \"tools/tests/test_car.py::test_num2\": True,\n        }\n        merged = _merged_lastfailed_content(last_failed_source, last_failed_dest)\n        self.assertEqual(merged, last_failed_merged)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005358", "source": "def is_namedtuple(type_: Type[Any]) -> bool:\n    from pydantic.v1.utils import lenient_issubclass\n    return lenient_issubclass(type_, tuple) and hasattr(type_, '_fields')", "target": "def test_error_details() -> None:\n    def act_on_error_details(_: ErrorDetails) -> None:\n        pass\n    v = SchemaValidator({'type': 'int'})\n    try:\n        v.validate_python('not an int')\n    except ValidationError as err:\n        for details in err.errors(include_url=False):\n            act_on_error_details(details)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005359", "source": "def collect_dataclass_fields(\n    cls: type[StandardDataclass],\n    *,\n    config_wrapper: ConfigWrapper,\n    ns_resolver: NsResolver | None = None,\n    typevars_map: dict[Any, Any] | None = None,\n) -> dict[str, FieldInfo]:\n    FieldInfo_ = import_cached_field_info()\n    fields: dict[str, FieldInfo] = {}\n    ns_resolver = ns_resolver or NsResolver()\n    dataclass_fields = cls.__dataclass_fields__\n    for base in reversed(cls.__mro__):\n        if not dataclasses.is_dataclass(base):\n            continue\n        with ns_resolver.push(base):\n            for ann_name, dataclass_field in dataclass_fields.items():\n                base_anns = _typing_extra.safe_get_annotations(base)\n                if ann_name not in base_anns:\n                    continue\n                globalns, localns = ns_resolver.types_namespace\n                ann_type, evaluated = _typing_extra.try_eval_type(dataclass_field.type, globalns, localns)\n                if _typing_extra.is_classvar_annotation(ann_type):\n                    continue\n                if (\n                    not dataclass_field.init\n                    and dataclass_field.default is dataclasses.MISSING\n                    and dataclass_field.default_factory is dataclasses.MISSING\n                ):\n                    continue\n                if isinstance(dataclass_field.default, FieldInfo_):\n                    if dataclass_field.default.init_var:\n                        if dataclass_field.default.init is False:\n                            raise PydanticUserError(\n                                f'Dataclass field {ann_name} has init=False and init_var=True, but these are mutually exclusive.',\n                                code='clashing-init-and-init-var',\n                            )\n                        continue\n                    field_info = FieldInfo_.from_annotated_attribute(\n                        ann_type, dataclass_field.default, _source=AnnotationSource.DATACLASS\n                    )\n                    field_info._original_assignment = dataclass_field.default\n                else:\n                    field_info = FieldInfo_.from_annotated_attribute(\n                        ann_type, dataclass_field, _source=AnnotationSource.DATACLASS\n                    )\n                    field_info._original_assignment = dataclass_field\n                if not evaluated:\n                    field_info._complete = False\n                    field_info._original_annotation = ann_type\n                fields[ann_name] = field_info\n                update_field_from_config(config_wrapper, ann_name, field_info)\n                if field_info.default is not PydanticUndefined and isinstance(\n                    getattr(cls, ann_name, field_info), FieldInfo_\n                ):\n                    setattr(cls, ann_name, field_info.default)\n    if typevars_map:\n        for field in fields.values():\n            field.apply_typevars_map(typevars_map)\n    if config_wrapper.use_attribute_docstrings:\n        _update_fields_from_docstrings(\n            cls,\n            fields,\n            use_inspect=not hasattr(cls, '__is_pydantic_dataclass__'),\n        )\n    return fields", "target": "def test_from_attributes_function(input_value, expected):\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={'a': core_schema.model_field(schema=core_schema.any_schema())}, from_attributes=True\n        )\n    )\n    model_dict, model_extra, fields_set = v.validate_python(input_value)\n    assert model_dict == expected\n    assert model_extra is None\n    assert fields_set == {'a'}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005360", "source": "def pydantic_general_metadata(**metadata: Any) -> BaseMetadata:\n    return _general_metadata_cls()(metadata)", "target": "def test_extra_behavior_allow_keys_validation() -> None:\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            {}, extra_behavior='allow', extras_keys_schema=core_schema.str_schema(max_length=3)\n        )\n    )\n    m, model_extra, fields_set = v.validate_python({'ext': 123})\n    assert m == {}\n    assert model_extra == {'ext': 123}\n    assert fields_set == {'ext'}\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python({'extra_too_long': 123})\n    assert exc_info.value.errors()[0]['type'] == 'string_too_long'"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005361", "source": "def _check_output(items: list[str], encoding: str = \"utf-8\") -> str:\n    from subprocess import CalledProcessError, check_output, STDOUT\n    try:\n        return check_output(items, stderr=STDOUT).decode(encoding)\n    except CalledProcessError as e:\n        msg = f\"Command `{' '.join(e.cmd)}` returned non-zero exit code {e.returncode}\"\n        stdout = e.stdout.decode(encoding) if e.stdout is not None else \"\"\n        stderr = e.stderr.decode(encoding) if e.stderr is not None else \"\"\n        print(f\"stdout: \\n{stdout}\")\n        print(f\"stderr: \\n{stderr}\")\n        if len(stderr) == 0:\n            msg += f\"\\n```\\n{stdout}```\"\n        else:\n            msg += f\"\\nstdout:\\n```\\n{stdout}```\\nstderr:\\n```\\n{stderr}```\"\n        raise RuntimeError(msg) from e", "target": "def test_peek(self, input_: str = \"abcdef\") -> None:\n        iter_ = PeekableIterator(input_)\n        for idx, c in enumerate(iter_):\n            if idx + 1 < len(input_):\n                self.assertEqual(iter_.peek(), input_[idx + 1])\n            else:\n                self.assertTrue(iter_.peek() is None)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005362", "source": "def with_config(config: ConfigDict | None = None, /, **kwargs: Any) -> Callable[[_TypeT], _TypeT]:\n    if config is not None and kwargs:\n        raise ValueError('Cannot specify both `config` and keyword arguments')\n    if len(kwargs) == 1 and (kwargs_conf := kwargs.get('config')) is not None:\n        warnings.warn(\n            'Passing `config` as a keyword argument is deprecated. Pass `config` as a positional argument instead',\n            category=PydanticDeprecatedSince211,\n            stacklevel=2,\n        )\n        final_config = cast(ConfigDict, kwargs_conf)\n    else:\n        final_config = config if config is not None else cast(ConfigDict, kwargs)\n    def inner(class_: _TypeT, /) -> _TypeT:\n        from ._internal._utils import is_model_class\n        if is_model_class(class_):\n            raise PydanticUserError(\n                f'Cannot use `with_config` on {class_.__name__} as it is a Pydantic model',\n                code='with-config-on-model',\n            )\n        class_.__pydantic_config__ = final_config\n        return class_\n    return inner", "target": "def test_on_config():\n    v = SchemaValidator(cs.str_schema(), config=CoreConfig(str_max_length=5))\n    assert 'max_length:Some(5)' in plain_repr(v)\n    assert v.isinstance_python('test') is True\n    assert v.isinstance_python('test long') is False"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005363", "source": "def is_path_exist(path: Union[str, Path, None]) -> bool:\n    return bool(path and get_path(path).exists())", "target": "def test_copy_dir_to_new_dir(self):\n        src = self.tmp_path / \"srcdir\"\n        (src / \"a\").mkdir(parents=True)\n        (src / \"a\" / \"f.txt\").write_text(\"content\")\n        dst = self.tmp_path / \"destdir\"\n        copy(src, dst)\n        self.assertEqual((dst / \"a\" / \"f.txt\").read_text(), \"content\")"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005364", "source": "def isoformat(o: Union[datetime.date, datetime.time]) -> str:\n    return o.isoformat()", "target": "def test_list_json():\n    s = SchemaSerializer(core_schema.list_schema(core_schema.json_schema()))\n    v = ['a', [1, 2], None]\n    assert s.to_python(v) == v\n    assert s.to_python(v, round_trip=True) == ['\"a\"', '[1,2]', 'null']\n    assert s.to_python(v, mode='json') == v\n    assert s.to_python(v, mode='json', round_trip=True) == ['\"a\"', '[1,2]', 'null']\n    assert s.to_json(v) == b'[\"a\",[1,2],null]'\n    assert s.to_json(v, round_trip=True) == b'[\"\\\\\"a\\\\\"\",\"[1,2]\",\"null\"]'"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005365", "source": "def make_class_validator(type_: Type[T]) -> Callable[[Any], Type[T]]:\n    def class_validator(v: Any) -> Type[T]:\n        if lenient_issubclass(v, type_):\n            return v\n        raise errors.SubclassError(expected_class=type_)\n    return class_validator", "target": "def test_validators_build(benchmark) -> None:\n    class Base1(BaseModel):\n        a: int\n        @field_validator('a', mode='after')\n        @classmethod\n        def val_a(cls, value: int) -> int: ...\n        @computed_field\n        def prop(self) -> int: ...\n    class Bare:\n        @computed_field\n        def prop_bare(self) -> int: ...\n    class Sub1(Base1):\n        @computed_field\n        def prop_2(self) -> int: ...\n        @computed_field\n        def prop_3(self) -> int: ...\n        @computed_field\n        def prop_4(self) -> int: ...\n    @benchmark\n    def bench() -> None:\n        class SubS(Sub1, Bare, defer_build=True):\n            @computed_field\n            def prop_5(self) -> int: ...\n            @computed_field\n            def prop_6(self) -> int: ...\n            @computed_field\n            def prop_7(self) -> int: ..."}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005366", "source": "def load_weights(mod: nn.Module, names: list[str], params: tuple[Tensor, ...]) -> None:\n    for name, p in zip(names, params):\n        _set_nested_attr(mod, name.split(\".\"), p)", "target": "def test_simple(self) -> None:\n        @retries_decorator()\n        def foo(x: int, y: int) -> int:\n            return x + y\n        self.assertEqual(foo(3, 4), 7)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005367", "source": "def dataclass(\n        *,\n        init: Literal[False] = False,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool = False,\n        config: ConfigDict | type[object] | None = None,\n        validate_on_init: bool | None = None,\n        kw_only: bool = ...,\n        slots: bool = ...,\n    ) -> Callable[[type[_T]], type[PydanticDataclass]]:\n        ...", "target": "def test_aliases(py_and_json: PyAndJson):\n    schema = core_schema.dataclass_args_schema(\n        'MyDataclass',\n        [\n            core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), validation_alias='Apple'),\n            core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), validation_alias=['Banana', 1]),\n            core_schema.dataclass_field(\n                name='c', schema=core_schema.int_schema(), validation_alias=['Carrot', 'v'], init_only=True\n            ),\n        ],\n        collect_init_only=True,\n    )\n    v = py_and_json(schema)\n    assert v.validate_test({'Apple': 'a', 'Banana': ['x', 'false'], 'Carrot': {'v': '42'}}) == (\n        {'a': 'a', 'b': False},\n        (42,),\n    )"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005368", "source": "def dataclass(\n        *,\n        init: Literal[False] = False,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool | None = None,\n        config: ConfigDict | type[object] | None = None,\n        validate_on_init: bool | None = None,\n    ) -> Callable[[type[_T]], type[PydanticDataclass]]:\n        ...", "target": "def test_serialization_alias():\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bytes_schema(), serialization_alias='BAR'),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    s = SchemaSerializer(schema)\n    assert s.to_python(Foo(a='hello', b=b'more'), by_alias=True) == IsStrictDict(a='hello', BAR=b'more')\n    assert s.to_python(Foo(a='hello', b=b'more'), mode='json', by_alias=True) == IsStrictDict(a='hello', BAR='more')\n    j = s.to_json(Foo(a='hello', b=b'more'), by_alias=True)\n    if on_pypy:\n        assert json.loads(j) == {'a': 'hello', 'BAR': 'more'}\n    else:\n        assert j == b'{\"a\":\"hello\",\"BAR\":\"more\"}'"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005369", "source": "def _check_protected_namespaces(\n    protected_namespaces: tuple[str | Pattern[str], ...],\n    ann_name: str,\n    bases: tuple[type[Any], ...],\n    cls_name: str,\n) -> None:\n    BaseModel = import_cached_base_model()\n    for protected_namespace in protected_namespaces:\n        ns_violation = False\n        if isinstance(protected_namespace, Pattern):\n            ns_violation = protected_namespace.match(ann_name) is not None\n        elif isinstance(protected_namespace, str):\n            ns_violation = ann_name.startswith(protected_namespace)\n        if ns_violation:\n            for b in bases:\n                if hasattr(b, ann_name):\n                    if not (issubclass(b, BaseModel) and ann_name in getattr(b, '__pydantic_fields__', {})):\n                        raise ValueError(\n                            f'Field {ann_name!r} conflicts with member {getattr(b, ann_name)}'\n                            f' of protected namespace {protected_namespace!r}.'\n                        )\n            else:\n                valid_namespaces: list[str] = []\n                for pn in protected_namespaces:\n                    if isinstance(pn, Pattern):\n                        if not pn.match(ann_name):\n                            valid_namespaces.append(f're.compile({pn.pattern!r})')\n                    else:\n                        if not ann_name.startswith(pn):\n                            valid_namespaces.append(f\"'{pn}'\")\n                valid_namespaces_str = f'({\", \".join(valid_namespaces)}{\",)\" if len(valid_namespaces) == 1 else \")\"}'\n                warnings.warn(\n                    f'Field {ann_name!r} in {cls_name!r} conflicts with protected namespace {protected_namespace!r}.\\n\\n'\n                    f\"You may be able to solve this by setting the 'protected_namespaces' configuration to {valid_namespaces_str}.\",\n                    UserWarning,\n                    stacklevel=5,\n                )", "target": "def test_from_attributes_type_error():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'a': core_schema.model_field(schema=core_schema.int_schema()),\n                'b': core_schema.model_field(schema=core_schema.int_schema()),\n                'c': core_schema.model_field(schema=core_schema.str_schema()),\n            },\n            from_attributes=True,\n            model_name='MyModel',\n        )\n    )\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python('123')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'model_attributes_type',\n            'loc': (),\n            'msg': 'Input should be a valid dictionary or object to extract fields from',\n            'input': '123',\n        }\n    ]\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_json('123')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'model_type',\n            'loc': (),\n            'msg': 'Input should be an object',\n            'input': 123,\n            'ctx': {'class_name': 'MyModel'},\n        }\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005370", "source": "def wrap(cls: Type[Any]) -> 'DataclassClassOrWrapper':\n        should_use_proxy = (\n            use_proxy\n            if use_proxy is not None\n            else (\n                is_builtin_dataclass(cls)\n                and (cls.__bases__[0] is object or set(dir(cls)) == set(dir(cls.__bases__[0])))\n            )\n        )\n        if should_use_proxy:\n            dc_cls_doc = ''\n            dc_cls = DataclassProxy(cls)\n            default_validate_on_init = False\n        else:\n            dc_cls_doc = cls.__doc__ or ''\n            if sys.version_info >= (3, 10):\n                dc_cls = dataclasses.dataclass(\n                    cls,\n                    init=init,\n                    repr=repr,\n                    eq=eq,\n                    order=order,\n                    unsafe_hash=unsafe_hash,\n                    frozen=frozen,\n                    kw_only=kw_only,\n                )\n            else:\n                dc_cls = dataclasses.dataclass(\n                    cls, init=init, repr=repr, eq=eq, order=order, unsafe_hash=unsafe_hash, frozen=frozen\n                )\n            default_validate_on_init = True\n        should_validate_on_init = default_validate_on_init if validate_on_init is None else validate_on_init\n        _add_pydantic_validation_attributes(cls, the_config, should_validate_on_init, dc_cls_doc)\n        dc_cls.__pydantic_model__.__try_update_forward_refs__(**{cls.__name__: cls})\n        return dc_cls", "target": "def test_validate_assignment_function():\n    @dataclasses.dataclass\n    class MyDataclass:\n        field_a: str\n        field_b: int\n        field_c: int\n    calls = []\n    def func(x, info):\n        calls.append(str(info))\n        return x * 2\n    v = SchemaValidator(\n        core_schema.dataclass_schema(\n            MyDataclass,\n            core_schema.dataclass_args_schema(\n                'MyDataclass',\n                [\n                    core_schema.dataclass_field('field_a', core_schema.str_schema()),\n                    core_schema.dataclass_field(\n                        'field_b',\n                        core_schema.with_info_after_validator_function(func, core_schema.int_schema()),\n                    ),\n                    core_schema.dataclass_field('field_c', core_schema.int_schema()),\n                ],\n            ),\n            ['field_a', 'field_b', 'field_c'],\n        )\n    )\n    m = v.validate_python({'field_a': 'x', 'field_b': 123, 'field_c': 456})\n    assert m.field_a == 'x'\n    assert m.field_b == 246\n    assert m.field_c == 456\n    assert calls == [\"ValidationInfo(config=None, context=None, data={'field_a': 'x'}, field_name='field_b')\"]\n    v.validate_assignment(m, 'field_b', '111')\n    assert m.field_b == 222\n    assert calls == [\n        \"ValidationInfo(config=None, context=None, data={'field_a': 'x'}, field_name='field_b')\",\n        \"ValidationInfo(config=None, context=None, data={'field_a': 'x', 'field_c': 456}, field_name='field_b')\",\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005371", "source": "def _general_metadata_cls() -> type[BaseMetadata]:\n    from annotated_types import BaseMetadata\n    class _PydanticGeneralMetadata(PydanticMetadata, BaseMetadata):\n        def __init__(self, metadata: Any):\n            self.__dict__ = metadata\n    return _PydanticGeneralMetadata", "target": "def test_validate_assignment_ignore_extra():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(fields={'field_a': core_schema.model_field(schema=core_schema.str_schema())})\n    )\n    assert v.validate_python({'field_a': 'test'}) == ({'field_a': 'test'}, None, {'field_a'})\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_assignment({'field_a': 'test'}, 'other_field', 456)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'no_such_attribute',\n            'loc': ('other_field',),\n            'msg': \"Object has no attribute 'other_field'\",\n            'input': 456,\n            'ctx': {'attribute': 'other_field'},\n        }\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005372", "source": "def pydantic_encoder(obj: Any) -> Any:\n    warnings.warn(\n        '`pydantic_encoder` is deprecated, use `pydantic_core.to_jsonable_python` instead.',\n        category=PydanticDeprecatedSince20,\n        stacklevel=2,\n    )\n    from dataclasses import asdict, is_dataclass\n    BaseModel = import_cached_base_model()\n    if isinstance(obj, BaseModel):\n        return obj.model_dump()\n    elif is_dataclass(obj):\n        return asdict(obj)\n    for base in obj.__class__.__mro__[:-1]:\n        try:\n            encoder = ENCODERS_BY_TYPE[base]\n        except KeyError:\n            continue\n        return encoder(obj)\n    else:\n        raise TypeError(f\"Object of type '{obj.__class__.__name__}' is not JSON serializable\")", "target": "def test_json_bytes_hex_round_trip():\n    data = b'hello'\n    encoded = b'\"68656c6c6f\"'\n    assert to_json(data, bytes_mode='hex') == encoded\n    v = SchemaValidator(core_schema.bytes_schema(), config=CoreConfig(val_json_bytes='hex'))\n    assert v.validate_json(encoded) == data\n    assert to_json({'key': data}, bytes_mode='hex') == b'{\"key\":\"68656c6c6f\"}'\n    v = SchemaValidator(\n        core_schema.dict_schema(keys_schema=core_schema.str_schema(), values_schema=core_schema.bytes_schema()),\n        config=CoreConfig(val_json_bytes='hex'),\n    )\n    assert v.validate_json('{\"key\":\"68656c6c6f\"}') == {'key': data}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005373", "source": "def decimal_encoder(dec_value: Decimal) -> Union[int, float]:\n    exponent = dec_value.as_tuple().exponent\n    if isinstance(exponent, int) and exponent >= 0:\n        return int(dec_value)\n    else:\n        return float(dec_value)", "target": "def test_list_int(py_and_json: PyAndJson, input_value, expected):\n    v = py_and_json(core_schema.json_schema(core_schema.list_schema(core_schema.int_schema())))\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=expected.message) as exc_info:\n            v.validate_test(input_value)\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        assert v.validate_test(input_value) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005374", "source": "def _check_protected_namespaces(\n    protected_namespaces: tuple[str | Pattern[str], ...],\n    ann_name: str,\n    bases: tuple[type[Any], ...],\n    cls_name: str,\n) -> None:\n    BaseModel = import_cached_base_model()\n    for protected_namespace in protected_namespaces:\n        ns_violation = False\n        if isinstance(protected_namespace, Pattern):\n            ns_violation = protected_namespace.match(ann_name) is not None\n        elif isinstance(protected_namespace, str):\n            ns_violation = ann_name.startswith(protected_namespace)\n        if ns_violation:\n            for b in bases:\n                if hasattr(b, ann_name):\n                    if not (issubclass(b, BaseModel) and ann_name in getattr(b, '__pydantic_fields__', {})):\n                        raise ValueError(\n                            f'Field {ann_name!r} conflicts with member {getattr(b, ann_name)}'\n                            f' of protected namespace {protected_namespace!r}.'\n                        )\n            else:\n                valid_namespaces: list[str] = []\n                for pn in protected_namespaces:\n                    if isinstance(pn, Pattern):\n                        if not pn.match(ann_name):\n                            valid_namespaces.append(f're.compile({pn.pattern!r})')\n                    else:\n                        if not ann_name.startswith(pn):\n                            valid_namespaces.append(f\"'{pn}'\")\n                valid_namespaces_str = f'({\", \".join(valid_namespaces)}{\",)\" if len(valid_namespaces) == 1 else \")\"}'\n                warnings.warn(\n                    f'Field {ann_name!r} in {cls_name!r} conflicts with protected namespace {protected_namespace!r}.\\n\\n'\n                    f\"You may be able to solve this by setting the 'protected_namespaces' configuration to {valid_namespaces_str}.\",\n                    UserWarning,\n                    stacklevel=5,\n                )", "target": "def test_alias_path(py_and_json: PyAndJson, input_value, expected):\n    v = py_and_json(\n        {\n            'type': 'model-fields',\n            'fields': {\n                'field_a': {'validation_alias': ['foo', 'bar'], 'type': 'model-field', 'schema': {'type': 'int'}}\n            },\n        }\n    )\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=expected.message):\n            v.validate_test(input_value)\n    else:\n        output = v.validate_test(input_value)\n        assert output == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005375", "source": "def core_config(self, title: str | None) -> core_schema.CoreConfig:\n        config = self.config_dict\n        if config.get('schema_generator') is not None:\n            warnings.warn(\n                'The `schema_generator` setting has been deprecated since v2.10. This setting no longer has any effect.',\n                PydanticDeprecatedSince210,\n                stacklevel=2,\n            )\n        if (populate_by_name := config.get('populate_by_name')) is not None:\n            if config.get('validate_by_name') is None:\n                config['validate_by_alias'] = True\n                config['validate_by_name'] = populate_by_name\n        if config.get('validate_by_alias') is False and config.get('validate_by_name') is None:\n            config['validate_by_name'] = True\n        if (not config.get('validate_by_alias', True)) and (not config.get('validate_by_name', False)):\n            raise PydanticUserError(\n                'At least one of `validate_by_alias` or `validate_by_name` must be set to True.',\n                code='validate-by-alias-and-name-false',\n            )\n        return core_schema.CoreConfig(\n            **{\n                k: v\n                for k, v in (\n                    ('title', config.get('title') or title or None),\n                    ('extra_fields_behavior', config.get('extra')),\n                    ('allow_inf_nan', config.get('allow_inf_nan')),\n                    ('str_strip_whitespace', config.get('str_strip_whitespace')),\n                    ('str_to_lower', config.get('str_to_lower')),\n                    ('str_to_upper', config.get('str_to_upper')),\n                    ('strict', config.get('strict')),\n                    ('ser_json_timedelta', config.get('ser_json_timedelta')),\n                    ('ser_json_temporal', config.get('ser_json_temporal')),\n                    ('val_temporal_unit', config.get('val_temporal_unit')),\n                    ('ser_json_bytes', config.get('ser_json_bytes')),\n                    ('val_json_bytes', config.get('val_json_bytes')),\n                    ('ser_json_inf_nan', config.get('ser_json_inf_nan')),\n                    ('from_attributes', config.get('from_attributes')),\n                    ('loc_by_alias', config.get('loc_by_alias')),\n                    ('revalidate_instances', config.get('revalidate_instances')),\n                    ('validate_default', config.get('validate_default')),\n                    ('str_max_length', config.get('str_max_length')),\n                    ('str_min_length', config.get('str_min_length')),\n                    ('hide_input_in_errors', config.get('hide_input_in_errors')),\n                    ('coerce_numbers_to_str', config.get('coerce_numbers_to_str')),\n                    ('regex_engine', config.get('regex_engine')),\n                    ('validation_error_cause', config.get('validation_error_cause')),\n                    ('cache_strings', config.get('cache_strings')),\n                    ('validate_by_alias', config.get('validate_by_alias')),\n                    ('validate_by_name', config.get('validate_by_name')),\n                    ('serialize_by_alias', config.get('serialize_by_alias')),\n                    ('url_preserve_empty_path', config.get('url_preserve_empty_path')),\n                )\n                if v is not None\n            }\n        )", "target": "def test_cache_strings():\n    v = SchemaValidator(cs.str_schema())\n    assert 'cache_strings=True' in plain_repr(v)\n    v = SchemaValidator(cs.str_schema(), config=CoreConfig(cache_strings=True))\n    assert 'cache_strings=True' in plain_repr(v)\n    v = SchemaValidator(cs.str_schema(), config=CoreConfig(cache_strings=False))\n    assert 'cache_strings=False' in plain_repr(v)\n    v = SchemaValidator(cs.str_schema(), config=CoreConfig(cache_strings='keys'))\n    assert \"cache_strings='keys'\" in plain_repr(v)"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005376", "source": "def check_for_functorch():\n    try:\n        import functorch\n        return True\n    except ImportError:\n        return False", "target": "def test_compute_diff(self) -> None:\n        diff = self.repo.diff(\"HEAD\")\n        sha = _shasum(diff)\n        self.assertEqual(len(sha), 64)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005377", "source": "def dataclass(\n        *,\n        init: bool = True,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool = False,\n        config: Union[ConfigDict, Type[object], None] = None,\n        validate_on_init: Optional[bool] = None,\n        use_proxy: Optional[bool] = None,\n    ) -> Callable[[Type[_T]], 'DataclassClassOrWrapper']:\n        ...", "target": "def test_dataclass_field_after_validator():\n    @dataclasses.dataclass\n    class Foo:\n        a: int\n        b: str\n        @classmethod\n        def validate_b(cls, v: str, info: core_schema.ValidationInfo) -> str:\n            assert v == 'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return 'hello world!'\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(\n                    name='b',\n                    schema=core_schema.with_info_after_validator_function(Foo.validate_b, core_schema.str_schema()),\n                ),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 1, 'b': b'hello'})\n    assert dataclasses.asdict(foo) == {'a': 1, 'b': 'hello world!'}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005378", "source": "def validated_setattr(instance: PydanticDataclass, name: str, value: Any, /) -> None:\n                if frozen_:\n                    return original_setattr(instance, name, value)\n                inst_cls = type(instance)\n                attr = getattr(inst_cls, name, None)\n                if isinstance(attr, property):\n                    attr.__set__(instance, value)\n                elif isinstance(attr, functools.cached_property):\n                    instance.__dict__.__setitem__(name, value)\n                else:\n                    inst_cls.__pydantic_validator__.validate_assignment(instance, name, value)", "target": "def test_dataclass_field_plain_validator():\n    @dataclasses.dataclass\n    class Foo:\n        a: int\n        b: str\n        @classmethod\n        def validate_b(cls, v: bytes, info: core_schema.ValidationInfo) -> str:\n            assert v == b'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return 'hello world!'\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(\n                    name='b', schema=core_schema.with_info_plain_validator_function(Foo.validate_b)\n                ),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 1, 'b': b'hello'})\n    assert dataclasses.asdict(foo) == {'a': 1, 'b': 'hello world!'}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005379", "source": "def custom_pydantic_encoder(type_encoders: dict[Any, Callable[[type[Any]], Any]], obj: Any) -> Any:\n    warnings.warn(\n        '`custom_pydantic_encoder` is deprecated, use `BaseModel.model_dump` instead.',\n        category=PydanticDeprecatedSince20,\n        stacklevel=2,\n    )\n    for base in obj.__class__.__mro__[:-1]:\n        try:\n            encoder = type_encoders[base]\n        except KeyError:\n            continue\n        return encoder(obj)\n    else:\n        return pydantic_encoder(obj)", "target": "def test_null():\n    assert SchemaValidator(core_schema.none_schema()).validate_json('null') is None"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005380", "source": "def str2bool(value: Optional[str]) -> bool:\n    if not value:\n        return False\n    if not isinstance(value, str):\n        raise ValueError(\n            f\"Expected a string value for boolean conversion, got {type(value)}\"\n        )\n    value = value.strip().lower()\n    true_value_set = {\"1\", \"true\", \"t\", \"yes\", \"y\", \"on\", \"enable\", \"enabled\", \"found\"}\n    false_value_set = {\"0\", \"false\", \"f\", \"no\", \"n\", \"off\", \"disable\"}\n    if value in true_value_set:\n        return True\n    if value in false_value_set:\n        return False\n    raise ValueError(f\"Invalid string value for boolean conversion: {value}\")", "target": "def test_changes_and_restores(self):\n        start = Path.cwd()\n        with tempfile.TemporaryDirectory() as td:\n            target = Path(td) / \"wd\"\n            target.mkdir()\n            with working_directory(str(target)):\n                self.assertEqual(Path.cwd().resolve(), target.resolve())\n        self.assertEqual(Path.cwd(), start)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005381", "source": "def create_dataclass(cls: type[Any]) -> type[PydanticDataclass]:\n        from ._internal._utils import is_model_class\n        if is_model_class(cls):\n            raise PydanticUserError(\n                f'Cannot create a Pydantic dataclass from {cls.__name__} as it is already a Pydantic model',\n                code='dataclass-on-model',\n            )\n        original_cls = cls\n        has_dataclass_base = any(dataclasses.is_dataclass(base) for base in cls.__bases__)\n        if not has_dataclass_base and config is not None and hasattr(cls, '__pydantic_config__'):\n            warn(\n                f'`config` is set via both the `dataclass` decorator and `__pydantic_config__` for dataclass {cls.__name__}. '\n                f'The `config` specification from `dataclass` decorator will take priority.',\n                category=UserWarning,\n                stacklevel=2,\n            )\n        config_dict = config if config is not None else getattr(cls, '__pydantic_config__', None)\n        config_wrapper = _config.ConfigWrapper(config_dict)\n        decorators = _decorators.DecoratorInfos.build(cls, replace_wrapped_methods=True)\n        decorators.update_from_config(config_wrapper)\n        original_doc = cls.__doc__\n        if _pydantic_dataclasses.is_stdlib_dataclass(cls):\n            original_doc = None\n            bases = (cls,)\n            if issubclass(cls, Generic):\n                generic_base = Generic[cls.__parameters__]\n                bases = bases + (generic_base,)\n            cls = types.new_class(cls.__name__, bases)\n        if frozen is not None:\n            frozen_ = frozen\n            if config_wrapper.frozen:\n                warn(\n                    f'`frozen` is set via both the `dataclass` decorator and `config` for dataclass {cls.__name__!r}.'\n                    'This is not recommended. The `frozen` specification on `dataclass` will take priority.',\n                    category=UserWarning,\n                    stacklevel=2,\n                )\n        else:\n            frozen_ = config_wrapper.frozen or False\n        cls_anns = _typing_extra.safe_get_annotations(cls)\n        for field_name in cls_anns:\n            field_value = getattr(cls, field_name, None)\n            if isinstance(field_value, FieldInfo):\n                setattr(cls, field_name, _pydantic_dataclasses.as_dataclass_field(field_value))\n        with _pydantic_dataclasses.patch_base_fields(cls):\n            cls = dataclasses.dataclass(\n                cls,\n                init=True,\n                repr=repr,\n                eq=eq,\n                order=order,\n                unsafe_hash=unsafe_hash,\n                frozen=frozen_,\n                **kwargs,\n            )\n        if config_wrapper.validate_assignment:\n            original_setattr = cls.__setattr__\n            @functools.wraps(cls.__setattr__)\n            def validated_setattr(instance: PydanticDataclass, name: str, value: Any, /) -> None:\n                if frozen_:\n                    return original_setattr(instance, name, value)\n                inst_cls = type(instance)\n                attr = getattr(inst_cls, name, None)\n                if isinstance(attr, property):\n                    attr.__set__(instance, value)\n                elif isinstance(attr, functools.cached_property):\n                    instance.__dict__.__setitem__(name, value)\n                else:\n                    inst_cls.__pydantic_validator__.validate_assignment(instance, name, value)\n            cls.__setattr__ = validated_setattr.__get__(None, cls)\n            if slots and not hasattr(cls, '__setstate__'):\n                def _dataclass_getstate(self: Any) -> list[Any]:\n                    return [getattr(self, f.name) for f in dataclasses.fields(self)]\n                def _dataclass_setstate(self: Any, state: list[Any]) -> None:\n                    for field, value in zip(dataclasses.fields(self), state):\n                        object.__setattr__(self, field.name, value)\n                cls.__getstate__ = _dataclass_getstate\n                cls.__setstate__ = _dataclass_setstate\n        cls.__is_pydantic_dataclass__ = True\n        cls.__pydantic_decorators__ = decorators\n        cls.__doc__ = original_doc\n        firstlineno = getattr(original_cls, '__firstlineno__', None)\n        cls.__module__ = original_cls.__module__\n        if sys.version_info >= (3, 13) and firstlineno is not None:\n            original_cls.__firstlineno__ = firstlineno\n            cls.__firstlineno__ = firstlineno\n        cls.__qualname__ = original_cls.__qualname__\n        cls.__pydantic_fields_complete__ = classmethod(_pydantic_fields_complete)\n        cls.__pydantic_complete__ = False\n        _pydantic_dataclasses.complete_dataclass(cls, config_wrapper, raise_errors=False)\n        return cls", "target": "def test_dataclass_subclass_subclass_revalidate():\n    v = SchemaValidator(\n        core_schema.dataclass_schema(\n            FooDataclass,\n            core_schema.dataclass_args_schema(\n                'FooDataclass',\n                [\n                    core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                    core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n                ],\n            ),\n            ['a', 'b'],\n            revalidate_instances='subclass-instances',\n            strict=True,\n        )\n    )\n    foo = FooDataclass(a='hello', b=True)\n    assert v.validate_python(foo) is foo\n    sub_foo = FooDataclassSame(a='hello', b='True')\n    sub_foo2 = v.validate_python(sub_foo)\n    assert sub_foo2 is not sub_foo\n    assert type(sub_foo2) is FooDataclass\n    assert dataclasses.asdict(sub_foo2) == dict(a='hello', b=True)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005382", "source": "def custom_pydantic_encoder(type_encoders: Dict[Any, Callable[[Type[Any]], Any]], obj: Any) -> Any:\n    for base in obj.__class__.__mro__[:-1]:\n        try:\n            encoder = type_encoders[base]\n        except KeyError:\n            continue\n        return encoder(obj)\n    else:\n        return pydantic_encoder(obj)", "target": "def test_dict_any_value():\n    v = SchemaValidator(core_schema.dict_schema(keys_schema=core_schema.str_schema()))\n    assert v.validate_json('{\"1\": 1, \"2\": \"a\", \"3\": null}') == {'1': 1, '2': 'a', '3': None}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005383", "source": "def _dataclass_getstate(self: Any) -> list[Any]:\n                    return [getattr(self, f.name) for f in dataclasses.fields(self)]", "target": "def test_serialization_exclude():\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(\n                    name='a', schema=core_schema.str_schema(), serialization_exclude_if=lambda x: x == 'bye'\n                ),\n                core_schema.dataclass_field(name='b', schema=core_schema.bytes_schema(), serialization_exclude=True),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    s = SchemaSerializer(schema)\n    assert s.to_python(Foo(a='hello', b=b'more')) == {'a': 'hello'}\n    assert s.to_python(Foo(a='hello', b=b'more'), mode='json') == {'a': 'hello'}\n    assert s.to_python(Foo(a='bye', b=b'more'), mode='json') == {}\n    j = s.to_json(Foo(a='hello', b=b'more'))\n    if on_pypy:\n        assert json.loads(j) == {'a': 'hello'}\n    else:\n        assert j == b'{\"a\":\"hello\"}'\n    j = s.to_json(Foo(a='bye', b=b'more'))\n    if on_pypy:\n        assert json.loads(j) == {}\n    else:\n        assert j == b'{}'"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005384", "source": "def handle_extra_init(self: 'Dataclass', *args: Any, **kwargs: Any) -> None:\n        if config.extra == Extra.ignore:\n            init(self, *args, **{k: v for k, v in kwargs.items() if k in self.__dataclass_fields__})\n        elif config.extra == Extra.allow:\n            for k, v in kwargs.items():\n                self.__dict__.setdefault(k, v)\n            init(self, *args, **{k: v for k, v in kwargs.items() if k in self.__dataclass_fields__})\n        else:\n            init(self, *args, **kwargs)", "target": "def test_dataclass_args_init_only_no_fields(py_and_json: PyAndJson, input_value, expected):\n    schema = core_schema.dataclass_args_schema(\n        'MyDataclass', [core_schema.dataclass_field(name='a', schema=core_schema.str_schema())], collect_init_only=True\n    )\n    v = py_and_json(schema)\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=expected.message) as exc_info:\n            v.validate_test(input_value)\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        assert v.validate_test(input_value) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005385", "source": "def load_weights(mod: nn.Module, names: list[str], params: tuple[Tensor, ...]) -> None:\n    for name, p in zip(names, params):\n        _set_nested_attr(mod, name.split(\".\"), p)", "target": "def test_iterator(self, input_: str = \"abcdef\") -> None:\n        iter_ = PeekableIterator(input_)\n        for idx, c in enumerate(iter_):\n            self.assertEqual(c, input_[idx])"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005386", "source": "def _general_metadata_cls() -> type[BaseMetadata]:\n    from annotated_types import BaseMetadata\n    class _PydanticGeneralMetadata(PydanticMetadata, BaseMetadata):\n        def __init__(self, metadata: Any):\n            self.__dict__ = metadata\n    return _PydanticGeneralMetadata", "target": "def test_extra_behavior_ignore(\n    config: Union[core_schema.CoreConfig, None],\n    schema_extra_behavior_kw: dict[str, Any],\n    validate_fn_extra_kw: Union[ExtraBehavior, None],\n):\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            {'f': core_schema.model_field(core_schema.str_schema())}, **schema_extra_behavior_kw\n        ),\n        config=config,\n    )\n    m, model_extra, fields_set = v.validate_python({'f': 'x', 'extra_field': 123}, extra=validate_fn_extra_kw)\n    assert m == {'f': 'x'}\n    assert model_extra is None\n    assert fields_set == {'f'}\n    v.validate_assignment(m, 'f', 'y', extra=validate_fn_extra_kw)\n    assert m['f'] == 'y'\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_assignment(m, 'not_f', 'xyz', extra=validate_fn_extra_kw)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'no_such_attribute',\n            'loc': ('not_f',),\n            'msg': \"Object has no attribute 'not_f'\",\n            'input': 'xyz',\n            'ctx': {'attribute': 'not_f'},\n        }\n    ]\n    assert 'not_f' not in m"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005387", "source": "def _check_output(items: list[str], encoding: str = \"utf-8\") -> str:\n    from subprocess import CalledProcessError, check_output, STDOUT\n    try:\n        return check_output(items, stderr=STDOUT).decode(encoding)\n    except CalledProcessError as e:\n        msg = f\"Command `{' '.join(e.cmd)}` returned non-zero exit code {e.returncode}\"\n        stdout = e.stdout.decode(encoding) if e.stdout is not None else \"\"\n        stderr = e.stderr.decode(encoding) if e.stderr is not None else \"\"\n        print(f\"stdout: \\n{stdout}\")\n        print(f\"stderr: \\n{stderr}\")\n        if len(stderr) == 0:\n            msg += f\"\\n```\\n{stdout}```\"\n        else:\n            msg += f\"\\nstdout:\\n```\\n{stdout}```\\nstderr:\\n```\\n{stderr}```\"\n        raise RuntimeError(msg) from e", "target": "def test_iterator(self, input_: str = \"abcdef\") -> None:\n        iter_ = PeekableIterator(input_)\n        for idx, c in enumerate(iter_):\n            self.assertEqual(c, input_[idx])"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005388", "source": "def _update_fields_from_docstrings(cls: type[Any], fields: dict[str, FieldInfo], use_inspect: bool = False) -> None:\n    fields_docs = extract_docstrings_from_cls(cls, use_inspect=use_inspect)\n    for ann_name, field_info in fields.items():\n        if field_info.description is None and ann_name in fields_docs:\n            field_info.description = fields_docs[ann_name]", "target": "def test_allow_extra_wrong():\n    with pytest.raises(SchemaError, match='Invalid extra_behavior: `wrong`'):\n        SchemaValidator(\n            schema=core_schema.model_fields_schema(fields={}), config=CoreConfig(extra_fields_behavior='wrong')\n        )"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005389", "source": "def collect_model_fields(\n    cls: type[BaseModel],\n    config_wrapper: ConfigWrapper,\n    ns_resolver: NsResolver,\n    *,\n    typevars_map: Mapping[TypeVar, Any] | None = None,\n) -> tuple[dict[str, FieldInfo], set[str]]:\n    FieldInfo_ = import_cached_field_info()\n    BaseModel_ = import_cached_base_model()\n    bases = cls.__bases__\n    parent_fields_lookup: dict[str, FieldInfo] = {}\n    for base in reversed(bases):\n        if model_fields := getattr(base, '__pydantic_fields__', None):\n            parent_fields_lookup.update(model_fields)\n    type_hints = _typing_extra.get_model_type_hints(cls, ns_resolver=ns_resolver)\n    annotations = _typing_extra.safe_get_annotations(cls)\n    fields: dict[str, FieldInfo] = {}\n    class_vars: set[str] = set()\n    for ann_name, (ann_type, evaluated) in type_hints.items():\n        if ann_name == 'model_config':\n            continue\n        _check_protected_namespaces(\n            protected_namespaces=config_wrapper.protected_namespaces,\n            ann_name=ann_name,\n            bases=bases,\n            cls_name=cls.__name__,\n        )\n        if _typing_extra.is_classvar_annotation(ann_type):\n            class_vars.add(ann_name)\n            continue\n        assigned_value = getattr(cls, ann_name, PydanticUndefined)\n        if assigned_value is not PydanticUndefined and (\n            any(getattr(BaseModel_, depr_name, None) is assigned_value for depr_name in _deprecated_method_names)\n            or (\n                hasattr(assigned_value, '__func__')\n                and any(\n                    getattr(getattr(BaseModel_, depr_name, None), '__func__', None) is assigned_value.__func__\n                    for depr_name in _deprecated_classmethod_names\n                )\n            )\n        ):\n            assigned_value = PydanticUndefined\n        if not is_valid_field_name(ann_name):\n            continue\n        if cls.__pydantic_root_model__ and ann_name != 'root':\n            raise NameError(\n                f\"Unexpected field with name {ann_name!r}; only 'root' is allowed as a field of a `RootModel`\"\n            )\n        generic_origin = getattr(cls, '__pydantic_generic_metadata__', {}).get('origin')\n        for base in bases:\n            dataclass_fields = {\n                field.name for field in (dataclasses.fields(base) if dataclasses.is_dataclass(base) else ())\n            }\n            if hasattr(base, ann_name):\n                if base is generic_origin:\n                    continue\n                if ann_name in dataclass_fields:\n                    continue\n                if ann_name not in annotations:\n                    continue\n                warnings.warn(\n                    f'Field name \"{ann_name}\" in \"{cls.__qualname__}\" shadows an attribute in parent '\n                    f'\"{base.__qualname__}\"',\n                    UserWarning,\n                    stacklevel=4,\n                )\n        if assigned_value is PydanticUndefined:\n            if ann_name in annotations or ann_name not in parent_fields_lookup:\n                field_info = FieldInfo_.from_annotation(ann_type, _source=AnnotationSource.CLASS)\n                field_info._original_annotation = ann_type\n                if not evaluated:\n                    field_info._complete = False\n            else:\n                parent_field_info = parent_fields_lookup[ann_name]._copy()\n                if typevars_map:\n                    field_info = _recreate_field_info(\n                        parent_field_info, ns_resolver=ns_resolver, typevars_map=typevars_map, lenient=True\n                    )\n                else:\n                    field_info = parent_field_info\n        else:\n            if isinstance(assigned_value, FieldInfo_) and ismethoddescriptor(assigned_value.default):\n                default = assigned_value.default.__get__(None, cls)\n                assigned_value.default = default\n                assigned_value._attributes_set['default'] = default\n            field_info = FieldInfo_.from_annotated_attribute(ann_type, assigned_value, _source=AnnotationSource.CLASS)\n            field_info._original_assignment = assigned_value\n            field_info._original_annotation = ann_type\n            if not evaluated:\n                field_info._complete = False\n            elif 'final' in field_info._qualifiers and not field_info.is_required():\n                warnings.warn(\n                    f'Annotation {ann_name!r} is marked as final and has a default value. Pydantic treats {ann_name!r} as a '\n                    'class variable, but it will be considered as a normal field in V3 to be aligned with dataclasses. If you '\n                    f'still want {ann_name!r} to be considered as a class variable, annotate it as: `ClassVar[<type>] = <default>.`',\n                    category=PydanticDeprecatedSince211,\n                    stacklevel=4,\n                )\n                class_vars.add(ann_name)\n                continue\n            try:\n                delattr(cls, ann_name)\n            except AttributeError:\n                pass\n        decorators: DecoratorInfos = cls.__dict__['__pydantic_decorators__']\n        if ann_name in decorators.computed_fields:\n            raise TypeError(\n                f'Field {ann_name!r} of class {cls.__name__!r} overrides symbol of same name in a parent class. '\n                'This override with a computed_field is incompatible.'\n            )\n        fields[ann_name] = field_info\n        if field_info._complete:\n            update_field_from_config(config_wrapper, ann_name, field_info)\n    if config_wrapper.use_attribute_docstrings:\n        _update_fields_from_docstrings(cls, fields)\n    return fields, class_vars", "target": "def test_from_attributes_error():\n    class Foobar:\n        def __init__(self):\n            self.a = 1\n        @property\n        def b(self):\n            raise RuntimeError('intentional error')\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'a': core_schema.model_field(schema=core_schema.int_schema()),\n                'b': core_schema.model_field(schema=core_schema.int_schema()),\n            },\n            from_attributes=True,\n        )\n    )\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python(Foobar())\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'get_attribute_error',\n            'loc': ('b',),\n            'msg': 'Error extracting attribute: RuntimeError: intentional error',\n            'input': HasRepr(IsStr(regex='.+Foobar object at.+')),\n            'ctx': {'error': 'RuntimeError: intentional error'},\n        }\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005390", "source": "def dataclass(\n        _cls: type[_T],\n        *,\n        init: Literal[False] = False,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool | None = None,\n        config: ConfigDict | type[object] | None = None,\n        validate_on_init: bool | None = None,\n        kw_only: bool = ...,\n        slots: bool = ...,\n    ) -> type[PydanticDataclass]: ...", "target": "def test_by_alias_and_name_config_interaction(config, runtime, expected) -> None:\n    @dataclasses.dataclass\n    class Foo:\n        my_field: str\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(\n                    name='my_field', schema=core_schema.str_schema(), serialization_alias='my_alias'\n                ),\n            ],\n        ),\n        ['my_field'],\n        config=core_schema.CoreConfig(serialize_by_alias=config or False),\n    )\n    s = SchemaSerializer(schema)\n    assert s.to_python(Foo(my_field='hello'), by_alias=runtime) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005391", "source": "def patch_id(self, ref: Union[str, list[str]]) -> list[tuple[str, str]]:\n        is_list = isinstance(ref, list)\n        if is_list:\n            if len(ref) == 0:\n                return []\n            ref = \" \".join(ref)\n        rc = _check_output(\n            [\"sh\", \"-c\", f\"git -C {self.repo_dir} show {ref}|git patch-id --stable\"]\n        ).strip()\n        return [cast(tuple[str, str], x.split(\" \", 1)) for x in rc.split(\"\\n\")]", "target": "def test_compute_diff(self) -> None:\n        diff = self.repo.diff(\"HEAD\")\n        sha = _shasum(diff)\n        self.assertEqual(len(sha), 64)"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005392", "source": "def push(self, branch: str, dry_run: bool, retry: int = 3) -> None:\n        for cnt in range(retry):\n            try:\n                if dry_run:\n                    self._run_git(\"push\", \"--dry-run\", self.remote, branch)\n                else:\n                    self._run_git(\"push\", self.remote, branch)\n            except RuntimeError as e:\n                print(f\"{cnt} push attempt failed with {e}\")\n                self.fetch()\n                self._run_git(\"rebase\", f\"{self.remote}/{branch}\")", "target": "def test_peek(self, input_: str = \"abcdef\") -> None:\n        iter_ = PeekableIterator(input_)\n        for idx, c in enumerate(iter_):\n            if idx + 1 < len(input_):\n                self.assertEqual(iter_.peek(), input_[idx + 1])\n            else:\n                self.assertTrue(iter_.peek() is None)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005393", "source": "def _check_protected_namespaces(\n    protected_namespaces: tuple[str | Pattern[str], ...],\n    ann_name: str,\n    bases: tuple[type[Any], ...],\n    cls_name: str,\n) -> None:\n    BaseModel = import_cached_base_model()\n    for protected_namespace in protected_namespaces:\n        ns_violation = False\n        if isinstance(protected_namespace, Pattern):\n            ns_violation = protected_namespace.match(ann_name) is not None\n        elif isinstance(protected_namespace, str):\n            ns_violation = ann_name.startswith(protected_namespace)\n        if ns_violation:\n            for b in bases:\n                if hasattr(b, ann_name):\n                    if not (issubclass(b, BaseModel) and ann_name in getattr(b, '__pydantic_fields__', {})):\n                        raise ValueError(\n                            f'Field {ann_name!r} conflicts with member {getattr(b, ann_name)}'\n                            f' of protected namespace {protected_namespace!r}.'\n                        )\n            else:\n                valid_namespaces: list[str] = []\n                for pn in protected_namespaces:\n                    if isinstance(pn, Pattern):\n                        if not pn.match(ann_name):\n                            valid_namespaces.append(f're.compile({pn.pattern!r})')\n                    else:\n                        if not ann_name.startswith(pn):\n                            valid_namespaces.append(f\"'{pn}'\")\n                valid_namespaces_str = f'({\", \".join(valid_namespaces)}{\",)\" if len(valid_namespaces) == 1 else \")\"}'\n                warnings.warn(\n                    f'Field {ann_name!r} in {cls_name!r} conflicts with protected namespace {protected_namespace!r}.\\n\\n'\n                    f\"You may be able to solve this by setting the 'protected_namespaces' configuration to {valid_namespaces_str}.\",\n                    UserWarning,\n                    stacklevel=5,\n                )", "target": "def test_ignore_extra():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'field_a': core_schema.model_field(schema=core_schema.str_schema()),\n                'field_b': core_schema.model_field(schema=core_schema.int_schema()),\n            }\n        )\n    )\n    assert v.validate_python({'field_a': b'123', 'field_b': 1, 'field_c': 123}) == (\n        {'field_a': '123', 'field_b': 1},\n        None,\n        {'field_b', 'field_a'},\n    )"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005394", "source": "def patch_base_fields(cls: type[Any]) -> Generator[None]:\n    original_fields_list: list[tuple[DcFields, DcFields]] = []\n    for base in cls.__mro__[1:]:\n        dc_fields: dict[str, dataclasses.Field[Any]] = base.__dict__.get('__dataclass_fields__', {})\n        dc_fields_with_pydantic_field_defaults = {\n            field_name: field\n            for field_name, field in dc_fields.items()\n            if isinstance(field.default, FieldInfo)\n            and (field.default.description is not None or field.default.kw_only or field.default.repr is not True)\n        }\n        if dc_fields_with_pydantic_field_defaults:\n            original_fields_list.append((dc_fields, dc_fields_with_pydantic_field_defaults))\n            for field_name, field in dc_fields_with_pydantic_field_defaults.items():\n                default = cast(FieldInfo, field.default)\n                new_dc_field = copy.copy(field)\n                if sys.version_info >= (3, 10) and default.kw_only:\n                    new_dc_field.kw_only = True\n                if default.repr is not True:\n                    new_dc_field.repr = default.repr\n                dc_fields[field_name] = new_dc_field\n    try:\n        yield\n    finally:\n        for fields, original_fields in original_fields_list:\n            for field_name, original_field in original_fields.items():\n                fields[field_name] = original_field", "target": "def test_properties():\n    @dataclasses.dataclass\n    class FooProp:\n        a: str\n        b: bytes\n        @property\n        def c(self) -> str:\n            return f'{self.a} {self.b.decode()}'\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'FooProp',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bytes_schema()),\n            ],\n            computed_fields=[core_schema.computed_field('c', core_schema.str_schema())],\n        ),\n        ['a', 'b'],\n    )\n    s = SchemaSerializer(schema)\n    assert s.to_python(FooProp(a='hello', b=b'more')) == IsStrictDict(a='hello', b=b'more', c='hello more')\n    assert s.to_python(FooProp(a='hello', b=b'more'), mode='json') == IsStrictDict(a='hello', b='more', c='hello more')\n    j = s.to_json(FooProp(a='hello', b=b'more'))\n    if on_pypy:\n        assert json.loads(j) == {'a': 'hello', 'b': 'more', 'c': 'hello more'}\n    else:\n        assert j == b'{\"a\":\"hello\",\"b\":\"more\",\"c\":\"hello more\"}'\n    assert s.to_python(FooProp(a='hello', b=b'more'), exclude={'b'}) == IsStrictDict(a='hello', c='hello more')\n    assert s.to_json(FooProp(a='hello', b=b'more'), include={'a'}) == b'{\"a\":\"hello\"}'"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005395", "source": "def _dataclass_setstate(self: Any, state: list[Any]) -> None:\n                    for field, value in zip(dataclasses.fields(self), state):\n                        object.__setattr__(self, field.name, value)", "target": "def test_dataclass_slots_field_after_validator():\n    @dataclasses.dataclass(slots=True)\n    class Foo:\n        a: int\n        b: str\n        @classmethod\n        def validate_b(cls, v: str, info: core_schema.ValidationInfo) -> str:\n            assert v == 'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return 'hello world!'\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(\n                    name='b',\n                    schema=core_schema.with_info_after_validator_function(Foo.validate_b, core_schema.str_schema()),\n                ),\n            ],\n        ),\n        ['a', 'b'],\n        slots=True,\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 1, 'b': b'hello'})\n    assert dataclasses.asdict(foo) == {'a': 1, 'b': 'hello world!'}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005396", "source": "def update_field_forward_refs(field: 'ModelField', globalns: Any, localns: Any) -> None:\n    prepare = False\n    if field.type_.__class__ == ForwardRef:\n        prepare = True\n        field.type_ = evaluate_forwardref(field.type_, globalns, localns or None)\n    if field.outer_type_.__class__ == ForwardRef:\n        prepare = True\n        field.outer_type_ = evaluate_forwardref(field.outer_type_, globalns, localns or None)\n    if prepare:\n        field.prepare()\n    if field.sub_fields:\n        for sub_f in field.sub_fields:\n            update_field_forward_refs(sub_f, globalns=globalns, localns=localns)\n    if field.discriminator_key is not None:\n        field.prepare_discriminated_union_sub_fields()", "target": "def test_schema_validator() -> None:\n    SchemaValidator({'type': 'int'})"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005397", "source": "def is_union(tp: Optional[Type[Any]]) -> bool:\n        return tp is Union or tp is types.UnionType", "target": "def test_schema_typing_error() -> None:\n    _: CoreSchema = {'type': 'wrong'}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005398", "source": "def is_valid_field_name(name: str) -> bool:\n    return not name.startswith('_')", "target": "def test_frozen_field():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'name': core_schema.model_field(schema=core_schema.str_schema()),\n                'age': core_schema.model_field(schema=core_schema.int_schema()),\n                'is_developer': core_schema.model_field(\n                    schema=core_schema.with_default_schema(schema=core_schema.bool_schema(), default=True), frozen=True\n                ),\n            }\n        )\n    )\n    r1, model_extra, fields_set = v.validate_python({'name': 'Samuel', 'age': '36'})\n    assert r1 == {'name': 'Samuel', 'age': 36, 'is_developer': True}\n    assert model_extra is None\n    assert fields_set == {'name', 'age'}\n    v.validate_assignment(r1, 'age', '35')\n    assert r1 == {'name': 'Samuel', 'age': 35, 'is_developer': True}\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_assignment(r1, 'is_developer', False)\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'frozen_field', 'loc': ('is_developer',), 'msg': 'Field is frozen', 'input': False}\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005399", "source": "def _apply_field_title_generator_to_field_info(\n    title_generator: Callable[[str, FieldInfo], str],\n    field_name: str,\n    field_info: FieldInfo,\n):\n    if field_info.title is None:\n        title = title_generator(field_name, field_info)\n        if not isinstance(title, str):\n            raise TypeError(f'field_title_generator {title_generator} must return str, not {title.__class__}')\n        field_info.title = title", "target": "def test_on_error_bad_default(self):\n        with pytest.raises(SchemaError, match=\"'on_error = default' requires a `default` or `default_factory`\"):\n            SchemaValidator(\n                schema=core_schema.model_fields_schema(\n                    fields={\n                        'x': core_schema.model_field(\n                            schema=core_schema.with_default_schema(schema=core_schema.str_schema(), on_error='default')\n                        )\n                    }\n                )\n            )"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005400", "source": "def rebuild_dataclass_fields(\n    cls: type[PydanticDataclass],\n    *,\n    config_wrapper: ConfigWrapper,\n    ns_resolver: NsResolver,\n    typevars_map: Mapping[TypeVar, Any],\n) -> dict[str, FieldInfo]:\n    FieldInfo_ = import_cached_field_info()\n    rebuilt_fields: dict[str, FieldInfo] = {}\n    with ns_resolver.push(cls):\n        for f_name, field_info in cls.__pydantic_fields__.items():\n            if field_info._complete:\n                rebuilt_fields[f_name] = field_info\n            else:\n                existing_desc = field_info.description\n                ann = _typing_extra.eval_type(\n                    field_info._original_annotation,\n                    *ns_resolver.types_namespace,\n                )\n                ann = _generics.replace_types(ann, typevars_map)\n                new_field = FieldInfo_.from_annotated_attribute(\n                    ann,\n                    field_info._original_assignment,\n                    _source=AnnotationSource.DATACLASS,\n                )\n                new_field.description = new_field.description if new_field.description is not None else existing_desc\n                update_field_from_config(config_wrapper, f_name, new_field)\n                rebuilt_fields[f_name] = new_field\n    return rebuilt_fields", "target": "def test_paths_allow_by_name(py_and_json: PyAndJson, input_value):\n    v = py_and_json(\n        {\n            'type': 'model-fields',\n            'fields': {\n                'field_a': {\n                    'validation_alias': [['foo', 'bar'], ['foo']],\n                    'type': 'model-field',\n                    'schema': {'type': 'int'},\n                }\n            },\n        },\n        config=CoreConfig(validate_by_name=True),\n    )\n    assert v.validate_test(input_value) == ({'field_a': 42}, None, {'field_a'})"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005401", "source": "def push(self, config_wrapper: ConfigWrapper | ConfigDict | None):\n        if config_wrapper is None:\n            yield\n            return\n        if not isinstance(config_wrapper, ConfigWrapper):\n            config_wrapper = ConfigWrapper(config_wrapper, check=False)\n        self._config_wrapper_stack.append(config_wrapper)\n        try:\n            yield\n        finally:\n            self._config_wrapper_stack.pop()", "target": "def test_on_model_class():\n    v = SchemaValidator(\n        cs.model_schema(\n            cls=MyModel,\n            config=CoreConfig(str_max_length=5),\n            schema=cs.model_fields_schema(fields={'f': cs.model_field(schema=cs.str_schema())}),\n        )\n    )\n    assert 'max_length:Some(5)' in plain_repr(v)\n    assert v.isinstance_python({'f': 'test'}) is True\n    assert v.isinstance_python({'f': 'test long'}) is False"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005402", "source": "def env_path(\n    name: str,\n    default: Optional[Union[str, Path]] = None,\n    resolve: bool = True,\n) -> Path:\n    path = env_path_optional(name, default, resolve)\n    if not path:\n        raise ValueError(f\"Missing path value for {name}\")\n    return path", "target": "def test_env_path_optional_unset_returns_default_str(self):\n        default_str = \"x/y\"\n        with patch.dict(os.environ, {}, clear=True):\n            p = m.env_path_optional(\"P\", default=default_str)\n            self.assertIsInstance(p, Path)\n            self.assertIsNotNone(p)\n            if p:\n                self.assertTrue(p.is_absolute())\n                self.assertEqual(p.parts[-2:], (\"x\", \"y\"))"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005403", "source": "def ensure_dir_exists(path: Union[str, Path]) -> Path:\n    path_obj = get_path(path)\n    path_obj.mkdir(parents=True, exist_ok=True)\n    return path_obj", "target": "def test_copy_dir_into_existing_dir_overwrite_true_merges(self):\n        src = self.tmp_path / \"srcdir\"\n        dst = self.tmp_path / \"destdir\"\n        (src / \"x\").mkdir(parents=True)\n        (src / \"x\" / \"new.txt\").write_text(\"new\")\n        dst.mkdir()\n        (dst / \"existing.txt\").write_text(\"old\")\n        copy(src, dst)\n        self.assertEqual((dst / \"existing.txt\").read_text(), \"old\")\n        self.assertEqual((dst / \"x\" / \"new.txt\").read_text(), \"new\")"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005404", "source": "def _is_field_cached_property(obj: 'Dataclass', k: str) -> bool:\n        return isinstance(getattr(type(obj), k, None), cached_property)", "target": "def test_extra_behavior_allow(\n    config: Union[core_schema.CoreConfig, None],\n    schema_extra_behavior_kw: dict[str, Any],\n    validate_fn_extra_kw: Union[ExtraBehavior, None],\n):\n    @dataclasses.dataclass\n    class MyModel:\n        f: str\n    v = SchemaValidator(\n        core_schema.dataclass_schema(\n            MyModel,\n            core_schema.dataclass_args_schema(\n                'MyModel', [core_schema.dataclass_field('f', core_schema.str_schema())], **schema_extra_behavior_kw\n            ),\n            ['f'],\n            config=config,\n        )\n    )\n    m: MyModel = v.validate_python({'f': 'x', 'extra_field': '123'}, extra=validate_fn_extra_kw)\n    assert m.f == 'x'\n    assert getattr(m, 'extra_field') == '123'\n    v.validate_assignment(m, 'f', 'y', extra=validate_fn_extra_kw)\n    assert m.f == 'y'\n    v.validate_assignment(m, 'not_f', '123', extra=validate_fn_extra_kw)\n    assert getattr(m, 'not_f') == '123'"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005405", "source": "def _call_initvar(*args: Any, **kwargs: Any) -> NoReturn:\n        raise TypeError(\"'InitVar' object is not callable\")", "target": "def test_extra_behavior_ignore(\n    config: Union[core_schema.CoreConfig, None],\n    schema_extra_behavior_kw: dict[str, Any],\n    validate_fn_extra_kw: Union[ExtraBehavior, None],\n):\n    @dataclasses.dataclass\n    class MyModel:\n        f: str\n    v = SchemaValidator(\n        core_schema.dataclass_schema(\n            MyModel,\n            core_schema.dataclass_args_schema(\n                'MyModel', [core_schema.dataclass_field('f', core_schema.str_schema())], **schema_extra_behavior_kw\n            ),\n            ['f'],\n        ),\n        config=config,\n    )\n    m: MyModel = v.validate_python({'f': 'x', 'extra_field': 123}, extra=validate_fn_extra_kw)\n    assert m.f == 'x'\n    assert not hasattr(m, 'extra_field')\n    v.validate_assignment(m, 'f', 'y', extra=validate_fn_extra_kw)\n    assert m.f == 'y'\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_assignment(m, 'not_f', 'xyz', extra=validate_fn_extra_kw)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'no_such_attribute',\n            'loc': ('not_f',),\n            'msg': \"Object has no attribute 'not_f'\",\n            'input': 'xyz',\n            'ctx': {'attribute': 'not_f'},\n        }\n    ]\n    assert not hasattr(m, 'not_f')"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005406", "source": "def is_union(tp: Optional[Type[Any]]) -> bool:\n        return tp is Union", "target": "def test_schema_validator() -> None:\n    SchemaValidator({'type': 'int'})"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005407", "source": "def is_pydantic_dataclass(class_: type[Any], /) -> TypeGuard[type[PydanticDataclass]]:\n    try:\n        return '__is_pydantic_dataclass__' in class_.__dict__ and dataclasses.is_dataclass(class_)\n    except AttributeError:\n        return False", "target": "def test_dataclass_args_init_with_default(input_value, extra_behavior, expected):\n    @dataclasses.dataclass\n    class Foo:\n        a: str\n        b: str\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(\n                    name='b',\n                    schema=core_schema.with_default_schema(schema=core_schema.str_schema(), default='HELLO'),\n                    init=False,\n                ),\n            ],\n            extra_behavior=extra_behavior,\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)) as exc_info:\n            v.validate_python(input_value)\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        assert dataclasses.asdict(v.validate_python(input_value)) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005408", "source": "def validated_setattr(instance: PydanticDataclass, name: str, value: Any, /) -> None:\n                if frozen_:\n                    return original_setattr(instance, name, value)\n                inst_cls = type(instance)\n                attr = getattr(inst_cls, name, None)\n                if isinstance(attr, property):\n                    attr.__set__(instance, value)\n                elif isinstance(attr, functools.cached_property):\n                    instance.__dict__.__setitem__(name, value)\n                else:\n                    inst_cls.__pydantic_validator__.validate_assignment(instance, name, value)", "target": "def test_dataclass_self_init_alias_field_name():\n    @dataclasses.dataclass(init=False)\n    class Foo:\n        a: str\n        b: bool\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), validation_alias='aAlias'),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), validation_alias=['bAlias', 0]),\n            ],\n        ),\n        ['a', 'b'],\n        config={'loc_by_alias': False},\n    )\n    v = SchemaValidator(schema)\n    def __init__(self, *args, **kwargs):\n        v.validate_python(ArgsKwargs(args, kwargs), self_instance=self)\n    Foo.__init__ = __init__\n    foo = Foo(aAlias=b'hello', bAlias=['True'])\n    assert dataclasses.asdict(foo) == {'a': 'hello', 'b': True}\n    with pytest.raises(ValidationError) as exc_info:\n        Foo(aAlias=b'hello', bAlias=['wrong'])\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'bool_parsing',\n            'loc': ('b',),\n            'msg': 'Input should be a valid boolean, unable to interpret input',\n            'input': 'wrong',\n        }\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005409", "source": "def _update_fields_from_docstrings(cls: type[Any], fields: dict[str, FieldInfo], use_inspect: bool = False) -> None:\n    fields_docs = extract_docstrings_from_cls(cls, use_inspect=use_inspect)\n    for ann_name, field_info in fields.items():\n        if field_info.description is None and ann_name in fields_docs:\n            field_info.description = fields_docs[ann_name]", "target": "def test_with_default():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'field_a': core_schema.model_field(schema=core_schema.str_schema()),\n                'field_b': core_schema.model_field(\n                    schema=core_schema.with_default_schema(schema=core_schema.int_schema(), default=666)\n                ),\n            }\n        )\n    )\n    assert v.validate_python({'field_a': b'abc'}) == ({'field_a': 'abc', 'field_b': 666}, None, {'field_a'})\n    assert v.validate_python({'field_a': b'abc', 'field_b': 1}) == (\n        {'field_a': 'abc', 'field_b': 1},\n        None,\n        {'field_b', 'field_a'},\n    )"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005410", "source": "def _update_fields_from_docstrings(cls: type[Any], fields: dict[str, FieldInfo], use_inspect: bool = False) -> None:\n    fields_docs = extract_docstrings_from_cls(cls, use_inspect=use_inspect)\n    for ann_name, field_info in fields.items():\n        if field_info.description is None and ann_name in fields_docs:\n            field_info.description = fields_docs[ann_name]", "target": "def test_aliases_debug():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'field_a': core_schema.model_field(\n                    validation_alias=[['foo', 'bar', 'bat'], ['foo', 3]], schema=core_schema.int_schema()\n                )\n            }\n        )\n    )\n    print(repr(v))\n    assert repr(v).startswith('SchemaValidator(title=\"model-fields\", validator=ModelFields(')\n    assert 'PathChoices(' in repr(v)"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005411", "source": "def _load_model(x: GPTModelConfig, device=\"cuda\", precision=torch.bfloat16):\n    with torch.device(\"meta\"):\n        model = x.module.from_name(x.name)\n    model = model.to(dtype=precision)\n    if x.mode == \"int8\":\n        print(\"Using int8 weight-only quantization!\")\n        model = x.quantizer(model).convert_for_runtime()\n    state_dict = model.state_dict()\n    for k, v in state_dict.items():\n        state_dict[k] = torch.nn.Parameter(\n            torch.randn(v.shape, device=device).to(dtype=v.dtype),\n            requires_grad=v.requires_grad,\n        )\n    model.load_state_dict(state_dict, assign=True)\n    return model.eval()", "target": "def testNonContiguous(self):\n        x = torch.tensor([100, 200, 300])[::2]\n        assert not x.is_contiguous()\n        assert x[0] == 100\n        assert x[1] == 300\n        return x"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005412", "source": "def dataclass(\n        _cls: type[_T],\n        *,\n        init: Literal[False] = False,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool | None = None,\n        config: ConfigDict | type[object] | None = None,\n        validate_on_init: bool | None = None,\n    ) -> type[PydanticDataclass]: ...", "target": "def test_dataclass_slots_field_before_validator():\n    @dataclasses.dataclass(slots=True)\n    class Foo:\n        a: int\n        b: str\n        @classmethod\n        def validate_b(cls, v: bytes, info: core_schema.ValidationInfo) -> bytes:\n            assert v == b'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return b'hello world!'\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(\n                    name='b',\n                    schema=core_schema.with_info_before_validator_function(Foo.validate_b, core_schema.str_schema()),\n                ),\n            ],\n        ),\n        ['a', 'b'],\n        slots=True,\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 1, 'b': b'hello'})\n    assert dataclasses.asdict(foo) == {'a': 1, 'b': 'hello world!'}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005413", "source": "def _validate_dataclass(cls: Type['DataclassT'], v: Any) -> 'DataclassT':\n    with set_validation(cls, True):\n        if isinstance(v, cls):\n            v.__pydantic_validate_values__()\n            return v\n        elif isinstance(v, (list, tuple)):\n            return cls(*v)\n        elif isinstance(v, dict):\n            return cls(**v)\n        else:\n            raise DataclassTypeError(class_name=cls.__name__)", "target": "def test_dataclass_self_init_alias():\n    @dataclasses.dataclass(init=False)\n    class Foo:\n        a: str\n        b: bool\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), validation_alias='aAlias'),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), validation_alias=['bAlias', 0]),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    def __init__(self, *args, **kwargs):\n        v.validate_python(ArgsKwargs(args, kwargs), self_instance=self)\n    Foo.__init__ = __init__\n    foo = Foo(aAlias=b'hello', bAlias=['True'])\n    assert dataclasses.is_dataclass(foo)\n    assert dataclasses.asdict(foo) == {'a': 'hello', 'b': True}\n    with pytest.raises(ValidationError) as exc_info:\n        Foo(aAlias=b'hello', bAlias=['wrong'])\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'bool_parsing',\n            'loc': ('bAlias', 0),\n            'msg': 'Input should be a valid boolean, unable to interpret input',\n            'input': 'wrong',\n        }\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005414", "source": "def parse_stmts(stmts: str) -> tuple[str, str]:\n    stmts = textwrap.dedent(stmts).strip()\n    lines: list[str] = stmts.splitlines(keepends=False)\n    assert len(lines) >= 3, f\"Invalid string:\\n{stmts}\"\n    column_header_pattern = r\"^Python\\s{35}\\| C\\+\\+(\\s*)$\"\n    signature_pattern = r\"^: f\\((.*)\\)( -> (.+))?\\s*$\"\n    separation_pattern = r\"^[-]{40} | [-]{40}$\"\n    code_pattern = r\"^(.{40}) \\|($| (.*)$)\"\n    column_match = re.search(column_header_pattern, lines[0])\n    if column_match is None:\n        raise ValueError(\n            f\"Column header `{lines[0]}` \"\n            f\"does not match pattern `{column_header_pattern}`\"\n        )\n    assert re.search(separation_pattern, lines[1])\n    py_lines: list[str] = []\n    cpp_lines: list[str] = []\n    for l in lines[2:]:\n        l_match = re.search(code_pattern, l)\n        if l_match is None:\n            raise ValueError(f\"Invalid line `{l}`\")\n        py_lines.append(l_match.groups()[0])\n        cpp_lines.append(l_match.groups()[2] or \"\")\n        l_from_stmts = f\"{py_lines[-1]:<40} | {cpp_lines[-1]:<40}\".rstrip()\n        assert l_from_stmts == l.rstrip(), f\"Failed to round trip `{l}`\"\n    return \"\\n\".join(py_lines), \"\\n\".join(cpp_lines)", "target": "def test_multiple_vars_and_missing_cleanup(self):\n        v1, v2 = \"TEST_ENV_V1\", \"TEST_ENV_V2\"\n        os.environ.pop(v1, None)\n        os.environ[v2] = \"keep\"\n        with temp_environ({v1: \"a\", v2: \"b\"}):\n            self.assertEqual(os.environ[v1], \"a\")\n            self.assertEqual(os.environ[v2], \"b\")\n        self.assertNotIn(v1, os.environ)\n        self.assertEqual(os.environ[v2], \"keep\")"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005415", "source": "def get_args(v: Any) -> Any:\n    pydantic_generic_metadata: PydanticGenericMetadata | None = getattr(v, '__pydantic_generic_metadata__', None)\n    if pydantic_generic_metadata:\n        return pydantic_generic_metadata.get('args')\n    return typing_extensions.get_args(v)", "target": "def test_fastapi_startup_perf(benchmark: Any):\n    data_models = create_data_models()\n    T = TypeVar('T')\n    class GetModel(BaseModel, Generic[T]):\n        res: T\n    class GetModel2(GetModel[T], Generic[T]):\n        foo: str\n        bar: str\n    class GetManyModel(BaseModel, Generic[T]):\n        res: list[T]\n    class GetManyModel2(GetManyModel[T], Generic[T]):\n        foo: str\n        bar: str\n    class GetManyModel3(BaseModel, Generic[T]):\n        res: dict[str, T]\n    class GetManyModel4(BaseModel, Generic[T]):\n        res: dict[str, list[T]]\n    class PutModel(BaseModel, Generic[T]):\n        data: T\n    class PutModel2(PutModel[T], Generic[T]):\n        foo: str\n        bar: str\n    class PutManyModel(BaseModel, Generic[T]):\n        data: list[T]\n    class PutManyModel2(PutManyModel[T], Generic[T]):\n        foo: str\n        bar: str\n    api_models: list[Any] = [\n        GetModel,\n        GetModel2,\n        GetManyModel,\n        GetManyModel2,\n        GetManyModel3,\n        GetManyModel4,\n        PutModel,\n        PutModel2,\n        PutManyModel,\n        PutManyModel2,\n    ]\n    assert len(data_models) == INNER_DATA_MODEL_COUNT + OUTER_DATA_MODEL_COUNT\n    def bench():\n        concrete_api_models = []\n        adapters = []\n        for outer_api_model in api_models:\n            for data_model in data_models:\n                concrete_api_model = outer_api_model[\n                    data_model\n                ]\n                concrete_api_models.append(concrete_api_model)\n                adapt = TypeAdapter(Annotated[concrete_api_model, FieldInfo(description='foo')])\n                adapters.append(adapt)\n                adapt = TypeAdapter(Annotated[concrete_api_model, FieldInfo(description='bar')])\n                adapters.append(adapt)\n        assert len(concrete_api_models) == len(data_models) * len(api_models)\n        assert len(adapters) == len(concrete_api_models) * 2\n    benchmark(bench)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005416", "source": "def dataclass(\n        _cls: type[_T],\n        *,\n        init: Literal[False] = False,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool | None = None,\n        config: ConfigDict | type[object] | None = None,\n        validate_on_init: bool | None = None,\n        kw_only: bool = ...,\n        slots: bool = ...,\n    ) -> type[PydanticDataclass]: ...", "target": "def test_dataclass_field_wrap_validator2():\n    @dataclasses.dataclass\n    class Foo:\n        a: int\n        b: str\n        @classmethod\n        def validate_b(\n            cls, v: bytes, nxt: core_schema.ValidatorFunctionWrapHandler, info: core_schema.ValidationInfo\n        ) -> bytes:\n            assert v == b'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return nxt(b'hello world!')\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(\n                    name='b',\n                    schema=core_schema.with_info_wrap_validator_function(Foo.validate_b, core_schema.str_schema()),\n                ),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 1, 'b': b'hello'})\n    assert dataclasses.asdict(foo) == {'a': 1, 'b': 'hello world!'}"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005417", "source": "def secs_to_ms(time_s):\n    return time_s * 1e3", "target": "def test_ghstack_branches_not_in_sync(self) -> None:\n        head_ref = \"gh/clee2000/1/head\"\n        self._skip_if_ref_does_not_exist(head_ref)\n        self.assertFalse(are_ghstack_branches_in_sync(self.repo, head_ref))"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005418", "source": "def wrap(cls: Type[Any]) -> 'DataclassClassOrWrapper':\n        should_use_proxy = (\n            use_proxy\n            if use_proxy is not None\n            else (\n                is_builtin_dataclass(cls)\n                and (cls.__bases__[0] is object or set(dir(cls)) == set(dir(cls.__bases__[0])))\n            )\n        )\n        if should_use_proxy:\n            dc_cls_doc = ''\n            dc_cls = DataclassProxy(cls)\n            default_validate_on_init = False\n        else:\n            dc_cls_doc = cls.__doc__ or ''\n            if sys.version_info >= (3, 10):\n                dc_cls = dataclasses.dataclass(\n                    cls,\n                    init=init,\n                    repr=repr,\n                    eq=eq,\n                    order=order,\n                    unsafe_hash=unsafe_hash,\n                    frozen=frozen,\n                    kw_only=kw_only,\n                )\n            else:\n                dc_cls = dataclasses.dataclass(\n                    cls, init=init, repr=repr, eq=eq, order=order, unsafe_hash=unsafe_hash, frozen=frozen\n                )\n            default_validate_on_init = True\n        should_validate_on_init = default_validate_on_init if validate_on_init is None else validate_on_init\n        _add_pydantic_validation_attributes(cls, the_config, should_validate_on_init, dc_cls_doc)\n        dc_cls.__pydantic_model__.__try_update_forward_refs__(**{cls.__name__: cls})\n        return dc_cls", "target": "def test_by_alias_and_name_config_interaction(config, runtime, expected) -> None:\n    @dataclasses.dataclass\n    class Foo:\n        my_field: str\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(\n                    name='my_field', schema=core_schema.str_schema(), serialization_alias='my_alias'\n                ),\n            ],\n        ),\n        ['my_field'],\n        config=core_schema.CoreConfig(serialize_by_alias=config or False),\n    )\n    s = SchemaSerializer(schema)\n    assert s.to_python(Foo(my_field='hello'), by_alias=runtime) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005419", "source": "def copy(src: Union[str, Path], dst: Union[str, Path]) -> None:\n    src_path = get_path(src, resolve=True)\n    dst_path = get_path(dst, resolve=True)\n    if not src_path.exists():\n        raise FileNotFoundError(f\"Source does not exist: {src_path}\")\n    dst_path.parent.mkdir(parents=True, exist_ok=True)\n    if src_path.is_file():\n        shutil.copy2(src_path, dst_path)\n    elif src_path.is_dir():\n        shutil.copytree(src_path, dst_path, dirs_exist_ok=True)\n    else:\n        raise ValueError(f\"Unsupported path type: {src_path}\")", "target": "def test_is_str_path_exist(self):\n        p = self.tmp_path / \"x.txt\"\n        p.write_text(\"1\")\n        self.assertTrue(is_path_exist(str(p)))\n        self.assertTrue(is_path_exist(p))\n        self.assertFalse(is_path_exist(str(self.tmp_path / \"missing\")))\n        self.assertFalse(is_path_exist(self.tmp_path / \"missing\"))\n        self.assertFalse(is_path_exist(\"\"))"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005420", "source": "def _general_metadata_cls() -> type[BaseMetadata]:\n    from annotated_types import BaseMetadata\n    class _PydanticGeneralMetadata(PydanticMetadata, BaseMetadata):\n        def __init__(self, metadata: Any):\n            self.__dict__ = metadata\n    return _PydanticGeneralMetadata", "target": "def test_from_attributes(input_value, expected, from_attributes_mode):\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'a': core_schema.model_field(schema=core_schema.int_schema()),\n                'b': core_schema.model_field(schema=core_schema.int_schema()),\n                'c': core_schema.model_field(schema=core_schema.str_schema()),\n            },\n            from_attributes=from_attributes_mode == 'schema',\n        )\n    )\n    kwargs = {}\n    if from_attributes_mode == 'validation':\n        kwargs['from_attributes'] = True\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            val = v.validate_python(input_value, **kwargs)\n            print(f'UNEXPECTED OUTPUT: {val!r}')\n    else:\n        output = v.validate_python(input_value, **kwargs)\n        assert output == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005421", "source": "def _pydantic_fields_complete(cls: type[PydanticDataclass]) -> bool:\n    return all(field_info._complete for field_info in cls.__pydantic_fields__.values())", "target": "def test_dataclass_slots_field_before_validator():\n    @dataclasses.dataclass(slots=True)\n    class Foo:\n        a: int\n        b: str\n        @classmethod\n        def validate_b(cls, v: bytes, info: core_schema.ValidationInfo) -> bytes:\n            assert v == b'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return b'hello world!'\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(\n                    name='b',\n                    schema=core_schema.with_info_before_validator_function(Foo.validate_b, core_schema.str_schema()),\n                ),\n            ],\n        ),\n        ['a', 'b'],\n        slots=True,\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 1, 'b': b'hello'})\n    assert dataclasses.asdict(foo) == {'a': 1, 'b': 'hello world!'}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005422", "source": "def pydantic_encoder(obj: Any) -> Any:\n    from dataclasses import asdict, is_dataclass\n    from pydantic.v1.main import BaseModel\n    if isinstance(obj, BaseModel):\n        return obj.dict()\n    elif is_dataclass(obj):\n        return asdict(obj)\n    for base in obj.__class__.__mro__[:-1]:\n        try:\n            encoder = ENCODERS_BY_TYPE[base]\n        except KeyError:\n            continue\n        return encoder(obj)\n    else:\n        raise TypeError(f\"Object of type '{obj.__class__.__name__}' is not JSON serializable\")", "target": "def test_json_bytes_hex_round_trip():\n    data = b'hello'\n    encoded = b'\"68656c6c6f\"'\n    assert to_json(data, bytes_mode='hex') == encoded\n    v = SchemaValidator(core_schema.bytes_schema(), config=CoreConfig(val_json_bytes='hex'))\n    assert v.validate_json(encoded) == data\n    assert to_json({'key': data}, bytes_mode='hex') == b'{\"key\":\"68656c6c6f\"}'\n    v = SchemaValidator(\n        core_schema.dict_schema(keys_schema=core_schema.str_schema(), values_schema=core_schema.bytes_schema()),\n        config=CoreConfig(val_json_bytes='hex'),\n    )\n    assert v.validate_json('{\"key\":\"68656c6c6f\"}') == {'key': data}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005423", "source": "def _check_protected_namespaces(\n    protected_namespaces: tuple[str | Pattern[str], ...],\n    ann_name: str,\n    bases: tuple[type[Any], ...],\n    cls_name: str,\n) -> None:\n    BaseModel = import_cached_base_model()\n    for protected_namespace in protected_namespaces:\n        ns_violation = False\n        if isinstance(protected_namespace, Pattern):\n            ns_violation = protected_namespace.match(ann_name) is not None\n        elif isinstance(protected_namespace, str):\n            ns_violation = ann_name.startswith(protected_namespace)\n        if ns_violation:\n            for b in bases:\n                if hasattr(b, ann_name):\n                    if not (issubclass(b, BaseModel) and ann_name in getattr(b, '__pydantic_fields__', {})):\n                        raise ValueError(\n                            f'Field {ann_name!r} conflicts with member {getattr(b, ann_name)}'\n                            f' of protected namespace {protected_namespace!r}.'\n                        )\n            else:\n                valid_namespaces: list[str] = []\n                for pn in protected_namespaces:\n                    if isinstance(pn, Pattern):\n                        if not pn.match(ann_name):\n                            valid_namespaces.append(f're.compile({pn.pattern!r})')\n                    else:\n                        if not ann_name.startswith(pn):\n                            valid_namespaces.append(f\"'{pn}'\")\n                valid_namespaces_str = f'({\", \".join(valid_namespaces)}{\",)\" if len(valid_namespaces) == 1 else \")\"}'\n                warnings.warn(\n                    f'Field {ann_name!r} in {cls_name!r} conflicts with protected namespace {protected_namespace!r}.\\n\\n'\n                    f\"You may be able to solve this by setting the 'protected_namespaces' configuration to {valid_namespaces_str}.\",\n                    UserWarning,\n                    stacklevel=5,\n                )", "target": "def test_with_default_factory():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'x': core_schema.model_field(\n                    schema=core_schema.with_default_schema(\n                        schema=core_schema.str_schema(), default_factory=lambda: 'pikachu'\n                    )\n                )\n            }\n        )\n    )\n    assert v.validate_python({}) == ({'x': 'pikachu'}, None, set())\n    assert v.validate_python({'x': 'bulbi'}) == ({'x': 'bulbi'}, None, {'x'})"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005424", "source": "def dataclass(\n        _cls: type[_T],\n        *,\n        init: Literal[False] = False,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool | None = None,\n        config: ConfigDict | type[object] | None = None,\n        validate_on_init: bool | None = None,\n        kw_only: bool = ...,\n        slots: bool = ...,\n    ) -> type[PydanticDataclass]: ...", "target": "def test_dataclass():\n    schema = core_schema.dataclass_schema(\n        FooDataclass,\n        core_schema.dataclass_args_schema(\n            'FooDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 'hello', 'b': True})\n    assert dataclasses.is_dataclass(foo)\n    assert foo.a == 'hello'\n    assert foo.b is True\n    assert dataclasses.asdict(v.validate_python(FooDataclass(a='hello', b=True))) == {'a': 'hello', 'b': True}\n    with pytest.raises(ValidationError, match='Input should be an instance of FooDataclass') as exc_info:\n        v.validate_python({'a': 'hello', 'b': True}, strict=True)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'dataclass_exact_type',\n            'loc': (),\n            'msg': 'Input should be an instance of FooDataclass',\n            'input': {'a': 'hello', 'b': True},\n            'ctx': {'class_name': 'FooDataclass'},\n        }\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005425", "source": "def _apply_field_title_generator_to_field_info(\n    title_generator: Callable[[str, FieldInfo], str],\n    field_name: str,\n    field_info: FieldInfo,\n):\n    if field_info.title is None:\n        title = title_generator(field_name, field_info)\n        if not isinstance(title, str):\n            raise TypeError(f'field_title_generator {title_generator} must return str, not {title.__class__}')\n        field_info.title = title", "target": "def test_on_error_default(self, py_and_json: PyAndJson):\n        v = py_and_json(\n            {\n                'type': 'model-fields',\n                'fields': {\n                    'x': {\n                        'type': 'model-field',\n                        'schema': {\n                            'type': 'default',\n                            'schema': {'type': 'str'},\n                            'on_error': 'default',\n                            'default': 'pika',\n                        },\n                    }\n                },\n            }\n        )\n        assert v.validate_test({'x': 'foo'}) == ({'x': 'foo'}, None, {'x'})\n        assert v.validate_test({'x': ['foo']}) == ({'x': 'pika'}, None, {'x'})"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005426", "source": "def _call_initvar(*args: Any, **kwargs: Any) -> NoReturn:\n        raise TypeError(\"'InitVar' object is not callable\")", "target": "def test_dataclass_field_plain_validator():\n    @dataclasses.dataclass\n    class Foo:\n        a: int\n        b: str\n        @classmethod\n        def validate_b(cls, v: bytes, info: core_schema.ValidationInfo) -> str:\n            assert v == b'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return 'hello world!'\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(\n                    name='b', schema=core_schema.with_info_plain_validator_function(Foo.validate_b)\n                ),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 1, 'b': b'hello'})\n    assert dataclasses.asdict(foo) == {'a': 1, 'b': 'hello world!'}"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005427", "source": "def get_commit(self, ref: str) -> GitCommit:\n        return parse_fuller_format(\n            self._run_git(\"show\", \"--format=fuller\", \"--date=unix\", \"--shortstat\", ref)\n        )", "target": "def test_compute_diff(self) -> None:\n        diff = self.repo.diff(\"HEAD\")\n        sha = _shasum(diff)\n        self.assertEqual(len(sha), 64)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005428", "source": "def _is_field_cached_property(obj: 'Dataclass', k: str) -> bool:\n        return isinstance(getattr(type(obj), k, None), cached_property)", "target": "def test_frozen_field():\n    @dataclasses.dataclass\n    class MyModel:\n        f: str\n    v = SchemaValidator(\n        core_schema.dataclass_schema(\n            MyModel,\n            core_schema.dataclass_args_schema(\n                'MyModel', [core_schema.dataclass_field('f', core_schema.str_schema(), frozen=True)]\n            ),\n            ['f'],\n        )\n    )\n    m = v.validate_python({'f': 'x'})\n    assert m.f == 'x'\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_assignment(m, 'f', 'y')\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'frozen_field', 'loc': ('f',), 'msg': 'Field is frozen', 'input': 'y'}\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005429", "source": "def create_pydantic_model_from_dataclass(\n    dc_cls: Type['Dataclass'],\n    config: Type[Any] = BaseConfig,\n    dc_cls_doc: Optional[str] = None,\n) -> Type['BaseModel']:\n    field_definitions: Dict[str, Any] = {}\n    for field in dataclasses.fields(dc_cls):\n        default: Any = Undefined\n        default_factory: Optional['NoArgAnyCallable'] = None\n        field_info: FieldInfo\n        if field.default is not dataclasses.MISSING:\n            default = field.default\n        elif field.default_factory is not dataclasses.MISSING:\n            default_factory = field.default_factory\n        else:\n            default = Required\n        if isinstance(default, FieldInfo):\n            field_info = default\n            dc_cls.__pydantic_has_field_info_default__ = True\n        else:\n            field_info = Field(default=default, default_factory=default_factory, **field.metadata)\n        field_definitions[field.name] = (field.type, field_info)\n    validators = gather_all_validators(dc_cls)\n    model: Type['BaseModel'] = create_model(\n        dc_cls.__name__,\n        __config__=config,\n        __module__=dc_cls.__module__,\n        __validators__=validators,\n        __cls_kwargs__={'__resolve_forward_refs__': False},\n        **field_definitions,\n    )\n    model.__doc__ = dc_cls_doc if dc_cls_doc is not None else dc_cls.__doc__ or ''\n    return model", "target": "def test_dataclass_json_duplicate_keys():\n    @dataclasses.dataclass\n    class MyDataclass:\n        name: str\n        age: int\n    schema = core_schema.dataclass_schema(\n        MyDataclass,\n        core_schema.dataclass_args_schema(\n            'MyDataclass',\n            [\n                core_schema.dataclass_field(name='name', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='age', schema=core_schema.int_schema()),\n            ],\n        ),\n        ['name', 'age'],\n    )\n    v = SchemaValidator(schema)\n    json_with_duplicates = '{\"name\": \"Alice\", \"age\": 30, \"name\": \"Bob\", \"age\": 25}'\n    result = v.validate_json(json_with_duplicates)\n    assert result.name == 'Bob', \"Last value for 'name' should win\"\n    assert result.age == 25, \"Last value for 'age' should win\"\n    assert dataclasses.asdict(result) == {'name': 'Bob', 'age': 25}\n    json_multiple_duplicates = '{\"name\": \"First\", \"age\": 1, \"name\": \"Second\", \"name\": \"Third\", \"age\": 3}'\n    result2 = v.validate_json(json_multiple_duplicates)\n    assert result2.name == 'Third', 'Last value among multiple duplicates should win'\n    assert result2.age == 3\n    assert dataclasses.asdict(result2) == {'name': 'Third', 'age': 3}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005430", "source": "def dataclass(\n        *,\n        init: bool = True,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool = False,\n        config: Union[ConfigDict, Type[object], None] = None,\n        validate_on_init: Optional[bool] = None,\n        use_proxy: Optional[bool] = None,\n        kw_only: bool = ...,\n    ) -> Callable[[Type[_T]], 'DataclassClassOrWrapper']:\n        ...", "target": "def test_alias_allow_pop(py_and_json: PyAndJson):\n    schema = core_schema.dataclass_schema(\n        BasicDataclass,\n        core_schema.dataclass_args_schema(\n            'BasicDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), validation_alias='FieldA'),\n            ],\n        ),\n        ['a'],\n        config=core_schema.CoreConfig(validate_by_name=True, validate_by_alias=True),\n    )\n    v = py_and_json(schema)\n    assert v.validate_test({'FieldA': 'hello'}) == BasicDataclass(a='hello')\n    assert v.validate_test({'a': 'hello'}) == BasicDataclass(a='hello')\n    assert v.validate_test(\n        {\n            'FieldA': 'hello',\n            'a': 'world',\n        }\n    ) == BasicDataclass(a='hello')\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'foobar': 'hello'})"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005431", "source": "def set_validation(cls: Type['DataclassT'], value: bool) -> Generator[Type['DataclassT'], None, None]:\n    original_run_validation = cls.__pydantic_run_validation__\n    try:\n        cls.__pydantic_run_validation__ = value\n        yield cls\n    finally:\n        cls.__pydantic_run_validation__ = original_run_validation", "target": "def test_by_alias_and_name_config_interaction(\n    config_by_alias: Union[bool, None],\n    config_by_name: Union[bool, None],\n    runtime_by_alias: Union[bool, None],\n    runtime_by_name: Union[bool, None],\n) -> None:\n    if config_by_alias is False and config_by_name is False and runtime_by_alias is False and runtime_by_name is False:\n        pytest.skip(\"Can't have both by_alias and by_name as effectively False\")\n    core_config = {\n        **({'validate_by_alias': config_by_alias} if config_by_alias is not None else {}),\n        **({'validate_by_name': config_by_name} if config_by_name is not None else {}),\n    }\n    @dataclasses.dataclass\n    class MyDataclass:\n        my_field: int\n    schema = core_schema.dataclass_schema(\n        MyDataclass,\n        core_schema.dataclass_args_schema(\n            'MyDataclass',\n            [\n                core_schema.dataclass_field(\n                    name='my_field', schema=core_schema.int_schema(), validation_alias='my_alias'\n                ),\n            ],\n        ),\n        ['my_field'],\n        config=core_schema.CoreConfig(**core_config),\n    )\n    s = SchemaValidator(schema)\n    alias_allowed = next(x for x in (runtime_by_alias, config_by_alias, True) if x is not None)\n    name_allowed = next(x for x in (runtime_by_name, config_by_name, False) if x is not None)\n    if alias_allowed:\n        assert dataclasses.asdict(\n            s.validate_python({'my_alias': 1}, by_alias=runtime_by_alias, by_name=runtime_by_name)\n        ) == {'my_field': 1}\n    if name_allowed:\n        assert dataclasses.asdict(\n            s.validate_python({'my_field': 1}, by_alias=runtime_by_alias, by_name=runtime_by_name)\n        ) == {'my_field': 1}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005432", "source": "def create_dataclass(cls: type[Any]) -> type[PydanticDataclass]:\n        from ._internal._utils import is_model_class\n        if is_model_class(cls):\n            raise PydanticUserError(\n                f'Cannot create a Pydantic dataclass from {cls.__name__} as it is already a Pydantic model',\n                code='dataclass-on-model',\n            )\n        original_cls = cls\n        has_dataclass_base = any(dataclasses.is_dataclass(base) for base in cls.__bases__)\n        if not has_dataclass_base and config is not None and hasattr(cls, '__pydantic_config__'):\n            warn(\n                f'`config` is set via both the `dataclass` decorator and `__pydantic_config__` for dataclass {cls.__name__}. '\n                f'The `config` specification from `dataclass` decorator will take priority.',\n                category=UserWarning,\n                stacklevel=2,\n            )\n        config_dict = config if config is not None else getattr(cls, '__pydantic_config__', None)\n        config_wrapper = _config.ConfigWrapper(config_dict)\n        decorators = _decorators.DecoratorInfos.build(cls, replace_wrapped_methods=True)\n        decorators.update_from_config(config_wrapper)\n        original_doc = cls.__doc__\n        if _pydantic_dataclasses.is_stdlib_dataclass(cls):\n            original_doc = None\n            bases = (cls,)\n            if issubclass(cls, Generic):\n                generic_base = Generic[cls.__parameters__]\n                bases = bases + (generic_base,)\n            cls = types.new_class(cls.__name__, bases)\n        if frozen is not None:\n            frozen_ = frozen\n            if config_wrapper.frozen:\n                warn(\n                    f'`frozen` is set via both the `dataclass` decorator and `config` for dataclass {cls.__name__!r}.'\n                    'This is not recommended. The `frozen` specification on `dataclass` will take priority.',\n                    category=UserWarning,\n                    stacklevel=2,\n                )\n        else:\n            frozen_ = config_wrapper.frozen or False\n        cls_anns = _typing_extra.safe_get_annotations(cls)\n        for field_name in cls_anns:\n            field_value = getattr(cls, field_name, None)\n            if isinstance(field_value, FieldInfo):\n                setattr(cls, field_name, _pydantic_dataclasses.as_dataclass_field(field_value))\n        with _pydantic_dataclasses.patch_base_fields(cls):\n            cls = dataclasses.dataclass(\n                cls,\n                init=True,\n                repr=repr,\n                eq=eq,\n                order=order,\n                unsafe_hash=unsafe_hash,\n                frozen=frozen_,\n                **kwargs,\n            )\n        if config_wrapper.validate_assignment:\n            original_setattr = cls.__setattr__\n            @functools.wraps(cls.__setattr__)\n            def validated_setattr(instance: PydanticDataclass, name: str, value: Any, /) -> None:\n                if frozen_:\n                    return original_setattr(instance, name, value)\n                inst_cls = type(instance)\n                attr = getattr(inst_cls, name, None)\n                if isinstance(attr, property):\n                    attr.__set__(instance, value)\n                elif isinstance(attr, functools.cached_property):\n                    instance.__dict__.__setitem__(name, value)\n                else:\n                    inst_cls.__pydantic_validator__.validate_assignment(instance, name, value)\n            cls.__setattr__ = validated_setattr.__get__(None, cls)\n            if slots and not hasattr(cls, '__setstate__'):\n                def _dataclass_getstate(self: Any) -> list[Any]:\n                    return [getattr(self, f.name) for f in dataclasses.fields(self)]\n                def _dataclass_setstate(self: Any, state: list[Any]) -> None:\n                    for field, value in zip(dataclasses.fields(self), state):\n                        object.__setattr__(self, field.name, value)\n                cls.__getstate__ = _dataclass_getstate\n                cls.__setstate__ = _dataclass_setstate\n        cls.__is_pydantic_dataclass__ = True\n        cls.__pydantic_decorators__ = decorators\n        cls.__doc__ = original_doc\n        firstlineno = getattr(original_cls, '__firstlineno__', None)\n        cls.__module__ = original_cls.__module__\n        if sys.version_info >= (3, 13) and firstlineno is not None:\n            original_cls.__firstlineno__ = firstlineno\n            cls.__firstlineno__ = firstlineno\n        cls.__qualname__ = original_cls.__qualname__\n        cls.__pydantic_fields_complete__ = classmethod(_pydantic_fields_complete)\n        cls.__pydantic_complete__ = False\n        _pydantic_dataclasses.complete_dataclass(cls, config_wrapper, raise_errors=False)\n        return cls", "target": "def test_dataclass():\n    schema = core_schema.dataclass_schema(\n        FooDataclass,\n        core_schema.dataclass_args_schema(\n            'FooDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 'hello', 'b': True})\n    assert dataclasses.is_dataclass(foo)\n    assert foo.a == 'hello'\n    assert foo.b is True\n    assert dataclasses.asdict(v.validate_python(FooDataclass(a='hello', b=True))) == {'a': 'hello', 'b': True}\n    with pytest.raises(ValidationError, match='Input should be an instance of FooDataclass') as exc_info:\n        v.validate_python({'a': 'hello', 'b': True}, strict=True)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'dataclass_exact_type',\n            'loc': (),\n            'msg': 'Input should be an instance of FooDataclass',\n            'input': {'a': 'hello', 'b': True},\n            'ctx': {'class_name': 'FooDataclass'},\n        }\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005433", "source": "def _dataclass_validate_assignment_setattr(self: 'Dataclass', name: str, value: Any) -> None:\n    if self.__pydantic_initialised__:\n        d = dict(self.__dict__)\n        d.pop(name, None)\n        known_field = self.__pydantic_model__.__fields__.get(name, None)\n        if known_field:\n            value, error_ = known_field.validate(value, d, loc=name, cls=self.__class__)\n            if error_:\n                raise ValidationError([error_], self.__class__)\n    object.__setattr__(self, name, value)", "target": "def test_extra_behavior_ignore(\n    config: Union[core_schema.CoreConfig, None],\n    schema_extra_behavior_kw: dict[str, Any],\n    validate_fn_extra_kw: Union[ExtraBehavior, None],\n):\n    @dataclasses.dataclass\n    class MyModel:\n        f: str\n    v = SchemaValidator(\n        core_schema.dataclass_schema(\n            MyModel,\n            core_schema.dataclass_args_schema(\n                'MyModel', [core_schema.dataclass_field('f', core_schema.str_schema())], **schema_extra_behavior_kw\n            ),\n            ['f'],\n        ),\n        config=config,\n    )\n    m: MyModel = v.validate_python({'f': 'x', 'extra_field': 123}, extra=validate_fn_extra_kw)\n    assert m.f == 'x'\n    assert not hasattr(m, 'extra_field')\n    v.validate_assignment(m, 'f', 'y', extra=validate_fn_extra_kw)\n    assert m.f == 'y'\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_assignment(m, 'not_f', 'xyz', extra=validate_fn_extra_kw)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'no_such_attribute',\n            'loc': ('not_f',),\n            'msg': \"Object has no attribute 'not_f'\",\n            'input': 'xyz',\n            'ctx': {'attribute': 'not_f'},\n        }\n    ]\n    assert not hasattr(m, 'not_f')"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005434", "source": "def rebuild_dataclass(\n    cls: type[PydanticDataclass],\n    *,\n    force: bool = False,\n    raise_errors: bool = True,\n    _parent_namespace_depth: int = 2,\n    _types_namespace: MappingNamespace | None = None,\n) -> bool | None:\n    if not force and cls.__pydantic_complete__:\n        return None\n    for attr in ('__pydantic_core_schema__', '__pydantic_validator__', '__pydantic_serializer__'):\n        if attr in cls.__dict__ and not isinstance(getattr(cls, attr), _mock_val_ser.MockValSer):\n            delattr(cls, attr)\n    cls.__pydantic_complete__ = False\n    if _types_namespace is not None:\n        rebuild_ns = _types_namespace\n    elif _parent_namespace_depth > 0:\n        rebuild_ns = _typing_extra.parent_frame_namespace(parent_depth=_parent_namespace_depth, force=True) or {}\n    else:\n        rebuild_ns = {}\n    ns_resolver = _namespace_utils.NsResolver(\n        parent_namespace=rebuild_ns,\n    )\n    return _pydantic_dataclasses.complete_dataclass(\n        cls,\n        _config.ConfigWrapper(cls.__pydantic_config__, check=False),\n        raise_errors=raise_errors,\n        ns_resolver=ns_resolver,\n        _force_build=True,\n    )", "target": "def test_extra_custom_serializer():\n    @dataclasses.dataclass\n    class Model:\n        pass\n    schema = core_schema.dataclass_schema(\n        Model,\n        core_schema.dataclass_args_schema(\n            'Model',\n            [],\n            extra_behavior='allow',\n        ),\n        [],\n    )\n    s = SchemaSerializer(schema)\n    v = SchemaValidator(schema)\n    m = v.validate_python({'extra': 'extra'})\n    assert s.to_python(m) == {'extra': 'extra bam!'}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005435", "source": "def collect_dataclass_fields(\n    cls: type[StandardDataclass],\n    *,\n    config_wrapper: ConfigWrapper,\n    ns_resolver: NsResolver | None = None,\n    typevars_map: dict[Any, Any] | None = None,\n) -> dict[str, FieldInfo]:\n    FieldInfo_ = import_cached_field_info()\n    fields: dict[str, FieldInfo] = {}\n    ns_resolver = ns_resolver or NsResolver()\n    dataclass_fields = cls.__dataclass_fields__\n    for base in reversed(cls.__mro__):\n        if not dataclasses.is_dataclass(base):\n            continue\n        with ns_resolver.push(base):\n            for ann_name, dataclass_field in dataclass_fields.items():\n                base_anns = _typing_extra.safe_get_annotations(base)\n                if ann_name not in base_anns:\n                    continue\n                globalns, localns = ns_resolver.types_namespace\n                ann_type, evaluated = _typing_extra.try_eval_type(dataclass_field.type, globalns, localns)\n                if _typing_extra.is_classvar_annotation(ann_type):\n                    continue\n                if (\n                    not dataclass_field.init\n                    and dataclass_field.default is dataclasses.MISSING\n                    and dataclass_field.default_factory is dataclasses.MISSING\n                ):\n                    continue\n                if isinstance(dataclass_field.default, FieldInfo_):\n                    if dataclass_field.default.init_var:\n                        if dataclass_field.default.init is False:\n                            raise PydanticUserError(\n                                f'Dataclass field {ann_name} has init=False and init_var=True, but these are mutually exclusive.',\n                                code='clashing-init-and-init-var',\n                            )\n                        continue\n                    field_info = FieldInfo_.from_annotated_attribute(\n                        ann_type, dataclass_field.default, _source=AnnotationSource.DATACLASS\n                    )\n                    field_info._original_assignment = dataclass_field.default\n                else:\n                    field_info = FieldInfo_.from_annotated_attribute(\n                        ann_type, dataclass_field, _source=AnnotationSource.DATACLASS\n                    )\n                    field_info._original_assignment = dataclass_field\n                if not evaluated:\n                    field_info._complete = False\n                    field_info._original_annotation = ann_type\n                fields[ann_name] = field_info\n                update_field_from_config(config_wrapper, ann_name, field_info)\n                if field_info.default is not PydanticUndefined and isinstance(\n                    getattr(cls, ann_name, field_info), FieldInfo_\n                ):\n                    setattr(cls, ann_name, field_info.default)\n    if typevars_map:\n        for field in fields.values():\n            field.apply_typevars_map(typevars_map)\n    if config_wrapper.use_attribute_docstrings:\n        _update_fields_from_docstrings(\n            cls,\n            fields,\n            use_inspect=not hasattr(cls, '__is_pydantic_dataclass__'),\n        )\n    return fields", "target": "def test_bad_default_factory(default_factory, error_message):\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'x': core_schema.model_field(\n                    schema=core_schema.with_default_schema(\n                        schema=core_schema.str_schema(), default_factory=default_factory\n                    )\n                )\n            }\n        )\n    )\n    with pytest.raises(TypeError, match=re.escape(error_message)):\n        v.validate_python({})"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005436", "source": "def set_dataclass_fields(\n    cls: type[StandardDataclass],\n    config_wrapper: _config.ConfigWrapper,\n    ns_resolver: NsResolver | None = None,\n) -> None:\n    typevars_map = get_standard_typevars_map(cls)\n    fields = collect_dataclass_fields(\n        cls, ns_resolver=ns_resolver, typevars_map=typevars_map, config_wrapper=config_wrapper\n    )\n    cls.__pydantic_fields__ = fields", "target": "def test_function_validator_wrapping_args_schema_wrap() -> None:\n    calls: list[Any] = []\n    def func(*args: Any) -> Any:\n        assert len(args) == 2\n        input, handler = args\n        output = handler(input)\n        calls.append((input, output))\n        return output\n    @dataclasses.dataclass\n    class Model:\n        number: int = 1\n    cs = core_schema.dataclass_schema(\n        Model,\n        core_schema.no_info_wrap_validator_function(\n            func,\n            core_schema.dataclass_args_schema(\n                'Model', [core_schema.dataclass_field('number', core_schema.int_schema())]\n            ),\n        ),\n        ['number'],\n    )\n    v = SchemaValidator(cs)\n    instance: Model = v.validate_python({'number': 1})\n    assert instance.number == 1\n    assert calls == [({'number': 1}, ({'number': 1}, None))]\n    v.validate_assignment(instance, 'number', 2)\n    assert instance.number == 2\n    assert calls == [({'number': 1}, ({'number': 1}, None)), ({'number': 2}, ({'number': 2}, None))]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005437", "source": "def _dataclass_validate_values(self: 'Dataclass') -> None:\n    if getattr(self, '__pydantic_initialised__'):\n        return\n    if getattr(self, '__pydantic_has_field_info_default__', False):\n        input_data = {\n            k: v\n            for k, v in self.__dict__.items()\n            if not (isinstance(v, FieldInfo) or _is_field_cached_property(self, k))\n        }\n    else:\n        input_data = {k: v for k, v in self.__dict__.items() if not _is_field_cached_property(self, k)}\n    d, _, validation_error = validate_model(self.__pydantic_model__, input_data, cls=self.__class__)\n    if validation_error:\n        raise validation_error\n    self.__dict__.update(d)\n    object.__setattr__(self, '__pydantic_initialised__', True)", "target": "def test_dataclass_field_after_validator():\n    @dataclasses.dataclass\n    class Foo:\n        a: int\n        b: str\n        @classmethod\n        def validate_b(cls, v: str, info: core_schema.ValidationInfo) -> str:\n            assert v == 'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return 'hello world!'\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(\n                    name='b',\n                    schema=core_schema.with_info_after_validator_function(Foo.validate_b, core_schema.str_schema()),\n                ),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 1, 'b': b'hello'})\n    assert dataclasses.asdict(foo) == {'a': 1, 'b': 'hello world!'}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005438", "source": "def pydantic_general_metadata(**metadata: Any) -> BaseMetadata:\n    return _general_metadata_cls()(metadata)", "target": "def test_only_allow_alias(py_and_json) -> None:\n    v = py_and_json(\n        {\n            'type': 'model-fields',\n            'fields': {'field_a': {'validation_alias': 'FieldA', 'type': 'model-field', 'schema': {'type': 'int'}}},\n        },\n        config=CoreConfig(validate_by_name=False, validate_by_alias=True),\n    )\n    assert v.validate_test({'FieldA': '123'}) == ({'field_a': 123}, None, {'field_a'})\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'field_a': '123'})"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005439", "source": "def custom_pydantic_encoder(type_encoders: Dict[Any, Callable[[Type[Any]], Any]], obj: Any) -> Any:\n    for base in obj.__class__.__mro__[:-1]:\n        try:\n            encoder = type_encoders[base]\n        except KeyError:\n            continue\n        return encoder(obj)\n    else:\n        return pydantic_encoder(obj)", "target": "def test_json_bytes_base64_round_trip():\n    data = b'\\xd8\\x07\\xc1Tx$\\x91F%\\xf3\\xf3I\\xca\\xd8@\\x0c\\xee\\xc3\\xab\\xff\\x7f\\xd3\\xcd\\xcd\\xf9\\xc2\\x10\\xe4\\xa1\\xb01e'\n    encoded_std = b'\"2AfBVHgkkUYl8/NJythADO7Dq/9/083N+cIQ5KGwMWU=\"'\n    encoded_url = b'\"2AfBVHgkkUYl8_NJythADO7Dq_9_083N-cIQ5KGwMWU=\"'\n    assert to_json(data, bytes_mode='base64') == encoded_url\n    v = SchemaValidator(core_schema.bytes_schema(), config=CoreConfig(val_json_bytes='base64'))\n    assert v.validate_json(encoded_url) == data\n    assert v.validate_json(encoded_std) == data\n    with pytest.raises(ValidationError) as exc:\n        v.validate_json('\"wrong!\"')\n    [details] = exc.value.errors()\n    assert details['type'] == 'bytes_invalid_encoding'\n    assert to_json({'key': data}, bytes_mode='base64') == b'{\"key\":' + encoded_url + b'}'\n    v = SchemaValidator(\n        core_schema.dict_schema(keys_schema=core_schema.str_schema(), values_schema=core_schema.bytes_schema()),\n        config=CoreConfig(val_json_bytes='base64'),\n    )\n    assert v.validate_json(b'{\"key\":' + encoded_url + b'}') == {'key': data}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005440", "source": "def dataclass(\n        *,\n        init: Literal[False] = False,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool | None = None,\n        config: ConfigDict | type[object] | None = None,\n        validate_on_init: bool | None = None,\n    ) -> Callable[[type[_T]], type[PydanticDataclass]]:\n        ...", "target": "def test_dataclass_args_init_only_no_fields(py_and_json: PyAndJson, input_value, expected):\n    schema = core_schema.dataclass_args_schema(\n        'MyDataclass', [core_schema.dataclass_field(name='a', schema=core_schema.str_schema())], collect_init_only=True\n    )\n    v = py_and_json(schema)\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=expected.message) as exc_info:\n            v.validate_test(input_value)\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        assert v.validate_test(input_value) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005441", "source": "def ordered_dict_validator(v: Any) -> 'AnyOrderedDict':\n    if isinstance(v, OrderedDict):\n        return v\n    try:\n        return OrderedDict(v)\n    except (TypeError, ValueError):\n        raise errors.DictError()", "target": "def test_validators_build(benchmark) -> None:\n    class Base1(BaseModel):\n        a: int\n        @field_validator('a', mode='after')\n        @classmethod\n        def val_a(cls, value: int) -> int: ...\n        @computed_field\n        def prop(self) -> int: ...\n    class Bare:\n        @computed_field\n        def prop_bare(self) -> int: ...\n    class Sub1(Base1):\n        @computed_field\n        def prop_2(self) -> int: ...\n        @computed_field\n        def prop_3(self) -> int: ...\n        @computed_field\n        def prop_4(self) -> int: ...\n    @benchmark\n    def bench() -> None:\n        class SubS(Sub1, Bare, defer_build=True):\n            @computed_field\n            def prop_5(self) -> int: ...\n            @computed_field\n            def prop_6(self) -> int: ...\n            @computed_field\n            def prop_7(self) -> int: ..."}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005442", "source": "def isoformat(o: Union[datetime.date, datetime.time]) -> str:\n    return o.isoformat()", "target": "def test_float_no_remainder():\n    v = SchemaValidator(core_schema.int_schema())\n    assert v.validate_json('123.0') == 123"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005443", "source": "def as_dataclass_field(pydantic_field: FieldInfo) -> dataclasses.Field[Any]:\n    field_args: dict[str, Any] = {'default': pydantic_field}\n    if sys.version_info >= (3, 14) and pydantic_field.description is not None:\n        field_args['doc'] = pydantic_field.description\n    if sys.version_info >= (3, 10) and pydantic_field.kw_only:\n        field_args['kw_only'] = True\n    if pydantic_field.repr is not True:\n        field_args['repr'] = pydantic_field.repr\n    return dataclasses.field(**field_args)", "target": "def test_by_alias_and_name_config_interaction(config, runtime, expected) -> None:\n    @dataclasses.dataclass\n    class Foo:\n        my_field: str\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(\n                    name='my_field', schema=core_schema.str_schema(), serialization_alias='my_alias'\n                ),\n            ],\n        ),\n        ['my_field'],\n        config=core_schema.CoreConfig(serialize_by_alias=config or False),\n    )\n    s = SchemaSerializer(schema)\n    assert s.to_python(Foo(my_field='hello'), by_alias=runtime) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005444", "source": "def run_command(\n    cmd: str,\n    use_shell: bool = False,\n    log_cmd: bool = True,\n    cwd: Optional[str] = None,\n    env: Optional[dict] = None,\n    check: bool = True,\n) -> int:\n    if use_shell:\n        args = cmd\n        log_prefix = \"[shell]\"\n        executable = \"/bin/bash\"\n    else:\n        args = shlex.split(cmd)\n        log_prefix = \"[cmd]\"\n        executable = None\n    if log_cmd:\n        display_cmd = cmd if use_shell else \" \".join(args)\n        logger.info(\"%s %s\", log_prefix, display_cmd)\n    run_env = {**os.environ, **(env or {})}\n    proc = subprocess.run(\n        args,\n        shell=use_shell,\n        executable=executable,\n        stdout=sys.stdout,\n        stderr=sys.stderr,\n        cwd=cwd,\n        env=run_env,\n        check=False,\n    )\n    if check and proc.returncode != 0:\n        logger.error(\n            \"%s Command failed (exit %s): %s\", log_prefix, proc.returncode, cmd\n        )\n        raise subprocess.CalledProcessError(\n            proc.returncode, args if not use_shell else cmd\n        )\n    return proc.returncode", "target": "def test_changes_and_restores(self):\n        start = Path.cwd()\n        with tempfile.TemporaryDirectory() as td:\n            target = Path(td) / \"wd\"\n            target.mkdir()\n            with working_directory(str(target)):\n                self.assertEqual(Path.cwd().resolve(), target.resolve())\n        self.assertEqual(Path.cwd(), start)"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005445", "source": "def _flatten(\n    key_prefix: Label, sub_schema: Definition, result: FlatIntermediateDefinition\n) -> None:\n    for k, value in sub_schema.items():\n        if isinstance(k, tuple):\n            assert all(isinstance(ki, str) for ki in k)\n            key_suffix: Label = k\n        elif k is None:\n            key_suffix = ()\n        else:\n            assert isinstance(k, str)\n            key_suffix = (k,)\n        key: Label = key_prefix + key_suffix\n        if isinstance(value, (TimerArgs, GroupedBenchmark)):\n            assert key not in result, f\"duplicate key: {key}\"\n            result[key] = value\n        else:\n            assert isinstance(value, dict)\n            _flatten(key_prefix=key, sub_schema=value, result=result)", "target": "def test_merged_lastfailed_content_with_empty_source(self) -> None:\n        last_failed_source = {\n            \"\": True,\n        }\n        last_failed_dest = {\n            \"tools/tests/test_car.py::test_num1\": True,\n            \"tools/tests/test_car.py::test_num2\": True,\n        }\n        last_failed_merged = {\n            \"tools/tests/test_car.py::test_num1\": True,\n            \"tools/tests/test_car.py::test_num2\": True,\n        }\n        merged = _merged_lastfailed_content(last_failed_source, last_failed_dest)\n        self.assertEqual(merged, last_failed_merged)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005446", "source": "def isoformat(o: Union[datetime.date, datetime.time]) -> str:\n    return o.isoformat()", "target": "def test_json_bytes_base64_round_trip():\n    data = b'\\xd8\\x07\\xc1Tx$\\x91F%\\xf3\\xf3I\\xca\\xd8@\\x0c\\xee\\xc3\\xab\\xff\\x7f\\xd3\\xcd\\xcd\\xf9\\xc2\\x10\\xe4\\xa1\\xb01e'\n    encoded_std = b'\"2AfBVHgkkUYl8/NJythADO7Dq/9/083N+cIQ5KGwMWU=\"'\n    encoded_url = b'\"2AfBVHgkkUYl8_NJythADO7Dq_9_083N-cIQ5KGwMWU=\"'\n    assert to_json(data, bytes_mode='base64') == encoded_url\n    v = SchemaValidator(core_schema.bytes_schema(), config=CoreConfig(val_json_bytes='base64'))\n    assert v.validate_json(encoded_url) == data\n    assert v.validate_json(encoded_std) == data\n    with pytest.raises(ValidationError) as exc:\n        v.validate_json('\"wrong!\"')\n    [details] = exc.value.errors()\n    assert details['type'] == 'bytes_invalid_encoding'\n    assert to_json({'key': data}, bytes_mode='base64') == b'{\"key\":' + encoded_url + b'}'\n    v = SchemaValidator(\n        core_schema.dict_schema(keys_schema=core_schema.str_schema(), values_schema=core_schema.bytes_schema()),\n        config=CoreConfig(val_json_bytes='base64'),\n    )\n    assert v.validate_json(b'{\"key\":' + encoded_url + b'}') == {'key': data}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005447", "source": "def handle_extra_init(self: 'Dataclass', *args: Any, **kwargs: Any) -> None:\n        if config.extra == Extra.ignore:\n            init(self, *args, **{k: v for k, v in kwargs.items() if k in self.__dataclass_fields__})\n        elif config.extra == Extra.allow:\n            for k, v in kwargs.items():\n                self.__dict__.setdefault(k, v)\n            init(self, *args, **{k: v for k, v in kwargs.items() if k in self.__dataclass_fields__})\n        else:\n            init(self, *args, **kwargs)", "target": "def test_validate_assignment_function():\n    @dataclasses.dataclass\n    class MyDataclass:\n        field_a: str\n        field_b: int\n        field_c: int\n    calls = []\n    def func(x, info):\n        calls.append(str(info))\n        return x * 2\n    v = SchemaValidator(\n        core_schema.dataclass_schema(\n            MyDataclass,\n            core_schema.dataclass_args_schema(\n                'MyDataclass',\n                [\n                    core_schema.dataclass_field('field_a', core_schema.str_schema()),\n                    core_schema.dataclass_field(\n                        'field_b',\n                        core_schema.with_info_after_validator_function(func, core_schema.int_schema()),\n                    ),\n                    core_schema.dataclass_field('field_c', core_schema.int_schema()),\n                ],\n            ),\n            ['field_a', 'field_b', 'field_c'],\n        )\n    )\n    m = v.validate_python({'field_a': 'x', 'field_b': 123, 'field_c': 456})\n    assert m.field_a == 'x'\n    assert m.field_b == 246\n    assert m.field_c == 456\n    assert calls == [\"ValidationInfo(config=None, context=None, data={'field_a': 'x'}, field_name='field_b')\"]\n    v.validate_assignment(m, 'field_b', '111')\n    assert m.field_b == 222\n    assert calls == [\n        \"ValidationInfo(config=None, context=None, data={'field_a': 'x'}, field_name='field_b')\",\n        \"ValidationInfo(config=None, context=None, data={'field_a': 'x', 'field_c': 456}, field_name='field_b')\",\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005448", "source": "def benchmark_module(config, module, use_throughput_benchmark=False):\n    if use_throughput_benchmark:\n        return benchmark_using_throughput_benchmark(config, module)\n    module.forward(config.num_warmup_iters)\n    print(f\"Running module for {config.num_iters} iterations\")\n    start = time.time()\n    module.forward(config.num_iters)\n    end = time.time()\n    time_elapsed_s = end - start\n    return secs_to_ms(time_elapsed_s) / config.num_iters / NUM_LOOP_ITERS", "target": "def test_pr_with_missing_labels(\n        self, mocked_rn_labels: Any, mocked_gql: Any\n    ) -> None:\n        \"Test PR with no 'release notes:' label or 'topic: not user facing' label\"\n        pr = GitHubPR(\"pytorch\", \"pytorch\", 82169)\n        self.assertFalse(has_required_labels(pr))"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005449", "source": "def decode_one_token(\n    model: torch.nn.Module, x: torch.Tensor, input_pos: torch.Tensor, **sampling_kwargs\n) -> tuple[torch.Tensor, torch.Tensor]:\n    assert input_pos.shape[-1] == 1\n    logits = model(x, input_pos)\n    return sample(logits, **sampling_kwargs)", "target": "def testAliasWithOffset(self) -> list[Tensor]:\n        x = torch.tensor([100, 200])\n        a = [x[0], x[1]]\n        return a"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005450", "source": "def _check_protected_namespaces(\n    protected_namespaces: tuple[str | Pattern[str], ...],\n    ann_name: str,\n    bases: tuple[type[Any], ...],\n    cls_name: str,\n) -> None:\n    BaseModel = import_cached_base_model()\n    for protected_namespace in protected_namespaces:\n        ns_violation = False\n        if isinstance(protected_namespace, Pattern):\n            ns_violation = protected_namespace.match(ann_name) is not None\n        elif isinstance(protected_namespace, str):\n            ns_violation = ann_name.startswith(protected_namespace)\n        if ns_violation:\n            for b in bases:\n                if hasattr(b, ann_name):\n                    if not (issubclass(b, BaseModel) and ann_name in getattr(b, '__pydantic_fields__', {})):\n                        raise ValueError(\n                            f'Field {ann_name!r} conflicts with member {getattr(b, ann_name)}'\n                            f' of protected namespace {protected_namespace!r}.'\n                        )\n            else:\n                valid_namespaces: list[str] = []\n                for pn in protected_namespaces:\n                    if isinstance(pn, Pattern):\n                        if not pn.match(ann_name):\n                            valid_namespaces.append(f're.compile({pn.pattern!r})')\n                    else:\n                        if not ann_name.startswith(pn):\n                            valid_namespaces.append(f\"'{pn}'\")\n                valid_namespaces_str = f'({\", \".join(valid_namespaces)}{\",)\" if len(valid_namespaces) == 1 else \")\"}'\n                warnings.warn(\n                    f'Field {ann_name!r} in {cls_name!r} conflicts with protected namespace {protected_namespace!r}.\\n\\n'\n                    f\"You may be able to solve this by setting the 'protected_namespaces' configuration to {valid_namespaces_str}.\",\n                    UserWarning,\n                    stacklevel=5,\n                )", "target": "def test_fields_required_by_default_with_default():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'x': core_schema.model_field(schema=core_schema.str_schema()),\n                'y': core_schema.model_field(\n                    schema=core_schema.with_default_schema(schema=core_schema.str_schema(), default='bulbi')\n                ),\n            }\n        )\n    )\n    assert v.validate_python({'x': 'pika', 'y': 'chu'}) == ({'x': 'pika', 'y': 'chu'}, None, {'x', 'y'})\n    assert v.validate_python({'x': 'pika'}) == ({'x': 'pika', 'y': 'bulbi'}, None, {'x'})"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005451", "source": "def custom_pydantic_encoder(type_encoders: dict[Any, Callable[[type[Any]], Any]], obj: Any) -> Any:\n    warnings.warn(\n        '`custom_pydantic_encoder` is deprecated, use `BaseModel.model_dump` instead.',\n        category=PydanticDeprecatedSince20,\n        stacklevel=2,\n    )\n    for base in obj.__class__.__mro__[:-1]:\n        try:\n            encoder = type_encoders[base]\n        except KeyError:\n            continue\n        return encoder(obj)\n    else:\n        return pydantic_encoder(obj)", "target": "def test_json_bytes_hex_invalid():\n    v = SchemaValidator(core_schema.bytes_schema(), config=CoreConfig(val_json_bytes='hex'))\n    wrong_input = 'a'\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_json(json.dumps(wrong_input))\n    assert exc_info.value.errors(include_url=False, include_context=False) == [\n        {\n            'type': 'bytes_invalid_encoding',\n            'loc': (),\n            'msg': 'Data should be valid hex: Odd number of digits',\n            'input': wrong_input,\n        }\n    ]\n    wrong_input = 'ag'\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_json(json.dumps(wrong_input))\n    assert exc_info.value.errors(include_url=False, include_context=False) == [\n        {\n            'type': 'bytes_invalid_encoding',\n            'loc': (),\n            'msg': \"Data should be valid hex: Invalid character 'g' at position 1\",\n            'input': wrong_input,\n        }\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005452", "source": "def _check_finalvar(v: Optional[Type[Any]]) -> bool:\n    if v is None:\n        return False\n    return v.__class__ == Final.__class__ and (sys.version_info < (3, 8) or getattr(v, '_name', None) == 'Final')", "target": "def test_correct_function_signature() -> None:\n    def my_validator(value: Any, info: Any) -> str:\n        return str(value)\n    v = SchemaValidator(core_schema.with_info_plain_validator_function(my_validator))\n    assert v.validate_python(1) == '1'"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005453", "source": "def decimal_encoder(dec_value: Decimal) -> Union[int, float]:\n    exponent = dec_value.as_tuple().exponent\n    if isinstance(exponent, int) and exponent >= 0:\n        return int(dec_value)\n    else:\n        return float(dec_value)", "target": "def test_to_jsonable_python_fallback():\n    with pytest.raises(PydanticSerializationError, match=r'Unable to serialize unknown type: <.+\\.Foobar'):\n        to_jsonable_python(Foobar())\n    assert to_jsonable_python(Foobar(), serialize_unknown=True) == 'Foobar.__str__'\n    assert to_jsonable_python(Foobar(), serialize_unknown=True, fallback=fallback_func) == 'fallback:Foobar'\n    assert to_jsonable_python(Foobar(), fallback=fallback_func) == 'fallback:Foobar'"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005454", "source": "def _recreate_field_info(\n    field_info: FieldInfo,\n    ns_resolver: NsResolver,\n    typevars_map: Mapping[TypeVar, Any],\n    *,\n    lenient: bool,\n) -> FieldInfo:\n    FieldInfo_ = import_cached_field_info()\n    existing_desc = field_info.description\n    if lenient:\n        ann = _generics.replace_types(field_info._original_annotation, typevars_map)\n        ann, evaluated = _typing_extra.try_eval_type(\n            ann,\n            *ns_resolver.types_namespace,\n        )\n    else:\n        ann = _typing_extra.eval_type(\n            field_info._original_annotation,\n            *ns_resolver.types_namespace,\n        )\n        ann = _generics.replace_types(ann, typevars_map)\n        ann = _typing_extra.eval_type(\n            ann,\n            *ns_resolver.types_namespace,\n        )\n        evaluated = True\n    if (assign := field_info._original_assignment) is PydanticUndefined:\n        new_field = FieldInfo_.from_annotation(ann, _source=AnnotationSource.CLASS)\n    else:\n        new_field = FieldInfo_.from_annotated_attribute(ann, assign, _source=AnnotationSource.CLASS)\n        new_field._original_assignment = assign\n    new_field._original_annotation = ann\n    new_field.description = new_field.description if new_field.description is not None else existing_desc\n    if not evaluated:\n        new_field._complete = False\n    return new_field", "target": "def test_on_error_raise_by_default(self, py_and_json: PyAndJson):\n        v = py_and_json({'type': 'model-fields', 'fields': {'x': {'type': 'model-field', 'schema': {'type': 'str'}}}})\n        assert v.validate_test({'x': 'foo'}) == ({'x': 'foo'}, None, {'x'})\n        with pytest.raises(ValidationError) as exc_info:\n            v.validate_test({'x': ['foo']})\n        assert exc_info.value.errors(include_url=False) == [\n            {'input': ['foo'], 'type': 'string_type', 'loc': ('x',), 'msg': 'Input should be a valid string'}\n        ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005455", "source": "def _dataclass_getstate(self: Any) -> list[Any]:\n                    return [getattr(self, f.name) for f in dataclasses.fields(self)]", "target": "def test_dataclass_field_wrap_validator2():\n    @dataclasses.dataclass\n    class Foo:\n        a: int\n        b: str\n        @classmethod\n        def validate_b(\n            cls, v: bytes, nxt: core_schema.ValidatorFunctionWrapHandler, info: core_schema.ValidationInfo\n        ) -> bytes:\n            assert v == b'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return nxt(b'hello world!')\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(\n                    name='b',\n                    schema=core_schema.with_info_wrap_validator_function(Foo.validate_b, core_schema.str_schema()),\n                ),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 1, 'b': b'hello'})\n    assert dataclasses.asdict(foo) == {'a': 1, 'b': 'hello world!'}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005456", "source": "def dataclass(\n        *,\n        init: bool = True,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool = False,\n        config: Union[ConfigDict, Type[object], None] = None,\n        validate_on_init: Optional[bool] = None,\n        use_proxy: Optional[bool] = None,\n        kw_only: bool = ...,\n    ) -> Callable[[Type[_T]], 'DataclassClassOrWrapper']:\n        ...", "target": "def test_dataclass_args_init_only_no_fields(py_and_json: PyAndJson, input_value, expected):\n    schema = core_schema.dataclass_args_schema(\n        'MyDataclass', [core_schema.dataclass_field(name='a', schema=core_schema.str_schema())], collect_init_only=True\n    )\n    v = py_and_json(schema)\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=expected.message) as exc_info:\n            v.validate_test(input_value)\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        assert v.validate_test(input_value) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005457", "source": "def rebuild_model_fields(\n    cls: type[BaseModel],\n    *,\n    config_wrapper: ConfigWrapper,\n    ns_resolver: NsResolver,\n    typevars_map: Mapping[TypeVar, Any],\n) -> dict[str, FieldInfo]:\n    rebuilt_fields: dict[str, FieldInfo] = {}\n    with ns_resolver.push(cls):\n        for f_name, field_info in cls.__pydantic_fields__.items():\n            if field_info._complete:\n                rebuilt_fields[f_name] = field_info\n            else:\n                new_field = _recreate_field_info(\n                    field_info, ns_resolver=ns_resolver, typevars_map=typevars_map, lenient=False\n                )\n                update_field_from_config(config_wrapper, f_name, new_field)\n                rebuilt_fields[f_name] = new_field\n    return rebuilt_fields", "target": "def test_empty_string_aliases(py_and_json: PyAndJson):\n    v = py_and_json(\n        {\n            'type': 'model-fields',\n            'fields': {'field_a': {'validation_alias': '', 'type': 'model-field', 'schema': {'type': 'int'}}},\n        }\n    )\n    assert v.validate_test({'': 123}) == ({'field_a': 123}, None, {'field_a'})\n    v = py_and_json(\n        {\n            'type': 'model-fields',\n            'fields': {'field_a': {'validation_alias': ['', ''], 'type': 'model-field', 'schema': {'type': 'int'}}},\n        }\n    )\n    assert v.validate_test({'': {'': 123}}) == ({'field_a': 123}, None, {'field_a'})"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005458", "source": "def is_valid_field_name(name: str) -> bool:\n    return not name.startswith('_')", "target": "def test_on_error_default_factory(self, py_and_json: PyAndJson):\n        v = py_and_json(\n            {\n                'type': 'model-fields',\n                'fields': {\n                    'x': {\n                        'type': 'model-field',\n                        'schema': {\n                            'type': 'default',\n                            'schema': {'type': 'str'},\n                            'on_error': 'default',\n                            'default_factory': lambda: 'pika',\n                        },\n                    }\n                },\n            }\n        )\n        assert v.validate_test({'x': 'foo'}) == ({'x': 'foo'}, None, {'x'})\n        assert v.validate_test({'x': ['foo']}) == ({'x': 'pika'}, None, {'x'})"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005459", "source": "def decimal_encoder(dec_value: Decimal) -> Union[int, float]:\n    if dec_value.as_tuple().exponent >= 0:\n        return int(dec_value)\n    else:\n        return float(dec_value)", "target": "def test_typed_dict():\n    v = SchemaValidator(\n        core_schema.typed_dict_schema(\n            fields={\n                'field_a': core_schema.typed_dict_field(schema=core_schema.str_schema()),\n                'field_b': core_schema.typed_dict_field(schema=core_schema.int_schema()),\n            }\n        )\n    )\n    input_str = '{\"field_a\": \"abc\", \"field_b\": 1}'\n    assert v.validate_json(input_str) == {'field_a': 'abc', 'field_b': 1}\n    input_str = '{\"field_a\": \"a\", \"field_a\": \"b\", \"field_b\": 1}'\n    assert v.validate_json(input_str) == {'field_a': 'b', 'field_b': 1}\n    assert v.validate_json(input_str) == {'field_a': 'b', 'field_b': 1}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005460", "source": "def _pydantic_fields_complete(cls: type[PydanticDataclass]) -> bool:\n    return all(field_info._complete for field_info in cls.__pydantic_fields__.values())", "target": "def test_dataclass():\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bytes_schema()),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    s = SchemaSerializer(schema)\n    assert s.to_python(Foo(a='hello', b=b'more')) == IsStrictDict(a='hello', b=b'more')\n    assert s.to_python(Foo(a='hello', b=b'more'), mode='json') == IsStrictDict(a='hello', b='more')\n    j = s.to_json(Foo(a='hello', b=b'more'))\n    if on_pypy:\n        assert json.loads(j) == {'a': 'hello', 'b': 'more'}\n    else:\n        assert j == b'{\"a\":\"hello\",\"b\":\"more\"}'\n    assert s.to_python(Foo(a='hello', b=b'more'), exclude={'b'}) == IsStrictDict(a='hello')\n    assert s.to_json(Foo(a='hello', b=b'more'), include={'a'}) == b'{\"a\":\"hello\"}'"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005461", "source": "def complete_dataclass(\n    cls: type[Any],\n    config_wrapper: _config.ConfigWrapper,\n    *,\n    raise_errors: bool = True,\n    ns_resolver: NsResolver | None = None,\n    _force_build: bool = False,\n) -> bool:\n    original_init = cls.__init__\n    def __init__(__dataclass_self__: PydanticDataclass, *args: Any, **kwargs: Any) -> None:\n        __tracebackhide__ = True\n        s = __dataclass_self__\n        s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)\n    __init__.__qualname__ = f'{cls.__qualname__}.__init__'\n    cls.__init__ = __init__\n    cls.__pydantic_config__ = config_wrapper.config_dict\n    set_dataclass_fields(cls, config_wrapper=config_wrapper, ns_resolver=ns_resolver)\n    if not _force_build and config_wrapper.defer_build:\n        set_dataclass_mocks(cls)\n        return False\n    if hasattr(cls, '__post_init_post_parse__'):\n        warnings.warn(\n            'Support for `__post_init_post_parse__` has been dropped, the method will not be called',\n            PydanticDeprecatedSince20,\n        )\n    typevars_map = get_standard_typevars_map(cls)\n    gen_schema = GenerateSchema(\n        config_wrapper,\n        ns_resolver=ns_resolver,\n        typevars_map=typevars_map,\n    )\n    cls.__signature__ = LazyClassAttribute(\n        '__signature__',\n        partial(\n            generate_pydantic_signature,\n            init=original_init,\n            fields=cls.__pydantic_fields__,\n            validate_by_name=config_wrapper.validate_by_name,\n            extra=config_wrapper.extra,\n            is_dataclass=True,\n        ),\n    )\n    try:\n        schema = gen_schema.generate_schema(cls)\n    except PydanticUndefinedAnnotation as e:\n        if raise_errors:\n            raise\n        set_dataclass_mocks(cls, f'`{e.name}`')\n        return False\n    core_config = config_wrapper.core_config(title=cls.__name__)\n    try:\n        schema = gen_schema.clean_schema(schema)\n    except InvalidSchemaError:\n        set_dataclass_mocks(cls)\n        return False\n    cls = cast('type[PydanticDataclass]', cls)\n    cls.__pydantic_core_schema__ = schema\n    cls.__pydantic_validator__ = create_schema_validator(\n        schema, cls, cls.__module__, cls.__qualname__, 'dataclass', core_config, config_wrapper.plugin_settings\n    )\n    cls.__pydantic_serializer__ = SchemaSerializer(schema, core_config)\n    cls.__pydantic_complete__ = True\n    return True", "target": "def test_dataclass_subclass_subclass_revalidate():\n    v = SchemaValidator(\n        core_schema.dataclass_schema(\n            FooDataclass,\n            core_schema.dataclass_args_schema(\n                'FooDataclass',\n                [\n                    core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                    core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n                ],\n            ),\n            ['a', 'b'],\n            revalidate_instances='subclass-instances',\n            strict=True,\n        )\n    )\n    foo = FooDataclass(a='hello', b=True)\n    assert v.validate_python(foo) is foo\n    sub_foo = FooDataclassSame(a='hello', b='True')\n    sub_foo2 = v.validate_python(sub_foo)\n    assert sub_foo2 is not sub_foo\n    assert type(sub_foo2) is FooDataclass\n    assert dataclasses.asdict(sub_foo2) == dict(a='hello', b=True)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005462", "source": "def patch_base_fields(cls: type[Any]) -> Generator[None]:\n    original_fields_list: list[tuple[DcFields, DcFields]] = []\n    for base in cls.__mro__[1:]:\n        dc_fields: dict[str, dataclasses.Field[Any]] = base.__dict__.get('__dataclass_fields__', {})\n        dc_fields_with_pydantic_field_defaults = {\n            field_name: field\n            for field_name, field in dc_fields.items()\n            if isinstance(field.default, FieldInfo)\n            and (field.default.description is not None or field.default.kw_only or field.default.repr is not True)\n        }\n        if dc_fields_with_pydantic_field_defaults:\n            original_fields_list.append((dc_fields, dc_fields_with_pydantic_field_defaults))\n            for field_name, field in dc_fields_with_pydantic_field_defaults.items():\n                default = cast(FieldInfo, field.default)\n                new_dc_field = copy.copy(field)\n                if sys.version_info >= (3, 10) and default.kw_only:\n                    new_dc_field.kw_only = True\n                if default.repr is not True:\n                    new_dc_field.repr = default.repr\n                dc_fields[field_name] = new_dc_field\n    try:\n        yield\n    finally:\n        for fields, original_fields in original_fields_list:\n            for field_name, original_field in original_fields.items():\n                fields[field_name] = original_field", "target": "def test_serialization_alias():\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bytes_schema(), serialization_alias='BAR'),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    s = SchemaSerializer(schema)\n    assert s.to_python(Foo(a='hello', b=b'more'), by_alias=True) == IsStrictDict(a='hello', BAR=b'more')\n    assert s.to_python(Foo(a='hello', b=b'more'), mode='json', by_alias=True) == IsStrictDict(a='hello', BAR='more')\n    j = s.to_json(Foo(a='hello', b=b'more'), by_alias=True)\n    if on_pypy:\n        assert json.loads(j) == {'a': 'hello', 'BAR': 'more'}\n    else:\n        assert j == b'{\"a\":\"hello\",\"BAR\":\"more\"}'"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005463", "source": "def isoformat(o: Union[datetime.date, datetime.time]) -> str:\n    return o.isoformat()", "target": "def test_json_bytes_base64_no_padding():\n    v = SchemaValidator(core_schema.bytes_schema(), config=CoreConfig(val_json_bytes='base64'))\n    base_64_without_padding = 'bm8tcGFkZGluZw'\n    assert v.validate_json(json.dumps(base_64_without_padding)) == b'no-padding'"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005464", "source": "def _set_nested_attr(obj: nn.Module, names: list[str], value: Tensor) -> None:\n    if len(names) == 1:\n        setattr(obj, names[0], value)\n    else:\n        _set_nested_attr(getattr(obj, names[0]), names[1:], value)", "target": "def test_pr_with_missing_labels(\n        self, mocked_rn_labels: Any, mocked_gql: Any\n    ) -> None:\n        \"Test PR with no 'release notes:' label or 'topic: not user facing' label\"\n        pr = GitHubPR(\"pytorch\", \"pytorch\", 82169)\n        self.assertFalse(has_required_labels(pr))"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005465", "source": "def dataclass(\n    _cls: type[_T] | None = None,\n    *,\n    init: Literal[False] = False,\n    repr: bool = True,\n    eq: bool = True,\n    order: bool = False,\n    unsafe_hash: bool = False,\n    frozen: bool | None = None,\n    config: ConfigDict | type[object] | None = None,\n    validate_on_init: bool | None = None,\n    kw_only: bool = False,\n    slots: bool = False,\n) -> Callable[[type[_T]], type[PydanticDataclass]] | type[PydanticDataclass]:\n    assert init is False, 'pydantic.dataclasses.dataclass only supports init=False'\n    assert validate_on_init is not False, 'validate_on_init=False is no longer supported'\n    if sys.version_info >= (3, 10):\n        kwargs = {'kw_only': kw_only, 'slots': slots}\n    else:\n        kwargs = {}\n    def create_dataclass(cls: type[Any]) -> type[PydanticDataclass]:\n        from ._internal._utils import is_model_class\n        if is_model_class(cls):\n            raise PydanticUserError(\n                f'Cannot create a Pydantic dataclass from {cls.__name__} as it is already a Pydantic model',\n                code='dataclass-on-model',\n            )\n        original_cls = cls\n        has_dataclass_base = any(dataclasses.is_dataclass(base) for base in cls.__bases__)\n        if not has_dataclass_base and config is not None and hasattr(cls, '__pydantic_config__'):\n            warn(\n                f'`config` is set via both the `dataclass` decorator and `__pydantic_config__` for dataclass {cls.__name__}. '\n                f'The `config` specification from `dataclass` decorator will take priority.',\n                category=UserWarning,\n                stacklevel=2,\n            )\n        config_dict = config if config is not None else getattr(cls, '__pydantic_config__', None)\n        config_wrapper = _config.ConfigWrapper(config_dict)\n        decorators = _decorators.DecoratorInfos.build(cls, replace_wrapped_methods=True)\n        decorators.update_from_config(config_wrapper)\n        original_doc = cls.__doc__\n        if _pydantic_dataclasses.is_stdlib_dataclass(cls):\n            original_doc = None\n            bases = (cls,)\n            if issubclass(cls, Generic):\n                generic_base = Generic[cls.__parameters__]\n                bases = bases + (generic_base,)\n            cls = types.new_class(cls.__name__, bases)\n        if frozen is not None:\n            frozen_ = frozen\n            if config_wrapper.frozen:\n                warn(\n                    f'`frozen` is set via both the `dataclass` decorator and `config` for dataclass {cls.__name__!r}.'\n                    'This is not recommended. The `frozen` specification on `dataclass` will take priority.',\n                    category=UserWarning,\n                    stacklevel=2,\n                )\n        else:\n            frozen_ = config_wrapper.frozen or False\n        cls_anns = _typing_extra.safe_get_annotations(cls)\n        for field_name in cls_anns:\n            field_value = getattr(cls, field_name, None)\n            if isinstance(field_value, FieldInfo):\n                setattr(cls, field_name, _pydantic_dataclasses.as_dataclass_field(field_value))\n        with _pydantic_dataclasses.patch_base_fields(cls):\n            cls = dataclasses.dataclass(\n                cls,\n                init=True,\n                repr=repr,\n                eq=eq,\n                order=order,\n                unsafe_hash=unsafe_hash,\n                frozen=frozen_,\n                **kwargs,\n            )\n        if config_wrapper.validate_assignment:\n            original_setattr = cls.__setattr__\n            @functools.wraps(cls.__setattr__)\n            def validated_setattr(instance: PydanticDataclass, name: str, value: Any, /) -> None:\n                if frozen_:\n                    return original_setattr(instance, name, value)\n                inst_cls = type(instance)\n                attr = getattr(inst_cls, name, None)\n                if isinstance(attr, property):\n                    attr.__set__(instance, value)\n                elif isinstance(attr, functools.cached_property):\n                    instance.__dict__.__setitem__(name, value)\n                else:\n                    inst_cls.__pydantic_validator__.validate_assignment(instance, name, value)\n            cls.__setattr__ = validated_setattr.__get__(None, cls)\n            if slots and not hasattr(cls, '__setstate__'):\n                def _dataclass_getstate(self: Any) -> list[Any]:\n                    return [getattr(self, f.name) for f in dataclasses.fields(self)]\n                def _dataclass_setstate(self: Any, state: list[Any]) -> None:\n                    for field, value in zip(dataclasses.fields(self), state):\n                        object.__setattr__(self, field.name, value)\n                cls.__getstate__ = _dataclass_getstate\n                cls.__setstate__ = _dataclass_setstate\n        cls.__is_pydantic_dataclass__ = True\n        cls.__pydantic_decorators__ = decorators\n        cls.__doc__ = original_doc\n        firstlineno = getattr(original_cls, '__firstlineno__', None)\n        cls.__module__ = original_cls.__module__\n        if sys.version_info >= (3, 13) and firstlineno is not None:\n            original_cls.__firstlineno__ = firstlineno\n            cls.__firstlineno__ = firstlineno\n        cls.__qualname__ = original_cls.__qualname__\n        cls.__pydantic_fields_complete__ = classmethod(_pydantic_fields_complete)\n        cls.__pydantic_complete__ = False\n        _pydantic_dataclasses.complete_dataclass(cls, config_wrapper, raise_errors=False)\n        return cls\n    return create_dataclass if _cls is None else create_dataclass(_cls)", "target": "def test_dataclass_args_init(input_value, extra_behavior, expected):\n    @dataclasses.dataclass\n    class Foo:\n        a: str\n        b: str\n        def __post_init__(self):\n            self.b = self.a.upper()\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.str_schema(), init=False),\n            ],\n            extra_behavior=extra_behavior,\n        ),\n        ['a', 'b'],\n        post_init=True,\n    )\n    v = SchemaValidator(schema)\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)) as exc_info:\n            v.validate_python(input_value)\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        assert dataclasses.asdict(v.validate_python(input_value)) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005466", "source": "def is_pydantic_dataclass(class_: type[Any], /) -> TypeGuard[type[PydanticDataclass]]:\n    try:\n        return '__is_pydantic_dataclass__' in class_.__dict__ and dataclasses.is_dataclass(class_)\n    except AttributeError:\n        return False", "target": "def test_aliases(py_and_json: PyAndJson):\n    schema = core_schema.dataclass_args_schema(\n        'MyDataclass',\n        [\n            core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), validation_alias='Apple'),\n            core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), validation_alias=['Banana', 1]),\n            core_schema.dataclass_field(\n                name='c', schema=core_schema.int_schema(), validation_alias=['Carrot', 'v'], init_only=True\n            ),\n        ],\n        collect_init_only=True,\n    )\n    v = py_and_json(schema)\n    assert v.validate_test({'Apple': 'a', 'Banana': ['x', 'false'], 'Carrot': {'v': '42'}}) == (\n        {'a': 'a', 'b': False},\n        (42,),\n    )"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005467", "source": "def create_pydantic_model_from_dataclass(\n    dc_cls: Type['Dataclass'],\n    config: Type[Any] = BaseConfig,\n    dc_cls_doc: Optional[str] = None,\n) -> Type['BaseModel']:\n    field_definitions: Dict[str, Any] = {}\n    for field in dataclasses.fields(dc_cls):\n        default: Any = Undefined\n        default_factory: Optional['NoArgAnyCallable'] = None\n        field_info: FieldInfo\n        if field.default is not dataclasses.MISSING:\n            default = field.default\n        elif field.default_factory is not dataclasses.MISSING:\n            default_factory = field.default_factory\n        else:\n            default = Required\n        if isinstance(default, FieldInfo):\n            field_info = default\n            dc_cls.__pydantic_has_field_info_default__ = True\n        else:\n            field_info = Field(default=default, default_factory=default_factory, **field.metadata)\n        field_definitions[field.name] = (field.type, field_info)\n    validators = gather_all_validators(dc_cls)\n    model: Type['BaseModel'] = create_model(\n        dc_cls.__name__,\n        __config__=config,\n        __module__=dc_cls.__module__,\n        __validators__=validators,\n        __cls_kwargs__={'__resolve_forward_refs__': False},\n        **field_definitions,\n    )\n    model.__doc__ = dc_cls_doc if dc_cls_doc is not None else dc_cls.__doc__ or ''\n    return model", "target": "def test_function_validator_wrapping_args_schema_before() -> None:\n    calls: list[Any] = []\n    def func(*args: Any) -> Any:\n        calls.append(args)\n        return args[0]\n    @dataclasses.dataclass\n    class Model:\n        number: int = 1\n    cs = core_schema.dataclass_schema(\n        Model,\n        core_schema.no_info_before_validator_function(\n            func,\n            core_schema.dataclass_args_schema(\n                'Model', [core_schema.dataclass_field('number', core_schema.int_schema())]\n            ),\n        ),\n        ['number'],\n    )\n    v = SchemaValidator(cs)\n    instance: Model = v.validate_python({'number': 1})\n    assert instance.number == 1\n    assert calls == [({'number': 1},)]\n    v.validate_assignment(instance, 'number', 2)\n    assert instance.number == 2\n    assert calls == [({'number': 1},), ({'number': 2},)]"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005468", "source": "def get_temp_dir() -> str:\n    global _TEMPDIR\n    if _TEMPDIR is None:\n        _TEMPDIR = _make_temp_dir(\n            prefix=\"instruction_count_microbenchmarks\", gc_dev_shm=True\n        )\n        atexit.register(shutil.rmtree, path=_TEMPDIR)\n    return _TEMPDIR", "target": "def test_gh_get_labels_raises_with_no_pages(\n        self,\n        mock_request_for_labels: Any,\n        get_last_page_num_from_header: Any,\n    ) -> None:\n        with self.assertRaises(AssertionError) as err:\n            gh_get_labels(\"foo\", \"bar\")\n        self.assertIn(\"number of pages of labels\", str(err.exception))"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005469", "source": "def rebuild_dataclass(\n    cls: type[PydanticDataclass],\n    *,\n    force: bool = False,\n    raise_errors: bool = True,\n    _parent_namespace_depth: int = 2,\n    _types_namespace: MappingNamespace | None = None,\n) -> bool | None:\n    if not force and cls.__pydantic_complete__:\n        return None\n    for attr in ('__pydantic_core_schema__', '__pydantic_validator__', '__pydantic_serializer__'):\n        if attr in cls.__dict__ and not isinstance(getattr(cls, attr), _mock_val_ser.MockValSer):\n            delattr(cls, attr)\n    cls.__pydantic_complete__ = False\n    if _types_namespace is not None:\n        rebuild_ns = _types_namespace\n    elif _parent_namespace_depth > 0:\n        rebuild_ns = _typing_extra.parent_frame_namespace(parent_depth=_parent_namespace_depth, force=True) or {}\n    else:\n        rebuild_ns = {}\n    ns_resolver = _namespace_utils.NsResolver(\n        parent_namespace=rebuild_ns,\n    )\n    return _pydantic_dataclasses.complete_dataclass(\n        cls,\n        _config.ConfigWrapper(cls.__pydantic_config__, check=False),\n        raise_errors=raise_errors,\n        ns_resolver=ns_resolver,\n        _force_build=True,\n    )", "target": "def test_function_validator_wrapping_args_schema_before() -> None:\n    calls: list[Any] = []\n    def func(*args: Any) -> Any:\n        calls.append(args)\n        return args[0]\n    @dataclasses.dataclass\n    class Model:\n        number: int = 1\n    cs = core_schema.dataclass_schema(\n        Model,\n        core_schema.no_info_before_validator_function(\n            func,\n            core_schema.dataclass_args_schema(\n                'Model', [core_schema.dataclass_field('number', core_schema.int_schema())]\n            ),\n        ),\n        ['number'],\n    )\n    v = SchemaValidator(cs)\n    instance: Model = v.validate_python({'number': 1})\n    assert instance.number == 1\n    assert calls == [({'number': 1},)]\n    v.validate_assignment(instance, 'number', 2)\n    assert instance.number == 2\n    assert calls == [({'number': 1},), ({'number': 2},)]"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005470", "source": "def to_markdown_table(\n    res: TimingResultType, header: Optional[tuple[str, ...]] = None\n) -> str:\n    if header is None:\n        header = (\"model\", \"task\", \"mean\", \"var\")\n    out = \"\"\n    def write_line(*args):\n        nonlocal out\n        out += f\"| {' | '.join(str(a) for a in args)} |\\n\"\n    write_line(*header)\n    write_line(*[\"--\"] * len(header))\n    for model, tasks in res.items():\n        for task, line in tasks.items():\n            write_line(*(model, task) + line)\n    return out", "target": "def test_peek(self, input_: str = \"abcdef\") -> None:\n        iter_ = PeekableIterator(input_)\n        for idx, c in enumerate(iter_):\n            if idx + 1 < len(input_):\n                self.assertEqual(iter_.peek(), input_[idx + 1])\n            else:\n                self.assertTrue(iter_.peek() is None)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005471", "source": "def _apply_alias_generator_to_field_info(\n    alias_generator: Callable[[str], str] | AliasGenerator, field_name: str, field_info: FieldInfo\n):\n    if (\n        field_info.alias_priority is None\n        or field_info.alias_priority <= 1\n        or field_info.alias is None\n        or field_info.validation_alias is None\n        or field_info.serialization_alias is None\n    ):\n        alias, validation_alias, serialization_alias = None, None, None\n        if isinstance(alias_generator, AliasGenerator):\n            alias, validation_alias, serialization_alias = alias_generator.generate_aliases(field_name)\n        elif callable(alias_generator):\n            alias = alias_generator(field_name)\n            if not isinstance(alias, str):\n                raise TypeError(f'alias_generator {alias_generator} must return str, not {alias.__class__}')\n        if field_info.alias_priority is None or field_info.alias_priority <= 1:\n            field_info.alias_priority = 1\n        if field_info.alias_priority == 1:\n            field_info.serialization_alias = get_first_not_none(serialization_alias, alias)\n            field_info.validation_alias = get_first_not_none(validation_alias, alias)\n            field_info.alias = alias\n        if field_info.alias is None:\n            field_info.alias = alias\n        if field_info.serialization_alias is None:\n            field_info.serialization_alias = get_first_not_none(serialization_alias, alias)\n        if field_info.validation_alias is None:\n            field_info.validation_alias = get_first_not_none(validation_alias, alias)", "target": "def test_by_alias_and_name_config_interaction(\n    config_by_alias: Union[bool, None],\n    config_by_name: Union[bool, None],\n    runtime_by_alias: Union[bool, None],\n    runtime_by_name: Union[bool, None],\n) -> None:\n    if config_by_alias is False and config_by_name is False and runtime_by_alias is False and runtime_by_name is False:\n        pytest.skip(\"Can't have both by_alias and by_name as effectively False\")\n    class Model:\n        def __init__(self, my_field: int) -> None:\n            self.my_field = my_field\n    core_config = {\n        **({'validate_by_alias': config_by_alias} if config_by_alias is not None else {}),\n        **({'validate_by_name': config_by_name} if config_by_name is not None else {}),\n    }\n    schema = core_schema.model_schema(\n        Model,\n        core_schema.model_fields_schema(\n            {\n                'my_field': core_schema.model_field(core_schema.int_schema(), validation_alias='my_alias'),\n            }\n        ),\n        config=core_schema.CoreConfig(**core_config),\n    )\n    s = SchemaValidator(schema)\n    alias_allowed = next(x for x in (runtime_by_alias, config_by_alias, True) if x is not None)\n    name_allowed = next(x for x in (runtime_by_name, config_by_name, False) if x is not None)\n    if alias_allowed:\n        assert s.validate_python({'my_alias': 1}, by_alias=runtime_by_alias, by_name=runtime_by_name).my_field == 1\n    if name_allowed:\n        assert s.validate_python({'my_field': 1}, by_alias=runtime_by_alias, by_name=runtime_by_name).my_field == 1"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005472", "source": "def _update_fields_from_docstrings(cls: type[Any], fields: dict[str, FieldInfo], use_inspect: bool = False) -> None:\n    fields_docs = extract_docstrings_from_cls(cls, use_inspect=use_inspect)\n    for ann_name, field_info in fields.items():\n        if field_info.description is None and ann_name in fields_docs:\n            field_info.description = fields_docs[ann_name]", "target": "def test_from_attributes(input_value, expected, from_attributes_mode):\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'a': core_schema.model_field(schema=core_schema.int_schema()),\n                'b': core_schema.model_field(schema=core_schema.int_schema()),\n                'c': core_schema.model_field(schema=core_schema.str_schema()),\n            },\n            from_attributes=from_attributes_mode == 'schema',\n        )\n    )\n    kwargs = {}\n    if from_attributes_mode == 'validation':\n        kwargs['from_attributes'] = True\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            val = v.validate_python(input_value, **kwargs)\n            print(f'UNEXPECTED OUTPUT: {val!r}')\n    else:\n        output = v.validate_python(input_value, **kwargs)\n        assert output == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005473", "source": "def check_parameters_count(cls: Type[GenericModel], parameters: Tuple[Any, ...]) -> None:\n    actual = len(parameters)\n    expected = len(cls.__parameters__)\n    if actual != expected:\n        description = 'many' if actual > expected else 'few'\n        raise TypeError(f'Too {description} parameters for {cls.__name__}; actual {actual}, expected {expected}')", "target": "def test_fastapi_startup_perf(benchmark: Any):\n    data_models = create_data_models()\n    T = TypeVar('T')\n    class GetModel(BaseModel, Generic[T]):\n        res: T\n    class GetModel2(GetModel[T], Generic[T]):\n        foo: str\n        bar: str\n    class GetManyModel(BaseModel, Generic[T]):\n        res: list[T]\n    class GetManyModel2(GetManyModel[T], Generic[T]):\n        foo: str\n        bar: str\n    class GetManyModel3(BaseModel, Generic[T]):\n        res: dict[str, T]\n    class GetManyModel4(BaseModel, Generic[T]):\n        res: dict[str, list[T]]\n    class PutModel(BaseModel, Generic[T]):\n        data: T\n    class PutModel2(PutModel[T], Generic[T]):\n        foo: str\n        bar: str\n    class PutManyModel(BaseModel, Generic[T]):\n        data: list[T]\n    class PutManyModel2(PutManyModel[T], Generic[T]):\n        foo: str\n        bar: str\n    api_models: list[Any] = [\n        GetModel,\n        GetModel2,\n        GetManyModel,\n        GetManyModel2,\n        GetManyModel3,\n        GetManyModel4,\n        PutModel,\n        PutModel2,\n        PutManyModel,\n        PutManyModel2,\n    ]\n    assert len(data_models) == INNER_DATA_MODEL_COUNT + OUTER_DATA_MODEL_COUNT\n    def bench():\n        concrete_api_models = []\n        adapters = []\n        for outer_api_model in api_models:\n            for data_model in data_models:\n                concrete_api_model = outer_api_model[\n                    data_model\n                ]\n                concrete_api_models.append(concrete_api_model)\n                adapt = TypeAdapter(Annotated[concrete_api_model, FieldInfo(description='foo')])\n                adapters.append(adapt)\n                adapt = TypeAdapter(Annotated[concrete_api_model, FieldInfo(description='bar')])\n                adapters.append(adapt)\n        assert len(concrete_api_models) == len(data_models) * len(api_models)\n        assert len(adapters) == len(concrete_api_models) * 2\n    benchmark(bench)"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005474", "source": "def compute_branch_diffs(\n        self, from_branch: str, to_branch: str\n    ) -> tuple[list[str], list[str]]:\n        from_ref = self.rev_parse(from_branch)\n        to_ref = self.rev_parse(to_branch)\n        merge_base = self.get_merge_base(from_ref, to_ref)\n        from_commits = self.revlist(f\"{merge_base}..{from_ref}\")\n        to_commits = self.revlist(f\"{merge_base}..{to_ref}\")\n        from_ids = fuzzy_list_to_dict(self.patch_id(from_commits))\n        to_ids = fuzzy_list_to_dict(self.patch_id(to_commits))\n        for patch_id in set(from_ids).intersection(set(to_ids)):\n            from_values = from_ids[patch_id]\n            to_values = to_ids[patch_id]\n            if len(from_values) != len(to_values):\n                while len(from_values) > 0 and len(to_values) > 0:\n                    frc = self.get_commit(from_values.pop())\n                    toc = self.get_commit(to_values.pop())\n                    if frc.title != toc.title or frc.author_date != toc.author_date:\n                        if (\n                            \"pytorch/pytorch\" not in self.remote_url()\n                            or frc.commit_hash\n                            not in {\n                                \"0a6a1b27a464ba5be5f587cce2ee12ab8c504dbf\",\n                                \"6d0f4a1d545a8f161df459e8d4ccafd4b9017dbe\",\n                                \"edf909e58f06150f7be41da2f98a3b9de3167bca\",\n                                \"a58c6aea5a0c9f8759a4154e46f544c8b03b8db1\",\n                                \"7106d216c29ca16a3504aa2bedad948ebcf4abc2\",\n                            }\n                        ):\n                            raise RuntimeError(\n                                f\"Unexpected differences between {frc} and {toc}\"\n                            )\n                    from_commits.remove(frc.commit_hash)\n                    to_commits.remove(toc.commit_hash)\n                continue\n            for commit in from_values:\n                from_commits.remove(commit)\n            for commit in to_values:\n                to_commits.remove(commit)\n        if \"pytorch/pytorch\" in self.remote_url():\n            for excluded_commit in {\n                \"8e09e20c1dafcdbdb45c2d1574da68a32e54a3a5\",\n                \"5f37e5c2a39c3acb776756a17730b865f0953432\",\n                \"b5222584e6d6990c6585981a936defd1af14c0ba\",\n                \"84d9a2e42d5ed30ec3b8b4140c38dd83abbce88d\",\n                \"f211ec90a6cdc8a2a5795478b5b5c8d7d7896f7e\",\n            }:\n                if excluded_commit in from_commits:\n                    from_commits.remove(excluded_commit)\n        return (from_commits, to_commits)", "target": "def test_simple(self) -> None:\n        @retries_decorator()\n        def foo(x: int, y: int) -> int:\n            return x + y\n        self.assertEqual(foo(3, 4), 7)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005475", "source": "def _general_metadata_cls() -> type[BaseMetadata]:\n    from annotated_types import BaseMetadata\n    class _PydanticGeneralMetadata(PydanticMetadata, BaseMetadata):\n        def __init__(self, metadata: Any):\n            self.__dict__ = metadata\n    return _PydanticGeneralMetadata", "target": "def test_from_attributes_override_false():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={'a': core_schema.model_field(schema=core_schema.int_schema())}, from_attributes=True\n        )\n    )\n    with pytest.raises(ValidationError, match='Input should be a valid dictionary'):\n        v.validate_python(Cls(a=1), from_attributes=False)\n    assert v.validate_python(Cls(a=1)) == ({'a': 1}, None, {'a'})\n    assert v.isinstance_python(Cls(a=1)) is True\n    assert v.isinstance_python(Cls(a=1), from_attributes=False) is False"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005476", "source": "def revlist(self, revision_range: str) -> list[str]:\n        rc = self._run_git(\"rev-list\", revision_range, \"--\", \".\").strip()\n        return rc.split(\"\\n\") if len(rc) > 0 else []", "target": "def test_fails(self) -> None:\n        @retries_decorator(rc=0)\n        def foo(x: int, y: int) -> int:\n            return x + y\n        self.assertEqual(foo(\"a\", 4), 0)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005477", "source": "def pydantic_general_metadata(**metadata: Any) -> BaseMetadata:\n    return _general_metadata_cls()(metadata)", "target": "def test_from_attributes_function(input_value, expected):\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={'a': core_schema.model_field(schema=core_schema.any_schema())}, from_attributes=True\n        )\n    )\n    model_dict, model_extra, fields_set = v.validate_python(input_value)\n    assert model_dict == expected\n    assert model_extra is None\n    assert fields_set == {'a'}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005478", "source": "def fraction_validator(input_value: Any, /) -> Fraction:\n    if isinstance(input_value, Fraction):\n        return input_value\n    try:\n        return Fraction(input_value)\n    except ValueError:\n        raise PydanticCustomError('fraction_parsing', 'Input is not a valid fraction')", "target": "def test_validators_build(benchmark) -> None:\n    class Base1(BaseModel):\n        a: int\n        @field_validator('a', mode='after')\n        @classmethod\n        def val_a(cls, value: int) -> int: ...\n        @computed_field\n        def prop(self) -> int: ...\n    class Bare:\n        @computed_field\n        def prop_bare(self) -> int: ...\n    class Sub1(Base1):\n        @computed_field\n        def prop_2(self) -> int: ...\n        @computed_field\n        def prop_3(self) -> int: ...\n        @computed_field\n        def prop_4(self) -> int: ...\n    @benchmark\n    def bench() -> None:\n        class SubS(Sub1, Bare, defer_build=True):\n            @computed_field\n            def prop_5(self) -> int: ...\n            @computed_field\n            def prop_6(self) -> int: ...\n            @computed_field\n            def prop_7(self) -> int: ..."}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005479", "source": "def patch_base_fields(cls: type[Any]) -> Generator[None]:\n    original_fields_list: list[tuple[DcFields, DcFields]] = []\n    for base in cls.__mro__[1:]:\n        dc_fields: dict[str, dataclasses.Field[Any]] = base.__dict__.get('__dataclass_fields__', {})\n        dc_fields_with_pydantic_field_defaults = {\n            field_name: field\n            for field_name, field in dc_fields.items()\n            if isinstance(field.default, FieldInfo)\n            and (field.default.description is not None or field.default.kw_only or field.default.repr is not True)\n        }\n        if dc_fields_with_pydantic_field_defaults:\n            original_fields_list.append((dc_fields, dc_fields_with_pydantic_field_defaults))\n            for field_name, field in dc_fields_with_pydantic_field_defaults.items():\n                default = cast(FieldInfo, field.default)\n                new_dc_field = copy.copy(field)\n                if sys.version_info >= (3, 10) and default.kw_only:\n                    new_dc_field.kw_only = True\n                if default.repr is not True:\n                    new_dc_field.repr = default.repr\n                dc_fields[field_name] = new_dc_field\n    try:\n        yield\n    finally:\n        for fields, original_fields in original_fields_list:\n            for field_name, original_field in original_fields.items():\n                fields[field_name] = original_field", "target": "def test_dataclass_post_init_args_multiple():\n    dc_args = None\n    @dataclasses.dataclass\n    class Foo:\n        a: str\n        b: dataclasses.InitVar[bool]\n        c: dataclasses.InitVar[int]\n        def __post_init__(self, *args):\n            nonlocal dc_args\n            dc_args = args\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), init_only=True),\n                core_schema.dataclass_field(name='c', schema=core_schema.int_schema(), init_only=True),\n            ],\n            collect_init_only=True,\n        ),\n        ['a', 'b'],\n        post_init=True,\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': b'hello', 'b': 'true', 'c': '42'})\n    assert dataclasses.asdict(foo) == {'a': 'hello'}\n    assert dc_args == (True, 42)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005480", "source": "def _apply_field_title_generator_to_field_info(\n    title_generator: Callable[[str, FieldInfo], str],\n    field_name: str,\n    field_info: FieldInfo,\n):\n    if field_info.title is None:\n        title = title_generator(field_name, field_info)\n        if not isinstance(title, str):\n            raise TypeError(f'field_title_generator {title_generator} must return str, not {title.__class__}')\n        field_info.title = title", "target": "def test_config(config: CoreConfig, input_value, expected):\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'a': core_schema.model_field(schema=core_schema.int_schema()),\n                'b': core_schema.model_field(\n                    schema=core_schema.with_default_schema(schema=core_schema.float_schema(), default=4.2)\n                ),\n            }\n        ),\n        config=config,\n    )\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            val = v.validate_python(input_value)\n            print(f'UNEXPECTED OUTPUT: {val!r}')\n    else:\n        result = v.validate_python(input_value)\n        assert result == expected"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005481", "source": "def benchmark_using_throughput_benchmark(config, module):\n    print(\"Benchmarking via ThroughputBenchmark\")\n    bench = ThroughputBenchmark(module.module)\n    bench.add_input(*module.tensor_inputs)\n    stats = bench.benchmark(1, config.num_warmup_iters, config.num_iters)\n    return stats.latency_avg_ms / NUM_LOOP_ITERS", "target": "def test_restores_even_on_exception(self):\n        var = \"TEST_TMP_ENV_EXCEPTION\"\n        self.assertNotIn(var, os.environ)\n        with self.assertRaises(RuntimeError):\n            with temp_environ({var: \"x\"}):\n                self.assertEqual(os.environ[var], \"x\")\n                raise RuntimeError(\"boom\")\n        self.assertNotIn(var, os.environ)"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005482", "source": "def env_path_field(\n    name: str,\n    default: Union[str, Path] = \"\",\n    *,\n    resolve: bool = True,\n) -> Path:\n    return field(default_factory=lambda: env_path(name, default, resolve=resolve))", "target": "def test_env_path_optional_unset_returns_none_by_default(self):\n        with patch.dict(os.environ, {}, clear=True):\n            self.assertIsNone(m.env_path_optional(\"P\"))"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005483", "source": "def generate_dataclass_help(cls) -> str:\n    if not is_dataclass(cls):\n        raise TypeError(f\"{cls} is not a dataclass\")\n    def get_value(f):\n        if f.default is not MISSING:\n            return f.default\n        if f.default_factory is not MISSING:\n            try:\n                return f.default_factory()\n            except Exception as e:\n                return f\"<error: {e}>\"\n        return \"<required>\"\n    lines = [f\"{f.name:<22} = {repr(get_value(f))}\" for f in fields(cls)]\n    return indent(\"\\n\".join(lines), \"    \")", "target": "def test_env_path_optional_unset_returns_none_by_default(self):\n        with patch.dict(os.environ, {}, clear=True):\n            self.assertIsNone(m.env_path_optional(\"P\"))"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005484", "source": "def push(self, config_wrapper: ConfigWrapper | ConfigDict | None):\n        if config_wrapper is None:\n            yield\n            return\n        if not isinstance(config_wrapper, ConfigWrapper):\n            config_wrapper = ConfigWrapper(config_wrapper, check=False)\n        self._config_wrapper_stack.append(config_wrapper)\n        try:\n            yield\n        finally:\n            self._config_wrapper_stack.pop()", "target": "def test_hide_input_in_errors(config, input_str):\n    v = SchemaValidator(\n        cs.model_schema(\n            cls=MyModel, schema=cs.model_fields_schema(fields={'f': cs.model_field(schema=cs.str_schema())})\n        ),\n        config=config,\n    )\n    with pytest.raises(ValidationError, match=re.escape(f'Input should be a valid string [{input_str}]')):\n        assert v.validate_python({'f': 123})"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005485", "source": "def path_validator(v: Any) -> Path:\n    if isinstance(v, Path):\n        return v\n    try:\n        return Path(v)\n    except TypeError:\n        raise errors.PathError()", "target": "def test_validators_build(benchmark) -> None:\n    class Base1(BaseModel):\n        a: int\n        @field_validator('a', mode='after')\n        @classmethod\n        def val_a(cls, value: int) -> int: ...\n        @computed_field\n        def prop(self) -> int: ...\n    class Bare:\n        @computed_field\n        def prop_bare(self) -> int: ...\n    class Sub1(Base1):\n        @computed_field\n        def prop_2(self) -> int: ...\n        @computed_field\n        def prop_3(self) -> int: ...\n        @computed_field\n        def prop_4(self) -> int: ...\n    @benchmark\n    def bench() -> None:\n        class SubS(Sub1, Bare, defer_build=True):\n            @computed_field\n            def prop_5(self) -> int: ...\n            @computed_field\n            def prop_6(self) -> int: ...\n            @computed_field\n            def prop_7(self) -> int: ..."}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005486", "source": "def dataclass(\n    _cls: type[_T] | None = None,\n    *,\n    init: Literal[False] = False,\n    repr: bool = True,\n    eq: bool = True,\n    order: bool = False,\n    unsafe_hash: bool = False,\n    frozen: bool | None = None,\n    config: ConfigDict | type[object] | None = None,\n    validate_on_init: bool | None = None,\n    kw_only: bool = False,\n    slots: bool = False,\n) -> Callable[[type[_T]], type[PydanticDataclass]] | type[PydanticDataclass]:\n    assert init is False, 'pydantic.dataclasses.dataclass only supports init=False'\n    assert validate_on_init is not False, 'validate_on_init=False is no longer supported'\n    if sys.version_info >= (3, 10):\n        kwargs = {'kw_only': kw_only, 'slots': slots}\n    else:\n        kwargs = {}\n    def create_dataclass(cls: type[Any]) -> type[PydanticDataclass]:\n        from ._internal._utils import is_model_class\n        if is_model_class(cls):\n            raise PydanticUserError(\n                f'Cannot create a Pydantic dataclass from {cls.__name__} as it is already a Pydantic model',\n                code='dataclass-on-model',\n            )\n        original_cls = cls\n        has_dataclass_base = any(dataclasses.is_dataclass(base) for base in cls.__bases__)\n        if not has_dataclass_base and config is not None and hasattr(cls, '__pydantic_config__'):\n            warn(\n                f'`config` is set via both the `dataclass` decorator and `__pydantic_config__` for dataclass {cls.__name__}. '\n                f'The `config` specification from `dataclass` decorator will take priority.',\n                category=UserWarning,\n                stacklevel=2,\n            )\n        config_dict = config if config is not None else getattr(cls, '__pydantic_config__', None)\n        config_wrapper = _config.ConfigWrapper(config_dict)\n        decorators = _decorators.DecoratorInfos.build(cls, replace_wrapped_methods=True)\n        decorators.update_from_config(config_wrapper)\n        original_doc = cls.__doc__\n        if _pydantic_dataclasses.is_stdlib_dataclass(cls):\n            original_doc = None\n            bases = (cls,)\n            if issubclass(cls, Generic):\n                generic_base = Generic[cls.__parameters__]\n                bases = bases + (generic_base,)\n            cls = types.new_class(cls.__name__, bases)\n        if frozen is not None:\n            frozen_ = frozen\n            if config_wrapper.frozen:\n                warn(\n                    f'`frozen` is set via both the `dataclass` decorator and `config` for dataclass {cls.__name__!r}.'\n                    'This is not recommended. The `frozen` specification on `dataclass` will take priority.',\n                    category=UserWarning,\n                    stacklevel=2,\n                )\n        else:\n            frozen_ = config_wrapper.frozen or False\n        cls_anns = _typing_extra.safe_get_annotations(cls)\n        for field_name in cls_anns:\n            field_value = getattr(cls, field_name, None)\n            if isinstance(field_value, FieldInfo):\n                setattr(cls, field_name, _pydantic_dataclasses.as_dataclass_field(field_value))\n        with _pydantic_dataclasses.patch_base_fields(cls):\n            cls = dataclasses.dataclass(\n                cls,\n                init=True,\n                repr=repr,\n                eq=eq,\n                order=order,\n                unsafe_hash=unsafe_hash,\n                frozen=frozen_,\n                **kwargs,\n            )\n        if config_wrapper.validate_assignment:\n            original_setattr = cls.__setattr__\n            @functools.wraps(cls.__setattr__)\n            def validated_setattr(instance: PydanticDataclass, name: str, value: Any, /) -> None:\n                if frozen_:\n                    return original_setattr(instance, name, value)\n                inst_cls = type(instance)\n                attr = getattr(inst_cls, name, None)\n                if isinstance(attr, property):\n                    attr.__set__(instance, value)\n                elif isinstance(attr, functools.cached_property):\n                    instance.__dict__.__setitem__(name, value)\n                else:\n                    inst_cls.__pydantic_validator__.validate_assignment(instance, name, value)\n            cls.__setattr__ = validated_setattr.__get__(None, cls)\n            if slots and not hasattr(cls, '__setstate__'):\n                def _dataclass_getstate(self: Any) -> list[Any]:\n                    return [getattr(self, f.name) for f in dataclasses.fields(self)]\n                def _dataclass_setstate(self: Any, state: list[Any]) -> None:\n                    for field, value in zip(dataclasses.fields(self), state):\n                        object.__setattr__(self, field.name, value)\n                cls.__getstate__ = _dataclass_getstate\n                cls.__setstate__ = _dataclass_setstate\n        cls.__is_pydantic_dataclass__ = True\n        cls.__pydantic_decorators__ = decorators\n        cls.__doc__ = original_doc\n        firstlineno = getattr(original_cls, '__firstlineno__', None)\n        cls.__module__ = original_cls.__module__\n        if sys.version_info >= (3, 13) and firstlineno is not None:\n            original_cls.__firstlineno__ = firstlineno\n            cls.__firstlineno__ = firstlineno\n        cls.__qualname__ = original_cls.__qualname__\n        cls.__pydantic_fields_complete__ = classmethod(_pydantic_fields_complete)\n        cls.__pydantic_complete__ = False\n        _pydantic_dataclasses.complete_dataclass(cls, config_wrapper, raise_errors=False)\n        return cls\n    return create_dataclass if _cls is None else create_dataclass(_cls)", "target": "def test_custom_dataclass_names():\n    schema = core_schema.dataclass_schema(\n        FooParentDataclass,\n        core_schema.dataclass_args_schema(\n            'FooParentDataclass',\n            [\n                core_schema.dataclass_field(\n                    name='foo',\n                    schema=core_schema.union_schema(\n                        [\n                            core_schema.dataclass_schema(\n                                FooDataclass,\n                                core_schema.dataclass_args_schema(\n                                    'FooDataclass[dataclass_args_schema]',\n                                    [\n                                        core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                                        core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n                                    ],\n                                ),\n                                ['a', 'b'],\n                                cls_name='FooDataclass[cls_name]',\n                            ),\n                            core_schema.none_schema(),\n                        ]\n                    ),\n                )\n            ],\n        ),\n        ['foo'],\n    )\n    v = SchemaValidator(schema)\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python({'foo': 123})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'class_name': 'FooDataclass[dataclass_args_schema]'},\n            'input': 123,\n            'loc': ('foo', 'FooDataclass[cls_name]'),\n            'msg': 'Input should be a dictionary or an instance of FooDataclass[dataclass_args_schema]',\n            'type': 'dataclass_type',\n        },\n        {'input': 123, 'loc': ('foo', 'none'), 'msg': 'Input should be None', 'type': 'none_required'},\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005487", "source": "def env_bool(\n    name: str,\n    default: bool = False,\n) -> bool:\n    val = get_env(name)\n    if not val:\n        return default\n    return str2bool(val)", "target": "def test_env_path_optional_unset_returns_default_path_no_resolve(self):\n        d = Path(\"z\")\n        with patch.dict(os.environ, {}, clear=True):\n            p = m.env_path_optional(\"P\", default=d, resolve=False)\n            self.assertEqual(p, d)"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005488", "source": "def env_path_field(\n    name: str,\n    default: Union[str, Path] = \"\",\n    *,\n    resolve: bool = True,\n) -> Path:\n    return field(default_factory=lambda: env_path(name, default, resolve=resolve))", "target": "def test_get_env_not_exist_without_default(self):\n        with patch.dict(os.environ, {\"FOO\": \"bar\"}, clear=True):\n            self.assertEqual(m.get_env(\"TEST_NOT_EXIST\"), \"\")"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005489", "source": "def apply_discriminator(\n    schema: core_schema.CoreSchema,\n    discriminator: str | Discriminator,\n    definitions: dict[str, core_schema.CoreSchema] | None = None,\n) -> core_schema.CoreSchema:\n    from ..types import Discriminator\n    if isinstance(discriminator, Discriminator):\n        if isinstance(discriminator.discriminator, str):\n            discriminator = discriminator.discriminator\n        else:\n            return discriminator._convert_schema(schema)\n    return _ApplyInferredDiscriminator(discriminator, definitions or {}).apply(schema)", "target": "def test_efficiency_with_highly_nested_examples(benchmark) -> None:\n    @benchmark\n    def run():\n        for i in range(1, 12):\n            very_nested_input = build_nested_state(i)\n            any_state_adapter.validate_python(very_nested_input)"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005490", "source": "def patterns_to_regex(allowed_patterns: list[str]) -> Any:\n    rc = \"(\"\n    for idx, pattern in enumerate(allowed_patterns):\n        if idx > 0:\n            rc += \"|\"\n        pattern_ = PeekableIterator(pattern)\n        assert not any(c in pattern for c in \"{}()[]\\\\\")\n        for c in pattern_:\n            if c == \".\":\n                rc += \"\\\\.\"\n            elif c == \"+\":\n                rc += \"\\\\+\"\n            elif c == \"*\":\n                if pattern_.peek() == \"*\":\n                    next(pattern_)\n                    rc += \".*\"\n                else:\n                    rc += \"[^/]*\"\n            else:\n                rc += c\n    rc += \")\"\n    return re.compile(rc)", "target": "def test_compute_diff(self) -> None:\n        diff = self.repo.diff(\"HEAD\")\n        sha = _shasum(diff)\n        self.assertEqual(len(sha), 64)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005491", "source": "def pydantic_encoder(obj: Any) -> Any:\n    warnings.warn(\n        '`pydantic_encoder` is deprecated, use `pydantic_core.to_jsonable_python` instead.',\n        category=PydanticDeprecatedSince20,\n        stacklevel=2,\n    )\n    from dataclasses import asdict, is_dataclass\n    BaseModel = import_cached_base_model()\n    if isinstance(obj, BaseModel):\n        return obj.model_dump()\n    elif is_dataclass(obj):\n        return asdict(obj)\n    for base in obj.__class__.__mro__[:-1]:\n        try:\n            encoder = ENCODERS_BY_TYPE[base]\n        except KeyError:\n            continue\n        return encoder(obj)\n    else:\n        raise TypeError(f\"Object of type '{obj.__class__.__name__}' is not JSON serializable\")", "target": "def test_any_python(input_value, expected):\n    v = SchemaValidator(core_schema.json_schema())\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            v.validate_python(input_value)\n    else:\n        assert v.validate_python(input_value) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005492", "source": "def amend_commit_message(self, msg: str) -> None:\n        self._run_git(\"commit\", \"--amend\", \"-m\", msg)", "target": "def test_ghstack_branches_in_sync(self) -> None:\n        head_ref = \"gh/SS-JIA/206/head\"\n        self._skip_if_ref_does_not_exist(head_ref)\n        self.assertTrue(are_ghstack_branches_in_sync(self.repo, head_ref))"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005493", "source": "def complete_dataclass(\n    cls: type[Any],\n    config_wrapper: _config.ConfigWrapper,\n    *,\n    raise_errors: bool = True,\n    ns_resolver: NsResolver | None = None,\n    _force_build: bool = False,\n) -> bool:\n    original_init = cls.__init__\n    def __init__(__dataclass_self__: PydanticDataclass, *args: Any, **kwargs: Any) -> None:\n        __tracebackhide__ = True\n        s = __dataclass_self__\n        s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)\n    __init__.__qualname__ = f'{cls.__qualname__}.__init__'\n    cls.__init__ = __init__\n    cls.__pydantic_config__ = config_wrapper.config_dict\n    set_dataclass_fields(cls, config_wrapper=config_wrapper, ns_resolver=ns_resolver)\n    if not _force_build and config_wrapper.defer_build:\n        set_dataclass_mocks(cls)\n        return False\n    if hasattr(cls, '__post_init_post_parse__'):\n        warnings.warn(\n            'Support for `__post_init_post_parse__` has been dropped, the method will not be called',\n            PydanticDeprecatedSince20,\n        )\n    typevars_map = get_standard_typevars_map(cls)\n    gen_schema = GenerateSchema(\n        config_wrapper,\n        ns_resolver=ns_resolver,\n        typevars_map=typevars_map,\n    )\n    cls.__signature__ = LazyClassAttribute(\n        '__signature__',\n        partial(\n            generate_pydantic_signature,\n            init=original_init,\n            fields=cls.__pydantic_fields__,\n            validate_by_name=config_wrapper.validate_by_name,\n            extra=config_wrapper.extra,\n            is_dataclass=True,\n        ),\n    )\n    try:\n        schema = gen_schema.generate_schema(cls)\n    except PydanticUndefinedAnnotation as e:\n        if raise_errors:\n            raise\n        set_dataclass_mocks(cls, f'`{e.name}`')\n        return False\n    core_config = config_wrapper.core_config(title=cls.__name__)\n    try:\n        schema = gen_schema.clean_schema(schema)\n    except InvalidSchemaError:\n        set_dataclass_mocks(cls)\n        return False\n    cls = cast('type[PydanticDataclass]', cls)\n    cls.__pydantic_core_schema__ = schema\n    cls.__pydantic_validator__ = create_schema_validator(\n        schema, cls, cls.__module__, cls.__qualname__, 'dataclass', core_config, config_wrapper.plugin_settings\n    )\n    cls.__pydantic_serializer__ = SchemaSerializer(schema, core_config)\n    cls.__pydantic_complete__ = True\n    return True", "target": "def test_dataclass_exact_validation(revalidate_instances, input_value, expected):\n    schema = core_schema.dataclass_schema(\n        FooDataclass,\n        core_schema.dataclass_args_schema(\n            'FooDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n            ],\n        ),\n        ['a', 'b'],\n        revalidate_instances=revalidate_instances,\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python(input_value)\n    assert dataclasses.asdict(foo) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005494", "source": "def evaluate_forwardref(type_: ForwardRef, globalns: Any, localns: Any) -> Any:\n        return cast(Any, type_)._evaluate(globalns, localns, recursive_guard=set())", "target": "def test_type_error():\n    try:\n        PydanticKnownError('foobar')\n    except KeyError as exc:\n        assert str(exc) == '\"Invalid error type: \\'foobar\\'\"'\n    else:\n        raise AssertionError(\"PydanticKnownError('foobar') did not raise KeyError\")\n    e = PydanticKnownError('recursion_loop')\n    assert isinstance(e, PydanticKnownError)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005495", "source": "def is_valid_privateattr_name(name: str) -> bool:\n    return name.startswith('_') and not name.startswith('__')", "target": "def test_alias_extra_from_attributes():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            extra_behavior='allow',\n            from_attributes=True,\n            fields={\n                'field_a': core_schema.model_field(\n                    validation_alias=[['FieldA'], ['foo', 2]], schema=core_schema.int_schema()\n                )\n            },\n        )\n    )\n    assert v.validate_python({'FieldA': 1}) == ({'field_a': 1}, {}, {'field_a'})\n    assert v.validate_python(Cls(FieldA=1)) == ({'field_a': 1}, {}, {'field_a'})\n    assert v.validate_python(Cls(foo=[1, 2, 3])) == ({'field_a': 3}, {}, {'field_a'})\n    assert v.validate_python({'foo': [1, 2, 3]}) == ({'field_a': 3}, {}, {'field_a'})"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005496", "source": "def all_literal_values(type_: Type[Any]) -> Tuple[Any, ...]:\n    if not is_literal_type(type_):\n        return (type_,)\n    values = literal_values(type_)\n    return tuple(x for value in values for x in all_literal_values(value))", "target": "def test_type_error():\n    try:\n        PydanticKnownError('foobar')\n    except KeyError as exc:\n        assert str(exc) == '\"Invalid error type: \\'foobar\\'\"'\n    else:\n        raise AssertionError(\"PydanticKnownError('foobar') did not raise KeyError\")\n    e = PydanticKnownError('recursion_loop')\n    assert isinstance(e, PydanticKnownError)"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005497", "source": "def temp_environ(updates: dict[str, str]):\n    missing = object()\n    old: dict[str, str | object] = {k: os.environ.get(k, missing) for k in updates}\n    try:\n        os.environ.update(updates)\n        yield\n    finally:\n        for k, v in old.items():\n            if v is missing:\n                os.environ.pop(k, None)\n            else:\n                os.environ[k] = v", "target": "def test_pr_with_release_notes_label(\n        self, mocked_rn_labels: Any, mocked_gql: Any\n    ) -> None:\n        \"Test PR with 'release notes: nn' label\"\n        pr = GitHubPR(\"pytorch\", \"pytorch\", 71759)\n        self.assertTrue(has_required_labels(pr))"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005498", "source": "def _flatten(\n    key_prefix: Label, sub_schema: Definition, result: FlatIntermediateDefinition\n) -> None:\n    for k, value in sub_schema.items():\n        if isinstance(k, tuple):\n            assert all(isinstance(ki, str) for ki in k)\n            key_suffix: Label = k\n        elif k is None:\n            key_suffix = ()\n        else:\n            assert isinstance(k, str)\n            key_suffix = (k,)\n        key: Label = key_prefix + key_suffix\n        if isinstance(value, (TimerArgs, GroupedBenchmark)):\n            assert key not in result, f\"duplicate key: {key}\"\n            result[key] = value\n        else:\n            assert isinstance(value, dict)\n            _flatten(key_prefix=key, sub_schema=value, result=result)", "target": "def test_ghstack_branches_in_sync(self) -> None:\n        head_ref = \"gh/SS-JIA/206/head\"\n        self._skip_if_ref_does_not_exist(head_ref)\n        self.assertTrue(are_ghstack_branches_in_sync(self.repo, head_ref))"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005499", "source": "def greater_than_validator(x: Any, gt: Any) -> Any:\n    try:\n        if not (x > gt):\n            raise PydanticKnownError('greater_than', {'gt': _safe_repr(gt)})\n        return x\n    except TypeError:\n        raise TypeError(f\"Unable to apply constraint 'gt' to supplied value {x}\")", "target": "def test_validators_build(benchmark) -> None:\n    class Base1(BaseModel):\n        a: int\n        @field_validator('a', mode='after')\n        @classmethod\n        def val_a(cls, value: int) -> int: ...\n        @computed_field\n        def prop(self) -> int: ...\n    class Bare:\n        @computed_field\n        def prop_bare(self) -> int: ...\n    class Sub1(Base1):\n        @computed_field\n        def prop_2(self) -> int: ...\n        @computed_field\n        def prop_3(self) -> int: ...\n        @computed_field\n        def prop_4(self) -> int: ...\n    @benchmark\n    def bench() -> None:\n        class SubS(Sub1, Bare, defer_build=True):\n            @computed_field\n            def prop_5(self) -> int: ...\n            @computed_field\n            def prop_6(self) -> int: ...\n            @computed_field\n            def prop_7(self) -> int: ..."}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005500", "source": "def patterns_to_regex(allowed_patterns: list[str]) -> Any:\n    rc = \"(\"\n    for idx, pattern in enumerate(allowed_patterns):\n        if idx > 0:\n            rc += \"|\"\n        pattern_ = PeekableIterator(pattern)\n        assert not any(c in pattern for c in \"{}()[]\\\\\")\n        for c in pattern_:\n            if c == \".\":\n                rc += \"\\\\.\"\n            elif c == \"+\":\n                rc += \"\\\\+\"\n            elif c == \"*\":\n                if pattern_.peek() == \"*\":\n                    next(pattern_)\n                    rc += \".*\"\n                else:\n                    rc += \"[^/]*\"\n            else:\n                rc += c\n    rc += \")\"\n    return re.compile(rc)", "target": "def test_simple(self) -> None:\n        @retries_decorator()\n        def foo(x: int, y: int) -> int:\n            return x + y\n        self.assertEqual(foo(3, 4), 7)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005501", "source": "def new_post_init(self: 'Dataclass', *args: Any, **kwargs: Any) -> None:\n            if config.post_init_call == 'before_validation':\n                post_init(self, *args, **kwargs)\n            if self.__class__.__pydantic_run_validation__:\n                self.__pydantic_validate_values__()\n                if hasattr(self, '__post_init_post_parse__'):\n                    self.__post_init_post_parse__(*args, **kwargs)\n            if config.post_init_call == 'after_validation':\n                post_init(self, *args, **kwargs)", "target": "def test_serialization_exclude():\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(\n                    name='a', schema=core_schema.str_schema(), serialization_exclude_if=lambda x: x == 'bye'\n                ),\n                core_schema.dataclass_field(name='b', schema=core_schema.bytes_schema(), serialization_exclude=True),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    s = SchemaSerializer(schema)\n    assert s.to_python(Foo(a='hello', b=b'more')) == {'a': 'hello'}\n    assert s.to_python(Foo(a='hello', b=b'more'), mode='json') == {'a': 'hello'}\n    assert s.to_python(Foo(a='bye', b=b'more'), mode='json') == {}\n    j = s.to_json(Foo(a='hello', b=b'more'))\n    if on_pypy:\n        assert json.loads(j) == {'a': 'hello'}\n    else:\n        assert j == b'{\"a\":\"hello\"}'\n    j = s.to_json(Foo(a='bye', b=b'more'))\n    if on_pypy:\n        assert json.loads(j) == {}\n    else:\n        assert j == b'{}'"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005502", "source": "def _decorator(cls: type) -> type:\n        block = generate_dataclass_help(params_cls)\n        cls.__doc__ = (cls.__doc__ or \"\") + f\"\\n\\n{title}:\\n{block}\"\n        return cls", "target": "def test_env_path_optional_unset_returns_default_path_no_resolve(self):\n        d = Path(\"z\")\n        with patch.dict(os.environ, {}, clear=True):\n            p = m.env_path_optional(\"P\", default=d, resolve=False)\n            self.assertEqual(p, d)"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005503", "source": "def working_directory(path: str):\n    if not path:\n        yield\n        return\n    prev_cwd = os.getcwd()\n    try:\n        os.chdir(path)\n        yield\n    finally:\n        os.chdir(prev_cwd)", "target": "def test_ghstack_branches_in_sync(self) -> None:\n        head_ref = \"gh/SS-JIA/206/head\"\n        self._skip_if_ref_does_not_exist(head_ref)\n        self.assertTrue(are_ghstack_branches_in_sync(self.repo, head_ref))"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005504", "source": "def _apply_alias_generator_to_field_info(\n    alias_generator: Callable[[str], str] | AliasGenerator, field_name: str, field_info: FieldInfo\n):\n    if (\n        field_info.alias_priority is None\n        or field_info.alias_priority <= 1\n        or field_info.alias is None\n        or field_info.validation_alias is None\n        or field_info.serialization_alias is None\n    ):\n        alias, validation_alias, serialization_alias = None, None, None\n        if isinstance(alias_generator, AliasGenerator):\n            alias, validation_alias, serialization_alias = alias_generator.generate_aliases(field_name)\n        elif callable(alias_generator):\n            alias = alias_generator(field_name)\n            if not isinstance(alias, str):\n                raise TypeError(f'alias_generator {alias_generator} must return str, not {alias.__class__}')\n        if field_info.alias_priority is None or field_info.alias_priority <= 1:\n            field_info.alias_priority = 1\n        if field_info.alias_priority == 1:\n            field_info.serialization_alias = get_first_not_none(serialization_alias, alias)\n            field_info.validation_alias = get_first_not_none(validation_alias, alias)\n            field_info.alias = alias\n        if field_info.alias is None:\n            field_info.alias = alias\n        if field_info.serialization_alias is None:\n            field_info.serialization_alias = get_first_not_none(serialization_alias, alias)\n        if field_info.validation_alias is None:\n            field_info.validation_alias = get_first_not_none(validation_alias, alias)", "target": "def test_on_error_raise_explicit(self, py_and_json: PyAndJson):\n        v = py_and_json(\n            {\n                'type': 'model-fields',\n                'fields': {\n                    'x': {\n                        'type': 'model-field',\n                        'schema': {'type': 'default', 'schema': {'type': 'str'}, 'on_error': 'raise'},\n                    }\n                },\n            }\n        )\n        assert v.validate_test({'x': 'foo'}) == ({'x': 'foo'}, None, {'x'})\n        with pytest.raises(ValidationError) as exc_info:\n            v.validate_test({'x': ['foo']})\n        assert exc_info.value.errors(include_url=False) == [\n            {'input': ['foo'], 'type': 'string_type', 'loc': ('x',), 'msg': 'Input should be a valid string'}\n        ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005505", "source": "def dataclass(\n        _cls: Type[_T],\n        *,\n        init: bool = True,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool = False,\n        config: Union[ConfigDict, Type[object], None] = None,\n        validate_on_init: Optional[bool] = None,\n        use_proxy: Optional[bool] = None,\n        kw_only: bool = ...,\n    ) -> 'DataclassClassOrWrapper':\n        ...", "target": "def test_dataclass_self_init_alias_field_name():\n    @dataclasses.dataclass(init=False)\n    class Foo:\n        a: str\n        b: bool\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), validation_alias='aAlias'),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), validation_alias=['bAlias', 0]),\n            ],\n        ),\n        ['a', 'b'],\n        config={'loc_by_alias': False},\n    )\n    v = SchemaValidator(schema)\n    def __init__(self, *args, **kwargs):\n        v.validate_python(ArgsKwargs(args, kwargs), self_instance=self)\n    Foo.__init__ = __init__\n    foo = Foo(aAlias=b'hello', bAlias=['True'])\n    assert dataclasses.asdict(foo) == {'a': 'hello', 'b': True}\n    with pytest.raises(ValidationError) as exc_info:\n        Foo(aAlias=b'hello', bAlias=['wrong'])\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'bool_parsing',\n            'loc': ('b',),\n            'msg': 'Input should be a valid boolean, unable to interpret input',\n            'input': 'wrong',\n        }\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005506", "source": "def for_model(\n        cls,\n        bases: tuple[type[Any], ...],\n        namespace: dict[str, Any],\n        raw_annotations: dict[str, Any],\n        kwargs: dict[str, Any],\n    ) -> Self:\n        config_new = ConfigDict()\n        for base in bases:\n            config = getattr(base, 'model_config', None)\n            if config:\n                config_new.update(config.copy())\n        config_class_from_namespace = namespace.get('Config')\n        config_dict_from_namespace = namespace.get('model_config')\n        if raw_annotations.get('model_config') and config_dict_from_namespace is None:\n            raise PydanticUserError(\n                '`model_config` cannot be used as a model field name. Use `model_config` for model configuration.',\n                code='model-config-invalid-field-name',\n            )\n        if config_class_from_namespace and config_dict_from_namespace:\n            raise PydanticUserError('\"Config\" and \"model_config\" cannot be used together', code='config-both')\n        config_from_namespace = config_dict_from_namespace or prepare_config(config_class_from_namespace)\n        config_new.update(config_from_namespace)\n        for k in list(kwargs.keys()):\n            if k in config_keys:\n                config_new[k] = kwargs.pop(k)\n        return cls(config_new)", "target": "def test_on_model_class():\n    v = SchemaValidator(\n        cs.model_schema(\n            cls=MyModel,\n            config=CoreConfig(str_max_length=5),\n            schema=cs.model_fields_schema(fields={'f': cs.model_field(schema=cs.str_schema())}),\n        )\n    )\n    assert 'max_length:Some(5)' in plain_repr(v)\n    assert v.isinstance_python({'f': 'test'}) is True\n    assert v.isinstance_python({'f': 'test long'}) is False"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005507", "source": "def copy(src: Union[str, Path], dst: Union[str, Path]) -> None:\n    src_path = get_path(src, resolve=True)\n    dst_path = get_path(dst, resolve=True)\n    if not src_path.exists():\n        raise FileNotFoundError(f\"Source does not exist: {src_path}\")\n    dst_path.parent.mkdir(parents=True, exist_ok=True)\n    if src_path.is_file():\n        shutil.copy2(src_path, dst_path)\n    elif src_path.is_dir():\n        shutil.copytree(src_path, dst_path, dirs_exist_ok=True)\n    else:\n        raise ValueError(f\"Unsupported path type: {src_path}\")", "target": "def test_copy_dir_to_new_dir(self):\n        src = self.tmp_path / \"srcdir\"\n        (src / \"a\").mkdir(parents=True)\n        (src / \"a\" / \"f.txt\").write_text(\"content\")\n        dst = self.tmp_path / \"destdir\"\n        copy(src, dst)\n        self.assertEqual((dst / \"a\" / \"f.txt\").read_text(), \"content\")"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005508", "source": "def dataclass(\n    _cls: type[_T] | None = None,\n    *,\n    init: Literal[False] = False,\n    repr: bool = True,\n    eq: bool = True,\n    order: bool = False,\n    unsafe_hash: bool = False,\n    frozen: bool | None = None,\n    config: ConfigDict | type[object] | None = None,\n    validate_on_init: bool | None = None,\n    kw_only: bool = False,\n    slots: bool = False,\n) -> Callable[[type[_T]], type[PydanticDataclass]] | type[PydanticDataclass]:\n    assert init is False, 'pydantic.dataclasses.dataclass only supports init=False'\n    assert validate_on_init is not False, 'validate_on_init=False is no longer supported'\n    if sys.version_info >= (3, 10):\n        kwargs = {'kw_only': kw_only, 'slots': slots}\n    else:\n        kwargs = {}\n    def create_dataclass(cls: type[Any]) -> type[PydanticDataclass]:\n        from ._internal._utils import is_model_class\n        if is_model_class(cls):\n            raise PydanticUserError(\n                f'Cannot create a Pydantic dataclass from {cls.__name__} as it is already a Pydantic model',\n                code='dataclass-on-model',\n            )\n        original_cls = cls\n        has_dataclass_base = any(dataclasses.is_dataclass(base) for base in cls.__bases__)\n        if not has_dataclass_base and config is not None and hasattr(cls, '__pydantic_config__'):\n            warn(\n                f'`config` is set via both the `dataclass` decorator and `__pydantic_config__` for dataclass {cls.__name__}. '\n                f'The `config` specification from `dataclass` decorator will take priority.',\n                category=UserWarning,\n                stacklevel=2,\n            )\n        config_dict = config if config is not None else getattr(cls, '__pydantic_config__', None)\n        config_wrapper = _config.ConfigWrapper(config_dict)\n        decorators = _decorators.DecoratorInfos.build(cls, replace_wrapped_methods=True)\n        decorators.update_from_config(config_wrapper)\n        original_doc = cls.__doc__\n        if _pydantic_dataclasses.is_stdlib_dataclass(cls):\n            original_doc = None\n            bases = (cls,)\n            if issubclass(cls, Generic):\n                generic_base = Generic[cls.__parameters__]\n                bases = bases + (generic_base,)\n            cls = types.new_class(cls.__name__, bases)\n        if frozen is not None:\n            frozen_ = frozen\n            if config_wrapper.frozen:\n                warn(\n                    f'`frozen` is set via both the `dataclass` decorator and `config` for dataclass {cls.__name__!r}.'\n                    'This is not recommended. The `frozen` specification on `dataclass` will take priority.',\n                    category=UserWarning,\n                    stacklevel=2,\n                )\n        else:\n            frozen_ = config_wrapper.frozen or False\n        cls_anns = _typing_extra.safe_get_annotations(cls)\n        for field_name in cls_anns:\n            field_value = getattr(cls, field_name, None)\n            if isinstance(field_value, FieldInfo):\n                setattr(cls, field_name, _pydantic_dataclasses.as_dataclass_field(field_value))\n        with _pydantic_dataclasses.patch_base_fields(cls):\n            cls = dataclasses.dataclass(\n                cls,\n                init=True,\n                repr=repr,\n                eq=eq,\n                order=order,\n                unsafe_hash=unsafe_hash,\n                frozen=frozen_,\n                **kwargs,\n            )\n        if config_wrapper.validate_assignment:\n            original_setattr = cls.__setattr__\n            @functools.wraps(cls.__setattr__)\n            def validated_setattr(instance: PydanticDataclass, name: str, value: Any, /) -> None:\n                if frozen_:\n                    return original_setattr(instance, name, value)\n                inst_cls = type(instance)\n                attr = getattr(inst_cls, name, None)\n                if isinstance(attr, property):\n                    attr.__set__(instance, value)\n                elif isinstance(attr, functools.cached_property):\n                    instance.__dict__.__setitem__(name, value)\n                else:\n                    inst_cls.__pydantic_validator__.validate_assignment(instance, name, value)\n            cls.__setattr__ = validated_setattr.__get__(None, cls)\n            if slots and not hasattr(cls, '__setstate__'):\n                def _dataclass_getstate(self: Any) -> list[Any]:\n                    return [getattr(self, f.name) for f in dataclasses.fields(self)]\n                def _dataclass_setstate(self: Any, state: list[Any]) -> None:\n                    for field, value in zip(dataclasses.fields(self), state):\n                        object.__setattr__(self, field.name, value)\n                cls.__getstate__ = _dataclass_getstate\n                cls.__setstate__ = _dataclass_setstate\n        cls.__is_pydantic_dataclass__ = True\n        cls.__pydantic_decorators__ = decorators\n        cls.__doc__ = original_doc\n        firstlineno = getattr(original_cls, '__firstlineno__', None)\n        cls.__module__ = original_cls.__module__\n        if sys.version_info >= (3, 13) and firstlineno is not None:\n            original_cls.__firstlineno__ = firstlineno\n            cls.__firstlineno__ = firstlineno\n        cls.__qualname__ = original_cls.__qualname__\n        cls.__pydantic_fields_complete__ = classmethod(_pydantic_fields_complete)\n        cls.__pydantic_complete__ = False\n        _pydantic_dataclasses.complete_dataclass(cls, config_wrapper, raise_errors=False)\n        return cls\n    return create_dataclass if _cls is None else create_dataclass(_cls)", "target": "def test_dataclass_field_before_validator():\n    @dataclasses.dataclass\n    class Foo:\n        a: int\n        b: str\n        @classmethod\n        def validate_b(cls, v: bytes, info: core_schema.ValidationInfo) -> bytes:\n            assert v == b'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return b'hello world!'\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(\n                    name='b',\n                    schema=core_schema.with_info_before_validator_function(Foo.validate_b, core_schema.str_schema()),\n                ),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 1, 'b': b'hello'})\n    assert dataclasses.asdict(foo) == {'a': 1, 'b': 'hello world!'}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005509", "source": "def _add_pydantic_validation_attributes(\n    dc_cls: Type['Dataclass'],\n    config: Type[BaseConfig],\n    validate_on_init: bool,\n    dc_cls_doc: str,\n) -> None:\n    init = dc_cls.__init__\n    @wraps(init)\n    def handle_extra_init(self: 'Dataclass', *args: Any, **kwargs: Any) -> None:\n        if config.extra == Extra.ignore:\n            init(self, *args, **{k: v for k, v in kwargs.items() if k in self.__dataclass_fields__})\n        elif config.extra == Extra.allow:\n            for k, v in kwargs.items():\n                self.__dict__.setdefault(k, v)\n            init(self, *args, **{k: v for k, v in kwargs.items() if k in self.__dataclass_fields__})\n        else:\n            init(self, *args, **kwargs)\n    if hasattr(dc_cls, '__post_init__'):\n        try:\n            post_init = dc_cls.__post_init__.__wrapped__\n        except AttributeError:\n            post_init = dc_cls.__post_init__\n        @wraps(post_init)\n        def new_post_init(self: 'Dataclass', *args: Any, **kwargs: Any) -> None:\n            if config.post_init_call == 'before_validation':\n                post_init(self, *args, **kwargs)\n            if self.__class__.__pydantic_run_validation__:\n                self.__pydantic_validate_values__()\n                if hasattr(self, '__post_init_post_parse__'):\n                    self.__post_init_post_parse__(*args, **kwargs)\n            if config.post_init_call == 'after_validation':\n                post_init(self, *args, **kwargs)\n        setattr(dc_cls, '__init__', handle_extra_init)\n        setattr(dc_cls, '__post_init__', new_post_init)\n    else:\n        @wraps(init)\n        def new_init(self: 'Dataclass', *args: Any, **kwargs: Any) -> None:\n            handle_extra_init(self, *args, **kwargs)\n            if self.__class__.__pydantic_run_validation__:\n                self.__pydantic_validate_values__()\n            if hasattr(self, '__post_init_post_parse__'):\n                initvars_and_values: Dict[str, Any] = {}\n                for i, f in enumerate(self.__class__.__dataclass_fields__.values()):\n                    if f._field_type is dataclasses._FIELD_INITVAR:\n                        try:\n                            initvars_and_values[f.name] = args[i]\n                        except IndexError:\n                            initvars_and_values[f.name] = kwargs.get(f.name, f.default)\n                self.__post_init_post_parse__(**initvars_and_values)\n        setattr(dc_cls, '__init__', new_init)\n    setattr(dc_cls, '__pydantic_run_validation__', ClassAttribute('__pydantic_run_validation__', validate_on_init))\n    setattr(dc_cls, '__pydantic_initialised__', False)\n    setattr(dc_cls, '__pydantic_model__', create_pydantic_model_from_dataclass(dc_cls, config, dc_cls_doc))\n    setattr(dc_cls, '__pydantic_validate_values__', _dataclass_validate_values)\n    setattr(dc_cls, '__validate__', classmethod(_validate_dataclass))\n    setattr(dc_cls, '__get_validators__', classmethod(_get_validators))\n    if dc_cls.__pydantic_model__.__config__.validate_assignment and not dc_cls.__dataclass_params__.frozen:\n        setattr(dc_cls, '__setattr__', _dataclass_validate_assignment_setattr)", "target": "def test_serialization_exclude():\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(\n                    name='a', schema=core_schema.str_schema(), serialization_exclude_if=lambda x: x == 'bye'\n                ),\n                core_schema.dataclass_field(name='b', schema=core_schema.bytes_schema(), serialization_exclude=True),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    s = SchemaSerializer(schema)\n    assert s.to_python(Foo(a='hello', b=b'more')) == {'a': 'hello'}\n    assert s.to_python(Foo(a='hello', b=b'more'), mode='json') == {'a': 'hello'}\n    assert s.to_python(Foo(a='bye', b=b'more'), mode='json') == {}\n    j = s.to_json(Foo(a='hello', b=b'more'))\n    if on_pypy:\n        assert json.loads(j) == {'a': 'hello'}\n    else:\n        assert j == b'{\"a\":\"hello\"}'\n    j = s.to_json(Foo(a='bye', b=b'more'))\n    if on_pypy:\n        assert json.loads(j) == {}\n    else:\n        assert j == b'{}'"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005510", "source": "def fetch(self, ref: Optional[str] = None, branch: Optional[str] = None) -> None:\n        if branch is None and ref is None:\n            self._run_git(\"fetch\", self.remote)\n        elif branch is None:\n            self._run_git(\"fetch\", self.remote, ref)\n        else:\n            self._run_git(\"fetch\", self.remote, f\"{ref}:{branch}\")", "target": "def test_double_asterisks(self) -> None:\n        allowed_patterns = [\n            \"aten/src/ATen/native/**LinearAlgebra*\",\n        ]\n        patterns_re = patterns_to_regex(allowed_patterns)\n        fnames = [\n            \"aten/src/ATen/native/LinearAlgebra.cpp\",\n            \"aten/src/ATen/native/cpu/LinearAlgebraKernel.cpp\",\n        ]\n        for filename in fnames:\n            self.assertTrue(patterns_re.match(filename))"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005511", "source": "def compute_branch_diffs(\n        self, from_branch: str, to_branch: str\n    ) -> tuple[list[str], list[str]]:\n        from_ref = self.rev_parse(from_branch)\n        to_ref = self.rev_parse(to_branch)\n        merge_base = self.get_merge_base(from_ref, to_ref)\n        from_commits = self.revlist(f\"{merge_base}..{from_ref}\")\n        to_commits = self.revlist(f\"{merge_base}..{to_ref}\")\n        from_ids = fuzzy_list_to_dict(self.patch_id(from_commits))\n        to_ids = fuzzy_list_to_dict(self.patch_id(to_commits))\n        for patch_id in set(from_ids).intersection(set(to_ids)):\n            from_values = from_ids[patch_id]\n            to_values = to_ids[patch_id]\n            if len(from_values) != len(to_values):\n                while len(from_values) > 0 and len(to_values) > 0:\n                    frc = self.get_commit(from_values.pop())\n                    toc = self.get_commit(to_values.pop())\n                    if frc.title != toc.title or frc.author_date != toc.author_date:\n                        if (\n                            \"pytorch/pytorch\" not in self.remote_url()\n                            or frc.commit_hash\n                            not in {\n                                \"0a6a1b27a464ba5be5f587cce2ee12ab8c504dbf\",\n                                \"6d0f4a1d545a8f161df459e8d4ccafd4b9017dbe\",\n                                \"edf909e58f06150f7be41da2f98a3b9de3167bca\",\n                                \"a58c6aea5a0c9f8759a4154e46f544c8b03b8db1\",\n                                \"7106d216c29ca16a3504aa2bedad948ebcf4abc2\",\n                            }\n                        ):\n                            raise RuntimeError(\n                                f\"Unexpected differences between {frc} and {toc}\"\n                            )\n                    from_commits.remove(frc.commit_hash)\n                    to_commits.remove(toc.commit_hash)\n                continue\n            for commit in from_values:\n                from_commits.remove(commit)\n            for commit in to_values:\n                to_commits.remove(commit)\n        if \"pytorch/pytorch\" in self.remote_url():\n            for excluded_commit in {\n                \"8e09e20c1dafcdbdb45c2d1574da68a32e54a3a5\",\n                \"5f37e5c2a39c3acb776756a17730b865f0953432\",\n                \"b5222584e6d6990c6585981a936defd1af14c0ba\",\n                \"84d9a2e42d5ed30ec3b8b4140c38dd83abbce88d\",\n                \"f211ec90a6cdc8a2a5795478b5b5c8d7d7896f7e\",\n            }:\n                if excluded_commit in from_commits:\n                    from_commits.remove(excluded_commit)\n        return (from_commits, to_commits)", "target": "def test_is_iterable(self) -> None:\n        from collections.abc import Iterator\n        iter_ = PeekableIterator(\"\")\n        self.assertTrue(isinstance(iter_, Iterator))"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005512", "source": "def _apply_field_title_generator_to_field_info(\n    title_generator: Callable[[str, FieldInfo], str],\n    field_name: str,\n    field_info: FieldInfo,\n):\n    if field_info.title is None:\n        title = title_generator(field_name, field_info)\n        if not isinstance(title, str):\n            raise TypeError(f'field_title_generator {title_generator} must return str, not {title.__class__}')\n        field_info.title = title", "target": "def test_from_attributes_path_error():\n    class PropertyError:\n        @property\n        def foo(self):\n            raise RuntimeError('intentional error')\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'my_field': core_schema.model_field(\n                    validation_alias=[['foo', 'bar', 'bat'], ['foo', 3], ['spam']], schema=core_schema.int_schema()\n                )\n            },\n            from_attributes=True,\n        )\n    )\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python(PropertyError())\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'get_attribute_error',\n            'loc': ('my_field',),\n            'msg': 'Error extracting attribute: RuntimeError: intentional error',\n            'input': HasRepr(IsStr(regex='.+PropertyError object at.+')),\n            'ctx': {'error': 'RuntimeError: intentional error'},\n        }\n    ]"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005513", "source": "def update_field_from_config(config_wrapper: ConfigWrapper, field_name: str, field_info: FieldInfo) -> None:\n    field_title_generator = field_info.field_title_generator or config_wrapper.field_title_generator\n    if field_title_generator is not None:\n        _apply_field_title_generator_to_field_info(field_title_generator, field_name, field_info)\n    if config_wrapper.alias_generator is not None:\n        _apply_alias_generator_to_field_info(config_wrapper.alias_generator, field_name, field_info)", "target": "def test_wrap_on_error(self, py_and_json: PyAndJson):\n        def wrap_function(input_value, validator, info):\n            try:\n                return validator(input_value)\n            except ValidationError:\n                if isinstance(input_value, list):\n                    return str(len(input_value))\n                else:\n                    return repr(input_value)\n        v = py_and_json(\n            {\n                'type': 'model-fields',\n                'fields': {\n                    'x': {\n                        'type': 'model-field',\n                        'schema': {\n                            'type': 'default',\n                            'on_error': 'raise',\n                            'schema': {\n                                'type': 'function-wrap',\n                                'function': {'type': 'with-info', 'function': wrap_function},\n                                'schema': {'type': 'str'},\n                            },\n                        },\n                    }\n                },\n            }\n        )\n        assert v.validate_test({'x': 'foo'}) == ({'x': 'foo'}, None, {'x'})\n        assert v.validate_test({'x': ['foo']}) == ({'x': '1'}, None, {'x'})\n        assert v.validate_test({'x': ['foo', 'bar']}) == ({'x': '2'}, None, {'x'})\n        assert v.validate_test({'x': {'a': 'b'}}) == ({'x': \"{'a': 'b'}\"}, None, {'x'})"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005514", "source": "def rebuild_dataclass_fields(\n    cls: type[PydanticDataclass],\n    *,\n    config_wrapper: ConfigWrapper,\n    ns_resolver: NsResolver,\n    typevars_map: Mapping[TypeVar, Any],\n) -> dict[str, FieldInfo]:\n    FieldInfo_ = import_cached_field_info()\n    rebuilt_fields: dict[str, FieldInfo] = {}\n    with ns_resolver.push(cls):\n        for f_name, field_info in cls.__pydantic_fields__.items():\n            if field_info._complete:\n                rebuilt_fields[f_name] = field_info\n            else:\n                existing_desc = field_info.description\n                ann = _typing_extra.eval_type(\n                    field_info._original_annotation,\n                    *ns_resolver.types_namespace,\n                )\n                ann = _generics.replace_types(ann, typevars_map)\n                new_field = FieldInfo_.from_annotated_attribute(\n                    ann,\n                    field_info._original_assignment,\n                    _source=AnnotationSource.DATACLASS,\n                )\n                new_field.description = new_field.description if new_field.description is not None else existing_desc\n                update_field_from_config(config_wrapper, f_name, new_field)\n                rebuilt_fields[f_name] = new_field\n    return rebuilt_fields", "target": "def test_extra_behavior_allow_keys_validation() -> None:\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            {}, extra_behavior='allow', extras_keys_schema=core_schema.str_schema(max_length=3)\n        )\n    )\n    m, model_extra, fields_set = v.validate_python({'ext': 123})\n    assert m == {}\n    assert model_extra == {'ext': 123}\n    assert fields_set == {'ext'}\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python({'extra_too_long': 123})\n    assert exc_info.value.errors()[0]['type'] == 'string_too_long'"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005515", "source": "def create_pydantic_model_from_dataclass(\n    dc_cls: Type['Dataclass'],\n    config: Type[Any] = BaseConfig,\n    dc_cls_doc: Optional[str] = None,\n) -> Type['BaseModel']:\n    field_definitions: Dict[str, Any] = {}\n    for field in dataclasses.fields(dc_cls):\n        default: Any = Undefined\n        default_factory: Optional['NoArgAnyCallable'] = None\n        field_info: FieldInfo\n        if field.default is not dataclasses.MISSING:\n            default = field.default\n        elif field.default_factory is not dataclasses.MISSING:\n            default_factory = field.default_factory\n        else:\n            default = Required\n        if isinstance(default, FieldInfo):\n            field_info = default\n            dc_cls.__pydantic_has_field_info_default__ = True\n        else:\n            field_info = Field(default=default, default_factory=default_factory, **field.metadata)\n        field_definitions[field.name] = (field.type, field_info)\n    validators = gather_all_validators(dc_cls)\n    model: Type['BaseModel'] = create_model(\n        dc_cls.__name__,\n        __config__=config,\n        __module__=dc_cls.__module__,\n        __validators__=validators,\n        __cls_kwargs__={'__resolve_forward_refs__': False},\n        **field_definitions,\n    )\n    model.__doc__ = dc_cls_doc if dc_cls_doc is not None else dc_cls.__doc__ or ''\n    return model", "target": "def test_dataclass_args(py_and_json: PyAndJson, input_value, expected):\n    schema = core_schema.dataclass_args_schema(\n        'MyDataclass',\n        [\n            core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), kw_only=False),\n            core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), kw_only=False),\n        ],\n    )\n    v = py_and_json(schema)\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)) as exc_info:\n            v.validate_test(input_value)\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        assert v.validate_test(input_value) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005516", "source": "def convert_generics(tp: Type[Any]) -> Type[Any]:\n        origin = get_origin(tp)\n        if not origin or not hasattr(tp, '__args__'):\n            return tp\n        args = get_args(tp)\n        if origin is Annotated:\n            return Annotated[(convert_generics(args[0]), *args[1:])]\n        converted = tuple(\n            ForwardRef(arg) if isinstance(arg, str) and isinstance(tp, TypingGenericAlias) else convert_generics(arg)\n            for arg in args\n        )\n        if converted == args:\n            return tp\n        elif isinstance(tp, TypingGenericAlias):\n            return TypingGenericAlias(origin, converted)\n        elif isinstance(tp, TypesUnionType):\n            return functools.reduce(operator.or_, converted)\n        else:\n            try:\n                setattr(tp, '__args__', converted)\n            except AttributeError:\n                pass\n            return tp", "target": "def test_schema_validator_wrong() -> None:\n    try:\n        SchemaValidator({'type': 'bad'})\n    except SchemaError:\n        pass\n    else:\n        raise AssertionError('SchemaValidator did not raise SchemaError')"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005517", "source": "def _is_field_cached_property(obj: 'Dataclass', k: str) -> bool:\n        return False", "target": "def test_dataclass_field_after_validator():\n    @dataclasses.dataclass\n    class Foo:\n        a: int\n        b: str\n        @classmethod\n        def validate_b(cls, v: str, info: core_schema.ValidationInfo) -> str:\n            assert v == 'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return 'hello world!'\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(\n                    name='b',\n                    schema=core_schema.with_info_after_validator_function(Foo.validate_b, core_schema.str_schema()),\n                ),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 1, 'b': b'hello'})\n    assert dataclasses.asdict(foo) == {'a': 1, 'b': 'hello world!'}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005518", "source": "def _validate_dataclass(cls: Type['DataclassT'], v: Any) -> 'DataclassT':\n    with set_validation(cls, True):\n        if isinstance(v, cls):\n            v.__pydantic_validate_values__()\n            return v\n        elif isinstance(v, (list, tuple)):\n            return cls(*v)\n        elif isinstance(v, dict):\n            return cls(**v)\n        else:\n            raise DataclassTypeError(class_name=cls.__name__)", "target": "def test_extra_custom_serializer():\n    @dataclasses.dataclass\n    class Model:\n        pass\n    schema = core_schema.dataclass_schema(\n        Model,\n        core_schema.dataclass_args_schema(\n            'Model',\n            [],\n            extra_behavior='allow',\n        ),\n        [],\n    )\n    s = SchemaSerializer(schema)\n    v = SchemaValidator(schema)\n    m = v.validate_python({'extra': 'extra'})\n    assert s.to_python(m) == {'extra': 'extra bam!'}"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005519", "source": "def parse_stmts(stmts: str) -> tuple[str, str]:\n    stmts = textwrap.dedent(stmts).strip()\n    lines: list[str] = stmts.splitlines(keepends=False)\n    assert len(lines) >= 3, f\"Invalid string:\\n{stmts}\"\n    column_header_pattern = r\"^Python\\s{35}\\| C\\+\\+(\\s*)$\"\n    signature_pattern = r\"^: f\\((.*)\\)( -> (.+))?\\s*$\"\n    separation_pattern = r\"^[-]{40} | [-]{40}$\"\n    code_pattern = r\"^(.{40}) \\|($| (.*)$)\"\n    column_match = re.search(column_header_pattern, lines[0])\n    if column_match is None:\n        raise ValueError(\n            f\"Column header `{lines[0]}` \"\n            f\"does not match pattern `{column_header_pattern}`\"\n        )\n    assert re.search(separation_pattern, lines[1])\n    py_lines: list[str] = []\n    cpp_lines: list[str] = []\n    for l in lines[2:]:\n        l_match = re.search(code_pattern, l)\n        if l_match is None:\n            raise ValueError(f\"Invalid line `{l}`\")\n        py_lines.append(l_match.groups()[0])\n        cpp_lines.append(l_match.groups()[2] or \"\")\n        l_from_stmts = f\"{py_lines[-1]:<40} | {cpp_lines[-1]:<40}\".rstrip()\n        assert l_from_stmts == l.rstrip(), f\"Failed to round trip `{l}`\"\n    return \"\\n\".join(py_lines), \"\\n\".join(cpp_lines)", "target": "def test_pr_with_not_user_facing_label(\n        self, mocked_rn_labels: Any, mocked_gql: Any\n    ) -> None:\n        \"Test PR with 'topic: not user facing' label\"\n        pr = GitHubPR(\"pytorch\", \"pytorch\", 75095)\n        self.assertTrue(has_required_labels(pr))"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005520", "source": "def handle_extra_init(self: 'Dataclass', *args: Any, **kwargs: Any) -> None:\n        if config.extra == Extra.ignore:\n            init(self, *args, **{k: v for k, v in kwargs.items() if k in self.__dataclass_fields__})\n        elif config.extra == Extra.allow:\n            for k, v in kwargs.items():\n                self.__dict__.setdefault(k, v)\n            init(self, *args, **{k: v for k, v in kwargs.items() if k in self.__dataclass_fields__})\n        else:\n            init(self, *args, **kwargs)", "target": "def test_dataclass_slots_field_before_validator():\n    @dataclasses.dataclass(slots=True)\n    class Foo:\n        a: int\n        b: str\n        @classmethod\n        def validate_b(cls, v: bytes, info: core_schema.ValidationInfo) -> bytes:\n            assert v == b'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return b'hello world!'\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(\n                    name='b',\n                    schema=core_schema.with_info_before_validator_function(Foo.validate_b, core_schema.str_schema()),\n                ),\n            ],\n        ),\n        ['a', 'b'],\n        slots=True,\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 1, 'b': b'hello'})\n    assert dataclasses.asdict(foo) == {'a': 1, 'b': 'hello world!'}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005521", "source": "def get_args(t: Type[Any]) -> Tuple[Any, ...]:\n        if type(t).__name__ in AnnotatedTypeNames:\n            return t.__args__ + t.__metadata__\n        if isinstance(t, _GenericAlias):\n            res = t.__args__\n            if t.__origin__ is Callable and res and res[0] is not Ellipsis:\n                res = (list(res[:-1]), res[-1])\n            return res\n        return getattr(t, '__args__', ())", "target": "def test_ser_function_plain():\n    def f(input: Any, info: core_schema.SerializationInfo, /) -> str:\n        return str(info)\n    s = SchemaSerializer(\n        core_schema.any_schema(\n            serialization=core_schema.plain_serializer_function_ser_schema(\n                f, info_arg=True, return_schema=core_schema.str_schema()\n            )\n        )\n    )\n    assert s.to_python(123) == (\n        \"SerializationInfo(include=None, exclude=None, context=None, mode='python', by_alias=False, exclude_unset=False, \"\n        'exclude_defaults=False, exclude_none=False, exclude_computed_fields=False, round_trip=False, serialize_as_any=False)'\n    )"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005522", "source": "def wrap(cls: Type[Any]) -> 'DataclassClassOrWrapper':\n        should_use_proxy = (\n            use_proxy\n            if use_proxy is not None\n            else (\n                is_builtin_dataclass(cls)\n                and (cls.__bases__[0] is object or set(dir(cls)) == set(dir(cls.__bases__[0])))\n            )\n        )\n        if should_use_proxy:\n            dc_cls_doc = ''\n            dc_cls = DataclassProxy(cls)\n            default_validate_on_init = False\n        else:\n            dc_cls_doc = cls.__doc__ or ''\n            if sys.version_info >= (3, 10):\n                dc_cls = dataclasses.dataclass(\n                    cls,\n                    init=init,\n                    repr=repr,\n                    eq=eq,\n                    order=order,\n                    unsafe_hash=unsafe_hash,\n                    frozen=frozen,\n                    kw_only=kw_only,\n                )\n            else:\n                dc_cls = dataclasses.dataclass(\n                    cls, init=init, repr=repr, eq=eq, order=order, unsafe_hash=unsafe_hash, frozen=frozen\n                )\n            default_validate_on_init = True\n        should_validate_on_init = default_validate_on_init if validate_on_init is None else validate_on_init\n        _add_pydantic_validation_attributes(cls, the_config, should_validate_on_init, dc_cls_doc)\n        dc_cls.__pydantic_model__.__try_update_forward_refs__(**{cls.__name__: cls})\n        return dc_cls", "target": "def test_dataclass_field_after_validator():\n    @dataclasses.dataclass\n    class Foo:\n        a: int\n        b: str\n        @classmethod\n        def validate_b(cls, v: str, info: core_schema.ValidationInfo) -> str:\n            assert v == 'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return 'hello world!'\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(\n                    name='b',\n                    schema=core_schema.with_info_after_validator_function(Foo.validate_b, core_schema.str_schema()),\n                ),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 1, 'b': b'hello'})\n    assert dataclasses.asdict(foo) == {'a': 1, 'b': 'hello world!'}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005523", "source": "def takes_validated_data_argument(\n    default_factory: Callable[[], Any] | Callable[[dict[str, Any]], Any],\n) -> TypeIs[Callable[[dict[str, Any]], Any]]:\n    try:\n        sig = signature(default_factory)\n    except (ValueError, TypeError):\n        return False\n    parameters = list(sig.parameters.values())\n    return len(parameters) == 1 and can_be_positional(parameters[0]) and parameters[0].default is Parameter.empty", "target": "def test_str_config():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(fields={'field_a': core_schema.model_field(schema=core_schema.str_schema())}),\n        config=CoreConfig(str_max_length=5),\n    )\n    assert v.validate_python({'field_a': 'test'}) == ({'field_a': 'test'}, None, {'field_a'})\n    with pytest.raises(ValidationError, match='String should have at most 5 characters'):\n        v.validate_python({'field_a': 'test long'})"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005524", "source": "def pydantic_encoder(obj: Any) -> Any:\n    from dataclasses import asdict, is_dataclass\n    from pydantic.v1.main import BaseModel\n    if isinstance(obj, BaseModel):\n        return obj.dict()\n    elif is_dataclass(obj):\n        return asdict(obj)\n    for base in obj.__class__.__mro__[:-1]:\n        try:\n            encoder = ENCODERS_BY_TYPE[base]\n        except KeyError:\n            continue\n        return encoder(obj)\n    else:\n        raise TypeError(f\"Object of type '{obj.__class__.__name__}' is not JSON serializable\")", "target": "def test_float(input_value, expected):\n    v = SchemaValidator(core_schema.float_schema())\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            v.validate_json(input_value)\n    else:\n        assert v.validate_json(input_value) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005525", "source": "def _add_pydantic_validation_attributes(\n    dc_cls: Type['Dataclass'],\n    config: Type[BaseConfig],\n    validate_on_init: bool,\n    dc_cls_doc: str,\n) -> None:\n    init = dc_cls.__init__\n    @wraps(init)\n    def handle_extra_init(self: 'Dataclass', *args: Any, **kwargs: Any) -> None:\n        if config.extra == Extra.ignore:\n            init(self, *args, **{k: v for k, v in kwargs.items() if k in self.__dataclass_fields__})\n        elif config.extra == Extra.allow:\n            for k, v in kwargs.items():\n                self.__dict__.setdefault(k, v)\n            init(self, *args, **{k: v for k, v in kwargs.items() if k in self.__dataclass_fields__})\n        else:\n            init(self, *args, **kwargs)\n    if hasattr(dc_cls, '__post_init__'):\n        try:\n            post_init = dc_cls.__post_init__.__wrapped__\n        except AttributeError:\n            post_init = dc_cls.__post_init__\n        @wraps(post_init)\n        def new_post_init(self: 'Dataclass', *args: Any, **kwargs: Any) -> None:\n            if config.post_init_call == 'before_validation':\n                post_init(self, *args, **kwargs)\n            if self.__class__.__pydantic_run_validation__:\n                self.__pydantic_validate_values__()\n                if hasattr(self, '__post_init_post_parse__'):\n                    self.__post_init_post_parse__(*args, **kwargs)\n            if config.post_init_call == 'after_validation':\n                post_init(self, *args, **kwargs)\n        setattr(dc_cls, '__init__', handle_extra_init)\n        setattr(dc_cls, '__post_init__', new_post_init)\n    else:\n        @wraps(init)\n        def new_init(self: 'Dataclass', *args: Any, **kwargs: Any) -> None:\n            handle_extra_init(self, *args, **kwargs)\n            if self.__class__.__pydantic_run_validation__:\n                self.__pydantic_validate_values__()\n            if hasattr(self, '__post_init_post_parse__'):\n                initvars_and_values: Dict[str, Any] = {}\n                for i, f in enumerate(self.__class__.__dataclass_fields__.values()):\n                    if f._field_type is dataclasses._FIELD_INITVAR:\n                        try:\n                            initvars_and_values[f.name] = args[i]\n                        except IndexError:\n                            initvars_and_values[f.name] = kwargs.get(f.name, f.default)\n                self.__post_init_post_parse__(**initvars_and_values)\n        setattr(dc_cls, '__init__', new_init)\n    setattr(dc_cls, '__pydantic_run_validation__', ClassAttribute('__pydantic_run_validation__', validate_on_init))\n    setattr(dc_cls, '__pydantic_initialised__', False)\n    setattr(dc_cls, '__pydantic_model__', create_pydantic_model_from_dataclass(dc_cls, config, dc_cls_doc))\n    setattr(dc_cls, '__pydantic_validate_values__', _dataclass_validate_values)\n    setattr(dc_cls, '__validate__', classmethod(_validate_dataclass))\n    setattr(dc_cls, '__get_validators__', classmethod(_get_validators))\n    if dc_cls.__pydantic_model__.__config__.validate_assignment and not dc_cls.__dataclass_params__.frozen:\n        setattr(dc_cls, '__setattr__', _dataclass_validate_assignment_setattr)", "target": "def test_dataclass_field_wrap_validator1():\n    @dataclasses.dataclass\n    class Foo:\n        a: int\n        b: str\n        @classmethod\n        def validate_b(\n            cls, v: bytes, nxt: core_schema.ValidatorFunctionWrapHandler, info: core_schema.ValidationInfo\n        ) -> str:\n            assert v == b'hello'\n            v = nxt(v)\n            assert v == 'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return 'hello world!'\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(\n                    name='b',\n                    schema=core_schema.with_info_wrap_validator_function(Foo.validate_b, core_schema.str_schema()),\n                ),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 1, 'b': b'hello'})\n    assert dataclasses.asdict(foo) == {'a': 1, 'b': 'hello world!'}"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005526", "source": "def is_builtin_dataclass(_cls: Type[Any]) -> bool:\n    return (\n        dataclasses.is_dataclass(_cls)\n        and not hasattr(_cls, '__pydantic_model__')\n        and set(_cls.__dataclass_fields__).issuperset(set(getattr(_cls, '__annotations__', {})))\n    )", "target": "def test_dataclass():\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bytes_schema()),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    s = SchemaSerializer(schema)\n    assert s.to_python(Foo(a='hello', b=b'more')) == IsStrictDict(a='hello', b=b'more')\n    assert s.to_python(Foo(a='hello', b=b'more'), mode='json') == IsStrictDict(a='hello', b='more')\n    j = s.to_json(Foo(a='hello', b=b'more'))\n    if on_pypy:\n        assert json.loads(j) == {'a': 'hello', 'b': 'more'}\n    else:\n        assert j == b'{\"a\":\"hello\",\"b\":\"more\"}'\n    assert s.to_python(Foo(a='hello', b=b'more'), exclude={'b'}) == IsStrictDict(a='hello')\n    assert s.to_json(Foo(a='hello', b=b'more'), include={'a'}) == b'{\"a\":\"hello\"}'"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005527", "source": "def get_origin(tp: Type[Any]) -> Optional[Type[Any]]:\n        if type(tp).__name__ in AnnotatedTypeNames:\n            return cast(Type[Any], Annotated)\n        return _typing_get_origin(tp) or getattr(tp, '__origin__', None)", "target": "def test_schema_typing() -> None:\n    schema: CoreSchema = {\n        'type': 'union',\n        'choices': [{'type': 'int'}, {'type': 'int', 'ge': 1}, {'type': 'float', 'lt': 1.0}],\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = {\n        'type': 'tagged-union',\n        'discriminator': 'type',\n        'choices': {\n            'apple': {\n                'type': 'typed-dict',\n                'fields': {'pips': {'type': 'typed-dict-field', 'schema': {'type': 'int'}}},\n            },\n            'banana': {\n                'type': 'typed-dict',\n                'fields': {'curvature': {'type': 'typed-dict-field', 'schema': {'type': 'float'}}},\n            },\n        },\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'int', 'ge': 1}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'float', 'lt': 1.0}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'str', 'pattern': r'http://.*'}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'bool', 'strict': False}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'literal', 'expected': [1, '1']}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'any'}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'none'}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'bytes'}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'list', 'items_schema': {'type': 'str'}, 'min_length': 3}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'set', 'items_schema': {'type': 'str'}, 'max_length': 3}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'tuple', 'items_schema': [{'type': 'str'}], 'variadic_item_index': 0, 'max_length': 3}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'tuple', 'items_schema': [{'type': 'str'}, {'type': 'int'}]}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'frozenset', 'items_schema': {'type': 'str'}, 'max_length': 3}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'dict', 'keys_schema': {'type': 'str'}, 'values_schema': {'type': 'any'}}\n    SchemaValidator(schema)\n    schema: CoreSchema = {\n        'type': 'typed-dict',\n        'fields': {'bar': {'type': 'typed-dict-field', 'schema': {'type': 'str'}}},\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = {\n        'type': 'model',\n        'cls': Foo,\n        'schema': {'type': 'model-fields', 'fields': {'bar': {'type': 'model-field', 'schema': {'type': 'str'}}}},\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = {\n        'type': 'typed-dict',\n        'fields': {\n            'a': {'type': 'typed-dict-field', 'schema': {'type': 'str'}},\n            'b': {'type': 'typed-dict-field', 'schema': {'type': 'str'}, 'validation_alias': 'foobar'},\n            'c': {\n                'type': 'typed-dict-field',\n                'schema': {'type': 'str'},\n                'validation_alias': [['foobar', 0, 'bar'], ['foo']],\n            },\n            'd': {\n                'type': 'typed-dict-field',\n                'schema': {'type': 'default', 'schema': {'type': 'str'}, 'default': 'spam'},\n            },\n        },\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = {\n        'type': 'function-wrap',\n        'function': {'type': 'with-info', 'function': wrap_validator, 'field_name': 'foobar'},\n        'schema': {'type': 'str'},\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = core_schema.with_info_plain_validator_function(validator)\n    SchemaValidator(schema)\n    schema: CoreSchema = {\n        'type': 'definitions',\n        'schema': {'type': 'definition-ref', 'schema_ref': 'Branch'},\n        'definitions': [\n            {\n                'type': 'typed-dict',\n                'fields': {\n                    'name': {'type': 'typed-dict-field', 'schema': {'type': 'str'}},\n                    'sub_branch': {\n                        'type': 'typed-dict-field',\n                        'schema': {\n                            'type': 'default',\n                            'schema': {\n                                'type': 'nullable',\n                                'schema': {'type': 'definition-ref', 'schema_ref': 'Branch'},\n                            },\n                            'default': None,\n                        },\n                    },\n                },\n                'ref': 'Branch',\n            }\n        ],\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'date', 'le': date.today()}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'time', 'lt': time(12, 13, 14)}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'datetime', 'ge': datetime.now()}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'is-instance', 'cls': Foo}\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'callable'}\n    SchemaValidator(schema)\n    schema: CoreSchema = {\n        'type': 'arguments',\n        'arguments_schema': [\n            {'name': 'a', 'mode': 'positional_only', 'schema': {'type': 'int'}},\n            {'name': 'b', 'schema': {'type': 'str'}},\n            {'name': 'c', 'mode': 'keyword_only', 'schema': {'type': 'bool'}},\n        ],\n    }\n    SchemaValidator(schema)\n    schema: CoreSchema = {'type': 'call', 'arguments_schema': {'type': 'any'}, 'function': foo}\n    SchemaValidator(schema)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005528", "source": "def pydantic_encoder(obj: Any) -> Any:\n    warnings.warn(\n        '`pydantic_encoder` is deprecated, use `pydantic_core.to_jsonable_python` instead.',\n        category=PydanticDeprecatedSince20,\n        stacklevel=2,\n    )\n    from dataclasses import asdict, is_dataclass\n    BaseModel = import_cached_base_model()\n    if isinstance(obj, BaseModel):\n        return obj.model_dump()\n    elif is_dataclass(obj):\n        return asdict(obj)\n    for base in obj.__class__.__mro__[:-1]:\n        try:\n            encoder = ENCODERS_BY_TYPE[base]\n        except KeyError:\n            continue\n        return encoder(obj)\n    else:\n        raise TypeError(f\"Object of type '{obj.__class__.__name__}' is not JSON serializable\")", "target": "def test_any_schema_no_schema():\n    v = SchemaValidator(core_schema.json_schema())\n    assert 'validator:None' in plain_repr(v)\n    v = SchemaValidator(core_schema.json_schema(core_schema.any_schema()))\n    assert 'validator:None' in plain_repr(v)\n    v = SchemaValidator(core_schema.json_schema(core_schema.int_schema()))\n    assert 'validator:Some(' in plain_repr(v)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005529", "source": "def is_builtin_dataclass(_cls: Type[Any]) -> bool:\n    return (\n        dataclasses.is_dataclass(_cls)\n        and not hasattr(_cls, '__pydantic_model__')\n        and set(_cls.__dataclass_fields__).issuperset(set(getattr(_cls, '__annotations__', {})))\n    )", "target": "def test_only_validate_by_name(py_and_json) -> None:\n    schema = core_schema.dataclass_schema(\n        BasicDataclass,\n        core_schema.dataclass_args_schema(\n            'BasicDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), validation_alias='FieldA'),\n            ],\n        ),\n        ['a'],\n        config=core_schema.CoreConfig(validate_by_name=True, validate_by_alias=False),\n    )\n    v = py_and_json(schema)\n    assert v.validate_test({'a': 'hello'}) == BasicDataclass(a='hello')\n    with pytest.raises(ValidationError, match=r'a\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'FieldA': 'hello'})"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005530", "source": "def _general_metadata_cls() -> type[BaseMetadata]:\n    from annotated_types import BaseMetadata\n    class _PydanticGeneralMetadata(PydanticMetadata, BaseMetadata):\n        def __init__(self, metadata: Any):\n            self.__dict__ = metadata\n    return _PydanticGeneralMetadata", "target": "def test_fields_required_by_default_with_default():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'x': core_schema.model_field(schema=core_schema.str_schema()),\n                'y': core_schema.model_field(\n                    schema=core_schema.with_default_schema(schema=core_schema.str_schema(), default='bulbi')\n                ),\n            }\n        )\n    )\n    assert v.validate_python({'x': 'pika', 'y': 'chu'}) == ({'x': 'pika', 'y': 'chu'}, None, {'x', 'y'})\n    assert v.validate_python({'x': 'pika'}) == ({'x': 'pika', 'y': 'bulbi'}, None, {'x'})"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005531", "source": "def create_dataclass(cls: type[Any]) -> type[PydanticDataclass]:\n        from ._internal._utils import is_model_class\n        if is_model_class(cls):\n            raise PydanticUserError(\n                f'Cannot create a Pydantic dataclass from {cls.__name__} as it is already a Pydantic model',\n                code='dataclass-on-model',\n            )\n        original_cls = cls\n        has_dataclass_base = any(dataclasses.is_dataclass(base) for base in cls.__bases__)\n        if not has_dataclass_base and config is not None and hasattr(cls, '__pydantic_config__'):\n            warn(\n                f'`config` is set via both the `dataclass` decorator and `__pydantic_config__` for dataclass {cls.__name__}. '\n                f'The `config` specification from `dataclass` decorator will take priority.',\n                category=UserWarning,\n                stacklevel=2,\n            )\n        config_dict = config if config is not None else getattr(cls, '__pydantic_config__', None)\n        config_wrapper = _config.ConfigWrapper(config_dict)\n        decorators = _decorators.DecoratorInfos.build(cls, replace_wrapped_methods=True)\n        decorators.update_from_config(config_wrapper)\n        original_doc = cls.__doc__\n        if _pydantic_dataclasses.is_stdlib_dataclass(cls):\n            original_doc = None\n            bases = (cls,)\n            if issubclass(cls, Generic):\n                generic_base = Generic[cls.__parameters__]\n                bases = bases + (generic_base,)\n            cls = types.new_class(cls.__name__, bases)\n        if frozen is not None:\n            frozen_ = frozen\n            if config_wrapper.frozen:\n                warn(\n                    f'`frozen` is set via both the `dataclass` decorator and `config` for dataclass {cls.__name__!r}.'\n                    'This is not recommended. The `frozen` specification on `dataclass` will take priority.',\n                    category=UserWarning,\n                    stacklevel=2,\n                )\n        else:\n            frozen_ = config_wrapper.frozen or False\n        cls_anns = _typing_extra.safe_get_annotations(cls)\n        for field_name in cls_anns:\n            field_value = getattr(cls, field_name, None)\n            if isinstance(field_value, FieldInfo):\n                setattr(cls, field_name, _pydantic_dataclasses.as_dataclass_field(field_value))\n        with _pydantic_dataclasses.patch_base_fields(cls):\n            cls = dataclasses.dataclass(\n                cls,\n                init=True,\n                repr=repr,\n                eq=eq,\n                order=order,\n                unsafe_hash=unsafe_hash,\n                frozen=frozen_,\n                **kwargs,\n            )\n        if config_wrapper.validate_assignment:\n            original_setattr = cls.__setattr__\n            @functools.wraps(cls.__setattr__)\n            def validated_setattr(instance: PydanticDataclass, name: str, value: Any, /) -> None:\n                if frozen_:\n                    return original_setattr(instance, name, value)\n                inst_cls = type(instance)\n                attr = getattr(inst_cls, name, None)\n                if isinstance(attr, property):\n                    attr.__set__(instance, value)\n                elif isinstance(attr, functools.cached_property):\n                    instance.__dict__.__setitem__(name, value)\n                else:\n                    inst_cls.__pydantic_validator__.validate_assignment(instance, name, value)\n            cls.__setattr__ = validated_setattr.__get__(None, cls)\n            if slots and not hasattr(cls, '__setstate__'):\n                def _dataclass_getstate(self: Any) -> list[Any]:\n                    return [getattr(self, f.name) for f in dataclasses.fields(self)]\n                def _dataclass_setstate(self: Any, state: list[Any]) -> None:\n                    for field, value in zip(dataclasses.fields(self), state):\n                        object.__setattr__(self, field.name, value)\n                cls.__getstate__ = _dataclass_getstate\n                cls.__setstate__ = _dataclass_setstate\n        cls.__is_pydantic_dataclass__ = True\n        cls.__pydantic_decorators__ = decorators\n        cls.__doc__ = original_doc\n        firstlineno = getattr(original_cls, '__firstlineno__', None)\n        cls.__module__ = original_cls.__module__\n        if sys.version_info >= (3, 13) and firstlineno is not None:\n            original_cls.__firstlineno__ = firstlineno\n            cls.__firstlineno__ = firstlineno\n        cls.__qualname__ = original_cls.__qualname__\n        cls.__pydantic_fields_complete__ = classmethod(_pydantic_fields_complete)\n        cls.__pydantic_complete__ = False\n        _pydantic_dataclasses.complete_dataclass(cls, config_wrapper, raise_errors=False)\n        return cls", "target": "def test_dataclass_exact_validation(revalidate_instances, input_value, expected):\n    schema = core_schema.dataclass_schema(\n        FooDataclass,\n        core_schema.dataclass_args_schema(\n            'FooDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n            ],\n        ),\n        ['a', 'b'],\n        revalidate_instances=revalidate_instances,\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python(input_value)\n    assert dataclasses.asdict(foo) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005532", "source": "def handle_extra_init(self: 'Dataclass', *args: Any, **kwargs: Any) -> None:\n        if config.extra == Extra.ignore:\n            init(self, *args, **{k: v for k, v in kwargs.items() if k in self.__dataclass_fields__})\n        elif config.extra == Extra.allow:\n            for k, v in kwargs.items():\n                self.__dict__.setdefault(k, v)\n            init(self, *args, **{k: v for k, v in kwargs.items() if k in self.__dataclass_fields__})\n        else:\n            init(self, *args, **kwargs)", "target": "def test_serialization_exclude():\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(\n                    name='a', schema=core_schema.str_schema(), serialization_exclude_if=lambda x: x == 'bye'\n                ),\n                core_schema.dataclass_field(name='b', schema=core_schema.bytes_schema(), serialization_exclude=True),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    s = SchemaSerializer(schema)\n    assert s.to_python(Foo(a='hello', b=b'more')) == {'a': 'hello'}\n    assert s.to_python(Foo(a='hello', b=b'more'), mode='json') == {'a': 'hello'}\n    assert s.to_python(Foo(a='bye', b=b'more'), mode='json') == {}\n    j = s.to_json(Foo(a='hello', b=b'more'))\n    if on_pypy:\n        assert json.loads(j) == {'a': 'hello'}\n    else:\n        assert j == b'{\"a\":\"hello\"}'\n    j = s.to_json(Foo(a='bye', b=b'more'))\n    if on_pypy:\n        assert json.loads(j) == {}\n    else:\n        assert j == b'{}'"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005533", "source": "def _validate_dataclass(cls: Type['DataclassT'], v: Any) -> 'DataclassT':\n    with set_validation(cls, True):\n        if isinstance(v, cls):\n            v.__pydantic_validate_values__()\n            return v\n        elif isinstance(v, (list, tuple)):\n            return cls(*v)\n        elif isinstance(v, dict):\n            return cls(**v)\n        else:\n            raise DataclassTypeError(class_name=cls.__name__)", "target": "def test_dataclass_field_plain_validator():\n    @dataclasses.dataclass\n    class Foo:\n        a: int\n        b: str\n        @classmethod\n        def validate_b(cls, v: bytes, info: core_schema.ValidationInfo) -> str:\n            assert v == b'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return 'hello world!'\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(\n                    name='b', schema=core_schema.with_info_plain_validator_function(Foo.validate_b)\n                ),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 1, 'b': b'hello'})\n    assert dataclasses.asdict(foo) == {'a': 1, 'b': 'hello world!'}"}
{"task": "TEST_GENERATION", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "005534", "source": "def get_temp_dir() -> str:\n    global _TEMPDIR\n    if _TEMPDIR is None:\n        _TEMPDIR = _make_temp_dir(\n            prefix=\"instruction_count_microbenchmarks\", gc_dev_shm=True\n        )\n        atexit.register(shutil.rmtree, path=_TEMPDIR)\n    return _TEMPDIR", "target": "def test_is_iterable(self) -> None:\n        from collections.abc import Iterator\n        iter_ = PeekableIterator(\"\")\n        self.assertTrue(isinstance(iter_, Iterator))"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005535", "source": "def check_deprecated(config_dict: ConfigDict) -> None:\n    deprecated_removed_keys = V2_REMOVED_KEYS & config_dict.keys()\n    deprecated_renamed_keys = V2_RENAMED_KEYS.keys() & config_dict.keys()\n    if deprecated_removed_keys or deprecated_renamed_keys:\n        renamings = {k: V2_RENAMED_KEYS[k] for k in sorted(deprecated_renamed_keys)}\n        renamed_bullets = [f'* {k!r} has been renamed to {v!r}' for k, v in renamings.items()]\n        removed_bullets = [f'* {k!r} has been removed' for k in sorted(deprecated_removed_keys)]\n        message = '\\n'.join(['Valid config keys have changed in V2:'] + renamed_bullets + removed_bullets)\n        warnings.warn(message, UserWarning)", "target": "def test_field_priority_model():\n    v = SchemaValidator(\n        cs.model_schema(\n            cls=MyModel,\n            config=CoreConfig(str_max_length=10),\n            schema=cs.model_fields_schema(fields={'f': cs.model_field(schema=cs.str_schema(max_length=5))}),\n        )\n    )\n    assert 'max_length:Some(5)' in plain_repr(v)\n    assert v.isinstance_python({'f': 'test'}) is True\n    assert v.isinstance_python({'f': 'test long'}) is False"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005536", "source": "def make_dataclass_validator(dc_cls: Type['Dataclass'], config: Type[BaseConfig]) -> 'CallableGenerator':\n    yield from _get_validators(dataclass(dc_cls, config=config, use_proxy=True))", "target": "def test_by_alias_and_name_config_interaction(config, runtime, expected) -> None:\n    @dataclasses.dataclass\n    class Foo:\n        my_field: str\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(\n                    name='my_field', schema=core_schema.str_schema(), serialization_alias='my_alias'\n                ),\n            ],\n        ),\n        ['my_field'],\n        config=core_schema.CoreConfig(serialize_by_alias=config or False),\n    )\n    s = SchemaSerializer(schema)\n    assert s.to_python(Foo(my_field='hello'), by_alias=runtime) == expected"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005537", "source": "def collect_dataclass_fields(\n    cls: type[StandardDataclass],\n    *,\n    config_wrapper: ConfigWrapper,\n    ns_resolver: NsResolver | None = None,\n    typevars_map: dict[Any, Any] | None = None,\n) -> dict[str, FieldInfo]:\n    FieldInfo_ = import_cached_field_info()\n    fields: dict[str, FieldInfo] = {}\n    ns_resolver = ns_resolver or NsResolver()\n    dataclass_fields = cls.__dataclass_fields__\n    for base in reversed(cls.__mro__):\n        if not dataclasses.is_dataclass(base):\n            continue\n        with ns_resolver.push(base):\n            for ann_name, dataclass_field in dataclass_fields.items():\n                base_anns = _typing_extra.safe_get_annotations(base)\n                if ann_name not in base_anns:\n                    continue\n                globalns, localns = ns_resolver.types_namespace\n                ann_type, evaluated = _typing_extra.try_eval_type(dataclass_field.type, globalns, localns)\n                if _typing_extra.is_classvar_annotation(ann_type):\n                    continue\n                if (\n                    not dataclass_field.init\n                    and dataclass_field.default is dataclasses.MISSING\n                    and dataclass_field.default_factory is dataclasses.MISSING\n                ):\n                    continue\n                if isinstance(dataclass_field.default, FieldInfo_):\n                    if dataclass_field.default.init_var:\n                        if dataclass_field.default.init is False:\n                            raise PydanticUserError(\n                                f'Dataclass field {ann_name} has init=False and init_var=True, but these are mutually exclusive.',\n                                code='clashing-init-and-init-var',\n                            )\n                        continue\n                    field_info = FieldInfo_.from_annotated_attribute(\n                        ann_type, dataclass_field.default, _source=AnnotationSource.DATACLASS\n                    )\n                    field_info._original_assignment = dataclass_field.default\n                else:\n                    field_info = FieldInfo_.from_annotated_attribute(\n                        ann_type, dataclass_field, _source=AnnotationSource.DATACLASS\n                    )\n                    field_info._original_assignment = dataclass_field\n                if not evaluated:\n                    field_info._complete = False\n                    field_info._original_annotation = ann_type\n                fields[ann_name] = field_info\n                update_field_from_config(config_wrapper, ann_name, field_info)\n                if field_info.default is not PydanticUndefined and isinstance(\n                    getattr(cls, ann_name, field_info), FieldInfo_\n                ):\n                    setattr(cls, ann_name, field_info.default)\n    if typevars_map:\n        for field in fields.values():\n            field.apply_typevars_map(typevars_map)\n    if config_wrapper.use_attribute_docstrings:\n        _update_fields_from_docstrings(\n            cls,\n            fields,\n            use_inspect=not hasattr(cls, '__is_pydantic_dataclass__'),\n        )\n    return fields", "target": "def test_aliases_debug():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'field_a': core_schema.model_field(\n                    validation_alias=[['foo', 'bar', 'bat'], ['foo', 3]], schema=core_schema.int_schema()\n                )\n            }\n        )\n    )\n    print(repr(v))\n    assert repr(v).startswith('SchemaValidator(title=\"model-fields\", validator=ModelFields(')\n    assert 'PathChoices(' in repr(v)"}
{"task": "TEST_GENERATION", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "005538", "source": "def custom_pydantic_encoder(type_encoders: dict[Any, Callable[[type[Any]], Any]], obj: Any) -> Any:\n    warnings.warn(\n        '`custom_pydantic_encoder` is deprecated, use `BaseModel.model_dump` instead.',\n        category=PydanticDeprecatedSince20,\n        stacklevel=2,\n    )\n    for base in obj.__class__.__mro__[:-1]:\n        try:\n            encoder = type_encoders[base]\n        except KeyError:\n            continue\n        return encoder(obj)\n    else:\n        return pydantic_encoder(obj)", "target": "def test_to_jsonable_python():\n    assert to_jsonable_python([1, 2]) == [1, 2]\n    assert to_jsonable_python({1, 2}) == IsList(1, 2, check_order=False)\n    assert to_jsonable_python([1, b'x']) == [1, 'x']\n    assert to_jsonable_python([0, 1, 2, 3, 4], exclude={1, 3}) == [0, 2, 4]"}
