{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "002493", "source": "def name(self):\n        prefix = f\"{self.category()}_{self._name}_{self.backend()}\"\n        return prefix", "target": "def name(self):\n        prefix = f\"{self.category()}_{self._name}_{self.backend()}\"\n        return prefix", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "002494", "source": "def test_exclude():\n    s = SchemaSerializer(core_schema.dict_schema(serialization=core_schema.filter_dict_schema(exclude={'a', 'c'})))\n    assert s.to_python({'a': 1, 'b': 2, 'c': 3, 'd': 4}) == {'b': 2, 'd': 4}\n    assert s.to_json({'a': 1, 'b': 2, 'c': 3, 'd': 4}) == b'{\"b\":2,\"d\":4}'\n    assert s.to_python({'a': 1, 'b': 2, 'c': 3, 'd': 4}, exclude={'d'}) == {'b': 2}\n    assert s.to_python({'a': 1, 'b': 2, 'c': 3, 'd': 4}, exclude={'__all__'}) == {}\n    assert s.to_python({'a': 1, 'b': 2, 'c': 3, 'd': 4}, exclude={'d': ...}) == {'b': 2}\n    assert s.to_python({'a': 1, 'b': 2, 'c': 3, 'd': 4}, exclude={'d': {1}}) == {'b': 2, 'd': 4}\n    assert s.to_json({'a': 1, 'b': 2, 'c': 3, 'd': 4}, exclude={'d'}) == b'{\"b\":2}'", "target": "def test_exclude(schema_func, seq_f):\n    v = SchemaSerializer(\n        schema_func(core_schema.any_schema(), serialization=core_schema.filter_seq_schema(exclude={1, 3, 5}))\n    )\n    assert v.to_python(seq_f(0, 1, 2, 3)) == seq_f(0, 2)\n    assert v.to_python(seq_f('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h')) == seq_f('a', 'c', 'e', 'g', 'h')\n    assert v.to_python(seq_f('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'), mode='json') == ['a', 'c', 'e', 'g', 'h']\n    assert v.to_json(seq_f('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h')) == b'[\"a\",\"c\",\"e\",\"g\",\"h\"]'\n    assert v.to_python(seq_f('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'), exclude={6}) == seq_f('a', 'c', 'e', 'h')\n    assert v.to_python(seq_f('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'), exclude={-1, -2}) == seq_f('a', 'c', 'e')\n    assert v.to_json(seq_f('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'), exclude={6}) == b'[\"a\",\"c\",\"e\",\"h\"]'\n    assert v.to_json(seq_f('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'), exclude={-1, -2}) == b'[\"a\",\"c\",\"e\"]'", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "test", "example_id": "002495", "source": "def test_is_instance_cases(schema_class, input_val, value):\n    v = SchemaValidator(cs.is_instance_schema(cls=schema_class))\n    assert v.isinstance_python(input_val) == value", "target": "def s2(v: int) -> int:\n        return v + 2", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "test", "example_id": "002496", "source": "def outMeta(desc):\n            out_desc = desc.withType(desc.depth, 1)\n            return out_desc, out_desc, out_desc", "target": "def tagged_union_of_unions_schema(model_a_b_union_schema: core_schema.UnionSchema) -> core_schema.UnionSchema:\n    return core_schema.union_schema(\n        [\n            model_a_b_union_schema,\n            core_schema.tagged_union_schema(\n                discriminator='type_',\n                choices={\n                    'cat': core_schema.model_schema(\n                        cls=ModelCat,\n                        schema=core_schema.model_fields_schema(\n                            fields={\n                                'type_': core_schema.model_field(core_schema.literal_schema(['cat'])),\n                            },\n                        ),\n                    ),\n                    'dog': core_schema.model_schema(\n                        cls=ModelDog,\n                        schema=core_schema.model_fields_schema(\n                            fields={\n                                'type_': core_schema.model_field(core_schema.literal_schema(['dog'])),\n                            },\n                        ),\n                    ),\n                },\n            ),\n        ]\n    )", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|pydantic/pydantic", "data_split": "test", "example_id": "002497", "source": "def create_species_bunch(species_name, train, test, coverages, xgrid, ygrid):\n    bunch = Bunch(name=\" \".join(species_name.split(\"_\")[:2]))\n    species_name = species_name.encode(\"ascii\")\n    points = dict(test=test, train=train)\n    for label, pts in points.items():\n        pts = pts[pts[\"species\"] == species_name]\n        bunch[\"pts_%s\" % label] = pts\n        ix = np.searchsorted(xgrid, pts[\"dd long\"])\n        iy = np.searchsorted(ygrid, pts[\"dd lat\"])\n        bunch[\"cov_%s\" % label] = coverages[:, -iy, ix].T\n    return bunch", "target": "def test_frozenset_from_dict_items(input_value, items_schema, expected):\n    v = SchemaValidator(core_schema.tuple_schema(items_schema=[items_schema], variadic_item_index=0))\n    output = v.validate_python(input_value)\n    assert isinstance(output, tuple)\n    assert output == expected", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "test", "example_id": "002498", "source": "def test_function_wrap_str():\n    def f(input_value, validator, info):\n        return plain_repr(validator)\n    v = SchemaValidator(core_schema.with_info_wrap_validator_function(f, core_schema.str_schema()))\n    assert (\n        v.validate_python('input value')\n        == 'ValidatorCallable(Str(StrValidator{strict:false,coerce_numbers_to_str:false}))'\n    )", "target": "def chunker(seq, size):\n    return (seq[pos : pos + size] for pos in range(0, len(seq), size))", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|opencv/opencv", "data_split": "test", "example_id": "002499", "source": "def setup_torchbench_cwd():\n    original_dir = abspath(os.getcwd())\n    os.environ[\"KALDI_ROOT\"] = \"/tmp\"\n    for torchbench_dir in (\n        \"./torchbenchmark\",\n        \"../torchbenchmark\",\n        \"../torchbench\",\n        \"../benchmark\",\n        \"../../torchbenchmark\",\n        \"../../torchbench\",\n        \"../../benchmark\",\n        \"../../../torchbenchmark\",\n        \"../../../torchbench\",\n        \"../../../benchmark\",\n    ):\n        if exists(torchbench_dir):\n            break\n    if exists(torchbench_dir):\n        torchbench_dir = abspath(torchbench_dir)\n        os.chdir(torchbench_dir)\n        sys.path.append(torchbench_dir)\n    return original_dir", "target": "def test_custom_op_addC(self):\n            sz = (3, 3, 3)\n            in_mat = np.full(sz, 45, dtype=np.uint8)\n            sc = (50, 10, 20)\n            expected = in_mat + np.array(sc, dtype=np.uint8)\n            g_in  = cv.GMat()\n            g_sc  = cv.GScalar()\n            g_out = GAddC.on(g_in, g_sc, cv.CV_8UC1)\n            comp  = cv.GComputation(cv.GIn(g_in, g_sc), cv.GOut(g_out))\n            pkg = cv.gapi.kernels(GAddCImpl)\n            actual = comp.apply(cv.gin(in_mat, sc), args=cv.gapi.compile_args(pkg))\n            self.assertEqual(0.0, cv.norm(expected, actual, cv.NORM_INF))", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "002500", "source": "def test_alias_error_loc_field_names(py_and_json: PyAndJson):\n    v = py_and_json(\n        {\n            'type': 'model-fields',\n            'fields': {\n                'field_a': {\n                    'type': 'model-field',\n                    'schema': {'type': 'int'},\n                    'validation_alias': [['foo'], ['bar', 1, -1]],\n                }\n            },\n        },\n        {'loc_by_alias': False},\n    )\n    assert v.validate_test({'foo': 42}) == ({'field_a': 42}, None, {'field_a'})\n    assert v.validate_test({'bar': ['x', [1, 2, 42]]}) == ({'field_a': 42}, None, {'field_a'})\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_test({'foo': 'not_int'})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('field_a',),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'not_int',\n        }\n    ]\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_test({'bar': ['x', [1, 2, 'not_int']]})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('field_a',),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'not_int',\n        }\n    ]\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_test({})\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'missing', 'loc': ('field_a',), 'msg': 'Field required', 'input': {}}\n    ]", "target": "def test_alias_error_loc_field_names(py_and_json: PyAndJson):\n    v = py_and_json(\n        {\n            'type': 'typed-dict',\n            'fields': {\n                'field_a': {\n                    'type': 'typed-dict-field',\n                    'schema': {'type': 'int'},\n                    'validation_alias': [['foo'], ['bar', 1, -1]],\n                }\n            },\n            'config': {'loc_by_alias': False},\n        }\n    )\n    assert v.validate_test({'foo': 42}) == {'field_a': 42}\n    assert v.validate_test({'bar': ['x', [1, 2, 42]]}) == {'field_a': 42}\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_test({'foo': 'not_int'})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('field_a',),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'not_int',\n        }\n    ]\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_test({'bar': ['x', [1, 2, 'not_int']]})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('field_a',),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'not_int',\n        }\n    ]\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_test({})\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'missing', 'loc': ('field_a',), 'msg': 'Field required', 'input': {}}\n    ]", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "test", "example_id": "002501", "source": "def children_types(self) -> Tuple[ASTNodeType, ...]:\n        return (ASTNodeType.Class, ASTNodeType.Function,\n                ASTNodeType.Enumeration, ASTNodeType.Constant)", "target": "def children_types(self) -> Tuple[ASTNodeType, ...]:\n        return ()", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "002502", "source": "def compiled(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, w = args\n        torch._dynamo.mark_dynamic(x, 0)\n        compiled_rms_norm = torch.compile(\n            self.rms_norm_ref, mode=self.compile_mode, fullgraph=True\n        )\n        return lambda: compiled_rms_norm(x, w)", "target": "def compiled(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, w = args\n        torch._dynamo.mark_dynamic(x, 0)\n        compiled_layernorm = torch.compile(\n            self.layernorm_ref, mode=self.compile_mode, fullgraph=True\n        )\n        return lambda: compiled_layernorm(x, w, eps=1e-6)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "002503", "source": "def eager(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, target, dloss = args\n        loss = F.cross_entropy(x, target, reduction=\"none\")\n        return lambda: torch.autograd.grad(\n            loss, x, grad_outputs=dloss, retain_graph=True\n        )", "target": "def eager(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, w = args\n        return lambda: self.layernorm_ref(x, w)", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "test", "example_id": "002504", "source": "def relative_typename(self, module: str) -> str:\n        return self.full_typename", "target": "def relative_typename(self, module: str) -> str:\n        return self.type_format.format(self.types_separator.join(\n            item.relative_typename(module) for item in self\n        ))", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "test", "example_id": "002505", "source": "def get_nn_runners(*names):\n    return [nn_runners[name] for name in names]", "target": "def constrain(self: _Pipeline[_InT, _NewOutGe], constraint: annotated_types.Ge) -> _Pipeline[_InT, _NewOutGe]: ...", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "test", "example_id": "002506", "source": "def test_omit(py_and_json: PyAndJson):\n    def omit(v, info):\n        if v == 'omit':\n            raise PydanticOmit\n        elif v == 'error':\n            raise ValueError('error')\n        else:\n            return v\n    v = py_and_json(core_schema.with_info_plain_validator_function(omit))\n    assert v.validate_test('foo') == 'foo'\n    if v.validator_type == 'python':\n        assert v.isinstance_test('foo') is True\n    if v.validator_type == 'python':\n        assert v.isinstance_test('error') is False\n    with pytest.raises(SchemaError, match='Uncaught Omit error, please check your usage of `default` validators.'):\n        v.validate_test('omit')", "target": "def test_dataclass_path_field_with_default_value(self):\n        @dataclass\n        class C2:\n            out: Path = m.env_path_field(\"OUT\", default=\"some/dir\", resolve=False)\n        with patch.dict(os.environ, {}, clear=True):\n            c = C2()\n            self.assertEqual(c.out, Path(\"some/dir\"))", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "test", "example_id": "002507", "source": "def test_complex_json(value, expected):\n    v = SchemaSerializer(core_schema.complex_schema())\n    c = v.to_python(value)\n    c_json = v.to_python(value, mode='json')\n    json_str = v.to_json(value).decode()\n    assert c_json == expected\n    assert json_str == f'\"{expected}\"'\n    if math.isnan(value.imag):\n        assert math.isnan(c.imag)\n    else:\n        assert c.imag == value.imag\n    if math.isnan(value.real):\n        assert math.isnan(c.real)\n    else:\n        assert c.imag == value.imag", "target": "def complex_schema(\n    *,\n    strict: bool | None = None,\n    ref: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> ComplexSchema:\n    return _dict_not_none(\n        type='complex',\n        strict=strict,\n        ref=ref,\n        metadata=metadata,\n        serialization=serialization,\n    )", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "test", "example_id": "002508", "source": "def typename(self) -> str:\n        return self.type_node.full_typename", "target": "def typename(self) -> str:\n        return \"\"", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "002509", "source": "def _prepare_once(self):\n        self.x = torch.randn(4, 4, requires_grad=True)", "target": "def _prepare_once(self):\n        self.m = self.ModuleClass()\n        torch.set_float32_matmul_precision(\"high\")\n        self.input = torch.ones(10, device=self.device())", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pytorch/pytorch", "data_split": "test", "example_id": "002510", "source": "def runTracker(self):\n        framesCounter = 0\n        self.selection = True\n        xmin, ymin, xmax, ymax = self.render.getCurrentRect()\n        self.track_window = (xmin, ymin, xmax - xmin, ymax - ymin)\n        while True:\n            framesCounter += 1\n            self.frame = self.render.getNextFrame()\n            hsv = cv.cvtColor(self.frame, cv.COLOR_BGR2HSV)\n            mask = cv.inRange(hsv, np.array((0., 60., 32.)), np.array((180., 255., 255.)))\n            if self.selection:\n                x0, y0, x1, y1 = self.render.getCurrentRect() + 50\n                x0 -= 100\n                y0 -= 100\n                hsv_roi = hsv[y0:y1, x0:x1]\n                mask_roi = mask[y0:y1, x0:x1]\n                hist = cv.calcHist( [hsv_roi], [0], mask_roi, [16], [0, 180] )\n                cv.normalize(hist, hist, 0, 255, cv.NORM_MINMAX)\n                self.hist = hist.reshape(-1)\n                self.selection = False\n            if self.track_window and self.track_window[2] > 0 and self.track_window[3] > 0:\n                self.selection = None\n                prob = cv.calcBackProject([hsv], [0], self.hist, [0, 180], 1)\n                prob &= mask\n                term_crit = ( cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 1 )\n                _track_box, self.track_window = cv.CamShift(prob, self.track_window, term_crit)\n            trackingRect = np.array(self.track_window)\n            trackingRect[2] += trackingRect[0]\n            trackingRect[3] += trackingRect[1]\n            if intersectionRate(self.render.getCurrentRect(), trackingRect) < 0.4:\n                self.errors += 1\n            if framesCounter > self.framesNum:\n                break\n        self.assertLess(float(self.errors) / self.framesNum, 0.4)", "target": "def get_symbols(lib: str) -> list[tuple[str, str, str]]:\n    from subprocess import check_output\n    lines = check_output(f'nm \"{lib}\"|c++filt', shell=True)\n    return [x.split(\" \", 2) for x in lines.decode(\"latin1\").split(\"\\n\")[:-1]]", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "test", "example_id": "002511", "source": "def test_all_optional_fields_with_required_fields():\n    v = SchemaValidator(\n        core_schema.typed_dict_schema(\n            total=False,\n            fields={\n                'x': core_schema.typed_dict_field(schema=core_schema.str_schema(strict=True), required=True),\n                'y': core_schema.typed_dict_field(schema=core_schema.str_schema()),\n            },\n        )\n    )\n    assert v.validate_python({'x': 'pika', 'y': 'chu'}) == {'x': 'pika', 'y': 'chu'}\n    assert v.validate_python({'x': 'pika'}) == {'x': 'pika'}\n    with pytest.raises(ValidationError) as exc_info:\n        assert v.validate_python({'y': 'chu'}) == ({'y': 'chu'}, {'y'})\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'missing', 'loc': ('x',), 'msg': 'Field required', 'input': {'y': 'chu'}}\n    ]", "target": "def test_keyword_only_error_required(py_and_json: PyAndJson, input_value) -> None:\n    v = py_and_json(\n        cs.arguments_v3_schema(\n            [\n                cs.arguments_v3_parameter(name='a', schema=cs.int_schema(), mode='keyword_only'),\n            ]\n        )\n    )\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_test(input_value)\n    error = exc_info.value.errors()[0]\n    assert error['type'] == 'missing_keyword_only_argument'\n    assert error['loc'] == ('a',)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "test", "example_id": "002512", "source": "def test_filter_runtime():\n    s = SchemaSerializer(\n        core_schema.dict_schema(\n            core_schema.any_schema(), serialization=core_schema.filter_dict_schema(exclude={'0', '1'})\n        )\n    )\n    assert s.to_python({'0': 0, '1': 1, '2': 2, '3': 3}, include={'1', '2'}) == {'1': 1, '2': 2}\n    assert s.to_python({'0': 0, '1': 1, '2': 2, '3': 3}, include={'1', '2'}, exclude={'2', '3'}) == {'1': 1}", "target": "def have_git() -> bool:\n    try:\n        subprocess.check_output(['git', '--help'])\n        return True\n    except subprocess.CalledProcessError:\n        return False\n    except OSError:\n        return False", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "test", "example_id": "002513", "source": "def test_datetime_strict_json(input_value, expected):\n    v = SchemaValidator(cs.datetime_schema(strict=True))\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            v.validate_json(json.dumps(input_value))\n    else:\n        output = v.validate_json(json.dumps(input_value))\n        assert output == expected", "target": "def param(self):\n        return '::'.join(filter(None, [self.type_param, self.value_param]))", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "test", "example_id": "002514", "source": "def test_unused_ref():\n    v = SchemaValidator(\n        cs.typed_dict_schema(\n            fields={\n                'name': cs.typed_dict_field(schema=cs.str_schema()),\n                'other': cs.typed_dict_field(schema=cs.int_schema()),\n            },\n            ref='Branch',\n        )\n    )\n    assert v.validate_python({'name': 'root', 'other': '4'}) == {'name': 'root', 'other': 4}", "target": "def wrapped_func(*args, **kwargs):\n                    if self.has_failure:\n                        if ret_type_on_failure is None:\n                            return None\n                        return ret_type_on_failure()\n                    try:\n                        ret_type = func(*args, **kwargs)\n                    except Exception:\n                        self.has_failure = True\n                        warnings.warn(\n                            \"Typing stubs generation has failed.\\n{}\".format(\n                                traceback.format_exc()\n                            )\n                        )\n                        if ret_type_on_failure is None:\n                            return None\n                        return ret_type_on_failure()\n                    return ret_type", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "002515", "source": "def test_alias(py_and_json: PyAndJson, input_value, expected) -> None:\n    v = py_and_json(\n        cs.arguments_v3_schema(\n            [\n                cs.arguments_v3_parameter(name='a', schema=cs.int_schema(), alias='Foo', mode='positional_or_keyword'),\n            ]\n        )\n    )\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            v.validate_test(input_value)\n    else:\n        assert v.validate_test(input_value) == expected", "target": "def test_alias(py_and_json: PyAndJson, input_value, expected):\n    v = py_and_json(\n        {\n            'type': 'arguments',\n            'arguments_schema': [\n                {'name': 'a', 'mode': 'positional_or_keyword', 'schema': {'type': 'int'}, 'alias': 'Foo'}\n            ],\n        }\n    )\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            v.validate_test(input_value)\n    else:\n        assert v.validate_test(input_value) == expected", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "002516", "source": "def _work(self):\n        with (\n            fresh_cache(),\n            torch._inductor.config.patch(force_shape_pad=self._force_shape_pad),\n        ):\n            opt_m = torch.compile(backend=self.backend(), dynamic=self.is_dynamic())(\n                self.m.cuda() if self._is_gpu else self.m\n            )\n            opt_m(self.input)", "target": "def _work(self) -> None:\n        @torch.compile(\n            backend=\"inductor\",\n            fullgraph=True,\n            dynamic=self._dynamic,\n        )\n        def f(a, b):\n            z = torch.mm(a, b)\n            for i in range(200):\n                z = torch.mm(z, b)\n            return z\n        with fresh_cache(), torch._inductor.config.patch(max_autotune=True):\n            f(self.a, self.b)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "test", "example_id": "002517", "source": "def str_strip(self: _Pipeline[_InT, str]) -> _Pipeline[_InT, str]:\n        return self.transform(str.strip)", "target": "def test_tuple_fix_error():\n    v = SchemaValidator(core_schema.tuple_schema(items_schema=[core_schema.int_schema(), core_schema.str_schema()]))\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python([1])\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'missing', 'loc': (1,), 'msg': 'Field required', 'input': [1]}\n    ]", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|scikit-learn/scikit-learn", "data_split": "test", "example_id": "002518", "source": "def get_gomp_thread():\n    python_path = Path(sys.executable).resolve()\n    python_prefix = (\n        python_path.parent.parent\n    )\n    abiflags = getattr(sys, \"abiflags\", \"\")\n    python_version = (\n        f\"python{sys.version_info.major}.{sys.version_info.minor}{abiflags}\"\n    )\n    libtorch_cpu_path = (\n        python_prefix\n        / \"lib\"\n        / python_version\n        / \"site-packages\"\n        / \"torch\"\n        / \"lib\"\n        / \"libtorch_cpu.so\"\n    )\n    libgomp_path = \"/usr/lib64/libgomp.so.1\"\n    if not os.path.exists(libgomp_path):\n        libgomp_path = f\"/usr/lib/{os.uname().machine}-linux-gnu/libgomp.so.1\"\n    os.environ[\"GOMP_CPU_AFFINITY\"] = \"0-3\"\n    libgomp = ctypes.CDLL(libgomp_path)\n    libgomp = ctypes.CDLL(libtorch_cpu_path)\n    libgomp.omp_get_max_threads.restype = ctypes.c_int\n    libgomp.omp_get_max_threads.argtypes = []\n    omp_max_threads = libgomp.omp_get_max_threads()\n    return omp_max_threads", "target": "def _plot(\n    results,\n    metrics,\n    formats,\n    title,\n    x_ticks,\n    x_label,\n    format_markers=(\"x\", \"|\", \"o\", \"+\"),\n    metric_colors=(\"c\", \"m\", \"y\", \"k\", \"g\", \"r\", \"b\"),\n):\n    fig = plt.figure(\"scikit-learn multilabel metrics benchmarks\")\n    plt.title(title)\n    ax = fig.add_subplot(111)\n    for i, metric in enumerate(metrics):\n        for j, format in enumerate(formats):\n            ax.plot(\n                x_ticks,\n                results[i, j].flat,\n                label=\"{}, {}\".format(metric, format),\n                marker=format_markers[j],\n                color=metric_colors[i % len(metric_colors)],\n            )\n    ax.set_xlabel(x_label)\n    ax.set_ylabel(\"Time (s)\")\n    ax.legend()\n    plt.show()", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "test", "example_id": "002519", "source": "def get_pytorch_preprocess(img):\n    img = img.astype(np.float32)\n    img *= BASE_IMG_SCALE_FACTOR\n    img -= [0.485, 0.456, 0.406]\n    img /= [0.229, 0.224, 0.225]\n    return img", "target": "def test_properties():\n    @dataclasses.dataclass\n    class FooProp:\n        a: str\n        b: bytes\n        @property\n        def c(self) -> str:\n            return f'{self.a} {self.b.decode()}'\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'FooProp',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bytes_schema()),\n            ],\n            computed_fields=[core_schema.computed_field('c', core_schema.str_schema())],\n        ),\n        ['a', 'b'],\n    )\n    s = SchemaSerializer(schema)\n    assert s.to_python(FooProp(a='hello', b=b'more')) == IsStrictDict(a='hello', b=b'more', c='hello more')\n    assert s.to_python(FooProp(a='hello', b=b'more'), mode='json') == IsStrictDict(a='hello', b='more', c='hello more')\n    j = s.to_json(FooProp(a='hello', b=b'more'))\n    if on_pypy:\n        assert json.loads(j) == {'a': 'hello', 'b': 'more', 'c': 'hello more'}\n    else:\n        assert j == b'{\"a\":\"hello\",\"b\":\"more\",\"c\":\"hello more\"}'\n    assert s.to_python(FooProp(a='hello', b=b'more'), exclude={'b'}) == IsStrictDict(a='hello', c='hello more')\n    assert s.to_json(FooProp(a='hello', b=b'more'), include={'a'}) == b'{\"a\":\"hello\"}'", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "002520", "source": "def name(self) -> str:\n        prefix = f\"{self.category()}_{self.backend()}\"\n        if self.is_dynamic():\n            prefix += \"_dynamic\"\n        if self.device() == \"cuda\":\n            prefix += \"_gpu\"\n        return prefix", "target": "def name(self) -> str:\n        if self.enable_persistent_tma_matmul:\n            return \"triton_persistent_tma\"\n        else:\n            return \"triton\"", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "002521", "source": "def area(self) -> int:\n            return self.width * self.height", "target": "def area(self) -> float:\n            return self.side**2", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "test", "example_id": "002522", "source": "def collect_field_or_class_var_from_stmt(\n        self, stmt: AssignmentStmt, model_config: ModelConfigData, class_vars: dict[str, PydanticModelClassVar]\n    ) -> PydanticModelField | PydanticModelClassVar | None:\n        cls = self._cls\n        lhs = stmt.lvalues[0]\n        if not isinstance(lhs, NameExpr) or not _fields.is_valid_field_name(lhs.name) or lhs.name == 'model_config':\n            return None\n        if not stmt.new_syntax:\n            if (\n                isinstance(stmt.rvalue, CallExpr)\n                and isinstance(stmt.rvalue.callee, CallExpr)\n                and isinstance(stmt.rvalue.callee.callee, NameExpr)\n                and stmt.rvalue.callee.callee.fullname in DECORATOR_FULLNAMES\n            ):\n                return None\n            if lhs.name in class_vars:\n                return None\n            error_untyped_fields(self._api, stmt)\n            return None\n        lhs = stmt.lvalues[0]\n        if not isinstance(lhs, NameExpr):\n            return None\n        if not _fields.is_valid_field_name(lhs.name) or lhs.name == 'model_config':\n            return None\n        sym = cls.info.names.get(lhs.name)\n        if sym is None:\n            return None\n        node = sym.node\n        if isinstance(node, PlaceholderNode):\n            return None\n        if isinstance(node, TypeAlias):\n            self._api.fail(\n                'Type aliases inside BaseModel definitions are not supported at runtime',\n                node,\n            )\n            return None\n        if not isinstance(node, Var):\n            return None\n        if node.is_classvar:\n            return PydanticModelClassVar(lhs.name)\n        node_type = get_proper_type(node.type)\n        if isinstance(node_type, Instance) and node_type.type.fullname == 'dataclasses.InitVar':\n            self._api.fail(\n                'InitVar is not supported in BaseModel',\n                node,\n            )\n        has_default = self.get_has_default(stmt)\n        strict = self.get_strict(stmt)\n        if sym.type is None and node.is_final and node.is_inferred:\n            typ = self._api.analyze_simple_literal_type(stmt.rvalue, is_final=True)\n            if typ:\n                node.type = typ\n            else:\n                self._api.fail(\n                    'Need type argument for Final[...] with non-literal default in BaseModel',\n                    stmt,\n                )\n                node.type = AnyType(TypeOfAny.from_error)\n        if node.is_final and has_default:\n            return PydanticModelClassVar(lhs.name)\n        alias, has_dynamic_alias = self.get_alias_info(stmt)\n        if (\n            has_dynamic_alias\n            and not (model_config.validate_by_name or model_config.populate_by_name)\n            and self.plugin_config.warn_required_dynamic_aliases\n        ):\n            error_required_dynamic_aliases(self._api, stmt)\n        is_frozen = self.is_field_frozen(stmt)\n        init_type = self._infer_dataclass_attr_init_type(sym, lhs.name, stmt)\n        return PydanticModelField(\n            name=lhs.name,\n            has_dynamic_alias=has_dynamic_alias,\n            has_default=has_default,\n            strict=strict,\n            alias=alias,\n            is_frozen=is_frozen,\n            line=stmt.line,\n            column=stmt.column,\n            type=init_type,\n            info=cls.info,\n        )", "target": "def ip_v4_address_validator(input_value: Any, /) -> IPv4Address:\n    if isinstance(input_value, IPv4Address):\n        return input_value\n    try:\n        return IPv4Address(input_value)\n    except ValueError:\n        raise PydanticCustomError('ip_v4_address', 'Input is not a valid IPv4 address')", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "002523", "source": "def get_memory_bytes(self, args, kwargs) -> int:\n        (x,) = args\n        M, N = x.shape\n        return 2 * M * N * x.dtype.itemsize", "target": "def get_memory_bytes(self, args, kwargs) -> int:\n        x, dy = args\n        M, N = x.shape\n        return 3 * M * N * x.dtype.itemsize", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|opencv/opencv", "data_split": "test", "example_id": "002524", "source": "def _run_git(self, *args: Any) -> str:\n        if self.debug:\n            print(f\"+ git -C {self.repo_dir} {' '.join(args)}\")\n        return _check_output([\"git\", \"-C\", self.repo_dir] + list(args))", "target": "def test_parse_to_rect_not_convertible(self):\n        for not_convertible in (np.empty(shape=(4, 1)), (), [], np.array([]), (12, ),\n                                [3, 4, 5, 10, 123], {1: 2, 3:4, 5:10, 6:30},\n                                '1234', np.array([1, 2, 3, 4], dtype=np.float32),\n                                np.array([[1, 2], [3, 4], [5, 6], [6, 8]]), (1, 2, 5, 1.5)):\n            with self.assertRaises((TypeError), msg=get_no_exception_msg(not_convertible)):\n                _ = cv.utils.dumpRect(not_convertible)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "test", "example_id": "002525", "source": "def test_empty_string_field_name(py_and_json: PyAndJson):\n    v = py_and_json({'type': 'typed-dict', 'fields': {'': {'type': 'typed-dict-field', 'schema': {'type': 'int'}}}})\n    assert v.validate_test({'': 123}) == {'': 123}", "target": "def test_invalid_type():\n    with pytest.raises(SchemaError, match=\"TypeError: 'Foo' object cannot be cast as 'type\"):\n        SchemaValidator(core_schema.is_subclass_schema(Foo()))", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "test", "example_id": "002526", "source": "def model_serializer(\n    f: _ModelPlainSerializerT | _ModelWrapSerializerT | None = None,\n    /,\n    *,\n    mode: Literal['plain', 'wrap'] = 'plain',\n    when_used: WhenUsed = 'always',\n    return_type: Any = PydanticUndefined,\n) -> (\n    _ModelPlainSerializerT\n    | Callable[[_ModelWrapSerializerT], _ModelWrapSerializerT]\n    | Callable[[_ModelPlainSerializerT], _ModelPlainSerializerT]\n):\n    def dec(f: ModelSerializer) -> _decorators.PydanticDescriptorProxy[Any]:\n        dec_info = _decorators.ModelSerializerDecoratorInfo(mode=mode, return_type=return_type, when_used=when_used)\n        return _decorators.PydanticDescriptorProxy(f, dec_info)\n    if f is None:\n        return dec\n    else:\n        return dec(f)", "target": "def _prepare_once(self):\n        torch._dynamo.config.capture_scalar_outputs = True\n        torch.manual_seed(0)\n        self.splits = torch.randint(10, (self.N,))", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "test", "example_id": "002527", "source": "def _iter(\n    self: BaseModel,\n    to_dict: bool = False,\n    by_alias: bool = False,\n    include: AbstractSetIntStr | MappingIntStrAny | None = None,\n    exclude: AbstractSetIntStr | MappingIntStrAny | None = None,\n    exclude_unset: bool = False,\n    exclude_defaults: bool = False,\n    exclude_none: bool = False,\n) -> TupleGenerator:\n    if exclude is not None:\n        exclude = _utils.ValueItems.merge(\n            {k: v.exclude for k, v in self.__pydantic_fields__.items() if v.exclude is not None}, exclude\n        )\n    if include is not None:\n        include = _utils.ValueItems.merge(dict.fromkeys(self.__pydantic_fields__, True), include, intersect=True)\n    allowed_keys = _calculate_keys(self, include=include, exclude=exclude, exclude_unset=exclude_unset)\n    if allowed_keys is None and not (to_dict or by_alias or exclude_unset or exclude_defaults or exclude_none):\n        yield from self.__dict__.items()\n        if self.__pydantic_extra__:\n            yield from self.__pydantic_extra__.items()\n        return\n    value_exclude = _utils.ValueItems(self, exclude) if exclude is not None else None\n    value_include = _utils.ValueItems(self, include) if include is not None else None\n    if self.__pydantic_extra__ is None:\n        items = self.__dict__.items()\n    else:\n        items = list(self.__dict__.items()) + list(self.__pydantic_extra__.items())\n    for field_key, v in items:\n        if (allowed_keys is not None and field_key not in allowed_keys) or (exclude_none and v is None):\n            continue\n        if exclude_defaults:\n            try:\n                field = self.__pydantic_fields__[field_key]\n            except KeyError:\n                pass\n            else:\n                if not field.is_required() and field.default == v:\n                    continue\n        if by_alias and field_key in self.__pydantic_fields__:\n            dict_key = self.__pydantic_fields__[field_key].alias or field_key\n        else:\n            dict_key = field_key\n        if to_dict or value_include or value_exclude:\n            v = _get_value(\n                type(self),\n                v,\n                to_dict=to_dict,\n                by_alias=by_alias,\n                include=value_include and value_include.for_element(field_key),\n                exclude=value_exclude and value_exclude.for_element(field_key),\n                exclude_unset=exclude_unset,\n                exclude_defaults=exclude_defaults,\n                exclude_none=exclude_none,\n            )\n        yield dict_key, v", "target": "def input_data_strict():\n    from datetime import date, datetime, time\n    from uuid import UUID\n    input_data = input_data_lax()\n    input_data.update(\n        field_date=date(2010, 2, 3),\n        field_date_con=date(2020, 1, 1),\n        field_time=time(12, 0, 0),\n        field_time_con=time(12, 0, 0),\n        field_datetime=datetime(2020, 1, 1, 12, 13, 14),\n        field_datetime_con=datetime(2020, 1, 1),\n        field_uuid=UUID('12345678-1234-5678-1234-567812345678'),\n        field_decimal=Decimal('42.0'),\n    )\n    return input_data", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "test", "example_id": "002528", "source": "def outMeta(desc):\n            out_desc = desc.withType(desc.depth, 1)\n            return out_desc, out_desc, out_desc", "target": "def outMeta(mat_desc, scalar_desc, dtype):\n            return mat_desc", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "002529", "source": "def get_custom_getitem():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={'field_a': core_schema.model_field(validation_alias=['foo'], schema=core_schema.int_schema())}\n        )\n    )\n    assert v.validate_python(GetItemThing()) == ({'field_a': 321}, {}, {'field_a'})\n    assert v.validate_python({'bar': GetItemThing()}) == ({'field_a': 321}, {}, {'field_a'})", "target": "def get_custom_getitem():\n    v = SchemaValidator(\n        core_schema.typed_dict_schema(\n            fields={'field_a': core_schema.typed_dict_field(validation_alias=['foo'], schema=core_schema.int_schema())}\n        )\n    )\n    assert v.validate_python(GetItemThing()) == ({'field_a': 321}, {'field_a'})\n    assert v.validate_python({'bar': GetItemThing()}) == ({'field_a': 321}, {'field_a'})", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "test", "example_id": "002530", "source": "def test_dict_cases(input_value, expected):\n    v = SchemaValidator(cs.dict_schema(keys_schema=cs.str_schema(), values_schema=cs.str_schema()))\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            v.validate_python(input_value)\n    else:\n        assert v.validate_python(input_value) == expected", "target": "def get_memory_bytes(self, args, kwargs) -> int:\n        x, target, dloss = args\n        M, N = x.shape\n        return (\n            2 * M * N * x.dtype.itemsize\n            + M * target.dtype.itemsize\n            + M * dloss.dtype.itemsize\n        )", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pytorch/pytorch", "data_split": "test", "example_id": "002531", "source": "def test_parse_to_bool_not_convertible_extra(self):\n        for not_convertible in (np.array([False]), np.array([True])):\n            with self.assertRaises((TypeError, OverflowError),\n                                   msg=get_no_exception_msg(not_convertible)):\n                _ = cv.utils.dumpBool(not_convertible)", "target": "def forward(self, x, lengths):\n        for module in self.seq_module:\n            x = module(x)\n            mask = torch.BoolTensor(x.size()).fill_(0)\n            if x.is_cuda:\n                mask = mask.cuda()\n            for i, length in enumerate(lengths):\n                length = length.item()\n                if (mask[i].size(2) - length) > 0:\n                    mask[i].narrow(2, length, mask[i].size(2) - length).fill_(1)\n            x = x.masked_fill(mask, 0)\n        return x, lengths", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "test", "example_id": "002532", "source": "def test_houghcircles(self):\n        fn = \"samples/data/board.jpg\"\n        src = self.get_sample(fn, 1)\n        img = cv.cvtColor(src, cv.COLOR_BGR2GRAY)\n        img = cv.medianBlur(img, 5)\n        circles = cv.HoughCircles(img, cv.HOUGH_GRADIENT, 1, 10, np.array([]), 100, 30, 1, 30)[0]\n        testCircles = [[38, 181, 17.6],\n        [99.7, 166, 13.12],\n        [142.7, 160, 13.52],\n        [223.6, 110, 8.62],\n        [79.1, 206.7, 8.62],\n        [47.5, 351.6, 11.64],\n        [189.5, 354.4, 11.64],\n        [189.8, 298.9, 10.64],\n        [189.5, 252.4, 14.62],\n        [252.5, 393.4, 15.62],\n        [602.9, 467.5, 11.42],\n        [222, 210.4, 9.12],\n        [263.1, 216.7, 9.12],\n        [359.8, 222.6, 9.12],\n        [518.9, 120.9, 9.12],\n        [413.8, 113.4, 9.12],\n        [489, 127.2, 9.12],\n        [448.4, 121.3, 9.12],\n        [384.6, 128.9, 8.62]]\n        matches_counter = 0\n        for i in range(len(testCircles)):\n            for j in range(len(circles)):\n                tstCircle = circleApproximation(testCircles[i])\n                circle = circleApproximation(circles[j])\n                if convContoursIntersectiponRate(tstCircle, circle) > 0.6:\n                    matches_counter += 1\n        self.assertGreater(float(matches_counter) / len(testCircles), .5)\n        self.assertLess(float(len(circles) - matches_counter) / len(circles), .75)\n        circles_acc = cv.HoughCirclesWithAccumulator(\n            image=img,\n            method=cv.HOUGH_GRADIENT,\n            dp=1,\n            minDist=10,\n            circles=np.array([]),\n            param1=150,\n            param2=45,\n            minRadius=1,\n            maxRadius=30)\n        self.assertEqual(circles_acc.shape, (1, 2, 4))\n        self.assertEqual(circles_acc[0, 0, 3], 66.)\n        self.assertEqual(circles_acc[0, 1, 3], 62.)", "target": "def test_extra_behavior_allow_with_validate_fn_override(\n    config: Union[core_schema.CoreConfig, None],\n    schema_extra_behavior_kw: dict[str, Any],\n):\n    v = SchemaValidator(\n        core_schema.typed_dict_schema(\n            {'f': core_schema.typed_dict_field(core_schema.str_schema())},\n            **schema_extra_behavior_kw,\n            config=config,\n        )\n    )\n    m: dict[str, Any] = v.validate_python({'f': 'x', 'extra_field': '123'}, extra='allow')\n    assert m == {'f': 'x', 'extra_field': '123'}", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "test", "example_id": "002533", "source": "def constrain(self: _Pipeline[_InT, _OutT], constraint: _Eq) -> _Pipeline[_InT, _OutT]: ...", "target": "def tupleIntSumReturnTuple(\n        self, input: tuple[int, int, int]\n    ) -> tuple[tuple[int, int, int], int]:\n        sum = 0\n        for x in input:\n            sum += x\n        return (input, sum)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "test", "example_id": "002534", "source": "def get_normal_2d_mat():\n        rows = 10\n        cols = 20\n        cn = 3\n        image = np.zeros((rows, cols, cn), np.uint8)\n        image[:] = (1, 2, 127)\n        for i in range(rows):\n            for j in range(cols):\n                image[i, j, 1] = (i + j) % 256\n        return image", "target": "def test_p_or_k_optional(py_and_json: PyAndJson, input_value, expected):\n    v = py_and_json(\n        {\n            'type': 'arguments',\n            'arguments_schema': [\n                {\n                    'name': 'a',\n                    'mode': 'positional_or_keyword',\n                    'schema': {'type': 'default', 'schema': {'type': 'int'}, 'default': 1},\n                }\n            ],\n        }\n    )\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)) as exc_info:\n            v.validate_test(input_value)\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        assert v.validate_test(input_value) == expected", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|scikit-learn/scikit-learn", "data_split": "test", "example_id": "002535", "source": "def generate_vector(shape, dtype):\n    if np.issubdtype(dtype, np.integer):\n        return np.random.randint(0, 100, shape).astype(dtype)\n    else:\n        return np.random.normal(10., 12.5, shape).astype(dtype)", "target": "def random_labels(n_samples, n_classes):\n    return rng.randint(low=0, high=n_classes, size=n_samples)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "test", "example_id": "002536", "source": "def test_bytes_invalid_cpython():\n    s = SchemaSerializer(core_schema.bytes_schema())\n    with pytest.raises(UnicodeDecodeError, match=\"'utf-8' codec can't decode byte 0x81 in position 0: invalid utf-8\"):\n        s.to_python(b'\\x81', mode='json')", "target": "def set_discriminator_in_metadata(schema: CoreSchema, discriminator: Any) -> None:\n    metadata = cast('CoreMetadata', schema.setdefault('metadata', {}))\n    metadata['pydantic_internal_union_discriminator'] = discriminator", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "test", "example_id": "002537", "source": "def is_commit_hash(ref: str) -> bool:\n    \"True if ref is hexadecimal number, else false\"\n    try:\n        int(ref, 16)\n    except ValueError:\n        return False\n    return True", "target": "def parse_job_name(job_str):\n    return (part.strip() for part in job_str.split(\"/\"))", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "002538", "source": "def gen(error: bool):\n        yield 1\n        yield 2\n        if error:\n            raise RuntimeError('my error')\n        yield 3", "target": "def gen(error: bool):\n        yield 1\n        yield 2\n        if error:\n            raise RuntimeError('error')\n        yield 3", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "test", "example_id": "002539", "source": "def test_enum_keys():\n    class FooEnum(str, Enum):\n        APPLE = 'apple'\n        BANANA = 'banana'\n    class BarEnum(int, Enum):\n        ONE = 1\n    class PlainEnum(Enum):\n        TWO = 'two'\n    v = SchemaValidator(\n        core_schema.tagged_union_schema(\n            discriminator='foo',\n            choices={\n                BarEnum.ONE: core_schema.typed_dict_schema(\n                    fields={\n                        'foo': core_schema.typed_dict_field(schema=core_schema.int_schema()),\n                        'bar': core_schema.typed_dict_field(schema=core_schema.int_schema()),\n                    }\n                ),\n                FooEnum.BANANA: core_schema.typed_dict_schema(\n                    fields={\n                        'foo': core_schema.typed_dict_field(schema=core_schema.str_schema()),\n                        'spam': core_schema.typed_dict_field(\n                            schema=core_schema.list_schema(items_schema=core_schema.int_schema())\n                        ),\n                    }\n                ),\n                PlainEnum.TWO: core_schema.typed_dict_schema(\n                    fields={\n                        'foo': core_schema.typed_dict_field(schema=core_schema.any_schema()),\n                        'baz': core_schema.typed_dict_field(schema=core_schema.int_schema()),\n                    }\n                ),\n            },\n        )\n    )\n    assert v.validate_python({'foo': FooEnum.BANANA, 'spam': [1, 2, '3']}) == {'foo': FooEnum.BANANA, 'spam': [1, 2, 3]}\n    assert v.validate_python({'foo': BarEnum.ONE, 'bar': '123'}) == {'foo': BarEnum.ONE, 'bar': 123}\n    assert v.validate_python({'foo': PlainEnum.TWO, 'baz': '123'}) == {'foo': PlainEnum.TWO, 'baz': 123}\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python({'foo': FooEnum.APPLE, 'spam': [1, 2, '3']})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'union_tag_invalid',\n            'loc': (),\n            'msg': (\n                \"Input tag 'FooEnum.APPLE' found using 'foo' does not match any of the expected tags:\"\n                \" <BarEnum.ONE: 1>, <FooEnum.BANANA: 'banana'>, <PlainEnum.TWO: 'two'>\"\n            ),\n            'input': {'foo': FooEnum.APPLE, 'spam': [1, 2, '3']},\n            'ctx': {\n                'discriminator': \"'foo'\",\n                'tag': 'FooEnum.APPLE',\n                'expected_tags': \"<BarEnum.ONE: 1>, <FooEnum.BANANA: 'banana'>, <PlainEnum.TWO: 'two'>\",\n            },\n        }\n    ]", "target": "def get_times(json_data):\n    r = {}\n    for fwd_bwd in json_data:\n        for test_name in json_data[fwd_bwd]:\n            name = construct_name(fwd_bwd, test_name)\n            r[name] = json_data[fwd_bwd][test_name]\n    return r", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "002540", "source": "def test_extra_behavior_forbid(\n    config: Union[core_schema.CoreConfig, None],\n    schema_extra_behavior_kw: dict[str, Any],\n    validate_fn_extra_kw: Union[ExtraBehavior, None],\n):\n    @dataclasses.dataclass\n    class MyModel:\n        f: str\n    v = SchemaValidator(\n        core_schema.dataclass_schema(\n            MyModel,\n            core_schema.dataclass_args_schema(\n                'MyModel', [core_schema.dataclass_field('f', core_schema.str_schema())], **schema_extra_behavior_kw\n            ),\n            ['f'],\n        ),\n        config=config,\n    )\n    m: MyModel = v.validate_python({'f': 'x'}, extra=validate_fn_extra_kw)\n    assert m.f == 'x'\n    v.validate_assignment(m, 'f', 'y', extra=validate_fn_extra_kw)\n    assert m.f == 'y'\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_assignment(m, 'not_f', 'xyz', extra=validate_fn_extra_kw)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'no_such_attribute',\n            'loc': ('not_f',),\n            'msg': \"Object has no attribute 'not_f'\",\n            'input': 'xyz',\n            'ctx': {'attribute': 'not_f'},\n        }\n    ]\n    assert not hasattr(m, 'not_f')", "target": "def test_extra_behavior_forbid(\n    config: Union[core_schema.CoreConfig, None],\n    schema_extra_behavior_kw: dict[str, Any],\n    validate_fn_extra_kw: Union[ExtraBehavior, None],\n):\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            {'f': core_schema.model_field(core_schema.str_schema())}, **schema_extra_behavior_kw\n        ),\n        config=config,\n    )\n    m, model_extra, fields_set = v.validate_python({'f': 'x'}, extra=validate_fn_extra_kw)\n    assert m == {'f': 'x'}\n    assert fields_set == {'f'}\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python({'f': 'x', 'extra_field': 123}, extra=validate_fn_extra_kw)\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'extra_forbidden', 'loc': ('extra_field',), 'msg': 'Extra inputs are not permitted', 'input': 123}\n    ]\n    v.validate_assignment(m, 'f', 'y', extra=validate_fn_extra_kw)\n    assert m['f'] == 'y'\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_assignment(m, 'not_f', 'xyz', extra=validate_fn_extra_kw)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'no_such_attribute',\n            'loc': ('not_f',),\n            'msg': \"Object has no attribute 'not_f'\",\n            'input': 'xyz',\n            'ctx': {'attribute': 'not_f'},\n        }\n    ]\n    assert 'not_f' not in m", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "test", "example_id": "002541", "source": "def get_memory_usage():\n    process = psutil.Process()\n    main_memory = process.memory_full_info().pss\n    for child in process.children(recursive=True):\n        try:\n            child_mem = child.memory_full_info().pss\n            main_memory += child_mem\n        except (psutil.NoSuchProcess, psutil.AccessDenied, AttributeError):\n            print(f\"Failed to get PSS for {child}, falling back to USS\")\n            child_mem = child.memory_info().uss\n            main_memory += child_mem\n    return main_memory / (1024 * 1024)", "target": "def test_function_positional_only_default(import_execute):\n    m = import_execute(\n    )\n    foobar = m.create_function(validate)\n    assert foobar('1', 2) == (1, 2)\n    assert foobar('1') == (1, 42)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "002542", "source": "def compiled(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        (x,) = args\n        torch._dynamo.mark_dynamic(x, 0)\n        compiled_softmax = torch.compile(\n            lambda x: F.softmax(x, dim=-1), mode=self.compile_mode, fullgraph=True\n        )\n        return lambda: compiled_softmax(x)", "target": "def compiled(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, w, dy = args\n        y = torch.compile(self.rms_norm_ref, mode=self.compile_mode, fullgraph=True)(\n            x, w\n        )\n        return lambda: torch.autograd.grad(\n            y, [x, w], grad_outputs=dy, retain_graph=True\n        )", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "002543", "source": "def test_constraints_schema_validation_error(constraint: str) -> None:\n    with pytest.raises(SchemaError, match=f\"'{constraint}' must be coercible to a date instance\"):\n        SchemaValidator(cs.date_schema(**{constraint: 'bad_value'}))", "target": "def test_constraints_schema_validation_error(constraint: str) -> None:\n    with pytest.raises(SchemaError, match=f\"'{constraint}' must be coercible to a datetime instance\"):\n        SchemaValidator(cs.datetime_schema(**{constraint: 'bad_value'}))", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "test", "example_id": "002544", "source": "def quack(self, args, kwargs=None) -> Any:\n        from quack.rmsnorm import _rmsnorm_fwd\n        x, w = args\n        y = torch.empty_like(x)\n        def quack_fwd():\n            _rmsnorm_fwd(\n                x,\n                w,\n                out=y,\n                bias=None,\n                rstd=None,\n                residual=None,\n                residual_out=None,\n                eps=1e-6,\n            )\n            return y\n        return quack_fwd", "target": "def f(input_value, validator, info):\n        try:\n            return validator(input_value) * 2\n        except ValidationError as e:\n            assert e.title == 'ValidatorCallable'\n            assert str(e).startswith('1 validation error for ValidatorCallable\\n')\n            raise e", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "002545", "source": "def area(self) -> float:\n            return self.side**2", "target": "def area(self) -> None:\n            self.side = 0.0", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "test", "example_id": "002546", "source": "def test_bytes_invalid_all():\n    s = SchemaSerializer(core_schema.bytes_schema())\n    assert s.to_python(b'\\x81') == b'\\x81'\n    msg = 'Error serializing to JSON: invalid utf-8 sequence of 1 bytes from index 0'\n    with pytest.raises(PydanticSerializationError, match=msg):\n        s.to_json(b'\\x81')", "target": "def col_values(self, row: Row) -> list[str]:\n        o = self.open_nowrap_span\n        c = self.close_nowrap_span\n        return [\n            f'{o}`{row.field_type_str}`{c}',\n            f'{o}`{row.input_type_str}`{c}',\n            '' if row.strict else '',\n            f'{o}{row.input_source_str}{c}',\n            row.condition if row.condition else '',\n        ]", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "test", "example_id": "002547", "source": "def make_estimator(self, params):\n        representation, solver, n_jobs = params\n        penalty = \"l2\" if solver == \"lbfgs\" else \"l1\"\n        estimator = LogisticRegression(\n            solver=solver,\n            penalty=penalty,\n            tol=0.01,\n            n_jobs=n_jobs,\n            random_state=0,\n        )\n        return estimator", "target": "def make_estimator(self, params):\n        representation, n_jobs = params\n        n_estimators = 500 if Benchmark.data_size == \"large\" else 100\n        estimator = RandomForestClassifier(\n            n_estimators=n_estimators,\n            min_samples_split=10,\n            max_features=\"log2\",\n            n_jobs=n_jobs,\n            random_state=0,\n        )\n        return estimator", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "002548", "source": "def test_filter(benchmark):\n    v = SchemaSerializer(core_schema.list_schema(core_schema.any_schema()))\n    assert v.to_python(['a', 'b', 'c', 'd', 'e'], include={-1, -2}) == ['d', 'e']\n    @benchmark\n    def t():\n        v.to_python(['a', 'b', 'c', 'd', 'e'], include={-1, -2})", "target": "def test_filter():\n    v = SchemaSerializer(\n        core_schema.list_schema(\n            core_schema.any_schema(), serialization=core_schema.filter_seq_schema(include={1, 3, 5}, exclude={5, 6})\n        )\n    )\n    assert v.to_python([0, 1, 2, 3, 4, 5, 6, 7]) == [1, 3]", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|pydantic/pydantic", "data_split": "test", "example_id": "002549", "source": "def run_bench(X, clfs, plot_name, n_components, tol, alpha, l1_ratio):\n    start = time()\n    results = []\n    for name, clf_type, iter_range, clf_params in clfs:\n        print(\"Training %s:\" % name)\n        for rs, init in enumerate((\"nndsvd\", \"nndsvdar\", \"random\")):\n            print(\"    %s %s: \" % (init, \" \" * (8 - len(init))), end=\"\")\n            W, H = _initialize_nmf(X, n_components, init, 1e-6, rs)\n            for max_iter in iter_range:\n                clf_params[\"alpha\"] = alpha\n                clf_params[\"l1_ratio\"] = l1_ratio\n                clf_params[\"max_iter\"] = max_iter\n                clf_params[\"tol\"] = tol\n                clf_params[\"random_state\"] = rs\n                clf_params[\"init\"] = \"custom\"\n                clf_params[\"n_components\"] = n_components\n                this_loss, duration = bench_one(\n                    name, X, W, H, X.shape, clf_type, clf_params, init, n_components, rs\n                )\n                init_name = \"init='%s'\" % init\n                results.append((name, this_loss, duration, init_name))\n                print(\".\", end=\"\")\n                sys.stdout.flush()\n            print(\" \")\n    results_df = pandas.DataFrame(results, columns=\"method loss time init\".split())\n    print(\"Total time = %0.3f sec\\n\" % (time() - start))\n    plot_results(results_df, plot_name)\n    return results_df", "target": "def test_to_json_list_of_lists(benchmark):\n    data = [[i + j for j in range(10)] for i in range(1000)]\n    benchmark(to_json, data)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "002550", "source": "def test_var_kwargs_invalid_dict(py_and_json: PyAndJson) -> None:\n    v = py_and_json(\n        cs.arguments_v3_schema(\n            [\n                cs.arguments_v3_parameter(name='kwargs', schema=cs.int_schema(), mode='var_kwargs_uniform'),\n            ]\n        )\n    )\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_test({'kwargs': 'not_a_dict'})\n    error = exc_info.value.errors()[0]\n    assert error['type'] == 'dict_type'\n    assert error['loc'] == ('kwargs',)", "target": "def test_var_kwargs_invalid_dict(py_and_json: PyAndJson) -> None:\n    v = py_and_json(\n        cs.arguments_v3_schema(\n            [\n                cs.arguments_v3_parameter(\n                    name='kwargs', schema=cs.typed_dict_schema({}), mode='var_kwargs_unpacked_typed_dict'\n                ),\n            ]\n        )\n    )\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_test({'kwargs': 'not_a_dict'})\n    error = exc_info.value.errors()[0]\n    assert error['type'] == 'dict_type'\n    assert error['loc'] == ('kwargs',)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "002551", "source": "def test_empty_model():\n    v = SchemaValidator(core_schema.model_fields_schema(fields={}))\n    assert v.validate_python({}) == ({}, None, set())\n    with pytest.raises(\n        ValidationError, match=re.escape('Input should be a valid dictionary or instance of Model [type=model_type,')\n    ):\n        v.validate_python('x')", "target": "def test_empty_model():\n    v = SchemaValidator(core_schema.typed_dict_schema(fields={}))\n    assert v.validate_python({}) == {}\n    with pytest.raises(ValidationError, match=re.escape('Input should be a valid dictionary [type=dict_type,')):\n        v.validate_python('x')", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pytorch/pytorch", "data_split": "test", "example_id": "002552", "source": "def seek(self, offset, whence):\n                return self.f.seek(offset, whence)", "target": "def parse_args() -> Any:\n    from argparse import ArgumentParser\n    parser = ArgumentParser(\"Comment on a PR\")\n    parser.add_argument(\"pr_num\", type=int)\n    parser.add_argument(\"action\", type=str)\n    return parser.parse_args()", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "test", "example_id": "002553", "source": "def make_estimator(self, params):\n        estimator = LinearRegression()\n        return estimator", "target": "def make_estimator(self, params):\n        representation, precompute = params\n        estimator = Lasso(precompute=precompute, alpha=0.001, random_state=0)\n        return estimator", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "test", "example_id": "002554", "source": "def test_umat_merge_mertens(self):\n        if self.extraTestDataPath is None:\n            self.fail('Test data is not available')\n        test_data_path = os.path.join(self.extraTestDataPath, 'cv', 'hdr')\n        images, _ = load_exposure_seq(os.path.join(test_data_path, 'exposures'))\n        num_threads = cv.getNumThreads()\n        cv.setNumThreads(1)\n        merge = cv.createMergeMertens()\n        mat_result = merge.process(images)\n        umat_images = [cv.UMat(img) for img in images]\n        umat_result = merge.process(umat_images)\n        cv.setNumThreads(num_threads)\n        self.assertTrue(np.allclose(umat_result.get(), mat_result))", "target": "def test_union_list_bool_int():\n    v = SchemaValidator(\n        core_schema.union_schema(\n            choices=[\n                core_schema.list_schema(items_schema=core_schema.bool_schema()),\n                core_schema.list_schema(items_schema=core_schema.int_schema()),\n            ]\n        )\n    )\n    assert v.validate_python(['true', True, 'no']) == [True, True, False]\n    assert v.validate_python([5, 6, '789']) == [5, 6, 789]\n    assert v.validate_python(['1', '0']) == [1, 0]\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python([3, 'true'])\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'bool_parsing',\n            'loc': ('list[bool]', 0),\n            'msg': 'Input should be a valid boolean, unable to interpret input',\n            'input': 3,\n        },\n        {\n            'type': 'int_parsing',\n            'loc': ('list[int]', 1),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'true',\n        },\n    ]", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "test", "example_id": "002555", "source": "def apply_rotary_emb(x: Tensor, freqs_cis: Tensor) -> Tensor:\n    xshaped = x.float().reshape(*x.shape[:-1], -1, 2)\n    freqs_cis = freqs_cis.view(1, xshaped.size(1), 1, xshaped.size(3), 2)\n    x_out2 = torch.stack(\n        [\n            xshaped[..., 0] * freqs_cis[..., 0] - xshaped[..., 1] * freqs_cis[..., 1],\n            xshaped[..., 1] * freqs_cis[..., 0] + xshaped[..., 0] * freqs_cis[..., 1],\n        ],\n        -1,\n    )\n    x_out2 = x_out2.flatten(3)\n    return x_out2.type_as(x)", "target": "def test_enum_value():\n    class FooEnum(Enum):\n        foo = 'foo_value'\n        bar = 'bar_value'\n    v = SchemaValidator(core_schema.literal_schema([FooEnum.foo]))\n    assert v.validate_python(FooEnum.foo) == FooEnum.foo\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python('foo_value')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'literal_error',\n            'loc': (),\n            'msg': \"Input should be <FooEnum.foo: 'foo_value'>\",\n            'input': 'foo_value',\n            'ctx': {'expected': \"<FooEnum.foo: 'foo_value'>\"},\n        }\n    ]\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python('unknown')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'literal_error',\n            'loc': (),\n            'msg': \"Input should be <FooEnum.foo: 'foo_value'>\",\n            'input': 'unknown',\n            'ctx': {'expected': \"<FooEnum.foo: 'foo_value'>\"},\n        }\n    ]\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_json('\"foo_value\"')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'literal_error',\n            'loc': (),\n            'msg': \"Input should be <FooEnum.foo: 'foo_value'>\",\n            'input': 'foo_value',\n            'ctx': {'expected': \"<FooEnum.foo: 'foo_value'>\"},\n        }\n    ]", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "test", "example_id": "002556", "source": "def test_validation_error_loc_overrides():\n    class CustomLocOverridesError(ValidationError):\n        @override\n        def errors(\n            self, *, include_url: bool = True, include_context: bool = True, include_input: bool = True\n        ) -> list[ErrorDetails]:\n            errors = super().errors(\n                include_url=include_url, include_context=include_context, include_input=include_input\n            )\n            return [{**error, 'loc': error['loc'][1:]} for error in errors]\n    with pytest.raises(CustomLocOverridesError) as exception_info:\n        raise CustomLocOverridesError.from_exception_data(\n            'My CustomError',\n            [\n                InitErrorDetails(\n                    type='value_error',\n                    loc=(\n                        'hide_this',\n                        'myField',\n                    ),\n                    msg='This is my custom error.',\n                    input='something invalid',\n                    ctx={\n                        'myField': 'something invalid',\n                        'error': \"'something invalid' is not a valid value for 'myField'\",\n                    },\n                ),\n                InitErrorDetails(\n                    type='value_error',\n                    loc=(\n                        'hide_this',\n                        'myFieldToo',\n                    ),\n                    msg='This is my custom error.',\n                    input='something invalid',\n                    ctx={\n                        'myFieldToo': 'something invalid',\n                        'error': \"'something invalid' is not a valid value for 'myFieldToo'\",\n                    },\n                ),\n            ],\n        )\n    TestCase().assertCountEqual(\n        exception_info.value.errors(),\n        [\n            {\n                'type': 'value_error',\n                'loc': ('myField',),\n                'msg': \"Value error, 'something invalid' is not a valid value for 'myField'\",\n                'input': 'something invalid',\n                'ctx': {\n                    'error': \"'something invalid' is not a valid value for 'myField'\",\n                    'myField': 'something invalid',\n                },\n                'url': ANY,\n            },\n            {\n                'type': 'value_error',\n                'loc': ('myFieldToo',),\n                'msg': \"Value error, 'something invalid' is not a valid value for 'myFieldToo'\",\n                'input': 'something invalid',\n                'ctx': {\n                    'error': \"'something invalid' is not a valid value for 'myFieldToo'\",\n                    'myFieldToo': 'something invalid',\n                },\n                'url': ANY,\n            },\n        ],\n    )", "target": "def runTracker(self):\n        framesCounter = 0\n        self.selection = True\n        xmin, ymin, xmax, ymax = self.render.getCurrentRect()\n        self.track_window = (xmin, ymin, xmax - xmin, ymax - ymin)\n        while True:\n            framesCounter += 1\n            self.frame = self.render.getNextFrame()\n            hsv = cv.cvtColor(self.frame, cv.COLOR_BGR2HSV)\n            mask = cv.inRange(hsv, np.array((0., 60., 32.)), np.array((180., 255., 255.)))\n            if self.selection:\n                x0, y0, x1, y1 = self.render.getCurrentRect() + 50\n                x0 -= 100\n                y0 -= 100\n                hsv_roi = hsv[y0:y1, x0:x1]\n                mask_roi = mask[y0:y1, x0:x1]\n                hist = cv.calcHist( [hsv_roi], [0], mask_roi, [16], [0, 180] )\n                cv.normalize(hist, hist, 0, 255, cv.NORM_MINMAX)\n                self.hist = hist.reshape(-1)\n                self.selection = False\n            if self.track_window and self.track_window[2] > 0 and self.track_window[3] > 0:\n                self.selection = None\n                prob = cv.calcBackProject([hsv], [0], self.hist, [0, 180], 1)\n                prob &= mask\n                term_crit = ( cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 1 )\n                _track_box, self.track_window = cv.CamShift(prob, self.track_window, term_crit)\n            trackingRect = np.array(self.track_window)\n            trackingRect[2] += trackingRect[0]\n            trackingRect[3] += trackingRect[1]\n            if intersectionRate(self.render.getCurrentRect(), trackingRect) < 0.4:\n                self.errors += 1\n            if framesCounter > self.framesNum:\n                break\n        self.assertLess(float(self.errors) / self.framesNum, 0.4)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "test", "example_id": "002557", "source": "def required_definition_imports(self) -> Generator[str, None, None]:\n        return self.value.required_usage_imports", "target": "def required_definition_imports(self) -> Generator[str, None, None]:\n        yield \"import typing as _typing\"\n        yield from super().required_definition_imports", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "test", "example_id": "002558", "source": "def f(input_value):\n        return input_value + ' Changed'", "target": "def test_kwargs_typed_dict(py_and_json: PyAndJson, input_value, expected):\n    v = py_and_json(\n        {\n            'type': 'arguments',\n            'arguments_schema': [],\n            'var_kwargs_mode': 'unpacked-typed-dict',\n            'var_kwargs_schema': {\n                'type': 'typed-dict',\n                'fields': {\n                    'x': {\n                        'type': 'typed-dict-field',\n                        'schema': {'type': 'int', 'strict': True},\n                        'required': True,\n                    },\n                    'y': {\n                        'type': 'typed-dict-field',\n                        'schema': {'type': 'str'},\n                        'required': False,\n                        'validation_alias': 'z',\n                    },\n                },\n                'config': {'extra_fields_behavior': 'forbid'},\n            },\n        }\n    )\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            v.validate_test(input_value)\n    else:\n        assert v.validate_test(input_value) == expected", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "test", "example_id": "002559", "source": "def wrapped_func(*args, **kwargs):\n                    if self.has_failure:\n                        if ret_type_on_failure is None:\n                            return None\n                        return ret_type_on_failure()\n                    try:\n                        ret_type = func(*args, **kwargs)\n                    except Exception:\n                        self.has_failure = True\n                        warnings.warn(\n                            \"Typing stubs generation has failed.\\n{}\".format(\n                                traceback.format_exc()\n                            )\n                        )\n                        if ret_type_on_failure is None:\n                            return None\n                        return ret_type_on_failure()\n                    return ret_type", "target": "def test_list_no_copy():\n    v = SchemaValidator(cs.list_schema())\n    assert v.validate_python([1, 2, 3]) is not [1, 2, 3]", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|scikit-learn/scikit-learn", "data_split": "test", "example_id": "002560", "source": "def flatten_states(states):\n    states = list(zip(*states))\n    assert len(states) == 2\n    return [torch.stack(state) for state in states]", "target": "def shrunk_cov_score(X):\n    shrinkages = np.logspace(-2, 0, 30)\n    cv = GridSearchCV(ShrunkCovariance(), {\"shrinkage\": shrinkages})\n    return np.mean(cross_val_score(cv.fit(X).best_estimator_, X))", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "test", "example_id": "002561", "source": "def test_custom_serializer():\n    s = SchemaSerializer(core_schema.any_schema(serialization=core_schema.simple_ser_schema('generator')))\n    assert s.to_python(gen_ok(1, 2), mode='json') == [1, 2]\n    assert s.to_json(gen_ok(1, 2)) == b'[1,2]'", "target": "def test_build_error():\n    with pytest.raises(SchemaError, match='SchemaError: `expected` should have length > 0'):\n        SchemaValidator(cs.literal_schema(expected=[]))", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "test", "example_id": "002562", "source": "def quack_bwd():\n            _rmsnorm_bwd(\n                x,\n                w,\n                dy,\n                rstd,\n                dx,\n                dw_partial,\n                db_partial=None,\n                dresidual_out=None,\n                dresidual=None,\n                sm_count=sm_count,\n            )\n            dw = dw_partial.sum(dim=0).to(w.dtype)\n            return dx, dw", "target": "def query_job_sha(repo, sha):\n    params = {\n        \"queryVariables\": {\"sha\": sha, \"repo\": repo},\n    }\n    KEY_ID = os.environ[\"CH_KEY_ID\"]\n    KEY_SECRET = os.environ[\"CH_KEY_SECRET\"]\n    r = requests.post(\n        url=ARTIFACTS_QUERY_URL,\n        data=json.dumps(params),\n        headers={\"Content-Type\": \"application/json\"},\n        auth=(KEY_ID, KEY_SECRET),\n    )\n    return r.json()[\"data\"]", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pytorch/pytorch", "data_split": "test", "example_id": "002563", "source": "def test_python_986(self):\n        cntls = []\n        img = np.zeros((100,100,3), dtype=np.uint8)\n        color = (0,0,0)\n        cnts = np.array(cntls, dtype=np.int32).reshape((1, -1, 2))\n        try:\n            cv.fillPoly(img, cnts, color)\n            assert False\n        except:\n            assert True", "target": "def dropoutlstm_creator(script=True, **kwargs):\n    assert script is True\n    from .custom_lstms import LSTMState, script_lstm\n    input_size = kwargs[\"inputSize\"]\n    hidden_size = kwargs[\"hiddenSize\"]\n    seq_len = kwargs[\"seqLength\"]\n    batch_size = kwargs[\"miniBatch\"]\n    num_layers = kwargs[\"numLayers\"]\n    ge = script_lstm(input_size, hidden_size, num_layers, dropout=True).cuda()\n    input = torch.randn(seq_len, batch_size, input_size, device=\"cuda\")\n    states = [\n        LSTMState(\n            torch.randn(batch_size, hidden_size, device=\"cuda\"),\n            torch.randn(batch_size, hidden_size, device=\"cuda\"),\n        )\n        for _ in range(num_layers)\n    ]\n    return ModelDef(\n        inputs=[input, states],\n        params=ge.parameters(),\n        forward=ge,\n        backward_setup=lstm_backward_setup,\n        backward=simple_backward,\n    )", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "test", "example_id": "002564", "source": "def test_schema_as_string():\n    v = SchemaValidator(cs.bool_schema())\n    assert v.validate_python('tRuE') is True", "target": "def main():\n    result_path = sys.argv[1]\n    Benchmark().enable_compile_time_instruction_count().collect_all().append_results(\n        result_path\n    )", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "test", "example_id": "002565", "source": "def type_format(self) -> str:\n        return \"_typing.Sequence[{}]\"", "target": "def type_format(self) -> str:\n        if TypeNode.compatible_to_runtime_usage:\n            return \"_typing.Dict[{}]\"\n        return \"dict[{}]\"", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "test", "example_id": "002566", "source": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "target": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|opencv/opencv", "data_split": "test", "example_id": "002567", "source": "def full_profile(rnns, **args):\n    profile_args = []\n    for k, v in args.items():\n        profile_args.append(f\"--{k}={v}\")\n    profile_args.append(f\"--rnns {' '.join(rnns)}\")\n    profile_args.append(\"--internal-run\")\n    outpath = nvprof_output_filename(rnns, **args)\n    cmd = f\"{sys.executable} -m fastrnns.profile {' '.join(profile_args)}\"\n    rc, stdout, stderr = nvprof(cmd, outpath)\n    if rc != 0:\n        raise RuntimeError(f\"stderr: {stderr}\\nstdout: {stdout}\")", "target": "def load_tests(loader, tests, pattern):\n    cwd = os.getcwd()\n    config_file = 'opencv_apps_python_tests.cfg'\n    locations = [cwd, basedir]\n    if os.path.exists(config_file):\n        with open(config_file, 'r') as f:\n            locations += [str(s).strip() for s in f.readlines()]\n    else:\n        print('WARNING: OpenCV tests config file ({}) is missing, running subset of tests'.format(config_file))\n    tests_pattern = os.environ.get('OPENCV_APPS_TEST_FILTER', 'test_*') + '.py'\n    if tests_pattern != 'test_*.py':\n        print('Tests filter: {}'.format(tests_pattern))\n    processed = set()\n    for l in locations:\n        if not os.path.isabs(l):\n            l = os.path.normpath(os.path.join(cwd, l))\n        if l in processed:\n            continue\n        processed.add(l)\n        print('Discovering python tests from: {}'.format(l))\n        sys_path_modify = l not in sys.path\n        if sys_path_modify:\n            sys.path.append(l)\n        discovered_tests = loader.discover(l, pattern=tests_pattern, top_level_dir=l)\n        print('    found {} tests'.format(discovered_tests.countTestCases()))\n        tests.addTests(loader.discover(l, pattern=tests_pattern))\n        if sys_path_modify:\n            sys.path.remove(l)\n    return tests", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|scikit-learn/scikit-learn", "data_split": "test", "example_id": "002568", "source": "def root_validator(\n    *__args,\n    pre: bool = False,\n    skip_on_failure: bool = False,\n    allow_reuse: bool = False,\n) -> Any:\n    warn(\n        'Pydantic V1 style `@root_validator` validators are deprecated.'\n        ' You should migrate to Pydantic V2 style `@model_validator` validators,'\n        ' see the migration guide for more details',\n        DeprecationWarning,\n        stacklevel=2,\n    )\n    if __args:\n        return root_validator()(*__args)\n    if allow_reuse is True:\n        warn(_ALLOW_REUSE_WARNING_MESSAGE, DeprecationWarning, stacklevel=2)\n    mode: Literal['before', 'after'] = 'before' if pre is True else 'after'\n    if pre is False and skip_on_failure is not True:\n        raise PydanticUserError(\n            'If you use `@root_validator` with pre=False (the default) you MUST specify `skip_on_failure=True`.'\n            ' Note that `@root_validator` is deprecated and should be replaced with `@model_validator`.',\n            code='root-validator-pre-skip',\n        )\n    wrap = partial(_decorators_v1.make_v1_generic_root_validator, pre=pre)\n    def dec(f: Callable[..., Any] | classmethod[Any, Any, Any] | staticmethod[Any, Any]) -> Any:\n        if _decorators.is_instance_method_from_sig(f):\n            raise TypeError('`@root_validator` cannot be applied to instance methods')\n        res = _decorators.ensure_classmethod_based_on_signature(f)\n        dec_info = _decorators.RootValidatorDecoratorInfo(mode=mode)\n        return _decorators.PydanticDescriptorProxy(res, dec_info, shim=wrap)\n    return dec", "target": "def make_data(self, params):\n        (representation,) = params\n        if representation == \"dense\":\n            data = _synth_regression_dataset(n_samples=1000000, n_features=100)\n        else:\n            data = _synth_regression_sparse_dataset(\n                n_samples=10000, n_features=100000, density=0.01\n            )\n        return data", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "test", "example_id": "002569", "source": "def nvprof(cmd, outpath):\n    return system(f\"nvprof -o {outpath} {cmd}\")", "target": "def test_generator_any():\n    s = SchemaSerializer(core_schema.generator_schema(core_schema.any_schema()))\n    assert list(s.to_python(iter(['a', b'b', 3]))) == ['a', b'b', 3]\n    assert list(s.to_python(gen_ok('a', b'b', 3))) == ['a', b'b', 3]\n    assert s.to_python(iter(['a', b'b', 3]), mode='json') == ['a', 'b', 3]\n    assert s.to_json(iter(['a', b'b', 3])) == b'[\"a\",\"b\",3]'\n    assert s.to_json(gen_ok('a', b'b', 3)) == b'[\"a\",\"b\",3]'\n    with pytest.warns(\n        UserWarning,\n        match=r'Expected `generator` - serialized value may not be as expected \\[input_value=4, input_type=int\\]',\n    ):\n        assert s.to_python(4) == 4\n    with pytest.warns(\n        UserWarning,\n        match=r\"Expected `generator` - serialized value may not be as expected \\[input_value=\\('a', b'b', 3\\), input_type=tuple\\]\",\n    ):\n        assert s.to_python(('a', b'b', 3)) == ('a', b'b', 3)\n    with pytest.warns(\n        UserWarning,\n        match=r\"Expected `generator` - serialized value may not be as expected \\[input_value='abc', input_type=str\\]\",\n    ):\n        assert s.to_python('abc') == 'abc'\n    with pytest.raises(ValueError, match='oops'):\n        list(s.to_python(gen_error(1, 2)))\n    with pytest.raises(ValueError, match='oops'):\n        s.to_python(gen_error(1, 2), mode='json')\n    with pytest.raises(ValueError, match='oops'):\n        s.to_json(gen_error(1, 2))", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "002570", "source": "def get_memory_bytes(self, args, kwargs) -> int:\n        x, w = args\n        M, N = x.shape\n        return 2 * M * N * x.dtype.itemsize + N * w.dtype.itemsize", "target": "def get_memory_bytes(self, args, kwargs) -> int:\n        x, w, dy = args\n        M, N = x.shape\n        return (\n            2 * M * N * x.dtype.itemsize\n            + 2 * N * w.dtype.itemsize\n            + M * N * dy.dtype.itemsize\n        )", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "002571", "source": "def compiled(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, w = args\n        torch._dynamo.mark_dynamic(x, 0)\n        compiled_rms_norm = torch.compile(\n            self.rms_norm_ref, mode=self.compile_mode, fullgraph=True\n        )\n        return lambda: compiled_rms_norm(x, w)", "target": "def compiled(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, w, dy = args\n        y = torch.compile(self.rms_norm_ref, mode=self.compile_mode, fullgraph=True)(\n            x, w\n        )\n        return lambda: torch.autograd.grad(\n            y, [x, w], grad_outputs=dy, retain_graph=True\n        )", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "002572", "source": "def get_memory_bytes(self, args, kwargs) -> int:\n        x, target = args\n        M, N = x.shape\n        dtype = x.dtype\n        return (M * N + M + M) * dtype.itemsize", "target": "def get_memory_bytes(self, args, kwargs) -> int:\n        (x,) = args\n        M, N = x.shape\n        return 2 * M * N * x.dtype.itemsize", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "test", "example_id": "002573", "source": "def test_time_bound_ctx():\n    v = SchemaValidator(core_schema.time_schema(gt=time(12, 13, 14, 123_456)))\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python('12:13')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'greater_than',\n            'loc': (),\n            'msg': 'Input should be greater than 12:13:14.123456',\n            'input': '12:13',\n            'ctx': {'gt': '12:13:14.123456'},\n        }\n    ]", "target": "def decode_n_tokens(\n    model: torch.nn.Module,\n    cur_token: torch.Tensor,\n    input_pos: torch.Tensor,\n    num_new_tokens: int,\n    **sampling_kwargs,\n):\n    new_tokens, new_probs = [], []\n    for i in range(num_new_tokens):\n        with torch.nn.attention.sdpa_kernel(\n            torch.nn.attention.SDPBackend.MATH\n        ):\n            next_token, next_prob = decode_one_token(\n                model, cur_token, input_pos, **sampling_kwargs\n            )\n            input_pos += 1\n            new_tokens.append(next_token.clone())\n            new_probs.append(next_prob.clone())\n            cur_token = next_token.view(1, -1)\n    return new_tokens, new_probs", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|scikit-learn/scikit-learn", "data_split": "test", "example_id": "002574", "source": "def f(value, handler, _info):\n        return handler(value)", "target": "def plot(args):\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n    bench_results = Path(args.bench_results)\n    pr_name = args.pr_name\n    main_name = args.main_name\n    image_path = args.image_path\n    results_path = Path(bench_results)\n    pr_path = results_path / f\"{pr_name}.csv\"\n    main_path = results_path / f\"{main_name}.csv\"\n    image_path = results_path / image_path\n    df_pr = pd.read_csv(pr_path).assign(branch=pr_name)\n    df_main = pd.read_csv(main_path).assign(branch=main_name)\n    merged_data = pd.merge(\n        df_pr,\n        df_main,\n        on=[\"n_samples_test\", \"n_jobs\"],\n        suffixes=(\"_pr\", \"_main\"),\n    )\n    sns.set(style=\"whitegrid\", context=\"notebook\", font_scale=1.5)\n    fig, axes = plt.subplots(1, 2, figsize=(18, 6), sharex=True, sharey=True)\n    print(merged_data[\"n_jobs\"].unique())\n    ax = axes[0]\n    sns.lineplot(\n        data=merged_data,\n        x=\"n_samples_test\",\n        y=\"predict_time_pr\",\n        hue=\"n_jobs\",\n        style=\"n_jobs\",\n        markers=\"o\",\n        ax=ax,\n        legend=\"full\",\n    )\n    ax.set_title(f\"Predict Time vs. n_samples_test - {pr_name} branch\")\n    ax.set_ylabel(\"Predict Time (Seconds)\")\n    ax.set_xlabel(\"n_samples_test\")\n    ax = axes[1]\n    sns.lineplot(\n        data=merged_data,\n        x=\"n_samples_test\",\n        y=\"predict_time_main\",\n        hue=\"n_jobs\",\n        style=\"n_jobs\",\n        markers=\"X\",\n        dashes=True,\n        ax=ax,\n        legend=None,\n    )\n    ax.set_title(f\"Predict Time vs. n_samples_test - {main_name} branch\")\n    ax.set_ylabel(\"Predict Time\")\n    ax.set_xlabel(\"n_samples_test\")\n    plt.tight_layout()\n    fig.savefig(image_path, bbox_inches=\"tight\")\n    print(f\"Saved image to {image_path}\")", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "test", "example_id": "002575", "source": "def test_class_from_submodule_has_global_alias(self):\n        self.assertTrue(hasattr(cv.ml, \"Boost\"),\n                        msg=\"Class is not registered in the submodule\")\n        self.assertTrue(hasattr(cv, \"ml_Boost\"),\n                        msg=\"Class from submodule doesn't have alias in the \"\n                        \"global module\")\n        self.assertEqual(cv.ml.Boost, cv.ml_Boost,\n                         msg=\"Classes from submodules and global module don't refer \"\n                         \"to the same type\")", "target": "def test_warn_on_missing_field() -> None:\n    class AModel(BasicModel): ...\n    class BModel(BasicModel): ...\n    s = SchemaSerializer(\n        core_schema.model_schema(\n            BasicModel,\n            core_schema.model_fields_schema(\n                {\n                    'root': core_schema.model_field(\n                        core_schema.tagged_union_schema(\n                            choices={\n                                'a': core_schema.model_schema(\n                                    AModel,\n                                    core_schema.model_fields_schema(\n                                        {\n                                            'type': core_schema.model_field(core_schema.literal_schema(['a'])),\n                                            'a': core_schema.model_field(core_schema.int_schema()),\n                                        }\n                                    ),\n                                ),\n                                'b': core_schema.model_schema(\n                                    BModel,\n                                    core_schema.model_fields_schema(\n                                        {\n                                            'type': core_schema.model_field(core_schema.literal_schema(['b'])),\n                                            'b': core_schema.model_field(core_schema.int_schema()),\n                                        }\n                                    ),\n                                ),\n                            },\n                            discriminator='type',\n                        )\n                    ),\n                }\n            ),\n        )\n    )\n    with pytest.warns(\n        UserWarning, match='Expected 2 fields but got 1: Expected `AModel` - serialized value may not be as expected .+'\n    ):\n        value = BasicModel(root=AModel(type='a'))\n        s.to_python(value)", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "test", "example_id": "002576", "source": "def make_estimator(self, params):\n        estimator = HistGradientBoostingClassifier(\n            max_iter=100, max_leaf_nodes=15, early_stopping=False, random_state=0\n        )\n        return estimator", "target": "def make_estimator(self, params):\n        (kernel,) = params\n        estimator = SVC(\n            max_iter=100, tol=1e-16, kernel=kernel, random_state=0, gamma=\"scale\"\n        )\n        return estimator", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "test", "example_id": "002577", "source": "def test_on_error_default_factory(self, py_and_json: PyAndJson):\n        v = py_and_json(\n            {\n                'type': 'model-fields',\n                'fields': {\n                    'x': {\n                        'type': 'model-field',\n                        'schema': {\n                            'type': 'default',\n                            'schema': {'type': 'str'},\n                            'on_error': 'default',\n                            'default_factory': lambda: 'pika',\n                        },\n                    }\n                },\n            }\n        )\n        assert v.validate_test({'x': 'foo'}) == ({'x': 'foo'}, None, {'x'})\n        assert v.validate_test({'x': ['foo']}) == ({'x': 'pika'}, None, {'x'})", "target": "def my_function(a, b, c):\n        return a + b + c", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "test", "example_id": "002578", "source": "def test_typed_dict_validator_reuse() -> None:\n    def f(input_value: Any, info: core_schema.ValidationInfo) -> Any:\n        return f'{info.field_name}: {input_value}'\n    with pytest.warns(\n        DeprecationWarning, match='`field_name` argument on `with_info_plain_validator_function` is deprecated'\n    ):\n        validator = core_schema.with_info_plain_validator_function(f, field_name='x')\n    v = SchemaValidator(\n        core_schema.typed_dict_schema(\n            {\n                'x': core_schema.model_field(validator),\n                'y': core_schema.model_field(validator),\n            }\n        )\n    )\n    data = v.validate_python({'x': 'foo', 'y': 'bar'})\n    assert data['x'] == 'x: foo'\n    assert data['y'] == 'y: bar'", "target": "def get_shapes(self) -> tuple[tuple[int, ...], ...]:\n        return (\n            (32768, 256),\n            (32768, 512),\n            (32768, 1024),\n            (32768, 2048),\n            (32768, 4096),\n            (32768, 8192),\n            (32768, 16384),\n            (32768, 32768),\n            (32768, 65536),\n            (16384, 131072),\n            (8192, 262144),\n        )", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "test", "example_id": "002579", "source": "def _general_metadata_cls() -> type[BaseMetadata]:\n    from annotated_types import BaseMetadata\n    class _PydanticGeneralMetadata(PydanticMetadata, BaseMetadata):\n        def __init__(self, metadata: Any):\n            self.__dict__ = metadata\n    return _PydanticGeneralMetadata", "target": "def test_validation_error_loc_overrides():\n    class CustomLocOverridesError(ValidationError):\n        @override\n        def errors(\n            self, *, include_url: bool = True, include_context: bool = True, include_input: bool = True\n        ) -> list[ErrorDetails]:\n            errors = super().errors(\n                include_url=include_url, include_context=include_context, include_input=include_input\n            )\n            return [{**error, 'loc': error['loc'][1:]} for error in errors]\n    with pytest.raises(CustomLocOverridesError) as exception_info:\n        raise CustomLocOverridesError.from_exception_data(\n            'My CustomError',\n            [\n                InitErrorDetails(\n                    type='value_error',\n                    loc=(\n                        'hide_this',\n                        'myField',\n                    ),\n                    msg='This is my custom error.',\n                    input='something invalid',\n                    ctx={\n                        'myField': 'something invalid',\n                        'error': \"'something invalid' is not a valid value for 'myField'\",\n                    },\n                ),\n                InitErrorDetails(\n                    type='value_error',\n                    loc=(\n                        'hide_this',\n                        'myFieldToo',\n                    ),\n                    msg='This is my custom error.',\n                    input='something invalid',\n                    ctx={\n                        'myFieldToo': 'something invalid',\n                        'error': \"'something invalid' is not a valid value for 'myFieldToo'\",\n                    },\n                ),\n            ],\n        )\n    TestCase().assertCountEqual(\n        exception_info.value.errors(),\n        [\n            {\n                'type': 'value_error',\n                'loc': ('myField',),\n                'msg': \"Value error, 'something invalid' is not a valid value for 'myField'\",\n                'input': 'something invalid',\n                'ctx': {\n                    'error': \"'something invalid' is not a valid value for 'myField'\",\n                    'myField': 'something invalid',\n                },\n                'url': ANY,\n            },\n            {\n                'type': 'value_error',\n                'loc': ('myFieldToo',),\n                'msg': \"Value error, 'something invalid' is not a valid value for 'myFieldToo'\",\n                'input': 'something invalid',\n                'ctx': {\n                    'error': \"'something invalid' is not a valid value for 'myFieldToo'\",\n                    'myFieldToo': 'something invalid',\n                },\n                'url': ANY,\n            },\n        ],\n    )", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "test", "example_id": "002580", "source": "def test_revalidate():\n    class RootModel:\n        __slots__ = '__dict__', '__pydantic_fields_set__', '__pydantic_extra__', '__pydantic_private__'\n        root: list[int]\n    v = SchemaValidator(\n        core_schema.model_schema(\n            RootModel, core_schema.list_schema(core_schema.int_schema()), root_model=True, revalidate_instances='always'\n        )\n    )\n    m = RootModel()\n    m = v.validate_python([1, '2'], self_instance=m)\n    assert isinstance(m, RootModel)\n    assert m.root == [1, 2]\n    assert m.__pydantic_fields_set__ == {'root'}\n    m2 = v.validate_python(m)\n    assert m2 is not m\n    assert isinstance(m2, RootModel)\n    assert m2.root == [1, 2]\n    assert m.__pydantic_fields_set__ == {'root'}", "target": "def test_pr_with_release_notes_label(\n        self, mocked_rn_labels: Any, mocked_gql: Any\n    ) -> None:\n        \"Test PR with 'release notes: nn' label\"\n        pr = GitHubPR(\"pytorch\", \"pytorch\", 71759)\n        self.assertTrue(has_required_labels(pr))", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "002581", "source": "def wrap_function(value, handler, _info):\n        nonlocal calls\n        calls += 1\n        return handler(value)", "target": "def wrap_function(value, _info):\n        nonlocal calls\n        calls += 1\n        return value.__dict__", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "test", "example_id": "002582", "source": "def test_any_model():\n    @dataclasses.dataclass\n    class Foo:\n        a: str\n        b: bytes\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo', [core_schema.dataclass_field(name='a', schema=core_schema.str_schema())]\n        ),\n        ['a'],\n    )\n    Foo.__pydantic_validator__ = SchemaValidator(schema)\n    Foo.__pydantic_serializer__ = SchemaSerializer(schema)\n    s = SchemaSerializer(core_schema.any_schema())\n    assert s.to_python(Foo(a='hello', b=b'more')) == IsStrictDict(a='hello')\n    assert s.to_python(Foo(a='hello', b=b'more'), mode='json') == IsStrictDict(a='hello')\n    j = s.to_json(Foo(a='hello', b=b'more'))\n    if on_pypy:\n        assert json.loads(j) == {'a': 'hello'}\n    else:\n        assert j == b'{\"a\":\"hello\"}'\n    assert s.to_python(Foo(a='hello', b=b'more'), exclude={'a'}) == IsStrictDict()\n    assert s.to_json(Foo(a='hello', b=b'more'), exclude={'a'}) == b'{}'\n    assert s.to_python(Foo) == Foo\n    with pytest.raises(PydanticSerializationError, match=r\"Unable to serialize unknown type: <class 'type'>\"):\n        s.to_python(Foo, mode='json')\n    with pytest.raises(PydanticSerializationError, match=r\"Unable to serialize unknown type: <class 'type'>\"):\n        s.to_json(Foo)\n    assert s.to_python(Foo, mode='json', fallback=lambda x: x.__name__) == 'Foo'\n    assert s.to_json(Foo, fallback=lambda x: x.__name__) == b'\"Foo\"'", "target": "def test_union_frozenset_list(input_value, expected):\n    v = SchemaValidator(cs.union_schema(choices=[cs.frozenset_schema(), cs.list_schema()]))\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            v.validate_python(input_value)\n    else:\n        v.validate_python(input_value)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "002583", "source": "def test_dict_py():\n    v = SchemaValidator(cs.dict_schema(keys_schema=cs.date_schema(), values_schema=cs.int_schema()))\n    assert v.validate_python({date(2000, 1, 1): 2, date(2000, 1, 2): 4}) == {date(2000, 1, 1): 2, date(2000, 1, 2): 4}", "target": "def test_dict_py():\n    v = SchemaValidator(\n        core_schema.dict_schema(keys_schema=core_schema.timedelta_schema(), values_schema=core_schema.int_schema())\n    )\n    assert v.validate_python({timedelta(days=2, hours=1): 2, timedelta(days=2, hours=2): 4}) == {\n        timedelta(days=2, hours=1): 2,\n        timedelta(days=2, hours=2): 4,\n    }", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "test", "example_id": "002584", "source": "def test_dataclass():\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bytes_schema()),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    s = SchemaSerializer(schema)\n    assert s.to_python(Foo(a='hello', b=b'more')) == IsStrictDict(a='hello', b=b'more')\n    assert s.to_python(Foo(a='hello', b=b'more'), mode='json') == IsStrictDict(a='hello', b='more')\n    j = s.to_json(Foo(a='hello', b=b'more'))\n    if on_pypy:\n        assert json.loads(j) == {'a': 'hello', 'b': 'more'}\n    else:\n        assert j == b'{\"a\":\"hello\",\"b\":\"more\"}'\n    assert s.to_python(Foo(a='hello', b=b'more'), exclude={'b'}) == IsStrictDict(a='hello')\n    assert s.to_json(Foo(a='hello', b=b'more'), include={'a'}) == b'{\"a\":\"hello\"}'", "target": "def forward(self, input_):\n        if not self.training:\n            return F.softmax(input_, dim=-1)\n        else:\n            return input_", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "002585", "source": "def test_union():\n    v = SchemaValidator(cs.union_schema(choices=[cs.str_schema(strict=True), cs.bytes_schema(strict=True)]))\n    assert v.validate_python('oh, a string') == 'oh, a string'\n    assert v.validate_python(b'oh, bytes') == b'oh, bytes'", "target": "def test_union():\n    v = SchemaValidator(cs.union_schema(choices=[cs.str_schema(), cs.date_schema()]))\n    assert v.validate_python('2022-01-02') == '2022-01-02'\n    assert v.validate_python(date(2022, 1, 2)) == date(2022, 1, 2)\n    v = SchemaValidator(cs.union_schema(choices=[cs.date_schema(), cs.str_schema()]))\n    assert v.validate_python('2022-01-02') == '2022-01-02'\n    assert v.validate_python(date(2022, 1, 2)) == date(2022, 1, 2)", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "test", "example_id": "002586", "source": "def make_charuco_board(self):\n        if (self.aruco_marker_size>self.square_size):\n            print(\"Error: Aruco marker cannot be lager than chessboard square!\")\n            return\n        if (self.dict_file.split(\".\")[-1] == \"gz\"):\n            with gzip.open(self.dict_file, 'r') as fin:\n                json_bytes = fin.read()\n                json_str = json_bytes.decode('utf-8')\n                dictionary = json.loads(json_str)\n        else:\n            f = open(self.dict_file)\n            dictionary = json.load(f)\n        if (dictionary[\"nmarkers\"] < int(self.cols*self.rows/2)):\n            print(\"Error: Aruco dictionary contains less markers than it needs for chosen board. Please choose another dictionary or use smaller board than required for chosen board\")\n            return\n        markerSize_bits = dictionary[\"markersize\"]\n        side = self.aruco_marker_size / (markerSize_bits+2)\n        spacing = self.square_size\n        xspacing = (self.width - self.cols * self.square_size) / 2.0\n        yspacing = (self.height - self.rows * self.square_size) / 2.0\n        ch_ar_border = (self.square_size - self.aruco_marker_size)/2\n        if ch_ar_border < side*0.7:\n            print(\"Marker border {} is less than 70% of ArUco pin size {}. Please increase --square_size or decrease --marker_size for stable board detection\".format(ch_ar_border, int(side)))\n        marker_id = self.dict_offset\n        for y in range(0, self.rows):\n            for x in range(0, self.cols):\n                if x % 2 == y % 2:\n                    square = SVG(\"rect\", x=x * spacing + xspacing, y=y * spacing + yspacing, width=spacing,\n                                 height=spacing, fill=\"black\", stroke=\"none\")\n                    self.g.append(square)\n                else:\n                    img_mark = self._create_marker_bits(markerSize_bits, dictionary[\"marker_\"+str(marker_id)])\n                    marker_id +=1\n                    x_pos = x * spacing + xspacing\n                    y_pos = y * spacing + yspacing\n                    square = SVG(\"rect\", x=x_pos+ch_ar_border, y=y_pos+ch_ar_border, width=self.aruco_marker_size,\n                                             height=self.aruco_marker_size, fill=\"black\", stroke=\"none\")\n                    self.g.append(square)\n                    for x_ in range(len(img_mark[0])):\n                        y_ = 0\n                        while y_ < len(img_mark):\n                            y_start = y_\n                            while y_ < len(img_mark) and img_mark[y_][x_] != 0:\n                                y_ += 1\n                            if y_ > y_start:\n                                rect = SVG(\"rect\", x=x_pos+ch_ar_border+(x_)*side, y=y_pos+ch_ar_border+(y_start)*side, width=side,\n                                           height=(y_ - y_start)*side, fill=\"white\", stroke=\"none\")\n                                self.g.append(rect)\n                            y_ += 1\n                    for y_ in range(len(img_mark)):\n                        x_ = 0\n                        while x_ < len(img_mark[0]):\n                            x_start = x_\n                            while x_ < len(img_mark[0]) and img_mark[y_][x_] != 0:\n                                x_ += 1\n                            if x_ > x_start:\n                                rect = SVG(\"rect\", x=x_pos+ch_ar_border+(x_start)*side, y=y_pos+ch_ar_border+(y_)*side, width=(x_-x_start)*side,\n                                           height=side, fill=\"white\", stroke=\"none\")\n                                self.g.append(rect)\n                            x_ += 1", "target": "def func(*args: Any) -> Any:\n        calls.append(args)\n        return args[0]", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "test", "example_id": "002587", "source": "def validate_before(data) -> dict:\n        data['x'] = data['x'] + ' modified'\n        return data", "target": "def deserialize(cls, data: JsonDict) -> PydanticModelClassVar:\n        data = data.copy()\n        return cls(**data)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "002588", "source": "def validate_b(\n            cls, v: bytes, nxt: core_schema.ValidatorFunctionWrapHandler, info: core_schema.ValidationInfo\n        ) -> bytes:\n            assert v == b'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return nxt(b'hello world!')", "target": "def validate_b(cls, v: bytes, info: core_schema.ValidationInfo) -> bytes:\n            assert v == b'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return b'hello world!'", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "test", "example_id": "002589", "source": "def test_definition_list():\n    v = SchemaValidator(\n        core_schema.definitions_schema(\n            core_schema.definition_reference_schema('the-list'),\n            [core_schema.list_schema(core_schema.definition_reference_schema('the-list'), ref='the-list')],\n        )\n    )\n    assert ',definitions=[List(ListValidator{' in plain_repr(v)\n    assert v.validate_python([]) == []\n    assert v.validate_python([[]]) == [[]]\n    data = list()\n    data.append(data)\n    with pytest.raises(ValidationError) as exc_info:\n        assert v.validate_python(data)\n    assert exc_info.value.title == 'list[...]'\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'recursion_loop',\n            'loc': (0,),\n            'msg': 'Recursion error - cyclic reference detected',\n            'input': [IsList(length=1)],\n        }\n    ]", "target": "def test_any_json_round_trip(any_serializer, value):\n    assert any_serializer.to_python(value) == value\n    assert json.loads(any_serializer.to_json(value)) == value\n    assert any_serializer.to_python(value, mode='json') == value", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "test", "example_id": "002590", "source": "def required_definition_imports(self) -> Generator[str, None, None]:\n        yield from self.positive_branch_type.required_usage_imports\n        yield from self.negative_branch_type.required_usage_imports\n        yield from self._condition_required_imports", "target": "def required_definition_imports(self) -> Generator[str, None, None]:\n        for item in self:\n            yield from item.required_definition_imports", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "002591", "source": "def ser_x(self, v: Any, _) -> str:\n            assert self.x == 1_000\n            return f'{v:_}'", "target": "def ser_x(self, v: Any, serializer: core_schema.SerializerFunctionWrapHandler, _) -> str:\n            assert self.x == 1_000\n            x = serializer(v)\n            return f'{x:_}'", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "test", "example_id": "002592", "source": "def name(self):\n        prefix = f\"{self.category()}_{self._name}_{self.backend()}\"\n        return prefix", "target": "def test_infer_serialize():\n    class MyEnum(Enum):\n        complex_ = complex(1, 2)\n    v = SchemaSerializer(core_schema.enum_schema(MyEnum, list(MyEnum.__members__.values())))\n    assert v.to_json(MyEnum.complex_) == b'\"1+2j\"'", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "test", "example_id": "002593", "source": "def ser_x(self, v: Any, serializer: core_schema.SerializerFunctionWrapHandler, _) -> str:\n            assert self.x == 1_000\n            x = serializer(v)\n            return f'{x:_}'", "target": "def test_decimal(py_and_json: PyAndJson, input_value, expected):\n    v = py_and_json({'type': 'decimal'})\n    if v.validator_type == 'json' and isinstance(input_value, Decimal):\n        input_value = str(input_value)\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            v.validate_test(input_value)\n    else:\n        output = v.validate_test(input_value)\n        assert output == expected\n        assert isinstance(output, Decimal)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "test", "example_id": "002594", "source": "def scaleMask(self, mask):\n        return np.where((mask==cv.GC_FGD) + (mask==cv.GC_PR_FGD),255,0).astype('uint8')", "target": "def input_type_str(self) -> str:\n        return f'{self.input_type.__name__}' if hasattr(self.input_type, '__name__') else f'{self.input_type}'", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "test", "example_id": "002595", "source": "def test_function_before_raise():\n    def f(input_value, info):\n        raise ValueError('foobar')\n    v = SchemaValidator(core_schema.with_info_before_validator_function(f, core_schema.str_schema()))\n    with pytest.raises(ValidationError) as exc_info:\n        assert v.validate_python('input value') == 'input value Changed'\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'value_error',\n            'loc': (),\n            'msg': 'Value error, foobar',\n            'input': 'input value',\n            'ctx': {'error': HasRepr(repr(ValueError('foobar')))},\n        }\n    ]", "target": "def pydantic_encoder(obj: Any) -> Any:\n    warnings.warn(\n        '`pydantic_encoder` is deprecated, use `pydantic_core.to_jsonable_python` instead.',\n        category=PydanticDeprecatedSince20,\n        stacklevel=2,\n    )\n    from dataclasses import asdict, is_dataclass\n    BaseModel = import_cached_base_model()\n    if isinstance(obj, BaseModel):\n        return obj.model_dump()\n    elif is_dataclass(obj):\n        return asdict(obj)\n    for base in obj.__class__.__mro__[:-1]:\n        try:\n            encoder = ENCODERS_BY_TYPE[base]\n        except KeyError:\n            continue\n        return encoder(obj)\n    else:\n        raise TypeError(f\"Object of type '{obj.__class__.__name__}' is not JSON serializable\")", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "test", "example_id": "002596", "source": "def check_close_boxes(self, a, b, delta, angle_delta):\n        self.check_close_pairs(a[0], b[0], delta)\n        self.check_close_pairs(a[1], b[1], delta)\n        self.check_close_angles(a[2], b[2], angle_delta)", "target": "def test_function_plain_field_serializer_to_json():\n    @dataclasses.dataclass\n    class Model:\n        x: int\n        def ser_x(self, v: Any, _) -> str:\n            assert self.x == 1_000\n            return f'{v:_}'\n    s = SchemaSerializer(\n        core_schema.model_schema(\n            Model,\n            core_schema.model_fields_schema(\n                {\n                    'x': core_schema.model_field(\n                        core_schema.int_schema(\n                            serialization=core_schema.plain_serializer_function_ser_schema(\n                                Model.ser_x, is_field_serializer=True, info_arg=True\n                            )\n                        )\n                    )\n                }\n            ),\n        )\n    )\n    assert json.loads(s.to_json(Model(x=1000))) == {'x': '1_000'}", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "002597", "source": "def test_by_alias_and_name_config_interaction(config, runtime, expected) -> None:\n    class Model:\n        def __init__(self, my_field: int) -> None:\n            self.my_field = my_field\n    schema = core_schema.model_schema(\n        Model,\n        core_schema.model_fields_schema(\n            {\n                'my_field': core_schema.model_field(core_schema.int_schema(), serialization_alias='my_alias'),\n            }\n        ),\n        config=core_schema.CoreConfig(serialize_by_alias=config or False),\n    )\n    s = SchemaSerializer(schema)\n    assert s.to_python(Model(1), by_alias=runtime) == expected", "target": "def test_by_alias_and_name_config_interaction(config, runtime, expected) -> None:\n    class Model(TypedDict):\n        my_field: int\n    schema = core_schema.typed_dict_schema(\n        {\n            'my_field': core_schema.typed_dict_field(core_schema.int_schema(), serialization_alias='my_alias'),\n        },\n    )\n    s = SchemaSerializer(schema, config=core_schema.CoreConfig(serialize_by_alias=config or False))\n    assert s.to_python(Model(my_field=1), by_alias=runtime) == expected", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|pydantic/pydantic", "data_split": "test", "example_id": "002598", "source": "def add_missing_values(X_full, y_full, rng):\n    n_samples, n_features = X_full.shape\n    missing_rate = 0.75\n    n_missing_samples = int(n_samples * missing_rate)\n    missing_samples = np.zeros(n_samples, dtype=bool)\n    missing_samples[:n_missing_samples] = True\n    rng.shuffle(missing_samples)\n    missing_features = rng.randint(0, n_features, n_missing_samples)\n    X_missing = X_full.copy()\n    X_missing[missing_samples, missing_features] = np.nan\n    y_missing = y_full.copy()\n    return X_missing, y_missing", "target": "def test_model_root_function_assignment(mode: str, calls1: Any, calls2: Any):\n    calls: list[Any] = []\n    class Model:\n        __slots__ = '__dict__', '__pydantic_fields_set__', '__pydantic_extra__', '__pydantic_private__'\n        x: str\n        y: int\n        def __init__(self, **kwargs: Any) -> None:\n            self.__dict__.update(kwargs)\n    def f(input_value: Any, *args: Any) -> Any:\n        if mode == 'wrap':\n            handler, _ = args\n            calls.append({'value': input_value})\n            return handler(input_value)\n        else:\n            calls.append({'value': input_value})\n            return input_value\n    v = SchemaValidator(\n        core_schema.model_schema(\n            Model,\n            {\n                'type': f'function-{mode}',\n                'function': {'type': 'with-info', 'function': f},\n                'schema': core_schema.model_fields_schema(\n                    {\n                        'x': core_schema.model_field(core_schema.str_schema()),\n                        'y': core_schema.model_field(core_schema.int_schema()),\n                    }\n                ),\n            },\n        )\n    )\n    m = Model()\n    v.validate_python({'x': b'input', 'y': '123'}, self_instance=m)\n    assert m.x == 'input'\n    assert m.y == 123\n    assert calls == [calls1]\n    v.validate_assignment(m, 'x', b'different')\n    assert calls == [calls1, calls2]", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "test", "example_id": "002599", "source": "def model_serializer() -> SchemaSerializer:\n    return SchemaSerializer(\n        core_schema.union_schema(\n            [\n                core_schema.model_schema(\n                    ModelA,\n                    core_schema.model_fields_schema(\n                        {\n                            'a': core_schema.model_field(core_schema.bytes_schema()),\n                            'b': core_schema.model_field(\n                                core_schema.float_schema(\n                                    serialization=core_schema.format_ser_schema('0.1f', when_used='unless-none')\n                                )\n                            ),\n                        }\n                    ),\n                ),\n                core_schema.model_schema(\n                    ModelB,\n                    core_schema.model_fields_schema(\n                        {\n                            'c': core_schema.model_field(core_schema.bytes_schema()),\n                            'd': core_schema.model_field(\n                                core_schema.float_schema(\n                                    serialization=core_schema.format_ser_schema('0.2f', when_used='unless-none')\n                                )\n                            ),\n                        }\n                    ),\n                ),\n            ],\n        )\n    )", "target": "def intersectionRate(s1, s2):\n    area, _intersection = cv.intersectConvexConvex(np.array(s1), np.array(s2))\n    return 2 * area / (cv.contourArea(np.array(s1)) + cv.contourArea(np.array(s2)))", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|opencv/opencv", "data_split": "test", "example_id": "002600", "source": "def required_definition_imports(self) -> Generator[str, None, None]:\n        return self.value.required_usage_imports", "target": "def rm_one(d):\n    d = str(d)\n    d = os.path.abspath(d)\n    if os.path.exists(d):\n        if os.path.isdir(d):\n            log.info(\"Removing dir: %s\", d)\n            shutil.rmtree(d)\n        elif os.path.isfile(d):\n            log.info(\"Removing file: %s\", d)\n            os.remove(d)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "test", "example_id": "002601", "source": "def test_schema_validator_not_reused_when_unpickling() -> None:\n    s = SchemaValidator(\n        core_schema.model_schema(\n            cls=Model,\n            schema=core_schema.model_fields_schema(fields={}, model_name='Model'),\n            config={'title': 'Model'},\n            ref='Model:123',\n        )\n    )\n    Model.__pydantic_validator__ = s\n    assert 'Prebuilt' not in str(Model.__pydantic_validator__)\n    reconstructed = pickle.loads(pickle.dumps(Model.__pydantic_validator__))\n    assert 'Prebuilt' not in str(reconstructed)", "target": "def current_branch(self) -> Optional[str]:\n        try:\n            return self._run_git(\"symbolic-ref\", \"--short\", \"HEAD\").strip()\n        except RuntimeError:\n            return None", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "test", "example_id": "002602", "source": "def test_empty_string_field_name(py_and_json: PyAndJson):\n    v = py_and_json({'type': 'typed-dict', 'fields': {'': {'type': 'typed-dict-field', 'schema': {'type': 'int'}}}})\n    assert v.validate_test({'': 123}) == {'': 123}", "target": "def native_hardswish(a):\n    return torch._C._nn.hardswish(a * 3)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "002603", "source": "def deserialize(cls, info: TypeInfo, data: JsonDict, api: SemanticAnalyzerPluginInterface) -> PydanticModelField:\n        data = data.copy()\n        typ = deserialize_and_fixup_type(data.pop('type'), api)\n        return cls(type=typ, info=info, **data)", "target": "def deserialize(cls, data: JsonDict) -> PydanticModelClassVar:\n        data = data.copy()\n        return cls(**data)", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|opencv/opencv", "data_split": "test", "example_id": "002604", "source": "def str_(cls, ctype_name: Optional[str] = None,\n             required_modules: Tuple[str, ...] = ()):\n        if ctype_name is None:\n            ctype_name = \"string\"\n        return PrimitiveTypeNode(ctype_name, \"str\", required_modules=required_modules)", "target": "def test_all_parameters_can_be_passed(self):\n        img1 = self.get_sample(\"samples/data/right01.jpg\")\n        img2 = self.get_sample(\"samples/data/right02.jpg\")\n        orb = cv2.ORB.create()\n        kp1, des1 = orb.detectAndCompute(img1, None)\n        kp2, des2 = orb.detectAndCompute(img2, None)\n        FLANN_INDEX_KDTREE = 1\n        index_param = dict(algorithm=FLANN_INDEX_KDTREE, trees=4)\n        search_param = dict(checks=32, sorted=True, eps=0.5,\n                            explore_all_trees=False)\n        matcher = cv2.FlannBasedMatcher(index_param, search_param)\n        matches = matcher.knnMatch(np.float32(des1), np.float32(des2), k=2)\n        self.assertGreater(len(matches), 0)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|scikit-learn/scikit-learn", "data_split": "test", "example_id": "002605", "source": "def area(self) -> int:\n            return self.width * self.height", "target": "def generate_list(contributors):\n    lines = []\n    for contributor in contributors:\n        lines.append(\"- %s\" % (contributor[\"name\"],))\n    return \"\\n\".join(lines) + \"\\n\"", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "002606", "source": "def compiled(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, target = args\n        torch._dynamo.mark_dynamic(x, 0)\n        torch._dynamo.mark_dynamic(target, 0)\n        compiled_cross_entropy = torch.compile(\n            lambda x, target: F.cross_entropy(x, target, reduction=\"none\"),\n            mode=self.compile_mode,\n            fullgraph=True,\n        )\n        return lambda: compiled_cross_entropy(x, target)", "target": "def compiled(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, w, dy = args\n        compiled_layernorm = torch.compile(\n            self.layernorm_ref, mode=self.compile_mode, fullgraph=True\n        )\n        y = compiled_layernorm(x, w)\n        return lambda: torch.autograd.grad(\n            y, [x, w], grad_outputs=dy, retain_graph=True\n        )", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "test", "example_id": "002607", "source": "def test_neg_7200():\n    v = SchemaValidator(core_schema.datetime_schema(tz_constraint=-7200))\n    value = datetime.now(tz=timezone(timedelta(hours=-2)))\n    assert value is v.validate_python(value)\n    value = datetime.now()\n    with pytest.raises(ValidationError, match='Input should have timezone info'):\n        v.validate_python(value)\n    value = datetime.now(tz=timezone.utc)\n    with pytest.raises(ValidationError, match='Timezone offset of -7200 required, got 0'):\n        v.validate_python(value)\n    with pytest.raises(ValidationError, match='Timezone offset of -7200 required, got 0'):\n        v.validate_python('2022-06-08T12:13:14Z')", "target": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "test", "example_id": "002608", "source": "def compute_bench(alpha, n_samples, n_features, precompute):\n    lasso_results = []\n    lars_lasso_results = []\n    it = 0\n    for ns in n_samples:\n        for nf in n_features:\n            it += 1\n            print(\"==================\")\n            print(\"Iteration %s of %s\" % (it, max(len(n_samples), len(n_features))))\n            print(\"==================\")\n            n_informative = nf // 10\n            X, Y, coef_ = make_regression(\n                n_samples=ns,\n                n_features=nf,\n                n_informative=n_informative,\n                noise=0.1,\n                coef=True,\n            )\n            X /= np.sqrt(np.sum(X**2, axis=0))\n            gc.collect()\n            print(\"- benchmarking Lasso\")\n            clf = Lasso(alpha=alpha, fit_intercept=False, precompute=precompute)\n            tstart = time()\n            clf.fit(X, Y)\n            lasso_results.append(time() - tstart)\n            gc.collect()\n            print(\"- benchmarking LassoLars\")\n            clf = LassoLars(alpha=alpha, fit_intercept=False, precompute=precompute)\n            tstart = time()\n            clf.fit(X, Y)\n            lars_lasso_results.append(time() - tstart)\n    return lasso_results, lars_lasso_results", "target": "def compute_bench(samples_range, features_range):\n    it = 0\n    results = defaultdict(lambda: [])\n    max_it = len(samples_range) * len(features_range)\n    for n_samples in samples_range:\n        for n_features in features_range:\n            it += 1\n            print(\"====================\")\n            print(\"Iteration %03d of %03d\" % (it, max_it))\n            print(\"====================\")\n            dataset_kwargs = {\n                \"n_samples\": n_samples,\n                \"n_features\": n_features,\n                \"n_informative\": n_features // 10,\n                \"effective_rank\": min(n_samples, n_features) / 10,\n                \"bias\": 0.0,\n            }\n            print(\"n_samples: %d\" % n_samples)\n            print(\"n_features: %d\" % n_features)\n            X, y = make_regression(**dataset_kwargs)\n            gc.collect()\n            print(\"benchmarking lars_path (with Gram):\", end=\"\")\n            sys.stdout.flush()\n            tstart = time()\n            G = np.dot(X.T, X)\n            Xy = np.dot(X.T, y)\n            lars_path_gram(Xy=Xy, Gram=G, n_samples=y.size, method=\"lasso\")\n            delta = time() - tstart\n            print(\"%0.3fs\" % delta)\n            results[\"lars_path (with Gram)\"].append(delta)\n            gc.collect()\n            print(\"benchmarking lars_path (without Gram):\", end=\"\")\n            sys.stdout.flush()\n            tstart = time()\n            lars_path(X, y, method=\"lasso\")\n            delta = time() - tstart\n            print(\"%0.3fs\" % delta)\n            results[\"lars_path (without Gram)\"].append(delta)\n            gc.collect()\n            print(\"benchmarking lasso_path (with Gram):\", end=\"\")\n            sys.stdout.flush()\n            tstart = time()\n            lasso_path(X, y, precompute=True)\n            delta = time() - tstart\n            print(\"%0.3fs\" % delta)\n            results[\"lasso_path (with Gram)\"].append(delta)\n            gc.collect()\n            print(\"benchmarking lasso_path (without Gram):\", end=\"\")\n            sys.stdout.flush()\n            tstart = time()\n            lasso_path(X, y, precompute=False)\n            delta = time() - tstart\n            print(\"%0.3fs\" % delta)\n            results[\"lasso_path (without Gram)\"].append(delta)\n    return results", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "test", "example_id": "002609", "source": "def schema_using_defs() -> cs.CoreSchema:\n    definitions: list[cs.CoreSchema] = [\n        {'type': 'int', 'ref': 'int'},\n        {\n            'type': 'model',\n            'cls': MyModel,\n            'schema': {\n                'type': 'model-fields',\n                'fields': {\n                    str(c): {'type': 'model-field', 'schema': {'type': 'definition-ref', 'schema_ref': 'int'}}\n                    for c in range(N)\n                },\n            },\n            'ref': f'model_{N}',\n        },\n    ]\n    level = N\n    for level in reversed(range(N)):\n        definitions.append(\n            {\n                'type': 'model',\n                'cls': MyModel,\n                'schema': {\n                    'type': 'model-fields',\n                    'fields': {\n                        str(c): {\n                            'type': 'model-field',\n                            'schema': {'type': 'definition-ref', 'schema_ref': f'model_{level + 1}'},\n                        }\n                        for c in range(N)\n                    },\n                },\n                'ref': f'model_{level}',\n            }\n        )\n    return {\n        'type': 'definitions',\n        'definitions': definitions,\n        'schema': {'type': 'definition-ref', 'schema_ref': 'model_0'},\n    }", "target": "def subtract(x, y):\n        return x - y", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "test", "example_id": "002610", "source": "def definitions_schema(schema: CoreSchema, definitions: list[CoreSchema]) -> DefinitionsSchema:\n    return DefinitionsSchema(type='definitions', schema=schema, definitions=definitions)", "target": "def test_result_rotated_rect_issue_20930(self):\n        rr = cv.utils.testRotatedRect(10, 20, 100, 200, 45)\n        self.assertTrue(isinstance(rr, tuple), msg=type(rr))\n        self.assertEqual(len(rr), 3)\n        rrv = cv.utils.testRotatedRectVector(10, 20, 100, 200, 45)\n        self.assertTrue(isinstance(rrv, tuple), msg=type(rrv))\n        self.assertEqual(len(rrv), 10)\n        rr = rrv[0]\n        self.assertTrue(isinstance(rr, tuple), msg=type(rrv))\n        self.assertEqual(len(rr), 3)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "test", "example_id": "002611", "source": "def run(img0, img1):\n                    raise Exception('Error')\n                    return img0 + img1", "target": "def is_dynamic_alias_present(fields: list[PydanticModelField], has_alias_generator: bool) -> bool:\n        for field in fields:\n            if field.has_dynamic_alias:\n                return True\n        if has_alias_generator:\n            for field in fields:\n                if field.alias is None:\n                    return True\n        return False", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|opencv/opencv", "data_split": "test", "example_id": "002612", "source": "def sample_line(p1, p2, n, noise=0.0):\n    np.random.seed(10)\n    p1 = np.float32(p1)\n    t = np.random.rand(n,1)\n    return p1 + (p2-p1)*t + np.random.normal(size=(n, 2))*noise", "target": "def getTestList(self, white, black):\n        res = [t for t in white or self.tests if self.getAlias(t) not in black]\n        if len(res) == 0:\n            raise Err(\"No tests found\")\n        return set(res)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "test", "example_id": "002613", "source": "def type_format(self) -> str:\n        if TypeNode.compatible_to_runtime_usage:\n            return \"_typing.Optional[{}]\"\n        return \"{} | None\"", "target": "def type_format(self) -> str:\n        if TypeNode.compatible_to_runtime_usage:\n            return \"_typing.Dict[{}]\"\n        return \"dict[{}]\"", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "test", "example_id": "002614", "source": "def plain_serializer_function_ser_schema(\n    function: SerializerFunction,\n    *,\n    is_field_serializer: bool | None = None,\n    info_arg: bool | None = None,\n    return_schema: CoreSchema | None = None,\n    when_used: WhenUsed = 'always',\n) -> PlainSerializerFunctionSerSchema:\n    if when_used == 'always':\n        when_used = None\n    return _dict_not_none(\n        type='function-plain',\n        function=function,\n        is_field_serializer=is_field_serializer,\n        info_arg=info_arg,\n        return_schema=return_schema,\n        when_used=when_used,\n    )", "target": "def test_multi_host_url_dict_keys():\n    v = SchemaValidator(core_schema.multi_host_url_schema())\n    s = SchemaSerializer(core_schema.dict_schema(core_schema.multi_host_url_schema()))\n    url = v.validate_python('https://example.com,example.org/path')\n    assert s.to_python({url: 'foo'}) == {url: 'foo'}\n    assert s.to_python({url: 'foo'}, mode='json') == {'https://example.com,example.org/path': 'foo'}\n    assert s.to_json({url: 'foo'}) == b'{\"https://example.com,example.org/path\":\"foo\"}'", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "002615", "source": "def handler() -> T | None:\n            if adapter.rebuild(raise_errors=False, _parent_namespace_depth=5) is not False:\n                return attr_fn(adapter)\n            return None", "target": "def handler() -> T | None:\n            if cls.model_rebuild(raise_errors=False, _parent_namespace_depth=5) is not False:\n                return attr_fn(cls)\n            return None", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pytorch/pytorch", "data_split": "test", "example_id": "002616", "source": "def evaluate_model(model, digits, samples, labels):\n    resp = model.predict(samples)\n    err = (labels != resp).mean()\n    confusion = np.zeros((10, 10), np.int32)\n    for i, j in zip(labels, resp):\n        confusion[int(i), int(j)] += 1\n    return err, confusion", "target": "def forward(self, x):\n        r\n        x = x + self.pe[: x.size(0), :]\n        return self.dropout(x)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "test", "example_id": "002617", "source": "def test_pr_with_missing_labels(\n        self, mocked_rn_labels: Any, mocked_gql: Any\n    ) -> None:\n        \"Test PR with no 'release notes:' label or 'topic: not user facing' label\"\n        pr = GitHubPR(\"pytorch\", \"pytorch\", 82169)\n        self.assertFalse(has_required_labels(pr))", "target": "def tanh(a):\n    return (3 * a).tanh()", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "test", "example_id": "002618", "source": "def test_function_before():\n    def f(input_value, _info):\n        assert isinstance(input_value, dict)\n        input_value['field_a'] += b' XX'\n        return input_value\n    v = SchemaValidator(\n        {\n            'type': 'function-before',\n            'function': {'type': 'with-info', 'function': f},\n            'schema': core_schema.model_schema(\n                cls=MyModel,\n                schema=core_schema.model_fields_schema(\n                    fields={\n                        'field_a': core_schema.model_field(schema=core_schema.str_schema()),\n                        'field_b': core_schema.model_field(schema=core_schema.int_schema()),\n                    }\n                ),\n            ),\n        }\n    )\n    m = v.validate_python({'field_a': b'321', 'field_b': '12'})\n    assert isinstance(m, MyModel)\n    assert m.field_a == '321 XX'\n    assert m.field_b == 12\n    m2 = MyModel()\n    v.validate_python({'field_a': b'321', 'field_b': '12'}, self_instance=m2)\n    assert m2.__dict__ == {'field_a': '321 XX', 'field_b': 12}\n    assert m2.__pydantic_fields_set__ == {'field_a', 'field_b'}", "target": "def test_aliases_path_multiple(py_and_json: PyAndJson, input_value, expected):\n    v = py_and_json(\n        {\n            'type': 'typed-dict',\n            'fields': {\n                'field_a': {\n                    'validation_alias': [['foo', 'bar', 'bat'], ['foo', 3], ['spam']],\n                    'type': 'typed-dict-field',\n                    'schema': {'type': 'int'},\n                }\n            },\n            'config': {'loc_by_alias': False},\n        }\n    )\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=expected.message):\n            val = v.validate_test(input_value)\n            print(f'UNEXPECTED OUTPUT: {val!r}')\n    else:\n        output = v.validate_test(input_value)\n        assert output == expected", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|scikit-learn/scikit-learn", "data_split": "test", "example_id": "002619", "source": "def projectChessboard(squaresX, squaresY, squareSize, imageSize, cameraMatrix, rvec, tvec):\n    img = np.ones(imageSize, np.uint8) * 255\n    distCoeffs = np.zeros((5, 1), np.float64)\n    for y in range(squaresY):\n        startY = y * squareSize\n        for x in range(squaresX):\n            if (y % 2 != x % 2):\n                continue\n            startX = x * squareSize\n            squareCorners = np.array([[startX - squaresX*squareSize/2,\n                                       startY - squaresY*squareSize/2,\n                                       0]], np.float32)\n            squareCorners = np.stack((squareCorners[0],\n                                      squareCorners[0] + [squareSize, 0, 0],\n                                      squareCorners[0] + [squareSize, squareSize, 0],\n                                      squareCorners[0] + [0, squareSize, 0]))\n            projectedCorners, _ = cv.projectPoints(squareCorners, rvec, tvec, cameraMatrix, distCoeffs)\n            projectedCorners = projectedCorners.astype(np.int64)\n            projectedCorners = projectedCorners.reshape(1, 4, 2)\n            img = cv.fillPoly(img, [projectedCorners], 0)\n    return img", "target": "def bench(clfs):\n    for (\n        name,\n        clf,\n        iter_range,\n        train_losses,\n        train_scores,\n        test_scores,\n        durations,\n    ) in clfs:\n        print(\"training %s\" % name)\n        clf_type = type(clf)\n        clf_params = clf.get_params()\n        for n_iter in iter_range:\n            gc.collect()\n            train_loss, train_score, test_score, duration = bench_one(\n                name, clf_type, clf_params, n_iter\n            )\n            train_losses.append(train_loss)\n            train_scores.append(train_score)\n            test_scores.append(test_score)\n            durations.append(duration)\n            print(\"classifier: %s\" % name)\n            print(\"train_loss: %.8f\" % train_loss)\n            print(\"train_score: %.8f\" % train_score)\n            print(\"test_score: %.8f\" % test_score)\n            print(\"time for fit: %.8f seconds\" % duration)\n            print(\"\")\n        print(\"\")\n    return clfs", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|scikit-learn/scikit-learn", "data_split": "test", "example_id": "002620", "source": "def exec_file_wrapper(fpath, g_vars, l_vars):\n        execfile(fpath, g_vars, l_vars)", "target": "def has_openmp_flags(target):\n    target_sources = target[\"target_sources\"]\n    target_use_openmp_flags = any(\n        has_source_openmp_flags(target_source) for target_source in target_sources\n    )\n    if not target_use_openmp_flags:\n        return False\n    assert len(target_sources) == 2\n    compiler_source, linker_source = target_sources\n    assert \"compiler\" in compiler_source\n    assert \"linker\" in linker_source\n    compiler_use_openmp_flags = any(\n        \"openmp\" in arg for arg in compiler_source[\"parameters\"]\n    )\n    linker_use_openmp_flags = any(\n        \"openmp\" in arg for arg in linker_source[\"parameters\"]\n    )\n    assert compiler_use_openmp_flags == linker_use_openmp_flags\n    return compiler_use_openmp_flags", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "test", "example_id": "002621", "source": "def test_dict_error_key_other():\n    v = SchemaValidator(cs.dict_schema(values_schema=cs.int_schema()))\n    with pytest.raises(ValidationError, match='Input should be a valid integer') as exc_info:\n        v.validate_python({1: 2, (1, 2): 'wrong'})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('(1, 2)',),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'wrong',\n        }\n    ]", "target": "def test_gc_schema_serializer() -> None:\n    class BaseModel:\n        __schema__: SchemaSerializer\n        def __init_subclass__(cls) -> None:\n            cls.__schema__ = SchemaSerializer(\n                core_schema.model_schema(cls, GC_TEST_SCHEMA_INNER), config={'ser_json_timedelta': 'float'}\n            )\n    cache: WeakValueDictionary[int, Any] = WeakValueDictionary()\n    for _ in range(10_000):\n        class MyModel(BaseModel):\n            pass\n        cache[id(MyModel)] = MyModel\n        del MyModel\n    assert_gc(lambda: len(cache) == 0)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "002622", "source": "def dynamic_rnn(\n        sequences: list[Tensor],\n        hiddens: tuple[Tensor, Tensor],\n        w_ih: Tensor,\n        w_hh: Tensor,\n        b_ih: Tensor,\n        b_hh: Tensor,\n    ) -> tuple[list[Tensor], tuple[list[Tensor], list[Tensor]]]:\n        hx, cx = hiddens\n        hxs = hx.unbind(1)\n        cxs = cx.unbind(1)\n        outputs = []\n        hx_outs = []\n        cx_outs = []\n        for batch in range(len(sequences)):\n            output = []\n            hy, cy = hxs[batch], cxs[batch]\n            inputs = sequences[batch].unbind(0)\n            for seq_idx in range(len(inputs)):\n                hy, cy = cell(\n                    inputs[seq_idx].unsqueeze(0), (hy, cy), w_ih, w_hh, b_ih, b_hh\n                )\n                output += [hy]\n            outputs += [torch.stack(output)]\n            hx_outs += [hy.unsqueeze(0)]\n            cx_outs += [cy.unsqueeze(0)]\n        return outputs, (hx_outs, cx_outs)", "target": "def dynamic_rnn(\n        input: Tensor,\n        hidden: tuple[Tensor, Tensor],\n        w_ih: Tensor,\n        w_hh: Tensor,\n        b_ih: Tensor,\n        b_hh: Tensor,\n    ) -> tuple[Tensor, tuple[Tensor, Tensor]]:\n        hx, cx = hidden\n        outputs = []\n        inputs = input.unbind(0)\n        hy, cy = hx[0], cx[0]\n        for seq_idx in range(len(inputs)):\n            hy, cy = cell(inputs[seq_idx], (hy, cy), w_ih, w_hh, b_ih, b_hh)\n            outputs += [hy]\n        return torch.stack(outputs), (hy.unsqueeze(0), cy.unsqueeze(0))", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "test", "example_id": "002623", "source": "def test_function_wrap_repr():\n    def f(input_value, validator, info):\n        assert repr(validator) == str(validator)\n        return plain_repr(validator)\n    v = SchemaValidator(core_schema.with_info_wrap_validator_function(f, core_schema.str_schema()))\n    assert (\n        v.validate_python('input value')\n        == 'ValidatorCallable(Str(StrValidator{strict:false,coerce_numbers_to_str:false}))'\n    )", "target": "def test_after(py_and_json: PyAndJson):\n    def f(input_value, info):\n        return input_value + f'| context: {info.context}'\n    v = py_and_json(core_schema.with_info_after_validator_function(f, core_schema.str_schema()))\n    assert v.validate_test('foobar') == 'foobar| context: None'\n    assert v.validate_test('foobar', None, {1: 10}) == 'foobar| context: {1: 10}'\n    assert v.validate_test('foobar', None, 'frogspawn') == 'foobar| context: frogspawn'", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "002624", "source": "def get_memory_bytes(self, args, kwargs) -> int:\n        x, dy = args\n        M, N = x.shape\n        return 3 * M * N * x.dtype.itemsize", "target": "def get_memory_bytes(self, args, kwargs) -> int:\n        x, w = args\n        M, N = x.shape\n        return 2 * M * N * x.dtype.itemsize + N * w.dtype.itemsize", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "test", "example_id": "002625", "source": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "target": "def test_include_dict(schema_func, seq_f):\n    v = SchemaSerializer(\n        schema_func(core_schema.any_schema(), serialization=core_schema.filter_seq_schema(include={1, 3, 5}))\n    )\n    assert v.to_python(seq_f(0, 1, 2, 3, 4)) == seq_f(1, 3)\n    assert v.to_python(seq_f('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h')) == seq_f('b', 'd', 'f')\n    assert v.to_python(seq_f(0, 1, 2, 3, 4), include={2: None}) == seq_f(1, 2, 3)\n    assert v.to_python(seq_f(0, 1, 2, 3, 4), include={2: {1, 2}}) == seq_f(1, 2, 3)\n    assert v.to_python(seq_f(0, 1, 2, 3, 4), include={2}) == seq_f(1, 2, 3)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|scikit-learn/scikit-learn", "data_split": "test", "example_id": "002626", "source": "def test_aliases_path_multiple(py_and_json: PyAndJson, input_value, expected):\n    v = py_and_json(\n        {\n            'type': 'model-fields',\n            'fields': {\n                'field_a': {\n                    'validation_alias': [['foo', 'bar', 'bat'], ['foo', 3], ['spam']],\n                    'type': 'model-field',\n                    'schema': {'type': 'int'},\n                }\n            },\n        },\n        {'loc_by_alias': False},\n    )\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=expected.message):\n            val = v.validate_test(input_value)\n            print(f'UNEXPECTED OUTPUT: {val!r}')\n    else:\n        output = v.validate_test(input_value)\n        assert output == expected", "target": "def nn_accuracy(X, X_embedded, k=1):\n    knn = NearestNeighbors(n_neighbors=1, n_jobs=-1)\n    _, neighbors_X = knn.fit(X).kneighbors()\n    _, neighbors_X_embedded = knn.fit(X_embedded).kneighbors()\n    return np.mean(neighbors_X == neighbors_X_embedded)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "test", "example_id": "002627", "source": "def test_charuco_match_image_points(self):\n        aruco_dict = cv.aruco.getPredefinedDictionary(cv.aruco.DICT_4X4_50)\n        board_size = (3, 4)\n        board = cv.aruco.CharucoBoard(board_size, 5.0, 1.0, aruco_dict)\n        chessboard_corners = np.array(board.getChessboardCorners())[:, :2]\n        chessboard_ids = board.getIds()\n        obj_points, img_points = board.matchImagePoints(chessboard_corners, chessboard_ids)\n        self.assertEqual(chessboard_corners.shape[0], obj_points.shape[0])\n        self.assertEqual(img_points.shape[0], obj_points.shape[0])\n        self.assertEqual(2, img_points.shape[2])\n        np.testing.assert_array_equal(chessboard_corners, obj_points[:, :, :2].reshape(-1, 2))", "target": "def fn():\n        def validate(v, info):\n            return v\n        schema = core_schema.with_info_plain_validator_function(validate)\n        schema = core_schema.nullable_schema(schema)\n        validate.__pydantic_validator__ = SchemaValidator(schema)\n        return validate", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "test", "example_id": "002628", "source": "def run(self):\n        self.prepare()\n        try:\n            with working_directory(self.work_directory):\n                if self.test_type == TestInpuType.TEST_PLAN:\n                    if self.num_shards > 1:\n                        run_test_plan(\n                            self.test_plan,\n                            \"vllm\",\n                            sample_vllm_test_library(),\n                            self.shard_id,\n                            self.num_shards,\n                        )\n                    else:\n                        run_test_plan(\n                            self.test_plan, \"vllm\", sample_vllm_test_library()\n                        )\n                else:\n                    raise ValueError(f\"Unknown test type {self.test_type}\")\n        finally:\n            check_versions()", "target": "def test_other_type():\n    v = SchemaSerializer(core_schema.is_instance_schema(int))\n    assert plain_repr(v) == 'SchemaSerializer(serializer=Any(AnySerializer),definitions=[])'\n    assert v.to_json('foobar') == b'\"foobar\"'", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "test", "example_id": "002629", "source": "def typename(self) -> str:\n            return self.type_node.full_typename", "target": "def typename(self) -> str:\n        return \"\"", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "002630", "source": "def test_extra_behavior_ignore(\n    config: Union[core_schema.CoreConfig, None],\n    schema_extra_behavior_kw: dict[str, Any],\n    validate_fn_extra_kw: Union[ExtraBehavior, None],\n):\n    @dataclasses.dataclass\n    class MyModel:\n        f: str\n    v = SchemaValidator(\n        core_schema.dataclass_schema(\n            MyModel,\n            core_schema.dataclass_args_schema(\n                'MyModel', [core_schema.dataclass_field('f', core_schema.str_schema())], **schema_extra_behavior_kw\n            ),\n            ['f'],\n        ),\n        config=config,\n    )\n    m: MyModel = v.validate_python({'f': 'x', 'extra_field': 123}, extra=validate_fn_extra_kw)\n    assert m.f == 'x'\n    assert not hasattr(m, 'extra_field')\n    v.validate_assignment(m, 'f', 'y', extra=validate_fn_extra_kw)\n    assert m.f == 'y'\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_assignment(m, 'not_f', 'xyz', extra=validate_fn_extra_kw)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'no_such_attribute',\n            'loc': ('not_f',),\n            'msg': \"Object has no attribute 'not_f'\",\n            'input': 'xyz',\n            'ctx': {'attribute': 'not_f'},\n        }\n    ]\n    assert not hasattr(m, 'not_f')", "target": "def test_extra_behavior_ignore(\n    config: Union[core_schema.CoreConfig, None],\n    schema_extra_behavior_kw: dict[str, Any],\n    validate_fn_extra_kw: Union[ExtraBehavior, None],\n):\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            {'f': core_schema.model_field(core_schema.str_schema())}, **schema_extra_behavior_kw\n        ),\n        config=config,\n    )\n    m, model_extra, fields_set = v.validate_python({'f': 'x', 'extra_field': 123}, extra=validate_fn_extra_kw)\n    assert m == {'f': 'x'}\n    assert model_extra is None\n    assert fields_set == {'f'}\n    v.validate_assignment(m, 'f', 'y', extra=validate_fn_extra_kw)\n    assert m['f'] == 'y'\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_assignment(m, 'not_f', 'xyz', extra=validate_fn_extra_kw)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'no_such_attribute',\n            'loc': ('not_f',),\n            'msg': \"Object has no attribute 'not_f'\",\n            'input': 'xyz',\n            'ctx': {'attribute': 'not_f'},\n        }\n    ]\n    assert 'not_f' not in m", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "test", "example_id": "002631", "source": "def predict(est, data_test, target_test):\n    if args.no_predict:\n        return\n    tic = time()\n    predicted_test = est.predict(data_test)\n    predicted_proba_test = est.predict_proba(data_test)\n    toc = time()\n    roc_auc = roc_auc_score(target_test, predicted_proba_test[:, 1])\n    acc = accuracy_score(target_test, predicted_test)\n    print(f\"predicted in {toc - tic:.3f}s, ROC AUC: {roc_auc:.4f}, ACC: {acc:.4f}\")", "target": "def predict(est, data_test, target_test):\n    if args.no_predict:\n        return\n    tic = time()\n    predicted_test = est.predict(data_test)\n    predicted_proba_test = est.predict_proba(data_test)\n    toc = time()\n    roc_auc = roc_auc_score(target_test, predicted_proba_test[:, 1])\n    acc = accuracy_score(target_test, predicted_test)\n    print(f\"predicted in {toc - tic:.3f}s, ROC AUC: {roc_auc:.4f}, ACC: {acc:.4f}\")", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "test", "example_id": "002632", "source": "def make_data(self, params):\n        representation, precompute = params\n        if representation == \"dense\":\n            data = _synth_regression_dataset(n_samples=1000000, n_features=100)\n        else:\n            data = _synth_regression_sparse_dataset(\n                n_samples=50000, n_features=5000, density=0.01\n            )\n        return data", "target": "def make_data(self, params):\n        (representation,) = params\n        if representation == \"sparse\":\n            data = _20newsgroups_highdim_dataset()\n        else:\n            data = _20newsgroups_lowdim_dataset()\n        return data", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "test", "example_id": "002633", "source": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "target": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "test", "example_id": "002634", "source": "def set_fuser(fuser_name, executor_name):\n    assert fuser_name in [\"te\", \"old\", \"none\", \"default\"]\n    if fuser_name == \"te\":\n        torch._C._jit_set_profiling_executor(True)\n        torch._C._get_graph_executor_optimize(True)\n        torch._C._jit_override_can_fuse_on_cpu(False)\n        torch._C._jit_override_can_fuse_on_gpu(True)\n        torch._C._jit_set_texpr_fuser_enabled(True)\n    elif fuser_name == \"old\":\n        torch._C._jit_set_profiling_executor(False)\n        torch._C._get_graph_executor_optimize(False)\n        torch._C._jit_override_can_fuse_on_gpu(True)\n        torch._C._jit_set_texpr_fuser_enabled(False)\n    elif fuser_name == \"none\":\n        torch._C._jit_set_profiling_executor(False)\n        torch._C._get_graph_executor_optimize(False)\n        torch._C._jit_override_can_fuse_on_gpu(False)\n        torch._C._jit_override_can_fuse_on_cpu(False)\n        torch._C._jit_set_texpr_fuser_enabled(False)\n    elif fuser_name == \"default\":\n        pass\n    if executor_name == \"profiling\":\n        torch._C._jit_set_profiling_executor(True)\n        torch._C._get_graph_executor_optimize(True)\n    elif executor_name == \"simple\":\n        torch._C._get_graph_executor_optimize(False)\n    elif executor_name == \"legacy\":\n        torch._C._jit_set_profiling_executor(False)\n        torch._C._get_graph_executor_optimize(True)\n    elif executor_name == \"default\":\n        pass", "target": "def collect_dataclass_fields(\n    cls: type[StandardDataclass],\n    *,\n    config_wrapper: ConfigWrapper,\n    ns_resolver: NsResolver | None = None,\n    typevars_map: dict[Any, Any] | None = None,\n) -> dict[str, FieldInfo]:\n    FieldInfo_ = import_cached_field_info()\n    fields: dict[str, FieldInfo] = {}\n    ns_resolver = ns_resolver or NsResolver()\n    dataclass_fields = cls.__dataclass_fields__\n    for base in reversed(cls.__mro__):\n        if not dataclasses.is_dataclass(base):\n            continue\n        with ns_resolver.push(base):\n            for ann_name, dataclass_field in dataclass_fields.items():\n                base_anns = _typing_extra.safe_get_annotations(base)\n                if ann_name not in base_anns:\n                    continue\n                globalns, localns = ns_resolver.types_namespace\n                ann_type, evaluated = _typing_extra.try_eval_type(dataclass_field.type, globalns, localns)\n                if _typing_extra.is_classvar_annotation(ann_type):\n                    continue\n                if (\n                    not dataclass_field.init\n                    and dataclass_field.default is dataclasses.MISSING\n                    and dataclass_field.default_factory is dataclasses.MISSING\n                ):\n                    continue\n                if isinstance(dataclass_field.default, FieldInfo_):\n                    if dataclass_field.default.init_var:\n                        if dataclass_field.default.init is False:\n                            raise PydanticUserError(\n                                f'Dataclass field {ann_name} has init=False and init_var=True, but these are mutually exclusive.',\n                                code='clashing-init-and-init-var',\n                            )\n                        continue\n                    field_info = FieldInfo_.from_annotated_attribute(\n                        ann_type, dataclass_field.default, _source=AnnotationSource.DATACLASS\n                    )\n                    field_info._original_assignment = dataclass_field.default\n                else:\n                    field_info = FieldInfo_.from_annotated_attribute(\n                        ann_type, dataclass_field, _source=AnnotationSource.DATACLASS\n                    )\n                    field_info._original_assignment = dataclass_field\n                if not evaluated:\n                    field_info._complete = False\n                    field_info._original_annotation = ann_type\n                fields[ann_name] = field_info\n                update_field_from_config(config_wrapper, ann_name, field_info)\n                if field_info.default is not PydanticUndefined and isinstance(\n                    getattr(cls, ann_name, field_info), FieldInfo_\n                ):\n                    setattr(cls, ann_name, field_info.default)\n    if typevars_map:\n        for field in fields.values():\n            field.apply_typevars_map(typevars_map)\n    if config_wrapper.use_attribute_docstrings:\n        _update_fields_from_docstrings(\n            cls,\n            fields,\n            use_inspect=not hasattr(cls, '__is_pydantic_dataclass__'),\n        )\n    return fields", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "test", "example_id": "002635", "source": "def test_model_union():\n    class BaseUser:\n        def __init__(self, name: str):\n            self.name = name\n    class User(BaseUser):\n        def __init__(self, name: str, surname: str):\n            super().__init__(name)\n            self.surname = surname\n    class DBUser(User):\n        def __init__(self, name: str, surname: str, password_hash: str):\n            super().__init__(name, surname)\n            self.password_hash = password_hash\n    class Item:\n        def __init__(self, name: str, price: float):\n            self.name = name\n            self.price = price\n    user_schema = core_schema.model_schema(\n        User,\n        core_schema.model_fields_schema(\n            {\n                'name': core_schema.model_field(schema=core_schema.str_schema()),\n                'surname': core_schema.model_field(schema=core_schema.str_schema()),\n            }\n        ),\n    )\n    item_schema = core_schema.model_schema(\n        Item,\n        core_schema.model_fields_schema(\n            {\n                'name': core_schema.model_field(schema=core_schema.str_schema()),\n                'price': core_schema.model_field(schema=core_schema.float_schema()),\n            }\n        ),\n    )\n    s = SchemaSerializer(core_schema.union_schema([user_schema, item_schema]))\n    assert s.to_python(User(name='foo', surname='bar')) == {'name': 'foo', 'surname': 'bar'}\n    assert s.to_python(DBUser(name='foo', surname='bar', password_hash='x')) == {'name': 'foo', 'surname': 'bar'}\n    assert s.to_json(DBUser(name='foo', surname='bar', password_hash='x')) == b'{\"name\":\"foo\",\"surname\":\"bar\"}'", "target": "def replace_buildkite_placeholders(step: str, shard_id: int, num_shards: int) -> str:\n    mapping = {\n        \"$$BUILDKITE_PARALLEL_JOB_COUNT\": str(num_shards),\n        \"$$BUILDKITE_PARALLEL_JOB\": str(shard_id),\n    }\n    for k in sorted(mapping, key=len, reverse=True):\n        step = step.replace(k, mapping[k])\n    return step", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "test", "example_id": "002636", "source": "def intersectionRate(s1, s2):\n    x1, y1, x2, y2 = s1\n    s1 = np.array([[x1, y1], [x2,y1], [x2, y2], [x1, y2]])\n    area, _intersection = cv.intersectConvexConvex(s1, np.array(s2))\n    return 2 * area / (cv.contourArea(s1) + cv.contourArea(np.array(s2)))", "target": "def intersectionRate(s1, s2):\n    x1, y1, x2, y2 = s1\n    s1 = np.array([[x1, y1], [x2,y1], [x2, y2], [x1, y2]])\n    x1, y1, x2, y2 = s2\n    s2 = np.array([[x1, y1], [x2,y1], [x2, y2], [x1, y2]])\n    area, _intersection = cv.intersectConvexConvex(s1, s2)\n    return 2 * area / (cv.contourArea(s1) + cv.contourArea(s2))", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|opencv/opencv", "data_split": "test", "example_id": "002637", "source": "def int_(cls, ctype_name: Optional[str] = None,\n             required_modules: Tuple[str, ...] = ()):\n        if ctype_name is None:\n            ctype_name = \"int\"\n        return PrimitiveTypeNode(ctype_name, typename=\"int\", required_modules=required_modules)", "target": "def center_crop(self, img):\n        cols = img.shape[1]\n        rows = img.shape[0]\n        y1 = round((rows - self.frame_size) / 2)\n        y2 = round(y1 + self.frame_size)\n        x1 = round((cols - self.frame_size) / 2)\n        x2 = round(x1 + self.frame_size)\n        return img[y1:y2, x1:x2]", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "test", "example_id": "002638", "source": "def test_aware_specific():\n    v = SchemaValidator(core_schema.time_schema(tz_constraint=0))\n    value = time(12, 13, 15, tzinfo=timezone.utc)\n    assert value is v.validate_python(value)\n    assert v.validate_python('12:13:14Z') == time(12, 13, 14, tzinfo=timezone.utc)\n    value = time(12, 13, 14)\n    with pytest.raises(ValidationError, match='Input should have timezone info'):\n        v.validate_python(value)\n    value = time(12, 13, 15, tzinfo=timezone(timedelta(hours=1)))\n    with pytest.raises(ValidationError, match='Timezone offset of 0 required, got 3600') as exc_info:\n        v.validate_python(value)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'timezone_offset',\n            'loc': (),\n            'msg': 'Timezone offset of 0 required, got 3600',\n            'input': value,\n            'ctx': {'tz_expected': 0, 'tz_actual': 3600},\n        }\n    ]\n    with pytest.raises(ValidationError, match='Timezone offset of 0 required, got 3600'):\n        v.validate_python('12:13:14+01:00')", "target": "def test_error_display(pydantic_version):\n    v = SchemaValidator(\n        core_schema.arguments_schema(\n            [\n                core_schema.arguments_parameter('a', core_schema.int_schema()),\n                core_schema.arguments_parameter('b', core_schema.int_schema()),\n            ]\n        )\n    )\n    assert v.validate_python(ArgsKwargs((1,), {'b': '2'})) == ((1,), {'b': 2})\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python(ArgsKwargs((), {'a': 1}))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'missing_argument',\n            'loc': ('b',),\n            'msg': 'Missing required argument',\n            'input': ArgsKwargs((), {'a': 1}),\n        }\n    ]\n    assert str(exc_info.value) == (\n        '1 validation error for arguments\\n'\n        'b\\n'\n        '  Missing required argument [type=missing_argument, '\n        \"input_value=ArgsKwargs((), {'a': 1}), input_type=ArgsKwargs]\"\n        + (\n            f'\\n    For further information visit https://errors.pydantic.dev/{pydantic_version}/v/missing_argument'\n            if os.environ.get('PYDANTIC_ERRORS_INCLUDE_URL') != 'false'\n            else ''\n        )\n    )\n    assert exc_info.value.json(include_url=False) == (\n        '[{\"type\":\"missing_argument\",\"loc\":[\"b\"],\"msg\":\"Missing required argument\",'\n        '\"input\":\"ArgsKwargs((), {\\'a\\': 1})\"}]'\n    )", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "test", "example_id": "002639", "source": "def children_types(self) -> Tuple[ASTNodeType, ...]:\n        return ()", "target": "def children_types(self) -> Tuple[ASTNodeType, ...]:\n        return (ASTNodeType.Namespace, ASTNodeType.Class, ASTNodeType.Function,\n                ASTNodeType.Enumeration, ASTNodeType.Constant)", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "test", "example_id": "002640", "source": "def make_estimator(self, params):\n        (n_jobs,) = params\n        clf = RandomForestClassifier(random_state=0)\n        if Benchmark.data_size == \"large\":\n            n_estimators_list = [10, 25, 50, 100, 500]\n            max_depth_list = [5, 10, None]\n            max_features_list = [0.1, 0.4, 0.8, 1.0]\n        else:\n            n_estimators_list = [10, 25, 50]\n            max_depth_list = [5, 10]\n            max_features_list = [0.1, 0.4, 0.8]\n        param_grid = {\n            \"n_estimators\": n_estimators_list,\n            \"max_depth\": max_depth_list,\n            \"max_features\": max_features_list,\n        }\n        estimator = GridSearchCV(clf, param_grid, n_jobs=n_jobs, cv=4)\n        return estimator", "target": "def make_estimator(self, params):\n        (representation,) = params\n        n_estimators = 100 if Benchmark.data_size == \"large\" else 10\n        estimator = GradientBoostingClassifier(\n            n_estimators=n_estimators,\n            max_features=\"log2\",\n            subsample=0.5,\n            random_state=0,\n        )\n        return estimator", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "002641", "source": "def fn(x):\n        for _ in range(N):\n            x = x.sin()\n        return x", "target": "def fn():\n        for n in g.graph.nodes:\n            pass", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "test", "example_id": "002642", "source": "def test_by_alias_and_name_config_interaction(config, runtime, expected) -> None:\n    class Model(TypedDict):\n        my_field: int\n    schema = core_schema.typed_dict_schema(\n        {\n            'my_field': core_schema.typed_dict_field(core_schema.int_schema(), serialization_alias='my_alias'),\n        },\n    )\n    s = SchemaSerializer(schema, config=core_schema.CoreConfig(serialize_by_alias=config or False))\n    assert s.to_python(Model(my_field=1), by_alias=runtime) == expected", "target": "def test_simple(self) -> None:\n        @retries_decorator()\n        def foo(x: int, y: int) -> int:\n            return x + y\n        self.assertEqual(foo(3, 4), 7)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "test", "example_id": "002643", "source": "def types_separator(self) -> str:\n        return \", \"", "target": "def types_separator(self) -> str:\n        return \", \"", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "test", "example_id": "002644", "source": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "target": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|opencv/opencv", "data_split": "test", "example_id": "002645", "source": "def parse_arguments():\n    parser = argparse.ArgumentParser(\n        description='Copies generated typing stubs only when generation '\n        'succeeded. This is identified by presence of the `py.typed` file '\n        'inside typing stubs directory.'\n    )\n    parser.add_argument('--stubs_dir', type=str,\n                        help='Path to directory containing generated typing '\n                        'stubs file')\n    parser.add_argument('--output_dir', type=str,\n                        help='Path to output directory')\n    return parser.parse_args()", "target": "def update_html(file, soup):\n    s = str(soup)\n    s = s.replace('!space!', ' ')\n    if os.name == 'nt' or sys.version_info[0] == 3:\n        s = s.encode('utf-8', 'ignore')\n    with open(file, 'wb') as f:\n        f.write(s)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "002646", "source": "def quack(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, target = args\n        from quack.cross_entropy import _cross_entropy\n        return lambda: _cross_entropy(x, target)", "target": "def quack(self, args, kwargs=None) -> Any:\n        from quack.cross_entropy import cross_entropy\n        assert kwargs is None\n        x, target, dloss = args\n        loss = cross_entropy(x, target)\n        return lambda: torch.autograd.grad(\n            loss, x, grad_outputs=dloss, retain_graph=True\n        )", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "test", "example_id": "002647", "source": "def gh_graphql(query: str, **kwargs: Any) -> dict[str, Any]:\n    rc = gh_fetch_url(\n        \"https://api.github.com/graphql\",\n        data={\"query\": query, \"variables\": kwargs},\n        reader=json.load,\n    )\n    if \"errors\" in rc:\n        raise RuntimeError(\n            f\"GraphQL query {query}, args {kwargs} failed: {rc['errors']}\"\n        )\n    return cast(dict[str, Any], rc)", "target": "def is_subclass_schema(\n    cls: type[Any],\n    *,\n    cls_repr: str | None = None,\n    ref: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> IsInstanceSchema:\n    return _dict_not_none(\n        type='is-subclass', cls=cls, cls_repr=cls_repr, ref=ref, metadata=metadata, serialization=serialization\n    )", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "test", "example_id": "002648", "source": "def node_type(self) -> ASTNodeType:\n        return ASTNodeType.Namespace", "target": "def node_type(self) -> ASTNodeType:\n        return ASTNodeType.Constant", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|opencv/opencv", "data_split": "test", "example_id": "002649", "source": "def json_urlread(url):\n    try:\n        return json.loads(urlopen(url).read().decode(\"utf8\"))\n    except Exception:\n        print(\"Error reading\", url, file=sys.stderr)\n        raise", "target": "def test_fitline(self):\n        noise = 5\n        n = 200\n        r = 5 / 100.0\n        outn = int(n*r)\n        p0, p1 = (90, 80), (w-90, h-80)\n        line_points = sample_line(p0, p1, n-outn, noise)\n        outliers = np.random.rand(outn, 2) * (w, h)\n        points = np.vstack([line_points, outliers])\n        lines = []\n        for name in dist_func_names:\n            func = getattr(cv, name)\n            vx, vy, cx, cy = cv.fitLine(np.float32(points), func, 0, 0.01, 0.01)\n            line = [float(vx), float(vy), float(cx), float(cy)]\n            lines.append(line)\n        eps = 0.05\n        refVec =  (np.float32(p1) - p0) / cv.norm(np.float32(p1) - p0)\n        for i in range(len(lines)):\n            self.assertLessEqual(cv.norm(refVec - lines[i][0:2], cv.NORM_L2), eps)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "test", "example_id": "002650", "source": "def typename(self) -> str:\n        return \"_typing.Any\"", "target": "def typename(self) -> str:\n        if self._export_name is not None:\n            return self._export_name\n        return self.ctype_name", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "test", "example_id": "002651", "source": "def outMeta(arr_desc0, arr_desc1):\n        return cv.empty_array_desc(), cv.empty_array_desc()", "target": "def outMeta(desc1, desc2, depth):\n            return desc1", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "002652", "source": "def area(self) -> int:\n            return self.width * self.height", "target": "def area(self) -> int:\n            return self.width * self.height", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "test", "example_id": "002653", "source": "def test_dict_value_error(py_and_json: PyAndJson):\n    v = py_and_json({'type': 'dict', 'values_schema': {'type': 'int'}})\n    assert v.validate_test({'a': 2, 'b': '4'}) == {'a': 2, 'b': 4}\n    with pytest.raises(ValidationError, match='Input should be a valid integer') as exc_info:\n        v.validate_test({'a': 2, 'b': 'wrong'})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('b',),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'wrong',\n        }\n    ]", "target": "def description(self):\n        return \"runtime of a compiled add1 op small input\"", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "002654", "source": "def f(value, serializer, _info):\n        return f'result={serializer(len(value))}'", "target": "def f(value, serializer):\n        if value == 42:\n            return 42\n        return f'result={serializer(value)}'", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "test", "example_id": "002655", "source": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "target": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "test", "example_id": "002656", "source": "def _clean_schema_for_pretty_print(obj: Any, strip_metadata: bool = True) -> Any:\n    if isinstance(obj, Mapping):\n        new_dct = {}\n        for k, v in obj.items():\n            if k == 'metadata' and strip_metadata:\n                new_metadata = {}\n                for meta_k, meta_v in v.items():\n                    if meta_k in ('pydantic_js_functions', 'pydantic_js_annotation_functions'):\n                        new_metadata['js_metadata'] = '<stripped>'\n                    else:\n                        new_metadata[meta_k] = _clean_schema_for_pretty_print(meta_v, strip_metadata=strip_metadata)\n                if list(new_metadata.keys()) == ['js_metadata']:\n                    new_metadata = {'<stripped>'}\n                new_dct[k] = new_metadata\n            elif k in ('custom_init', 'root_model') and not v:\n                continue\n            else:\n                new_dct[k] = _clean_schema_for_pretty_print(v, strip_metadata=strip_metadata)\n        return new_dct\n    elif isinstance(obj, Sequence) and not isinstance(obj, str):\n        return [_clean_schema_for_pretty_print(v, strip_metadata=strip_metadata) for v in obj]\n    else:\n        return obj", "target": "def test_dict_length_constraints(kwargs: dict[str, Any], input_value, expected):\n    v = SchemaValidator(cs.dict_schema(**kwargs))\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            v.validate_python(input_value)\n    else:\n        assert v.validate_python(input_value) == expected", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "test", "example_id": "002657", "source": "def train(self, samples, responses):\n        self.model.setMaxDepth(20)\n        self.model.train(samples, cv.ml.ROW_SAMPLE, responses.astype(int))", "target": "def train(self, samples, responses):\n        self.model.train(samples, cv.ml.ROW_SAMPLE, responses)", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "test", "example_id": "002658", "source": "def full_typename(self) -> str:\n        return \"cv2.typing.\" + self.typename", "target": "def full_typename(self) -> str:\n        return '_typing.Callable[[{}], {}]'.format(\n            ', '.join(arg.full_typename for arg in self.arg_types),\n            self.ret_type.full_typename\n        )", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "002659", "source": "def test_constraints_schema_validation_error(constraint: str) -> None:\n    with pytest.raises(SchemaError, match=f\"'{constraint}' must be coercible to an integer\"):\n        SchemaValidator(cs.int_schema(**{constraint: 'bad_value'}))", "target": "def test_constraints_schema_validation_error(constraint: str) -> None:\n    with pytest.raises(SchemaError, match=f\"'{constraint}' must be coercible to a time instance\"):\n        SchemaValidator(core_schema.time_schema(**{constraint: 'bad_value'}))", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "test", "example_id": "002660", "source": "def ip_v6_network_validator(input_value: Any, /) -> IPv6Network:\n    if isinstance(input_value, IPv6Network):\n        return input_value\n    try:\n        return IPv6Network(input_value)\n    except ValueError:\n        raise PydanticCustomError('ip_v6_network', 'Input is not a valid IPv6 network')", "target": "def test_custom_op_split3(self):\n            sz = (4, 4)\n            in_ch1 = np.full(sz, 1, dtype=np.uint8)\n            in_ch2 = np.full(sz, 2, dtype=np.uint8)\n            in_ch3 = np.full(sz, 3, dtype=np.uint8)\n            in_mat = np.stack((in_ch1, in_ch2, in_ch3), axis=2)\n            g_in  = cv.GMat()\n            g_ch1, g_ch2, g_ch3 = GSplit3.on(g_in)\n            comp = cv.GComputation(cv.GIn(g_in), cv.GOut(g_ch1, g_ch2, g_ch3))\n            pkg = cv.gapi.kernels(GSplit3Impl)\n            ch1, ch2, ch3 = comp.apply(cv.gin(in_mat), args=cv.gapi.compile_args(pkg))\n            self.assertEqual(0.0, cv.norm(in_ch1, ch1, cv.NORM_INF))\n            self.assertEqual(0.0, cv.norm(in_ch2, ch2, cv.NORM_INF))\n            self.assertEqual(0.0, cv.norm(in_ch3, ch3, cv.NORM_INF))", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "test", "example_id": "002661", "source": "def test_json_big_int_key():\n    v = SchemaValidator(cs.dict_schema(keys_schema=cs.int_schema(), values_schema=cs.str_schema()))\n    big_integer = 1433352099889938534014333520998899385340\n    assert v.validate_python({big_integer: 'x'}) == {big_integer: 'x'}\n    assert v.validate_json('{\"' + str(big_integer) + '\": \"x\"}') == {big_integer: 'x'}\n    assert v.validate_strings({str(big_integer): 'x'}) == {big_integer: 'x'}", "target": "def f(input_value: Any, info: core_schema.ValidationInfo) -> Any:\n        assert info.field_name == 'x'\n        assert info.data == {}\n        assert isinstance(input_value, bytes)\n        return f'input: {input_value.decode()}'", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "test", "example_id": "002662", "source": "def attempt_rebuild_fn(attr_fn: Callable[[type[BaseModel]], T]) -> Callable[[], T | None]:\n        def handler() -> T | None:\n            if cls.model_rebuild(raise_errors=False, _parent_namespace_depth=5) is not False:\n                return attr_fn(cls)\n            return None\n        return handler", "target": "def main():\n    dldt_src_url = 'https://github.com/openvinotoolkit/openvino'\n    dldt_src_commit = '2021.4.2'\n    dldt_config = None\n    dldt_release = None\n    build_cache_dir_default = os.environ.get('BUILD_CACHE_DIR', '.build_cache')\n    build_subst_drive = os.environ.get('BUILD_SUBST_DRIVE', None)\n    parser = argparse.ArgumentParser(\n            description='Build OpenCV Windows package with Inference Engine (DLDT)',\n    )\n    parser.add_argument('output_dir', nargs='?', default='.', help='Output directory')\n    parser.add_argument('opencv_dir', nargs='?', default=os.path.join(SCRIPT_DIR, '../..'), help='Path to OpenCV source dir')\n    parser.add_argument('--build_cache_dir', default=build_cache_dir_default, help='Build cache directory (sources and binaries cache of build dependencies, default = \"%s\")' % build_cache_dir_default)\n    parser.add_argument('--build_subst_drive', default=build_subst_drive, help='Drive letter to workaround Windows limit for 260 symbols in path (error MSB3491)')\n    parser.add_argument('--cmake_option', action='append', help='Append OpenCV CMake option')\n    parser.add_argument('--cmake_option_dldt', action='append', help='Append CMake option for DLDT project')\n    parser.add_argument('--clean_dldt', action='store_true', help='Clean DLDT build and sysroot directories')\n    parser.add_argument('--clean_dldt_sysroot', action='store_true', help='Clean DLDT sysroot directories')\n    parser.add_argument('--clean_opencv', action='store_true', help='Clean OpenCV build directory')\n    parser.add_argument('--build_debug', action='store_true', help='Build debug binaries')\n    parser.add_argument('--build_tests', action='store_true', help='Build OpenCV tests')\n    parser.add_argument('--build_tests_dnn', action='store_true', help='Build OpenCV DNN accuracy and performance tests only')\n    parser.add_argument('--dldt_src_url', default=dldt_src_url, help='DLDT source URL (tag / commit, default: %s)' % dldt_src_url)\n    parser.add_argument('--dldt_src_branch', help='DLDT checkout branch')\n    parser.add_argument('--dldt_src_commit', default=dldt_src_commit, help='DLDT source commit / tag (default: %s)' % dldt_src_commit)\n    parser.add_argument('--dldt_src_git_clone_extra', action='append', help='DLDT git clone extra args')\n    parser.add_argument('--dldt_release', default=dldt_release, help='DLDT release code for INF_ENGINE_RELEASE, e.g 2021030000 (default: %s)' % dldt_release)\n    parser.add_argument('--dldt_reference_dir', help='DLDT reference git repository (optional)')\n    parser.add_argument('--dldt_src_dir', help='DLDT custom source repository (skip git checkout and patching, use for TESTING only)')\n    parser.add_argument('--dldt_config', default=dldt_config, help='Specify DLDT build configuration (defaults to evaluate from DLDT commit/branch)')\n    parser.add_argument('--override_patch_hashsum', default='', help='(script debug mode)')\n    args = parser.parse_args()\n    log.basicConfig(\n            format='%(asctime)s %(levelname)-8s %(message)s',\n            level=os.environ.get('LOGLEVEL', 'INFO'),\n            datefmt='%Y-%m-%d %H:%M:%S'\n    )\n    log.debug('Args: %s', args)\n    if not check_executable(['git', '--version']):\n        sys.exit(\"FATAL: 'git' is not available\")\n    if not check_executable(['cmake', '--version']):\n        sys.exit(\"FATAL: 'cmake' is not available\")\n    if os.path.realpath(args.output_dir) == os.path.realpath(SCRIPT_DIR):\n        raise Fail(\"Specify output_dir (building from script directory is not supported)\")\n    if os.path.realpath(args.output_dir) == os.path.realpath(args.opencv_dir):\n        raise Fail(\"Specify output_dir (building from OpenCV source directory is not supported)\")\n    if args.opencv_dir is not None and not os.path.isabs(args.opencv_dir):\n        args.opencv_dir = os.path.abspath(args.opencv_dir)\n    if not args.dldt_config:\n        if str(args.dldt_src_commit).startswith('releases/20'):\n            args.dldt_config = str(args.dldt_src_commit)[len('releases/'):].replace('/', '.')\n            if not args.dldt_src_branch:\n                args.dldt_src_branch = args.dldt_src_commit\n        elif str(args.dldt_src_branch).startswith('releases/20'):\n            args.dldt_config = str(args.dldt_src_branch)[len('releases/'):].replace('/', '.')\n        else:\n            args.dldt_config = args.dldt_src_commit\n    _opencv_dir = check_dir(args.opencv_dir)\n    _outdir = prepare_dir(args.output_dir)\n    _cachedir = prepare_dir(args.build_cache_dir)\n    ocv_hooks_dir = os.environ.get('OPENCV_CMAKE_HOOKS_DIR', None)\n    hooks_dir = os.path.join(SCRIPT_DIR, 'cmake-opencv-checks')\n    os.environ['OPENCV_CMAKE_HOOKS_DIR'] = hooks_dir if ocv_hooks_dir is None else (hooks_dir + ';' + ocv_hooks_dir)\n    builder_dldt = BuilderDLDT(args)\n    try:\n        builder_dldt.prepare_sources()\n        builder_dldt.build()\n        builder_dldt.make_sysroot()\n        builder_opencv = Builder(args)\n        builder_opencv.build(builder_dldt)\n        builder_opencv.copy_sysroot(builder_dldt)\n        builder_opencv.package_sources()\n    except:\n        builder_dldt.cleanup()\n        raise\n    log.info(\"=====\")\n    log.info(\"===== Build finished\")\n    log.info(\"=====\")", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "002663", "source": "def test_generator_error():\n    def gen(error: bool):\n        yield 1\n        yield 2\n        if error:\n            raise RuntimeError('my error')\n        yield 3\n    v = SchemaValidator(cs.frozenset_schema(items_schema=cs.int_schema()))\n    r = v.validate_python(gen(False))\n    assert r == {1, 2, 3}\n    assert isinstance(r, frozenset)\n    msg = r'Error iterating over object, error: RuntimeError: my error \\[type=iteration_error,'\n    with pytest.raises(ValidationError, match=msg):\n        v.validate_python(gen(True))", "target": "def test_generator_error():\n    def gen(error: bool):\n        yield 1\n        yield 2\n        if error:\n            raise RuntimeError('my error')\n        yield 3\n    v = SchemaValidator(cs.set_schema(items_schema=cs.int_schema()))\n    r = v.validate_python(gen(False))\n    assert r == {1, 2, 3}\n    assert isinstance(r, set)\n    msg = r'Error iterating over object, error: RuntimeError: my error \\[type=iteration_error,'\n    with pytest.raises(ValidationError, match=msg):\n        v.validate_python(gen(True))", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "test", "example_id": "002664", "source": "def make_data(self, params):\n        representation, solver, n_jobs = params\n        if Benchmark.data_size == \"large\":\n            if representation == \"sparse\":\n                data = _20newsgroups_highdim_dataset(n_samples=10000)\n            else:\n                data = _20newsgroups_lowdim_dataset(n_components=1e3)\n        else:\n            if representation == \"sparse\":\n                data = _20newsgroups_highdim_dataset(n_samples=2500)\n            else:\n                data = _20newsgroups_lowdim_dataset()\n        return data", "target": "def make_data(self, params):\n        representation, precompute = params\n        if representation == \"dense\":\n            data = _synth_regression_dataset(n_samples=1000000, n_features=100)\n        else:\n            data = _synth_regression_sparse_dataset(\n                n_samples=50000, n_features=5000, density=0.01\n            )\n        return data", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|pytorch/pytorch", "data_split": "test", "example_id": "002665", "source": "def get_estimator_path(benchmark, directory, params, save=False):\n    path = Path(__file__).resolve().parent / \"cache\"\n    path = (path / \"estimators\" / directory) if save else (path / \"tmp\")\n    filename = (\n        benchmark.__class__.__name__\n        + \"_estimator_\"\n        + \"_\".join(list(map(str, params)))\n        + \".pkl\"\n    )\n    return path / filename", "target": "def main():\n    result_path = sys.argv[1]\n    all = [\n        Benchmark(),\n    ]\n    for benchmark in all:\n        benchmark.enable_compile_time_instruction_count().collect_all().append_results(\n            result_path\n        )", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "002666", "source": "def rebuild(self) -> CoreSchema | None:\n        self._built_memo = None\n        if self._attempt_rebuild:\n            schema = self._attempt_rebuild()\n            if schema is not None:\n                return schema\n            else:\n                raise PydanticUserError(self._error_message, code=self._code)\n        return None", "target": "def rebuild(self) -> ValSer | None:\n        if self._attempt_rebuild:\n            val_ser = self._attempt_rebuild()\n            if val_ser is not None:\n                return val_ser\n            else:\n                raise PydanticUserError(self._error_message, code=self._code)\n        return None", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "test", "example_id": "002667", "source": "def run_once_functorch(\n    model: Callable, inp: InputsType, task: str, v: VType, maybe_check_consistency=False\n) -> None:\n    func = get_task_functorch(task)\n    if v is not None:\n        res = func(model, inp, v=v, strict=True)\n    else:\n        res = func(model, inp, strict=True)\n    if maybe_check_consistency:\n        af_func = get_task_func(task)\n        if v is not None:\n            expected = af_func(model, inp, v=v, strict=True)\n        else:\n            expected = af_func(model, inp, strict=True)\n        atol = 1e-2 if task == \"vhp\" else 5e-3\n        torch.testing.assert_close(\n            res,\n            expected,\n            rtol=1e-5,\n            atol=atol,\n            msg=f\"Consistency fail for task '{task}'\",\n        )", "target": "def test_cached_property_alias():\n    @dataclasses.dataclass\n    class Model:\n        width: int\n        height: int\n        @cached_property\n        def area(self) -> int:\n            return self.width * self.height\n    s = SchemaSerializer(\n        core_schema.model_schema(\n            Model,\n            core_schema.model_fields_schema(\n                {\n                    'width': core_schema.model_field(core_schema.int_schema()),\n                    'height': core_schema.model_field(core_schema.int_schema()),\n                },\n                computed_fields=[core_schema.computed_field('area', core_schema.int_schema())],\n            ),\n        )\n    )\n    assert s.to_python(Model(3, 4)) == {'width': 3, 'height': 4, 'area': 12}\n    assert s.to_python(Model(3, 4), mode='json') == {'width': 3, 'height': 4, 'area': 12}\n    assert s.to_json(Model(3, 4)) == b'{\"width\":3,\"height\":4,\"area\":12}'", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "test", "example_id": "002668", "source": "def make_data(self, params):\n        representation, solver = params\n        if representation == \"dense\":\n            data = _synth_regression_dataset(n_samples=500000, n_features=100)\n        else:\n            data = _synth_regression_sparse_dataset(\n                n_samples=100000, n_features=10000, density=0.005\n            )\n        return data", "target": "def make_data(self, params):\n        representation, precompute = params\n        if representation == \"dense\":\n            data = _synth_regression_dataset(n_samples=1000000, n_features=100)\n        else:\n            data = _synth_regression_sparse_dataset(\n                n_samples=50000, n_features=5000, density=0.01\n            )\n        return data", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "test", "example_id": "002669", "source": "def quack(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, target = args\n        from quack.cross_entropy import _cross_entropy\n        return lambda: _cross_entropy(x, target)", "target": "def wrapper_function(*args: Any, **kwargs: Any) -> Any:\n            return vd.call(*args, **kwargs)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "test", "example_id": "002670", "source": "def fn():\n        def validate(v, info):\n            return v\n        schema = core_schema.with_info_plain_validator_function(validate)\n        schema = core_schema.typed_dict_schema(\n            {'f': core_schema.typed_dict_field(schema)}, extra_behavior='allow', extras_schema=schema\n        )\n        validate.__pydantic_validator__ = SchemaValidator(schema)\n        return validate", "target": "def check_perf_csv(filename, threshold, threshold_scale):\n    try:\n        df = pd.read_csv(filename)\n    except FileNotFoundError:\n        print(f\"Error: File {filename} not found\")\n        sys.exit(1)\n    effective_threshold = threshold * threshold_scale\n    print(f\"Checking {filename} (speedup threshold >= {effective_threshold:.2f}x)\\n\")\n    failed = []\n    for _, row in df.iterrows():\n        model_name = row[\"name\"]\n        speedup = float(row[\"speedup\"])\n        abs_latency = float(row[\"abs_latency\"])\n        compilation_latency = float(row[\"compilation_latency\"])\n        compression_ratio = float(row[\"compression_ratio\"])\n        eager_peak_mem = float(row[\"eager_peak_mem\"])\n        dynamo_peak_mem = float(row[\"dynamo_peak_mem\"])\n        perf_summary = f\"{model_name:34} speedup={speedup:.3f}x\"\n        if pd.notna(abs_latency):\n            perf_summary += f\", latency={abs_latency:.1f} ms/iter\"\n        if pd.notna(compilation_latency):\n            perf_summary += f\", compile={compilation_latency:.3f}s\"\n        if pd.notna(compression_ratio):\n            perf_summary += f\", mem_ratio={1 / compression_ratio:.2f}x\"\n            if pd.notna(eager_peak_mem) and pd.notna(dynamo_peak_mem):\n                perf_summary += (\n                    f\" (eager={eager_peak_mem:.1f} GB, dynamo={dynamo_peak_mem:.1f} GB)\"\n                )\n        if speedup < effective_threshold:\n            failed.append((model_name, speedup))\n        print(perf_summary)\n    if failed:\n        print(\n            textwrap.dedent(\n                f\n            )\n        )\n        for name, sp in sorted(failed, key=lambda x: x[1]):\n            pct_from_target = (sp / effective_threshold - 1.0) * 100.0\n            print(\n                f\"  - {name}: {sp:.3f}x (< {effective_threshold:.2f}x; {pct_from_target:.1f}% from target)\"\n            )\n        sys.exit(1)\n    else:\n        print(\n            f\"\\nAll {len(df)} model(s) passed threshold check (>= {effective_threshold:.2f}x)\"\n        )", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "002671", "source": "def my_function(a, b, c):\n        return a + b + c", "target": "def my_function(a, b, c):\n        return a + b + c", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "test", "example_id": "002672", "source": "def compile_pattern(pattern: PatternType) -> re.Pattern[PatternType]:\n    try:\n        return re.compile(pattern)\n    except re.error:\n        raise PydanticCustomError('pattern_regex', 'Input should be a valid regular expression')", "target": "def split2d(img, cell_size, flatten=True):\n    h, w = img.shape[:2]\n    sx, sy = cell_size\n    cells = [np.hsplit(row, w//sx) for row in np.vsplit(img, h//sy)]\n    cells = np.array(cells)\n    if flatten:\n        cells = cells.reshape(-1, sy, sx)\n    return cells", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "test", "example_id": "002673", "source": "def train(self, samples, responses):\n        self.model.train(samples, cv.ml.ROW_SAMPLE, responses)", "target": "def train(self, samples, responses):\n        self.model.setType(cv.ml.SVM_C_SVC)\n        self.model.setC(1)\n        self.model.setKernel(cv.ml.SVM_RBF)\n        self.model.setGamma(.1)\n        self.model.train(samples, cv.ml.ROW_SAMPLE, responses.astype(int))", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "test", "example_id": "002674", "source": "def make_data(self, params):\n        representation, solver, n_jobs = params\n        if Benchmark.data_size == \"large\":\n            if representation == \"sparse\":\n                data = _20newsgroups_highdim_dataset(n_samples=10000)\n            else:\n                data = _20newsgroups_lowdim_dataset(n_components=1e3)\n        else:\n            if representation == \"sparse\":\n                data = _20newsgroups_highdim_dataset(n_samples=2500)\n            else:\n                data = _20newsgroups_lowdim_dataset()\n        return data", "target": "def make_data(self, params):\n        representation, precompute = params\n        if representation == \"dense\":\n            data = _synth_regression_dataset(n_samples=1000000, n_features=100)\n        else:\n            data = _synth_regression_sparse_dataset(\n                n_samples=50000, n_features=5000, density=0.01\n            )\n        return data", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "test", "example_id": "002675", "source": "def test_from_attributes_type_error():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'a': core_schema.model_field(schema=core_schema.int_schema()),\n                'b': core_schema.model_field(schema=core_schema.int_schema()),\n                'c': core_schema.model_field(schema=core_schema.str_schema()),\n            },\n            from_attributes=True,\n            model_name='MyModel',\n        )\n    )\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python('123')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'model_attributes_type',\n            'loc': (),\n            'msg': 'Input should be a valid dictionary or object to extract fields from',\n            'input': '123',\n        }\n    ]\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_json('123')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'model_type',\n            'loc': (),\n            'msg': 'Input should be an object',\n            'input': 123,\n            'ctx': {'class_name': 'MyModel'},\n        }\n    ]", "target": "def secs_to_ms(time_s):\n    return time_s * 1e3", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "002676", "source": "def test_generator_error():\n    def gen(error: bool):\n        yield 1\n        yield 2\n        if error:\n            raise RuntimeError('my error')\n        yield 3\n    v = SchemaValidator(cs.frozenset_schema(items_schema=cs.int_schema()))\n    r = v.validate_python(gen(False))\n    assert r == {1, 2, 3}\n    assert isinstance(r, frozenset)\n    msg = r'Error iterating over object, error: RuntimeError: my error \\[type=iteration_error,'\n    with pytest.raises(ValidationError, match=msg):\n        v.validate_python(gen(True))", "target": "def test_generator_error():\n    def gen(error: bool):\n        yield 1\n        yield 2\n        if error:\n            raise RuntimeError('error')\n        yield 3\n    v = SchemaValidator(core_schema.tuple_schema(items_schema=[core_schema.int_schema()], variadic_item_index=0))\n    assert v.validate_python(gen(False)) == (1, 2, 3)\n    msg = r'Error iterating over object, error: RuntimeError: error \\[type=iteration_error,'\n    with pytest.raises(ValidationError, match=msg):\n        v.validate_python(gen(True))", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "test", "example_id": "002677", "source": "def test_check_ref_used_ignores_metadata():\n    v = SchemaValidator(\n        core_schema.list_schema(\n            core_schema.int_schema(metadata={'type': 'definition-ref', 'schema_ref': 'foobar'}), ref='foobar'\n        )\n    )\n    assert v.validate_python([1, 2, 3]) == [1, 2, 3]", "target": "def check_loss(ref_loss, res_loss):\n    assert len(ref_loss) == len(res_loss)\n    length = len(ref_loss)\n    x = min(length, 10)\n    return sum(res_loss[-x:]) / 10 <= sum(ref_loss[-x:]) / 10 + 0.1", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "test", "example_id": "002678", "source": "def ser_x(self, v: Any, serializer: core_schema.SerializerFunctionWrapHandler, _) -> str:\n            assert self.x == 1_000\n            x = serializer(v)\n            return f'{x:_}'", "target": "def _hash_files(files):\n    hashes = {file: _hash_file(file) for file in files}\n    return hashes", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "test", "example_id": "002679", "source": "def test_filter_args_nested(params):\n    s = SchemaSerializer(core_schema.dict_schema(core_schema.str_schema(), core_schema.list_schema()))\n    include, exclude, expected = params['include'], params['exclude'], params['expected']\n    value = {'0': [0], '1': [0, 1], '2': [0, 1, 2], '3': [0, 1, 2, 3]}\n    assert s.to_python(value, include=include, exclude=exclude) == expected\n    assert s.to_python(value, mode='json', include=include, exclude=exclude) == expected\n    assert json.loads(s.to_json(value, include=include, exclude=exclude)) == expected", "target": "def test_bad_iter(items_schema):\n    class BadIter:\n        def __init__(self, success: bool):\n            self._success = success\n            self._index = 0\n        def __iter__(self):\n            return self\n        def __len__(self):\n            return 2\n        def __next__(self):\n            self._index += 1\n            if self._index == 1:\n                return 1\n            elif self._success:\n                raise StopIteration()\n            else:\n                raise RuntimeError('broken')\n    v = SchemaValidator(cs.list_schema(items_schema={'type': items_schema}))\n    assert v.validate_python(BadIter(True)) == [1]\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python(BadIter(False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'iteration_error',\n            'loc': (1,),\n            'msg': 'Error iterating over object, error: RuntimeError: broken',\n            'input': IsInstance(BadIter),\n            'ctx': {'error': 'RuntimeError: broken'},\n        }\n    ]", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "002680", "source": "def _work(self):\n        @torch.compile(\n            backend=self.backend(),\n            fullgraph=True,\n            dynamic=self.is_dynamic(),\n        )\n        def f(a, b):\n            result = a.clone()\n            for i in range(1000):\n                if i % 3 == 0:\n                    result = result + b\n                elif i % 3 == 1:\n                    result = result + 8 * b\n                else:\n                    result = result.sin()\n            return result\n        with fresh_cache():\n            f(self.a, self.b)", "target": "def _work(self) -> None:\n        @torch.compile(\n            backend=\"inductor\",\n            fullgraph=True,\n            dynamic=self._dynamic,\n        )\n        def f(a, b):\n            z = torch.mm(a, b)\n            for i in range(200):\n                z = torch.mm(z, b)\n            return z\n        with fresh_cache(), torch._inductor.config.patch(max_autotune=True):\n            f(self.a, self.b)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "test", "example_id": "002681", "source": "def some_method(self):\n            return self.path + '-success'", "target": "def test_parse_to_string_convertible(self):\n        try_to_convert = partial(self._try_to_convert, cv.utils.dumpString)\n        for convertible in (None, '', 's', 'str', str(123), np.str_('test2')):\n            expected = 'string: ' + (convertible if convertible else '')\n            actual = try_to_convert(convertible)\n            self.assertEqual(expected, actual,\n                             msg=get_conversion_error_msg(convertible, expected, actual))", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "002682", "source": "def test_frozenset_from_dict_items(input_value, items_schema, expected):\n    v = SchemaValidator(cs.frozenset_schema(items_schema=items_schema))\n    output = v.validate_python(input_value)\n    assert isinstance(output, frozenset)\n    assert output == expected", "target": "def test_frozenset_from_dict_items(input_value, items_schema, expected):\n    v = SchemaValidator(core_schema.tuple_schema(items_schema=[items_schema], variadic_item_index=0))\n    output = v.validate_python(input_value)\n    assert isinstance(output, tuple)\n    assert output == expected", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "002683", "source": "def test_list_int(input_value, expected):\n    v = SchemaValidator(cs.list_schema(items_schema=cs.int_schema()))\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            v.validate_python(input_value)\n    else:\n        assert v.validate_python(input_value) == expected", "target": "def test_list_int(py_and_json: PyAndJson, input_value, expected):\n    v = py_and_json(core_schema.json_schema(core_schema.list_schema(core_schema.int_schema())))\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=expected.message) as exc_info:\n            v.validate_test(input_value)\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        assert v.validate_test(input_value) == expected", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "test", "example_id": "002684", "source": "def children_types(self) -> Tuple[ASTNodeType, ...]:\n        return (ASTNodeType.Namespace, ASTNodeType.Class, ASTNodeType.Function,\n                ASTNodeType.Enumeration, ASTNodeType.Constant)", "target": "def children_types(self) -> Tuple[ASTNodeType, ...]:\n        return (ASTNodeType.Constant, )", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "test", "example_id": "002685", "source": "def get_deepspeech(device: torch.device) -> GetterReturnType:\n    sample_rate = 16000\n    window_size = 0.02\n    window = \"hamming\"\n    audio_conf = dict(\n        sample_rate=sample_rate, window_size=window_size, window=window, noise_dir=None\n    )\n    N = 10\n    num_classes = 10\n    spectrogram_size = 161\n    seq_length = 500\n    target_length = 10\n    labels = torch.rand(num_classes, device=device)\n    inputs = torch.rand(N, 1, spectrogram_size, seq_length, device=device)\n    inputs_sizes = (\n        torch.rand(N, device=device).mul(seq_length * 0.1).add(seq_length * 0.8)\n    )\n    targets = torch.rand(N, target_length, device=device)\n    targets_sizes = torch.full((N,), target_length, dtype=torch.int, device=device)\n    model = models.DeepSpeech(\n        rnn_type=nn.LSTM,\n        labels=labels,\n        rnn_hidden_size=1024,\n        nb_layers=5,\n        audio_conf=audio_conf,\n        bidirectional=True,\n    )\n    if has_functorch:\n        from functorch.experimental import replace_all_batch_norm_modules_\n        replace_all_batch_norm_modules_(model)\n    model = model.to(device)\n    criterion = nn.CTCLoss()\n    params, names = extract_weights(model)\n    def forward(*new_params: Tensor) -> Tensor:\n        load_weights(model, names, new_params)\n        out, out_sizes = model(inputs, inputs_sizes)\n        out = out.transpose(0, 1)\n        loss = criterion(out, targets, out_sizes, targets_sizes)\n        return loss\n    return forward, params", "target": "def test_dict():\n    v = SchemaValidator(\n        core_schema.dict_schema(keys_schema=core_schema.int_schema(), values_schema=core_schema.int_schema())\n    )\n    assert v.validate_json('{\"1\": 2, \"3\": 4}') == {1: 2, 3: 4}\n    assert json.loads('{\"1\": 1, \"1\": 2}') == {'1': 2}\n    assert v.validate_json('{\"1\": 1, \"1\": 2}') == {1: 2}", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "002686", "source": "def f(a):\n            xs = a.tolist()\n            y = 0\n            if self.use_loop:\n                for i in xs:\n                    y += i\n            else:\n                y = sum(xs)\n            return torch.tensor(y)", "target": "def f(a, b):\n            xs = b.tolist()\n            for x in xs:\n                torch._check(x >= 0)\n                torch._check(x <= self.N)\n            return a.split(xs)", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "test", "example_id": "002687", "source": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "target": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|opencv/opencv", "data_split": "test", "example_id": "002688", "source": "def filter_ciflow_tags(tags: set[str]) -> list[str]:\n    \"Return sorted list of ciflow tags\"\n    return sorted(\n        tag[:-2] for tag in tags if tag.startswith(\"ciflow/\") and tag.endswith(\"/*\")\n    )", "target": "def _try_to_convert(self, conversion, value):\n        try:\n            result = conversion(value).lower()\n        except Exception as e:\n            self.fail(\n                '{} \"{}\" is risen for conversion {} of type {}'.format(\n                    type(e).__name__, e, value, type(value).__name__\n                )\n            )\n        else:\n            return result", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "test", "example_id": "002689", "source": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "target": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "test", "example_id": "002690", "source": "def make_data(self, params):\n        (representation,) = params\n        if representation == \"sparse\":\n            data = _20newsgroups_highdim_dataset()\n        else:\n            data = _20newsgroups_lowdim_dataset()\n        return data", "target": "def make_data(self, params):\n        data = _synth_classification_dataset(\n            n_samples=10000, n_features=100, n_classes=5\n        )\n        return data", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "002691", "source": "def test_ignored_def():\n    s = SchemaSerializer(\n        core_schema.definitions_schema(\n            core_schema.list_schema(core_schema.int_schema()),\n            [core_schema.int_schema(ref='foobar', serialization=core_schema.to_string_ser_schema(when_used='always'))],\n        )\n    )\n    assert s.to_python([1, 2, 3]) == [1, 2, 3]", "target": "def test_ignored_def():\n    v = SchemaValidator(\n        core_schema.definitions_schema(\n            core_schema.list_schema(core_schema.int_schema()), [core_schema.int_schema(ref='foobar')]\n        )\n    )\n    assert v.validate_python([1, 2, '3']) == [1, 2, 3]\n    r = plain_repr(v)\n    assert r.startswith('SchemaValidator(title=\"list[int]\",')", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "test", "example_id": "002692", "source": "def predict(self, samples):\n        new_samples = self.unroll_samples(samples)\n        _ret, resp = self.model.predict(new_samples)\n        return resp.ravel().reshape(-1, self.class_n).argmax(1)", "target": "def predict(self, samples):\n        _ret, resp = self.model.predict(samples)\n        return resp.ravel()", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "002693", "source": "def _prepare_once(self):\n        self.a = torch.ones(1000, device=self.device())\n        self.b = torch.torch.ones(1000, device=self.device())", "target": "def _prepare_once(self):\n        self.x = torch.randn(4, 4, requires_grad=True)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "test", "example_id": "002694", "source": "def test_list_json():\n    s = SchemaSerializer(core_schema.list_schema(core_schema.json_schema()))\n    v = ['a', [1, 2], None]\n    assert s.to_python(v) == v\n    assert s.to_python(v, round_trip=True) == ['\"a\"', '[1,2]', 'null']\n    assert s.to_python(v, mode='json') == v\n    assert s.to_python(v, mode='json', round_trip=True) == ['\"a\"', '[1,2]', 'null']\n    assert s.to_json(v) == b'[\"a\",[1,2],null]'\n    assert s.to_json(v, round_trip=True) == b'[\"\\\\\"a\\\\\"\",\"[1,2]\",\"null\"]'", "target": "def ge(self: _Pipeline[_InT, _NewOutGe], ge: _NewOutGe) -> _Pipeline[_InT, _NewOutGe]:\n        return self.constrain(annotated_types.Ge(ge))", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "002695", "source": "def parse_args() -> Any:\n    from argparse import ArgumentParser\n    parser = ArgumentParser(\"Check PR labels\")\n    parser.add_argument(\"pr_num\", type=int)\n    parser.add_argument(\n        \"--exit-non-zero\",\n        action=\"store_true\",\n        help=\"Return a non-zero exit code if the PR does not have the required labels\",\n    )\n    return parser.parse_args()", "target": "def parse_args() -> Any:\n    from argparse import ArgumentParser\n    parser = ArgumentParser(\"Comment on a PR\")\n    parser.add_argument(\"pr_num\", type=int)\n    parser.add_argument(\"action\", type=str)\n    return parser.parse_args()", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "test", "example_id": "002696", "source": "def forward(self, idx: Tensor, input_pos: Optional[Tensor] = None) -> Tensor:\n        assert self.freqs_cis is not None, \"Caches must be initialized first\"\n        mask = self.causal_mask[None, None, input_pos]\n        freqs_cis = self.freqs_cis[input_pos]\n        x = self.tok_embeddings(idx)\n        for i, layer in enumerate(self.layers):\n            x = layer(x, input_pos, freqs_cis, mask)\n        x = self.norm(x)\n        logits = self.output(x)\n        return logits", "target": "def get_shapes(self) -> tuple[tuple[int, ...], ...]:\n        return (\n            (32768, 256),\n            (32768, 512),\n            (32768, 1024),\n            (32768, 2048),\n            (32768, 4096),\n            (32768, 8192),\n            (32768, 16384),\n            (32768, 32768),\n            (32768, 65536),\n        ) + extra_shapes_for_norm", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "002697", "source": "def test_filter_args_nested(params):\n    s = SchemaSerializer(core_schema.dict_schema(core_schema.str_schema(), core_schema.list_schema()))\n    include, exclude, expected = params['include'], params['exclude'], params['expected']\n    value = {'0': [0], '1': [0, 1], '2': [0, 1, 2], '3': [0, 1, 2, 3]}\n    assert s.to_python(value, include=include, exclude=exclude) == expected\n    assert s.to_python(value, mode='json', include=include, exclude=exclude) == expected\n    assert json.loads(s.to_json(value, include=include, exclude=exclude)) == expected", "target": "def test_filter_args_nested(params):\n    s = SchemaSerializer(core_schema.list_schema(core_schema.list_schema()))\n    include, exclude, expected = params['include'], params['exclude'], params['expected']\n    value = [[0], [0, 1], [0, 1, 2], [0, 1, 2, 3]]\n    assert s.to_python(value, include=include, exclude=exclude) == expected\n    assert s.to_python(value, mode='json', include=include, exclude=exclude) == expected\n    assert json.loads(s.to_json(value, include=include, exclude=exclude)) == expected", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "test", "example_id": "002698", "source": "def fallback_func(obj):\n        nonlocal v\n        v += 1\n        return FoobarCount(v)", "target": "def is_core_schema_field(\n    schema: CoreSchemaOrField,\n) -> TypeGuard[CoreSchemaField]:\n    return schema['type'] in _CORE_SCHEMA_FIELD_TYPES", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "test", "example_id": "002699", "source": "def detect_features(self, frame):\n        keypoints, descrs = self.detector.detectAndCompute(frame, None)\n        if descrs is None:\n            descrs = []\n        return keypoints, descrs", "target": "def constrain(self: _Pipeline[_InT, _NewOutLt], constraint: annotated_types.Lt) -> _Pipeline[_InT, _NewOutLt]: ...", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "test", "example_id": "002700", "source": "def f(input_value, info):\n        return input_value + ' Changed'", "target": "def fn(x):\n        for _ in range(N):\n            x = x.sin()\n        return x", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|opencv/opencv", "data_split": "test", "example_id": "002701", "source": "def test_aggregates_failures_and_raises(monkeypatch, patch_module):\n    run_test_plan = patch_module.module.run_test_plan\n    tests_map = {\n        \"mix\": {\n            \"title\": \"Some pass some fail\",\n            \"steps\": [\n                \"pytest test_a.py\",\n                \"pytest test_b.py\",\n                \"pytest test_c.py\",\n            ],\n        }\n    }\n    patch_module.run_command.side_effect = [0, 1, 2]\n    with pytest.raises(RuntimeError) as ei:\n        run_test_plan(\"mix\", \"cpu\", tests_map)\n    msg = str(ei.value)\n    assert \"2 pytest runs failed\" in msg\n    patch_module.logger.error.assert_called_once()\n    assert patch_module.run_command.call_count == 3", "target": "def read_linked_libs(lib_name, abis):\n    deps_lists = []\n    for abi in abis:\n         with open(path.join(ANDROID_PROJECT_DIR, \"OpenCV/src/main/cpp\", f\"linkedlibs.{lib_name}.{abi}.txt\")) as f:\n            text = f.read()\n            linked_libs = text.split(\";\")\n            linked_libs = [x.replace(\"$<LINK_ONLY:\", \"\").replace(\">\", \"\") for x in linked_libs]\n            deps_lists.append(linked_libs)\n    return merge_dependencies_lists(deps_lists)", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|pytorch/pytorch", "data_split": "test", "example_id": "002702", "source": "def euclidean_distances(X, n_jobs):\n    return pairwise_distances(X, metric=\"euclidean\", n_jobs=n_jobs)", "target": "def state_dict_hook(module, destination, prefix, local_metadata):\n        for name, param in module.named_parameters():\n            if isinstance(destination[name], torch.Tensor) and not isinstance(\n                destination[name], torch.nn.Parameter\n            ):\n                destination[name] = torch.nn.Parameter(destination[name])", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "test", "example_id": "002703", "source": "def test_function_before_error():\n    def my_function(input_value, info):\n        return input_value + 'x'\n    v = SchemaValidator(\n        {\n            'type': 'function-before',\n            'function': {'type': 'with-info', 'function': my_function},\n            'schema': cs.str_schema(max_length=5),\n        }\n    )\n    assert v.validate_python('1234') == '1234x'\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python('12345')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'string_too_long',\n            'loc': (),\n            'msg': 'String should have at most 5 characters',\n            'input': '12345x',\n            'ctx': {'max_length': 5},\n        }\n    ]\n    assert repr(exc_info.value).startswith('1 validation error for function-before[my_function(), constrained-str]\\n')", "target": "def check_deprecated(config_dict: ConfigDict) -> None:\n    deprecated_removed_keys = V2_REMOVED_KEYS & config_dict.keys()\n    deprecated_renamed_keys = V2_RENAMED_KEYS.keys() & config_dict.keys()\n    if deprecated_removed_keys or deprecated_renamed_keys:\n        renamings = {k: V2_RENAMED_KEYS[k] for k in sorted(deprecated_renamed_keys)}\n        renamed_bullets = [f'* {k!r} has been renamed to {v!r}' for k, v in renamings.items()]\n        removed_bullets = [f'* {k!r} has been removed' for k in sorted(deprecated_removed_keys)]\n        message = '\\n'.join(['Valid config keys have changed in V2:'] + renamed_bullets + removed_bullets)\n        warnings.warn(message, UserWarning)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "test", "example_id": "002704", "source": "def test_property():\n    @dataclasses.dataclass\n    class Model:\n        def __init__(self, **kwargs):\n            for key, value in kwargs.items():\n                setattr(self, key, value)\n        @property\n        def area(self) -> bytes:\n            a = self.width * self.height\n            return b'%d' % a\n    s = SchemaSerializer(\n        core_schema.model_schema(\n            Model,\n            core_schema.model_fields_schema(\n                {\n                    'width': core_schema.model_field(core_schema.int_schema()),\n                    'height': core_schema.model_field(core_schema.int_schema()),\n                },\n                computed_fields=[core_schema.computed_field('area', core_schema.bytes_schema())],\n            ),\n        )\n    )\n    assert s.to_python(Model(width=3, height=4)) == {'width': 3, 'height': 4, 'area': b'12'}\n    assert s.to_python(Model(width=3, height=4), mode='json') == {'width': 3, 'height': 4, 'area': '12'}\n    assert s.to_json(Model(width=3, height=4)) == b'{\"width\":3,\"height\":4,\"area\":\"12\"}'\n    assert s.to_python(Model(width=3, height=4), round_trip=True) == {'width': 3, 'height': 4}\n    assert s.to_json(Model(width=3, height=4), round_trip=True) == b'{\"width\":3,\"height\":4}'\n    assert s.to_python(Model(width=3, height=4), exclude_computed_fields=True) == {'width': 3, 'height': 4}\n    assert s.to_json(Model(width=3, height=4), exclude_computed_fields=True) == b'{\"width\":3,\"height\":4}'", "target": "def test_invalid_keyword_only_parameter_position() -> None:\n    with pytest.raises(\n        SchemaError, match=\"Keyword only parameter 'test' cannot follow variadic keyword only parameter\"\n    ):\n        SchemaValidator(\n            schema=cs.arguments_v3_schema(\n                [\n                    cs.arguments_v3_parameter(name='a', schema=cs.int_schema(), mode='var_kwargs_uniform'),\n                    cs.arguments_v3_parameter(name='test', schema=cs.int_schema(), mode='keyword_only'),\n                ]\n            )\n        )", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "test", "example_id": "002705", "source": "def node_type(self) -> ASTNodeType:\n        return ASTNodeType.Function", "target": "def node_type(self) -> ASTNodeType:\n        return ASTNodeType.Namespace", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "002706", "source": "def forward(self, x):\n        for i, l in enumerate(self.linears):\n            x = self.linears[i // 2](x) + l(x)\n        return x", "target": "def forward(self, x):\n        x = self.relu_a(x)\n        x = x + self.sub_mods(x)\n        return x + self.relu_b(x) + self.a", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "test", "example_id": "002707", "source": "def parse_args() -> Any:\n    from argparse import ArgumentParser\n    parser = ArgumentParser(\"Comment on a PR\")\n    parser.add_argument(\"pr_num\", type=int)\n    parser.add_argument(\"action\", type=str)\n    return parser.parse_args()", "target": "def test_set_ints_both(py_and_json: PyAndJson, input_value, expected):\n    v = py_and_json({'type': 'set', 'items_schema': {'type': 'int'}})\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            v.validate_test(input_value)\n    else:\n        assert v.validate_test(input_value) == expected", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "002708", "source": "def constrain(self: _Pipeline[_InT, _NewOutGt], constraint: annotated_types.Gt) -> _Pipeline[_InT, _NewOutGt]: ...", "target": "def constrain(\n        self: _Pipeline[_InT, _NewOutDatetime], constraint: annotated_types.Timezone\n    ) -> _Pipeline[_InT, _NewOutDatetime]: ...", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|opencv/opencv", "data_split": "test", "example_id": "002709", "source": "def typename(self) -> str:\n        return self._typename", "target": "def test_mat_construct_4d(self):\n            data = np.random.random([5, 10, 10, 3])\n            mat_data0 = cv.Mat(data)\n            assert isinstance(mat_data0, cv.Mat)\n            assert isinstance(mat_data0, np.ndarray)\n            self.assertEqual(mat_data0.wrap_channels, False)\n            res0 = cv.utils.dumpInputArray(mat_data0)\n            self.assertEqual(res0, \"InputArray: empty()=false kind=0x00010000 flags=0x01010000 total(-1)=1500 dims(-1)=4 size(-1)=[5 10 10 3] type(-1)=CV_64FC1\")\n            mat_data1 = cv.Mat(data, wrap_channels=True)\n            assert isinstance(mat_data1, cv.Mat)\n            assert isinstance(mat_data1, np.ndarray)\n            self.assertEqual(mat_data1.wrap_channels, True)\n            res1 = cv.utils.dumpInputArray(mat_data1)\n            self.assertEqual(res1, \"InputArray: empty()=false kind=0x00010000 flags=0x01010000 total(-1)=500 dims(-1)=3 size(-1)=[5 10 10] type(-1)=CV_64FC3\")\n            mat_data2 = cv.Mat(mat_data1)\n            assert isinstance(mat_data2, cv.Mat)\n            assert isinstance(mat_data2, np.ndarray)\n            self.assertEqual(mat_data2.wrap_channels, True)\n            res2 = cv.utils.dumpInputArray(mat_data2)\n            self.assertEqual(res2, \"InputArray: empty()=false kind=0x00010000 flags=0x01010000 total(-1)=500 dims(-1)=3 size(-1)=[5 10 10] type(-1)=CV_64FC3\")", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "test", "example_id": "002710", "source": "def test_iterator(self, input_: str = \"abcdef\") -> None:\n        iter_ = PeekableIterator(input_)\n        for idx, c in enumerate(iter_):\n            self.assertEqual(c, input_[idx])", "target": "def test_str_validation_w_lax() -> None:\n    s = SchemaValidator(cs.decimal_schema(strict=False))\n    assert s.validate_python('1.23') == Decimal('1.23')", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "test", "example_id": "002711", "source": "def int_(cls, ctype_name: Optional[str] = None,\n             required_modules: Tuple[str, ...] = ()):\n        if ctype_name is None:\n            ctype_name = \"int\"\n        return PrimitiveTypeNode(ctype_name, typename=\"int\", required_modules=required_modules)", "target": "def test_on_error_bad_default(self):\n        with pytest.raises(SchemaError, match=\"'on_error = default' requires a `default` or `default_factory`\"):\n            SchemaValidator(\n                schema=core_schema.typed_dict_schema(\n                    fields={\n                        'x': core_schema.typed_dict_field(\n                            schema=core_schema.with_default_schema(schema=core_schema.str_schema(), on_error='default')\n                        )\n                    }\n                )\n            )", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "test", "example_id": "002712", "source": "def add_class(self, name: str,\n                  bases: Sequence[\"weakref.ProxyType[ClassNode]\"] = (),\n                  properties: Sequence[ClassProperty] = ()) -> \"ClassNode\":\n        return self._add_child(ClassNode, name, bases=bases,\n                               properties=properties)", "target": "def field_serializer(\n    field: str,\n    /,\n    *fields: str,\n    mode: Literal['wrap'],\n    return_type: Any = ...,\n    when_used: WhenUsed = ...,\n    check_fields: bool | None = ...,\n) -> Callable[[_FieldWrapSerializerT], _FieldWrapSerializerT]: ...", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "002713", "source": "def test_function_plain_field_serializer_to_json():\n    @dataclasses.dataclass\n    class Model:\n        x: int\n        def ser_x(self, v: Any, _) -> str:\n            assert self.x == 1_000\n            return f'{v:_}'\n    s = SchemaSerializer(\n        core_schema.model_schema(\n            Model,\n            core_schema.model_fields_schema(\n                {\n                    'x': core_schema.model_field(\n                        core_schema.int_schema(\n                            serialization=core_schema.plain_serializer_function_ser_schema(\n                                Model.ser_x, is_field_serializer=True, info_arg=True\n                            )\n                        )\n                    )\n                }\n            ),\n        )\n    )\n    assert json.loads(s.to_json(Model(x=1000))) == {'x': '1_000'}", "target": "def test_function_plain_field_serializer_to_json():\n    class Model(TypedDict):\n        x: int\n    def ser_x(data: Model, v: Any, info: core_schema.FieldSerializationInfo) -> str:\n        assert data['x'] == 1_000\n        return f'{v:_}-{info.field_name}'\n    s = SchemaSerializer(\n        core_schema.typed_dict_schema(\n            {\n                'x': core_schema.typed_dict_field(\n                    core_schema.int_schema(\n                        serialization=core_schema.plain_serializer_function_ser_schema(\n                            ser_x, is_field_serializer=True, info_arg=True\n                        )\n                    )\n                )\n            }\n        )\n    )\n    assert json.loads(s.to_json(Model(x=1000))) == {'x': '1_000-x'}", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "002714", "source": "def compiled(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, target = args\n        torch._dynamo.mark_dynamic(x, 0)\n        torch._dynamo.mark_dynamic(target, 0)\n        compiled_cross_entropy = torch.compile(\n            lambda x, target: F.cross_entropy(x, target, reduction=\"none\"),\n            mode=self.compile_mode,\n            fullgraph=True,\n        )\n        return lambda: compiled_cross_entropy(x, target)", "target": "def compiled(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        (x,) = args\n        torch._dynamo.mark_dynamic(x, 0)\n        compiled_softmax = torch.compile(\n            lambda x: F.softmax(x, dim=-1), mode=self.compile_mode, fullgraph=True\n        )\n        return lambda: compiled_softmax(x)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "test", "example_id": "002715", "source": "def test_function_plain_field_serializer_to_python():\n    class Model(RootModel):\n        def ser_root(self, v: Any, _) -> str:\n            assert self.root == 1_000\n            return f'{v:_}'\n    s = SchemaSerializer(\n        core_schema.model_schema(\n            Model,\n            core_schema.int_schema(\n                serialization=core_schema.plain_serializer_function_ser_schema(\n                    Model.ser_root, is_field_serializer=True, info_arg=True\n                )\n            ),\n            root_model=True,\n        )\n    )\n    assert s.to_python(Model(1000)) == '1_000'", "target": "def test_dataclass_args_init(input_value, extra_behavior, expected):\n    @dataclasses.dataclass\n    class Foo:\n        a: str\n        b: str\n        def __post_init__(self):\n            self.b = self.a.upper()\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.str_schema(), init=False),\n            ],\n            extra_behavior=extra_behavior,\n        ),\n        ['a', 'b'],\n        post_init=True,\n    )\n    v = SchemaValidator(schema)\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)) as exc_info:\n            v.validate_python(input_value)\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        assert dataclasses.asdict(v.validate_python(input_value)) == expected", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "test", "example_id": "002716", "source": "def functions(self) -> Dict[str, FunctionNode]:\n        return self._children[ASTNodeType.Function]", "target": "def functions(self) -> Dict[str, FunctionNode]:\n        return self._children[ASTNodeType.Function]", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "test", "example_id": "002717", "source": "def predict(self, samples):\n        _ret, resp = self.model.predict(samples)\n        return resp.ravel()", "target": "def predict(self, samples):\n        _retval, results, _neigh_resp, _dists = self.model.findNearest(samples, k = 10)\n        return results.ravel()", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "test", "example_id": "002718", "source": "def constrain(self: _Pipeline[_InT, _NewOutLe], constraint: annotated_types.Le) -> _Pipeline[_InT, _NewOutLe]: ...", "target": "def test_simple():\n    v = SchemaValidator(core_schema.str_schema())\n    assert v.validate_python(b'abc') == 'abc'\n    assert v.isinstance_python(b'abc') is True\n    assert v.validate_python(b'abc', self_instance='foobar') == 'abc'\n    assert v.isinstance_python(b'abc', self_instance='foobar') is True\n    assert v.validate_json('\"abc\"') == 'abc'\n    assert v.validate_json('\"abc\"', self_instance='foobar') == 'abc'", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|opencv/opencv", "data_split": "test", "example_id": "002719", "source": "def full_typename(self) -> str:\n        return self.type_format.format(self.types_separator.join(\n            item.full_typename for item in self\n        ))", "target": "def run(img, order):\n                    return np.transpose(img, order)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "test", "example_id": "002720", "source": "def test_dataclass():\n    schema = core_schema.call_schema(\n        core_schema.arguments_schema(\n            [\n                core_schema.arguments_parameter('foo', core_schema.int_schema()),\n                core_schema.arguments_parameter('bar', core_schema.str_schema()),\n                core_schema.arguments_parameter('spam', core_schema.bytes_schema(), mode='keyword_only'),\n                core_schema.arguments_parameter('frog', core_schema.int_schema(), mode='keyword_only'),\n            ]\n        ),\n        DataClass,\n        serialization=core_schema.model_ser_schema(\n            DataClass,\n            core_schema.model_fields_schema(\n                {\n                    'foo': core_schema.model_field(core_schema.int_schema()),\n                    'bar': core_schema.model_field(core_schema.str_schema()),\n                    'spam': core_schema.model_field(core_schema.bytes_schema()),\n                }\n            ),\n        ),\n    )\n    v = SchemaValidator(schema)\n    dc = v.validate_python({'foo': 1, 'bar': 'bar-str', 'spam': 'bite', 'frog': 123})\n    assert dc == DataClass(foo=1, bar='bar-str', spam=b'bite', frog=123)\n    dc.class_var = 2\n    assert dataclasses.is_dataclass(dc)\n    s = SchemaSerializer(schema)\n    assert dataclasses.asdict(dc) == IsStrictDict(foo=1, bar='bar-str', spam=b'bite')\n    assert s.to_python(dc) == IsStrictDict(foo=1, bar='bar-str', spam=b'bite')\n    assert s.to_python(dc, mode='json') == {'foo': 1, 'bar': 'bar-str', 'spam': 'bite'}\n    assert json.loads(s.to_json(dc)) == {'foo': 1, 'bar': 'bar-str', 'spam': 'bite'}", "target": "def string_or_pathlike_(ctype_name: str = \"string\") -> UnionTypeNode:\n        return UnionTypeNode(\n            ctype_name,\n            items=(\n                PrimitiveTypeNode.str_(ctype_name),\n                PathLikeTypeNode(ctype_name)\n            )\n        )", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "002721", "source": "def eager(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, w, dy = args\n        y = self.rms_norm_ref(x, w)\n        return lambda: torch.autograd.grad(\n            y, [x, w], grad_outputs=dy, retain_graph=True\n        )", "target": "def eager(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, w = args\n        return lambda: self.layernorm_ref(x, w)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "test", "example_id": "002722", "source": "def test_str_enum():\n    class MyEnum(str, Enum):\n        a = 'a'\n        b = 'b'\n    v = SchemaSerializer(core_schema.enum_schema(MyEnum, list(MyEnum.__members__.values()), sub_type='str'))\n    assert v.to_python(MyEnum.a) is MyEnum.a\n    assert v.to_python(MyEnum.a, mode='json') == 'a'\n    assert v.to_json(MyEnum.a) == b'\"a\"'\n    with pytest.warns(\n        UserWarning,\n        match=r\"Expected `enum` - serialized value may not be as expected \\[input_value='a', input_type=str\\]\",\n    ):\n        assert v.to_python('a') == 'a'\n    with pytest.warns(\n        UserWarning,\n        match=r\"Expected `enum` - serialized value may not be as expected \\[input_value='a', input_type=str\\]\",\n    ):\n        assert v.to_json('a') == b'\"a\"'", "target": "def pytest_generate_tests(metafunc):\n    if metafunc.cls.__name__ == \"TestBenchNetwork\":\n        metafunc.parametrize(\"net_name\", all_nets, scope=\"class\")\n        metafunc.parametrize(\n            \"executor\", [metafunc.config.getoption(\"executor\")], scope=\"class\"\n        )\n        metafunc.parametrize(\n            \"fuser\", [metafunc.config.getoption(\"fuser\")], scope=\"class\"\n        )", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "test", "example_id": "002723", "source": "def make_generic_v1_field_validator(validator: V1Validator) -> core_schema.WithInfoValidatorFunction:\n    sig = signature(validator)\n    needs_values_kw = False\n    for param_num, (param_name, parameter) in enumerate(sig.parameters.items()):\n        if can_be_keyword(parameter) and param_name in ('field', 'config'):\n            raise PydanticUserError(\n                'The `field` and `config` parameters are not available in Pydantic V2, '\n                'please use the `info` parameter instead.',\n                code='validator-field-config-info',\n            )\n        if parameter.kind is Parameter.VAR_KEYWORD:\n            needs_values_kw = True\n        elif can_be_keyword(parameter) and param_name == 'values':\n            needs_values_kw = True\n        elif can_be_positional(parameter) and param_num == 0:\n            continue\n        elif parameter.default is Parameter.empty:\n            raise PydanticUserError(\n                f'Unsupported signature for V1 style validator {validator}: {sig} is not supported.',\n                code='validator-v1-signature',\n            )\n    if needs_values_kw:\n        val1 = cast(V1ValidatorWithValues, validator)\n        def wrapper1(value: Any, info: core_schema.ValidationInfo) -> Any:\n            return val1(value, values=info.data)\n        return wrapper1\n    else:\n        val2 = cast(V1OnlyValueValidator, validator)\n        def wrapper2(value: Any, _: core_schema.ValidationInfo) -> Any:\n            return val2(value)\n        return wrapper2", "target": "def recursively_defined_type_refs() -> set[str]:\n    visited = _generic_recursion_cache.get()\n    if not visited:\n        return set()\n    return visited.copy()", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "002724", "source": "def test_only_validate_by_name(py_and_json) -> None:\n    schema = core_schema.dataclass_schema(\n        BasicDataclass,\n        core_schema.dataclass_args_schema(\n            'BasicDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), validation_alias='FieldA'),\n            ],\n        ),\n        ['a'],\n        config=core_schema.CoreConfig(validate_by_name=True, validate_by_alias=False),\n    )\n    v = py_and_json(schema)\n    assert v.validate_test({'a': 'hello'}) == BasicDataclass(a='hello')\n    with pytest.raises(ValidationError, match=r'a\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'FieldA': 'hello'})", "target": "def test_only_validate_by_name(py_and_json) -> None:\n    v = py_and_json(\n        {\n            'type': 'model-fields',\n            'fields': {'field_a': {'validation_alias': 'FieldA', 'type': 'model-field', 'schema': {'type': 'int'}}},\n        },\n        config=CoreConfig(validate_by_name=True, validate_by_alias=False),\n    )\n    assert v.validate_test({'field_a': '123'}) == ({'field_a': 123}, None, {'field_a'})\n    with pytest.raises(ValidationError, match=r'field_a\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'FieldA': '123'})", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "test", "example_id": "002725", "source": "def predict(self, samples):\n        _retval, results, _neigh_resp, _dists = self.model.findNearest(samples, k = 10)\n        return results.ravel()", "target": "def predict(self, samples):\n        _ret, resp = self.model.predict(samples)\n        return resp.ravel()", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "test", "example_id": "002726", "source": "def compute_bench(samples_range, features_range, n_iter=3, rank=50):\n    it = 0\n    results = defaultdict(lambda: [])\n    max_it = len(samples_range) * len(features_range)\n    for n_samples in samples_range:\n        for n_features in features_range:\n            it += 1\n            print(\"====================\")\n            print(\"Iteration %03d of %03d\" % (it, max_it))\n            print(\"====================\")\n            X = make_low_rank_matrix(\n                n_samples, n_features, effective_rank=rank, tail_strength=0.2\n            )\n            gc.collect()\n            print(\"benchmarking scipy svd: \")\n            tstart = time()\n            svd(X, full_matrices=False)\n            results[\"scipy svd\"].append(time() - tstart)\n            gc.collect()\n            print(\"benchmarking scikit-learn randomized_svd: n_iter=0\")\n            tstart = time()\n            randomized_svd(X, rank, n_iter=0)\n            results[\"scikit-learn randomized_svd (n_iter=0)\"].append(time() - tstart)\n            gc.collect()\n            print(\"benchmarking scikit-learn randomized_svd: n_iter=%d \" % n_iter)\n            tstart = time()\n            randomized_svd(X, rank, n_iter=n_iter)\n            results[\"scikit-learn randomized_svd (n_iter=%d)\" % n_iter].append(\n                time() - tstart\n            )\n    return results", "target": "def compute_bench(samples_range, features_range):\n    it = 0\n    results = defaultdict(lambda: [])\n    max_it = len(samples_range) * len(features_range)\n    for n_samples in samples_range:\n        for n_features in features_range:\n            it += 1\n            print(\"====================\")\n            print(\"Iteration %03d of %03d\" % (it, max_it))\n            print(\"====================\")\n            dataset_kwargs = {\n                \"n_samples\": n_samples,\n                \"n_features\": n_features,\n                \"n_informative\": n_features // 10,\n                \"effective_rank\": min(n_samples, n_features) / 10,\n                \"bias\": 0.0,\n            }\n            print(\"n_samples: %d\" % n_samples)\n            print(\"n_features: %d\" % n_features)\n            X, y = make_regression(**dataset_kwargs)\n            gc.collect()\n            print(\"benchmarking lars_path (with Gram):\", end=\"\")\n            sys.stdout.flush()\n            tstart = time()\n            G = np.dot(X.T, X)\n            Xy = np.dot(X.T, y)\n            lars_path_gram(Xy=Xy, Gram=G, n_samples=y.size, method=\"lasso\")\n            delta = time() - tstart\n            print(\"%0.3fs\" % delta)\n            results[\"lars_path (with Gram)\"].append(delta)\n            gc.collect()\n            print(\"benchmarking lars_path (without Gram):\", end=\"\")\n            sys.stdout.flush()\n            tstart = time()\n            lars_path(X, y, method=\"lasso\")\n            delta = time() - tstart\n            print(\"%0.3fs\" % delta)\n            results[\"lars_path (without Gram)\"].append(delta)\n            gc.collect()\n            print(\"benchmarking lasso_path (with Gram):\", end=\"\")\n            sys.stdout.flush()\n            tstart = time()\n            lasso_path(X, y, precompute=True)\n            delta = time() - tstart\n            print(\"%0.3fs\" % delta)\n            results[\"lasso_path (with Gram)\"].append(delta)\n            gc.collect()\n            print(\"benchmarking lasso_path (without Gram):\", end=\"\")\n            sys.stdout.flush()\n            tstart = time()\n            lasso_path(X, y, precompute=False)\n            delta = time() - tstart\n            print(\"%0.3fs\" % delta)\n            results[\"lasso_path (without Gram)\"].append(delta)\n    return results", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "test", "example_id": "002727", "source": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "target": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "002728", "source": "def test_constraints_schema_validation_error(constraint: str) -> None:\n    with pytest.raises(SchemaError, match=f\"'{constraint}' must be coercible to a Decimal instance\"):\n        SchemaValidator(cs.decimal_schema(**{constraint: 'bad_value'}))", "target": "def test_constraints_schema_validation_error(constraint: str) -> None:\n    with pytest.raises(SchemaError, match=f\"'{constraint}' must be coercible to a date instance\"):\n        SchemaValidator(cs.date_schema(**{constraint: 'bad_value'}))", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "002729", "source": "def test_dataclass():\n    schema = core_schema.dataclass_args_schema(\n        'MyDataclass',\n        [\n            core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), kw_only=False),\n            core_schema.dataclass_field(\n                name='b', schema=core_schema.list_schema(core_schema.str_schema(min_length=2)), kw_only=False\n            ),\n        ],\n    )\n    v = SchemaValidator(schema)\n    assert v.validate_python({'a': 'x', 'b': ['ab', 'cd']}) == snapshot(({'a': 'x', 'b': ['ab', 'cd']}, None))\n    assert v.validate_python({'a': 'x', 'b': ['ab', 'cd']}, allow_partial=True) == snapshot(\n        ({'a': 'x', 'b': ['ab', 'cd']}, None)\n    )\n    with pytest.raises(ValidationError, match=r'b\\.1\\s+String should have at least 2 characters'):\n        v.validate_python({'a': 'x', 'b': ['ab', 'c']}, allow_partial=True)", "target": "def test_dataclass():\n    schema = core_schema.dataclass_schema(\n        FooDataclass,\n        core_schema.dataclass_args_schema(\n            'FooDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 'hello', 'b': True})\n    assert dataclasses.is_dataclass(foo)\n    assert foo.a == 'hello'\n    assert foo.b is True\n    assert dataclasses.asdict(v.validate_python(FooDataclass(a='hello', b=True))) == {'a': 'hello', 'b': True}\n    with pytest.raises(ValidationError, match='Input should be an instance of FooDataclass') as exc_info:\n        v.validate_python({'a': 'hello', 'b': True}, strict=True)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'dataclass_exact_type',\n            'loc': (),\n            'msg': 'Input should be an instance of FooDataclass',\n            'input': {'a': 'hello', 'b': True},\n            'ctx': {'class_name': 'FooDataclass'},\n        }\n    ]", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "test", "example_id": "002730", "source": "def test_json_error():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'field_a': core_schema.model_field(\n                    schema=core_schema.list_schema(items_schema=core_schema.int_schema())\n                )\n            }\n        )\n    )\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_json('{\"field_a\": [123, \"wrong\"]}')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('field_a', 1),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'wrong',\n        }\n    ]", "target": "def is_pydantic_dataclass(class_: type[Any], /) -> TypeGuard[type[PydanticDataclass]]:\n    try:\n        return '__is_pydantic_dataclass__' in class_.__dict__ and dataclasses.is_dataclass(class_)\n    except AttributeError:\n        return False", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "test", "example_id": "002731", "source": "def complex_schema(\n    *,\n    strict: bool | None = None,\n    ref: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> ComplexSchema:\n    return _dict_not_none(\n        type='complex',\n        strict=strict,\n        ref=ref,\n        metadata=metadata,\n        serialization=serialization,\n    )", "target": "def test_recursive_function_deeper_ref():\n    s = SchemaSerializer(\n        core_schema.typed_dict_schema(\n            {\n                'a': core_schema.typed_dict_field(\n                    core_schema.definitions_schema(\n                        core_schema.definition_reference_schema('my_ref'),\n                        [\n                            core_schema.typed_dict_schema(\n                                {'b': core_schema.typed_dict_field(core_schema.definition_reference_schema('my_ref'))},\n                                ref='my_ref',\n                            )\n                        ],\n                    )\n                )\n            },\n            serialization=core_schema.wrap_serializer_function_ser_schema(\n                function=lambda x, _handler: x, is_field_serializer=False\n            ),\n        )\n    )\n    assert s.to_python({'a': {'b': {'b': {}}}}) == {'a': {'b': {'b': {}}}}", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|opencv/opencv", "data_split": "test", "example_id": "002732", "source": "def softplus(a):\n    return (a * 1.0).exp().log1p() / 1.0", "target": "def test_array_with_custom_type(self):\n            @cv.gapi.op('custom.op', in_types=[cv.GArray.Any, cv.GArray.Any], out_types=[cv.GArray.Any])\n            class GConcat:\n                @staticmethod\n                def outMeta(arr_desc0, arr_desc1):\n                    return cv.empty_array_desc()\n            @cv.gapi.kernel(GConcat)\n            class GConcatImpl:\n                @staticmethod\n                def run(arr0, arr1):\n                    return arr0 + arr1\n            g_arr0 = cv.GArray.Any()\n            g_arr1 = cv.GArray.Any()\n            g_out  = GConcat.on(g_arr0, g_arr1)\n            comp = cv.GComputation(cv.GIn(g_arr0, g_arr1), cv.GOut(g_out))\n            arr0 = ((2, 2), 2.0)\n            arr1 = (3,    'str')\n            out = comp.apply(cv.gin(arr0, arr1),\n                             args=cv.gapi.compile_args(cv.gapi.kernels(GConcatImpl)))\n            self.assertEqual(arr0 + arr1, out)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "test", "example_id": "002733", "source": "def test_json_none():\n    v = SchemaValidator(cs.none_schema())\n    assert v.validate_json('null') is None\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_json('1')\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'none_required', 'loc': (), 'msg': 'Input should be null', 'input': 1}\n    ]", "target": "def test_exclude_none():\n    v = SchemaSerializer(\n        core_schema.typed_dict_schema(\n            {\n                'foo': core_schema.typed_dict_field(core_schema.nullable_schema(core_schema.int_schema())),\n                'bar': core_schema.typed_dict_field(core_schema.bytes_schema()),\n            },\n            extra_behavior='allow',\n        )\n    )\n    assert v.to_python({'foo': 1, 'bar': b'more', 'c': 3}) == {'foo': 1, 'bar': b'more', 'c': 3}\n    assert v.to_python({'foo': None, 'bar': b'more', 'c': None}) == {'foo': None, 'bar': b'more', 'c': None}\n    assert v.to_python({'foo': None, 'bar': b'more', 'c': None}, exclude_none=True) == {'bar': b'more'}\n    assert v.to_python({'foo': None, 'bar': b'more', 'c': None}, mode='json') == {'foo': None, 'bar': 'more', 'c': None}\n    assert v.to_python({'foo': None, 'bar': b'more', 'c': None}, mode='json', exclude_none=True) == {'bar': 'more'}\n    assert v.to_json({'foo': 1, 'bar': b'more', 'c': None}) == b'{\"foo\":1,\"bar\":\"more\",\"c\":null}'\n    assert v.to_json({'foo': None, 'bar': b'more'}) == b'{\"foo\":null,\"bar\":\"more\"}'\n    assert v.to_json({'foo': None, 'bar': b'more', 'c': None}, exclude_none=True) == b'{\"bar\":\"more\"}'", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|scikit-learn/scikit-learn", "data_split": "test", "example_id": "002734", "source": "def string_or_pathlike_(ctype_name: str = \"string\") -> UnionTypeNode:\n        return UnionTypeNode(\n            ctype_name,\n            items=(\n                PrimitiveTypeNode.str_(ctype_name),\n                PathLikeTypeNode(ctype_name)\n            )\n        )", "target": "def make_estimator(self, params):\n        estimator = HistGradientBoostingClassifier(\n            max_iter=100, max_leaf_nodes=15, early_stopping=False, random_state=0\n        )\n        return estimator", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "test", "example_id": "002735", "source": "def get_limits(dtype):\n    if not is_numeric(dtype):\n        return None, None\n    if np.issubdtype(dtype, np.integer):\n        info = np.iinfo(dtype)\n    else:\n        info = np.finfo(dtype)\n    return info.min, info.max", "target": "def f2(input_value, info):\n        info.context['f2'] = input_value\n        return input_value + f'| context: {info.context}'", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "002736", "source": "def foobar(a: int, b: int, *, c: int):\n        return a, b, c", "target": "def foobar(*args, **kwargs):\n        return args, kwargs", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "002737", "source": "def push(self, config_wrapper: ConfigWrapper | ConfigDict | None):\n        if config_wrapper is None:\n            yield\n            return\n        if not isinstance(config_wrapper, ConfigWrapper):\n            config_wrapper = ConfigWrapper(config_wrapper, check=False)\n        self._config_wrapper_stack.append(config_wrapper)\n        try:\n            yield\n        finally:\n            self._config_wrapper_stack.pop()", "target": "def push(self, typ: type[Any] | TypeAliasType, /) -> Generator[None]:\n        self._types_stack.append(typ)\n        self.__dict__.pop('types_namespace', None)\n        try:\n            yield\n        finally:\n            self._types_stack.pop()\n            self.__dict__.pop('types_namespace', None)", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "test", "example_id": "002738", "source": "def predict(est, data_test, target_test):\n    if args.no_predict:\n        return\n    tic = time()\n    predicted_test = est.predict(data_test)\n    predicted_proba_test = est.predict_proba(data_test)\n    toc = time()\n    roc_auc = roc_auc_score(target_test, predicted_proba_test[:, 1])\n    acc = accuracy_score(target_test, predicted_test)\n    print(f\"predicted in {toc - tic:.3f}s, ROC AUC: {roc_auc:.4f}, ACC: {acc:.4f}\")", "target": "def predict(est, data_test):\n    if args.no_predict:\n        return\n    tic = time()\n    est.predict(data_test)\n    toc = time()\n    print(f\"predicted in {toc - tic:.3f}s\")", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "002739", "source": "def test_alias_build_error(alias_schema, error):\n    with pytest.raises(SchemaError, match=error):\n        SchemaValidator(\n            schema={\n                'type': 'model-fields',\n                'fields': {'field_a': {'type': 'model-field', 'schema': {'type': 'int'}, **alias_schema}},\n            }\n        )", "target": "def test_alias_build_error(alias_schema, error):\n    with pytest.raises(SchemaError, match=error):\n        SchemaValidator(\n            schema={\n                'type': 'typed-dict',\n                'fields': {'field_a': {'type': 'typed-dict-field', 'schema': {'type': 'int'}, **alias_schema}},\n            }\n        )", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "002740", "source": "def _work(self):\n        @torch.compile(\n            backend=self.backend(),\n            fullgraph=True,\n            dynamic=self.is_dynamic(),\n        )\n        def f(a, b):\n            result = a.clone()\n            for i in range(1000):\n                if i % 3 == 0:\n                    result = result + b\n                elif i % 3 == 1:\n                    result = result + 8 * b\n                else:\n                    result = result.sin()\n            return result\n        with fresh_cache():\n            f(self.a, self.b)", "target": "def _work(self):\n        with (\n            fresh_cache(),\n            torch._inductor.config.patch(force_shape_pad=self._force_shape_pad),\n        ):\n            opt_m = torch.compile(backend=self.backend(), dynamic=self.is_dynamic())(\n                self.m.cuda() if self._is_gpu else self.m\n            )\n            opt_m(self.input)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "test", "example_id": "002741", "source": "def print_measurements(prefix, nelem, measurements):\n        measurements = sorted(measurements)\n        local_print(f\"{prefix:8s}:\")\n        for p in [50, 75, 90, 95]:\n            v = np.percentile(measurements, p)\n            local_print(f\"  p{p:02d}:  {v:1.3f}s  {nelem / v:6d}/s\")\n        local_print(\"\\n\")", "target": "def test_wrap_on_error(self, py_and_json: PyAndJson):\n        def wrap_function(input_value, validator, info):\n            try:\n                return validator(input_value)\n            except ValidationError:\n                if isinstance(input_value, list):\n                    return str(len(input_value))\n                else:\n                    return repr(input_value)\n        v = py_and_json(\n            {\n                'type': 'typed-dict',\n                'fields': {\n                    'x': {\n                        'type': 'typed-dict-field',\n                        'schema': {\n                            'type': 'default',\n                            'on_error': 'raise',\n                            'schema': {\n                                'type': 'function-wrap',\n                                'function': {'type': 'with-info', 'function': wrap_function},\n                                'schema': {'type': 'str'},\n                            },\n                        },\n                    }\n                },\n            }\n        )\n        assert v.validate_test({'x': 'foo'}) == {'x': 'foo'}\n        assert v.validate_test({'x': ['foo']}) == {'x': '1'}\n        assert v.validate_test({'x': ['foo', 'bar']}) == {'x': '2'}\n        assert v.validate_test({'x': {'a': 'b'}}) == {'x': \"{'a': 'b'}\"}", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pytorch/pytorch", "data_split": "test", "example_id": "002742", "source": "def test_simple(self):\n        images = [\n            self.get_sample('stitching/a1.png'),\n            self.get_sample('stitching/a2.png'),\n            self.get_sample('stitching/a3.png')\n        ]\n        images = [cv.resize(img, [100, 100]) for img in images]\n        finder = cv.detail_GraphCutSeamFinder('COST_COLOR_GRAD')\n        masks = [cv.UMat(255 * np.ones((img.shape[0], img.shape[1]), np.uint8)) for img in images]\n        images_f = [img.astype(np.float32) for img in images]\n        masks_warped = finder.find(images_f, [(0, 0), (75, 0), (150, 0)], masks)\n        self.assertIsNotNone(masks_warped)", "target": "def powspace(start, stop, pow, step):\n    start = math.log(start, pow)\n    stop = math.log(stop, pow)\n    steps = int((stop - start + 1) // step)\n    ret = torch.pow(pow, torch.linspace(start, stop, steps))\n    ret = torch.unique(ret)\n    return list(map(int, ret))", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|pytorch/pytorch", "data_split": "test", "example_id": "002743", "source": "def skip(self, params):\n        representation, precompute = params\n        if representation == \"sparse\" and precompute is False:\n            return True\n        return False", "target": "def name(self):\n        prefix = f\"{self.category()}_{self.backend()}\"\n        if self.is_dynamic():\n            prefix += \"_dynamic\"\n        if self.device() == \"cuda\":\n            prefix += \"_gpu\"\n        return prefix", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "test", "example_id": "002744", "source": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "target": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "test", "example_id": "002745", "source": "def process(self, frameworks, data_fetcher):\n        sorted_imgs_names = sorted(self.img_classes.keys())\n        correct_answers = [0] * len(frameworks)\n        samples_handled = 0\n        blobs_l1_diff = [0] * len(frameworks)\n        blobs_l1_diff_count = [0] * len(frameworks)\n        blobs_l_inf_diff = [sys.float_info.min] * len(frameworks)\n        inference_time = [0.0] * len(frameworks)\n        for x in xrange(0, len(sorted_imgs_names), self.batch_size):\n            sublist = sorted_imgs_names[x:x + self.batch_size]\n            batch = data_fetcher.get_batch(sublist)\n            samples_handled += len(sublist)\n            frameworks_out = []\n            fw_accuracy = []\n            for i in range(len(frameworks)):\n                start = time.time()\n                out = frameworks[i].get_output(batch)\n                end = time.time()\n                correct_answers[i] += get_correct_answers(sublist, self.img_classes, out)\n                fw_accuracy.append(100 * correct_answers[i] / float(samples_handled))\n                frameworks_out.append(out)\n                inference_time[i] += end - start\n                print(samples_handled, 'Accuracy for', frameworks[i].get_name() + ':', fw_accuracy[i], file=self.log)\n                print(\"Inference time, ms \", \\\n                    frameworks[i].get_name(), inference_time[i] / samples_handled * 1000, file=self.log)\n            for i in range(1, len(frameworks)):\n                log_str = frameworks[0].get_name() + \" vs \" + frameworks[i].get_name() + ':'\n                diff = np.abs(frameworks_out[0] - frameworks_out[i])\n                l1_diff = np.sum(diff) / diff.size\n                print(samples_handled, \"L1 difference\", log_str, l1_diff, file=self.log)\n                blobs_l1_diff[i] += l1_diff\n                blobs_l1_diff_count[i] += 1\n                if np.max(diff) > blobs_l_inf_diff[i]:\n                    blobs_l_inf_diff[i] = np.max(diff)\n                print(samples_handled, \"L_INF difference\", log_str, blobs_l_inf_diff[i], file=self.log)\n            self.log.flush()\n        for i in range(1, len(blobs_l1_diff)):\n            log_str = frameworks[0].get_name() + \" vs \" + frameworks[i].get_name() + ':'\n            print('Final l1 diff', log_str, blobs_l1_diff[i] / blobs_l1_diff_count[i], file=self.log)", "target": "def process(self, frameworks, data_fetcher):\n        samples_handled = 0\n        conf_mats = [np.zeros((data_fetcher.get_num_classes(), data_fetcher.get_num_classes())) for i in range(len(frameworks))]\n        blobs_l1_diff = [0] * len(frameworks)\n        blobs_l1_diff_count = [0] * len(frameworks)\n        blobs_l_inf_diff = [sys.float_info.min] * len(frameworks)\n        inference_time = [0.0] * len(frameworks)\n        for in_blob_dict, gt in data_fetcher:\n            frameworks_out = []\n            samples_handled += 1\n            for i in range(len(frameworks)):\n                start = time.time()\n                framework_name = frameworks[i].get_name()\n                out = frameworks[i].get_output(in_blob_dict[framework_name])\n                end = time.time()\n                segm = eval_segm_result(out)\n                conf_mats[i] += get_conf_mat(gt, segm[0])\n                frameworks_out.append(out)\n                inference_time[i] += end - start\n                pix_acc, mean_acc, miou = get_metrics(conf_mats[i])\n                name = frameworks[i].get_name()\n                print(samples_handled, 'Pixel accuracy, %s:' % name, 100 * pix_acc, file=self.log)\n                print(samples_handled, 'Mean accuracy, %s:' % name, 100 * mean_acc, file=self.log)\n                print(samples_handled, 'Mean IOU, %s:' % name, 100 * miou, file=self.log)\n                print(\"Inference time, ms \", \\\n                    frameworks[i].get_name(), inference_time[i] / samples_handled * 1000, file=self.log)\n            for i in range(1, len(frameworks)):\n                log_str = frameworks[0].get_name() + \" vs \" + frameworks[i].get_name() + ':'\n                diff = np.abs(frameworks_out[0] - frameworks_out[i])\n                l1_diff = np.sum(diff) / diff.size\n                print(samples_handled, \"L1 difference\", log_str, l1_diff, file=self.log)\n                blobs_l1_diff[i] += l1_diff\n                blobs_l1_diff_count[i] += 1\n                if np.max(diff) > blobs_l_inf_diff[i]:\n                    blobs_l_inf_diff[i] = np.max(diff)\n                print(samples_handled, \"L_INF difference\", log_str, blobs_l_inf_diff[i], file=self.log)\n            self.log.flush()\n        for i in range(1, len(blobs_l1_diff)):\n            log_str = frameworks[0].get_name() + \" vs \" + frameworks[i].get_name() + ':'\n            print('Final l1 diff', log_str, blobs_l1_diff[i] / blobs_l1_diff_count[i], file=self.log)", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pytorch/pytorch", "data_split": "test", "example_id": "002746", "source": "def create_python_fn_description(soup, variants):\n    language = 'Python:'\n    table = soup.new_tag('table')\n    heading_row = soup.new_tag('th')\n    table.append(\n        append(soup.new_tag('tr'),\n               append(soup.new_tag('th', colspan=999, style=\"text-align:left\"), language)))\n    for v in variants:\n        add_signature_to_table(soup, table, v, language, type)\n    return table", "target": "def test_env_path_returns_path_when_present(self):\n        tmp = Path(\"./b\").resolve()\n        with patch.dict(os.environ, {\"P\": str(tmp)}, clear=True):\n            p = m.env_path(\"P\", None, resolve=True)\n            self.assertEqual(p, tmp)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "test", "example_id": "002747", "source": "def make_gaussians(cluster_n, img_size):\n    points = []\n    ref_distrs = []\n    for _ in xrange(cluster_n):\n        mean = (0.1 + 0.8*random.rand(2)) * img_size\n        a = (random.rand(2, 2)-0.5)*img_size*0.1\n        cov = np.dot(a.T, a) + img_size*0.05*np.eye(2)\n        n = 100 + random.randint(900)\n        pts = random.multivariate_normal(mean, cov, n)\n        points.append( pts )\n        ref_distrs.append( (mean, cov) )\n    points = np.float32( np.vstack(points) )\n    return points, ref_distrs", "target": "def make_gaussians(cluster_n, img_size):\n    points = []\n    ref_distrs = []\n    sizes = []\n    for _ in xrange(cluster_n):\n        mean = (0.1 + 0.8*random.rand(2)) * img_size\n        a = (random.rand(2, 2)-0.5)*img_size*0.1\n        cov = np.dot(a.T, a) + img_size*0.05*np.eye(2)\n        n = 100 + random.randint(900)\n        pts = random.multivariate_normal(mean, cov, n)\n        points.append( pts )\n        ref_distrs.append( (mean, cov) )\n        sizes.append(n)\n    points = np.float32( np.vstack(points) )\n    return points, ref_distrs, sizes", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "test", "example_id": "002748", "source": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "target": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|opencv/opencv", "data_split": "test", "example_id": "002749", "source": "def _inner(fn):\n        if \"pre_grad_fusion_options\" in optimus_inductor_config:\n            torch._inductor.config.pre_grad_fusion_options = optimus_inductor_config[\n                \"pre_grad_fusion_options\"\n            ]\n        if \"post_grad_fusion_options\" in optimus_inductor_config:\n            torch._inductor.config.post_grad_fusion_options = optimus_inductor_config[\n                \"post_grad_fusion_options\"\n            ]\n        return torch.compile(\n            fn, backend=\"inductor\", fullgraph=nopython, mode=inductor_compile_mode\n        )", "target": "def functions(self) -> Dict[str, FunctionNode]:\n        return self._children[ASTNodeType.Function]", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "test", "example_id": "002750", "source": "def test_frozenset_no_validators_both(py_and_json: PyAndJson, input_value, expected):\n    v = py_and_json({'type': 'set'})\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=expected.message):\n            v.validate_test(input_value)\n    else:\n        assert v.validate_test(input_value) == expected", "target": "def test_dataclass_subclass(revalidate_instances, input_value, expected):\n    schema = core_schema.dataclass_schema(\n        FooDataclass,\n        core_schema.dataclass_args_schema(\n            'FooDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n            ],\n            extra_behavior='forbid',\n        ),\n        ['a', 'b'],\n        revalidate_instances=revalidate_instances,\n    )\n    v = SchemaValidator(schema)\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=expected.message) as exc_info:\n            print(v.validate_python(input_value))\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        dc = v.validate_python(input_value)\n        assert dataclasses.is_dataclass(dc)\n        assert dataclasses.asdict(dc) == expected", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|scikit-learn/scikit-learn", "data_split": "test", "example_id": "002751", "source": "def test_success_runs_all_steps_and_uses_env_and_workdir(monkeypatch, patch_module):\n    run_test_plan = patch_module.run_test_plan\n    tests_map = {\n        \"basic\": {\n            \"title\": \"Basic suite\",\n            \"package_install\": [],\n            \"working_directory\": \"tests\",\n            \"env_vars\": {\"GLOBAL_FLAG\": \"1\"},\n            \"steps\": [\n                \"export A=x && pytest -q\",\n                \"export B=y && pytest -q tests/unit\",\n            ],\n        }\n    }\n    patch_module.run_command.side_effect = [0, 0, 0]\n    run_test_plan(\"basic\", \"cpu\", tests_map)\n    calls = patch_module.run_command.call_args_list\n    cmds = [_get_cmd(c) for c in calls]\n    checks = [_get_check(c) for c in calls]\n    assert cmds == [\n        \"export A=x && pytest -q\",\n        \"export B=y && pytest -q tests/unit\",\n    ]\n    assert all(chk is False for chk in checks)\n    assert patch_module.workdir_calls == [\"tests\"]\n    assert patch_module.temp_calls == [{\"GLOBAL_FLAG\": \"1\"}]", "target": "def compute_bench(samples_range, features_range):\n    it = 0\n    results = dict()\n    lars = np.empty((len(features_range), len(samples_range)))\n    lars_gram = lars.copy()\n    omp = lars.copy()\n    omp_gram = lars.copy()\n    max_it = len(samples_range) * len(features_range)\n    for i_s, n_samples in enumerate(samples_range):\n        for i_f, n_features in enumerate(features_range):\n            it += 1\n            n_informative = n_features // 10\n            print(\"====================\")\n            print(\"Iteration %03d of %03d\" % (it, max_it))\n            print(\"====================\")\n            dataset_kwargs = {\n                \"n_samples\": 1,\n                \"n_components\": n_features,\n                \"n_features\": n_samples,\n                \"n_nonzero_coefs\": n_informative,\n                \"random_state\": 0,\n            }\n            print(\"n_samples: %d\" % n_samples)\n            print(\"n_features: %d\" % n_features)\n            y, X, _ = make_sparse_coded_signal(**dataset_kwargs)\n            X = np.asfortranarray(X.T)\n            gc.collect()\n            print(\"benchmarking lars_path (with Gram):\", end=\"\")\n            sys.stdout.flush()\n            tstart = time()\n            G = np.dot(X.T, X)\n            Xy = np.dot(X.T, y)\n            lars_path_gram(Xy=Xy, Gram=G, n_samples=y.size, max_iter=n_informative)\n            delta = time() - tstart\n            print(\"%0.3fs\" % delta)\n            lars_gram[i_f, i_s] = delta\n            gc.collect()\n            print(\"benchmarking lars_path (without Gram):\", end=\"\")\n            sys.stdout.flush()\n            tstart = time()\n            lars_path(X, y, Gram=None, max_iter=n_informative)\n            delta = time() - tstart\n            print(\"%0.3fs\" % delta)\n            lars[i_f, i_s] = delta\n            gc.collect()\n            print(\"benchmarking orthogonal_mp (with Gram):\", end=\"\")\n            sys.stdout.flush()\n            tstart = time()\n            orthogonal_mp(X, y, precompute=True, n_nonzero_coefs=n_informative)\n            delta = time() - tstart\n            print(\"%0.3fs\" % delta)\n            omp_gram[i_f, i_s] = delta\n            gc.collect()\n            print(\"benchmarking orthogonal_mp (without Gram):\", end=\"\")\n            sys.stdout.flush()\n            tstart = time()\n            orthogonal_mp(X, y, precompute=False, n_nonzero_coefs=n_informative)\n            delta = time() - tstart\n            print(\"%0.3fs\" % delta)\n            omp[i_f, i_s] = delta\n    results[\"time(LARS) / time(OMP)\\n (w/ Gram)\"] = lars_gram / omp_gram\n    results[\"time(LARS) / time(OMP)\\n (w/o Gram)\"] = lars / omp\n    return results", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "test", "example_id": "002752", "source": "def getToolchain(self, arch, target):\n        return None", "target": "def input_data_lax():\n    return {\n        'field_str': 'fo',\n        'field_str_con': 'fooba',\n        'field_int': 1,\n        'field_int_con': 8,\n        'field_float': 1.0,\n        'field_float_con': 10.0,\n        'field_decimal': 42.0,\n        'field_bool': True,\n        'field_bytes': b'foobar',\n        'field_bytes_con': b'foobar',\n        'field_date': '2010-02-03',\n        'field_date_con': '2020-01-01',\n        'field_time': '12:00:00',\n        'field_time_con': '12:00:00',\n        'field_datetime': '2020-01-01T12:13:14',\n        'field_datetime_con': '2020-01-01T00:00:00',\n        'field_uuid': '12345678-1234-5678-1234-567812345678',\n        'field_list_any': ['a', b'b', True, 1.0, None] * 10,\n        'field_list_str': ['a', 'b', 'c'] * 10,\n        'field_list_str_con': ['a', 'b', 'c'] * 10,\n        'field_set_any': {'a', b'b', True, 1.0, None},\n        'field_set_int': set(range(100)),\n        'field_set_int_con': set(range(42)),\n        'field_frozenset_any': frozenset({'a', b'b', True, 1.0, None}),\n        'field_frozenset_bytes': frozenset([f'{i}'.encode() for i in range(100)]),\n        'field_frozenset_bytes_con': frozenset([f'{i}'.encode() for i in range(42)]),\n        'field_tuple_var_len_any': ('a', b'b', True, 1.0, None),\n        'field_tuple_var_len_float': tuple(i + 0.5 for i in range(100)),\n        'field_tuple_var_len_float_con': tuple(i + 0.5 for i in range(42)),\n        'field_tuple_fix_len': ('a', 1, 1.0, True),\n        'field_dict_any': {'a': 'b', 1: True, 1.0: 1.0},\n        'field_dict_str_float': {f'{i}': i + 0.5 for i in range(100)},\n        'field_literal_1_int': 1,\n        'field_literal_1_str': 'foobar',\n        'field_literal_mult_int': 3,\n        'field_literal_mult_str': 'foo',\n        'field_literal_assorted': 'foo',\n        'field_list_nullable_int': [1, None, 2, None, 3, None, 4, None],\n        'field_union': {'field_str': 'foo', 'field_int': 1, 'field_float': 1.0},\n        'field_functions_model': {\n            'field_before': 'foo',\n            'field_after': 'foo',\n            'field_wrap': 'foo',\n            'field_plain': 'foo',\n        },\n        'field_recursive': {\n            'name': 'foo',\n            'sub_branch': {'name': 'bar', 'sub_branch': {'name': 'baz', 'sub_branch': None}},\n        },\n    }", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "test", "example_id": "002753", "source": "def test_by_alias_and_name_config_interaction(\n    config_by_alias: Union[bool, None],\n    config_by_name: Union[bool, None],\n    runtime_by_alias: Union[bool, None],\n    runtime_by_name: Union[bool, None],\n) -> None:\n    if config_by_alias is False and config_by_name is False and runtime_by_alias is False and runtime_by_name is False:\n        pytest.skip(\"Can't have both by_alias and by_name as effectively False\")\n    schema = core_schema.arguments_schema(\n        arguments=[\n            core_schema.arguments_parameter(name='my_field', schema=core_schema.int_schema(), alias='my_alias'),\n        ],\n        **({'validate_by_alias': config_by_alias} if config_by_alias is not None else {}),\n        **({'validate_by_name': config_by_name} if config_by_name is not None else {}),\n    )\n    s = SchemaValidator(schema)\n    alias_allowed = next(x for x in (runtime_by_alias, config_by_alias, True) if x is not None)\n    name_allowed = next(x for x in (runtime_by_name, config_by_name, False) if x is not None)\n    if alias_allowed:\n        assert s.validate_python(\n            ArgsKwargs((), {'my_alias': 1}), by_alias=runtime_by_alias, by_name=runtime_by_name\n        ) == (\n            (),\n            {'my_field': 1},\n        )\n    if name_allowed:\n        assert s.validate_python(\n            ArgsKwargs((), {'my_field': 1}), by_alias=runtime_by_alias, by_name=runtime_by_name\n        ) == (\n            (),\n            {'my_field': 1},\n        )", "target": "def test_by_alias_and_name_config_interaction(\n    config_by_alias: Union[bool, None],\n    config_by_name: Union[bool, None],\n    runtime_by_alias: Union[bool, None],\n    runtime_by_name: Union[bool, None],\n) -> None:\n    if config_by_alias is False and config_by_name is False and runtime_by_alias is False and runtime_by_name is False:\n        pytest.skip(\"Can't have both by_alias and by_name as effectively False\")\n    core_config = {\n        **({'validate_by_alias': config_by_alias} if config_by_alias is not None else {}),\n        **({'validate_by_name': config_by_name} if config_by_name is not None else {}),\n    }\n    @dataclasses.dataclass\n    class MyDataclass:\n        my_field: int\n    schema = core_schema.dataclass_schema(\n        MyDataclass,\n        core_schema.dataclass_args_schema(\n            'MyDataclass',\n            [\n                core_schema.dataclass_field(\n                    name='my_field', schema=core_schema.int_schema(), validation_alias='my_alias'\n                ),\n            ],\n        ),\n        ['my_field'],\n        config=core_schema.CoreConfig(**core_config),\n    )\n    s = SchemaValidator(schema)\n    alias_allowed = next(x for x in (runtime_by_alias, config_by_alias, True) if x is not None)\n    name_allowed = next(x for x in (runtime_by_name, config_by_name, False) if x is not None)\n    if alias_allowed:\n        assert dataclasses.asdict(\n            s.validate_python({'my_alias': 1}, by_alias=runtime_by_alias, by_name=runtime_by_name)\n        ) == {'my_field': 1}\n    if name_allowed:\n        assert dataclasses.asdict(\n            s.validate_python({'my_field': 1}, by_alias=runtime_by_alias, by_name=runtime_by_name)\n        ) == {'my_field': 1}", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pytorch/pytorch", "data_split": "test", "example_id": "002754", "source": "def typename(self) -> str:\n        if self._export_name is not None:\n            return self._export_name\n        return self.ctype_name", "target": "def read_triton_pin(device: str = \"cuda\") -> str:\n    triton_file = \"triton.txt\"\n    if device == \"xpu\":\n        triton_file = \"triton-xpu.txt\"\n    with open(REPO_DIR / \".ci\" / \"docker\" / \"ci_commit_pins\" / triton_file) as f:\n        return f.read().strip()", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "test", "example_id": "002755", "source": "def make_estimator(self, params):\n        representation, solver, n_jobs = params\n        penalty = \"l2\" if solver == \"lbfgs\" else \"l1\"\n        estimator = LogisticRegression(\n            solver=solver,\n            penalty=penalty,\n            tol=0.01,\n            n_jobs=n_jobs,\n            random_state=0,\n        )\n        return estimator", "target": "def make_estimator(self, params):\n        (kernel,) = params\n        estimator = SVC(\n            max_iter=100, tol=1e-16, kernel=kernel, random_state=0, gamma=\"scale\"\n        )\n        return estimator", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|opencv/opencv", "data_split": "test", "example_id": "002756", "source": "def contiguousChannelsLast(self, x: Tensor) -> Tensor:\n        return x.contiguous(memory_format=torch.channels_last)", "target": "def test_custom_op_addC(self):\n            sz = (3, 3, 3)\n            in_mat = np.full(sz, 45, dtype=np.uint8)\n            sc = (50, 10, 20)\n            expected = in_mat + np.array(sc, dtype=np.uint8)\n            g_in  = cv.GMat()\n            g_sc  = cv.GScalar()\n            g_out = GAddC.on(g_in, g_sc, cv.CV_8UC1)\n            comp  = cv.GComputation(cv.GIn(g_in, g_sc), cv.GOut(g_out))\n            pkg = cv.gapi.kernels(GAddCImpl)\n            actual = comp.apply(cv.gin(in_mat, sc), args=cv.gapi.compile_args(pkg))\n            self.assertEqual(0.0, cv.norm(expected, actual, cv.NORM_INF))", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pytorch/pytorch", "data_split": "test", "example_id": "002757", "source": "def test_registry(self):\n        self.check_name(cv.videoio_registry.getBackendName(cv.CAP_ANY));\n        self.check_name(cv.videoio_registry.getBackendName(cv.CAP_FFMPEG))\n        self.check_name(cv.videoio_registry.getBackendName(cv.CAP_OPENCV_MJPEG))\n        backends = cv.videoio_registry.getBackends()\n        for backend in backends:\n            self.check_name(cv.videoio_registry.getBackendName(backend))", "target": "def check_accuracy(actual_csv, expected_csv, expected_filename):\n    failed = []\n    improved = []\n    if \"rocm\" in expected_filename:\n        flaky_models.update(\n            {\n                \"Background_Matting\",\n                \"alexnet\",\n                \"demucs\",\n                \"densenet121\",\n                \"detectron2_fcos_r_50_fpn\",\n                \"doctr_det_predictor\",\n                \"doctr_reco_predictor\",\n                \"dpn107\",\n                \"fbnetv3_b\",\n                \"levit_128\",\n                \"llava\",\n                \"microbench_unbacked_tolist_sum\",\n                \"mnasnet1_0\",\n                \"mobilenet_v2\",\n                \"pytorch_CycleGAN_and_pix2pix\",\n                \"pytorch_stargan\",\n                \"repvgg_a2\",\n                \"resnet152\",\n                \"resnet18\",\n                \"resnet50\",\n                \"resnext50_32x4d\",\n                \"sam\",\n                \"sam_fast\",\n                \"shufflenet_v2_x1_0\",\n                \"squeezenet1_1\",\n                \"stable_diffusion_text_encoder\",\n                \"stable_diffusion_unet\",\n                \"swsl_resnext101_32x16d\",\n                \"torchrec_dlrm\",\n                \"vgg16\",\n                \"BERT_pytorch\",\n                \"coat_lite_mini\",\n                \"mobilenet_v3_large\",\n                \"vision_maskrcnn\",\n                \"meta-llama/Llama-3.2-1B\",\n                \"google/gemma-2-2b\",\n                \"google/gemma-3-4b-it\",\n                \"openai/whisper-tiny\",\n                \"Qwen/Qwen3-0.6B\",\n                \"mistralai/Mistral-7B-Instruct-v0.3\",\n                \"openai/gpt-oss-20b\",\n            }\n        )\n    for model in actual_csv[\"name\"]:\n        accuracy = get_field(actual_csv, model, \"accuracy\")\n        expected_accuracy = get_field(expected_csv, model, \"accuracy\")\n        if accuracy == expected_accuracy:\n            status = \"PASS\" if expected_accuracy == \"pass\" else \"XFAIL\"\n            print(f\"{model:34}  {status}\")\n            continue\n        elif model in flaky_models:\n            if accuracy == \"pass\":\n                status = \"PASS_BUT_FLAKY:\"\n            else:\n                status = \"FAIL_BUT_FLAKY:\"\n        elif accuracy != \"pass\":\n            status = \"FAIL:\"\n            failed.append(model)\n        else:\n            status = \"IMPROVED:\"\n            improved.append(model)\n        print(\n            f\"{model:34}  {status:9} accuracy={accuracy}, expected={expected_accuracy}\"\n        )\n    msg = \"\"\n    if failed or improved:\n        if failed:\n            msg += textwrap.dedent(\n                f\n            )\n        if improved:\n            msg += textwrap.dedent(\n                f\n            )\n        sha = os.getenv(\"SHA1\", \"{your CI commit sha}\")\n        msg += textwrap.dedent(\n            f\n        )\n    return failed or improved, msg", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "test", "example_id": "002758", "source": "def hessian_revrev(model, inp, strict=None):\n        return functional.hessian(model, inp, strict=False, vectorize=True)", "target": "def hessian_revrev(model, inp, v=None, strict=None):\n        argnums = tuple(range(len(inp)))\n        return ft.jacrev(ft.jacrev(model, argnums=argnums), argnums=argnums)(*inp)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "test", "example_id": "002759", "source": "def test_dataclass_self_init_post_init():\n    calls = []\n    @dataclasses.dataclass(init=False)\n    class Foo:\n        a: str\n        b: bool\n        c: dataclasses.InitVar[int]\n        def __init__(self, *args, **kwargs):\n            v.validate_python(ArgsKwargs(args, kwargs), self_instance=self)\n        def __post_init__(self, c):\n            calls.append(c)\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), kw_only=False),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), kw_only=False),\n                core_schema.dataclass_field(name='c', schema=core_schema.int_schema(), init_only=True),\n            ],\n            collect_init_only=True,\n        ),\n        ['a', 'b', 'c'],\n        post_init=True,\n    )\n    v = SchemaValidator(schema)\n    foo = Foo(b'hello', 'True', c='123')\n    assert dataclasses.is_dataclass(foo)\n    assert dataclasses.asdict(foo) == {'a': 'hello', 'b': True}\n    assert calls == [123]", "target": "def main():\n    args = parse_args()\n    train_dataloader, eval_dataloader = data_processing(\n        args.num_samples, args.batch_size\n    )\n    model = AutoModelForSequenceClassification.from_pretrained(\n        \"bert-base-cased\", num_labels=5\n    )\n    optimizer_cls = getattr(sys.modules[\"torch.optim\"], args.optimizer)\n    if \"capturable\" in inspect.signature(optimizer_cls).parameters.keys():\n        optimizer = optimizer_cls(model.parameters(), lr=args.lr, capturable=True)\n    else:\n        optimizer = optimizer_cls(model.parameters(), lr=args.lr)\n    native_start = time.time()\n    ref_loss, accuracy = model_training_evaluation(\n        None,\n        train_dataloader,\n        eval_dataloader,\n        model,\n        optimizer,\n        args.epochs,\n        args.evaluation,\n    )\n    native_end = time.time()\n    res_loss, accuracy = model_training_evaluation(\n        args.backend,\n        train_dataloader,\n        eval_dataloader,\n        model,\n        optimizer,\n        args.epochs,\n        args.evaluation,\n    )\n    dynamo_end = time.time()\n    if check_loss(ref_loss, res_loss):\n        print(\n            \"[PASSED] TorchDynamo end to end training loss is less than or equal to native PyTorch\"\n        )\n    else:\n        print(\n            \"[FAILED] TorchDynamo end to end training loss is greater than native Pytorch\"\n        )\n    if args.evaluation:\n        print(f\"Model accuracy: {accuracy}\")\n    native_elapsed = native_end - native_start\n    dynamo_elapsed = dynamo_end - native_end\n    print(\n        f\"Train model on {args.epochs} epochs with backend {args.backend} and optimizer {args.optimizer}:\"\n    )\n    print(f\"PyTorch spent {timedelta(seconds=native_elapsed / args.epochs)} per epoch\")\n    print(\n        f\"TorchDynamo spent {timedelta(seconds=dynamo_elapsed / args.epochs)} per epoch\"\n    )", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "test", "example_id": "002760", "source": "def types_separator(self) -> str:\n        if TypeNode.compatible_to_runtime_usage:\n            return \", \"\n        return \" | \"", "target": "def types_separator(self) -> str:\n        return \", \"", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "test", "example_id": "002761", "source": "def deep_update(mapping: dict[KeyType, Any], *updating_mappings: dict[KeyType, Any]) -> dict[KeyType, Any]:\n    updated_mapping = mapping.copy()\n    for updating_mapping in updating_mappings:\n        for k, v in updating_mapping.items():\n            if k in updated_mapping and isinstance(updated_mapping[k], dict) and isinstance(v, dict):\n                updated_mapping[k] = deep_update(updated_mapping[k], v)\n            else:\n                updated_mapping[k] = v\n    return updated_mapping", "target": "def scaleMask(self, mask):\n        return np.where((mask==cv.GC_FGD) + (mask==cv.GC_PR_FGD),255,0).astype('uint8')", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|opencv/opencv", "data_split": "test", "example_id": "002762", "source": "def name(self):\n        prefix = f\"{self.category()}_{self._name}_{self.backend()}\"\n        return prefix", "target": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "test", "example_id": "002763", "source": "def test_charuco_detector(self):\n        aruco_dict = cv.aruco.getPredefinedDictionary(cv.aruco.DICT_4X4_250)\n        board_size = (3, 3)\n        board = cv.aruco.CharucoBoard(board_size, 1.0, .8, aruco_dict)\n        charuco_detector = cv.aruco.CharucoDetector(board)\n        cell_size = 100\n        image = board.generateImage((cell_size*board_size[0], cell_size*board_size[1]))\n        list_gold_corners = []\n        for i in range(1, board_size[0]):\n            for j in range(1, board_size[1]):\n                list_gold_corners.append((j*cell_size, i*cell_size))\n        gold_corners = np.array(list_gold_corners, dtype=np.float32)\n        charucoCorners, charucoIds, markerCorners, markerIds = charuco_detector.detectBoard(image)\n        self.assertEqual(len(charucoIds), 4)\n        for i in range(0, 4):\n            self.assertEqual(charucoIds[i], i)\n        np.testing.assert_allclose(gold_corners, charucoCorners.reshape(-1, 2), 0.01, 0.1)", "target": "def append_args(value, info):\n        return f'{value} info={info}'", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "test", "example_id": "002764", "source": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "target": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "test", "example_id": "002765", "source": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "target": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|opencv/opencv", "data_split": "test", "example_id": "002766", "source": "def test_get_env_empty_returns_default(self):\n        with patch.dict(os.environ, {\"FOO\": \"\"}, clear=True):\n            self.assertEqual(m.get_env(\"FOO\", \"default\"), \"default\")", "target": "def full_typename(self) -> str:\n        return self.type_format.format(self.types_separator.join(\n            item.full_typename for item in self\n        ))", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "test", "example_id": "002767", "source": "def test_typed_dict_allow_extra():\n    v = SchemaSerializer(\n        core_schema.typed_dict_schema(\n            {\n                'foo': core_schema.typed_dict_field(core_schema.int_schema()),\n                'bar': core_schema.typed_dict_field(core_schema.bytes_schema()),\n            },\n            extra_behavior='allow',\n        )\n    )\n    assert v.to_python({'bar': b'more', 'b': 3, 'foo': 1, 'a': 4}) == IsStrictDict(bar=b'more', b=3, foo=1, a=4)\n    assert v.to_python({'bar': b'more', 'c': 3, 'foo': 1}, mode='json') == IsStrictDict(bar='more', c=3, foo=1)\n    assert v.to_json({'bar': b'more', 'c': 3, 'foo': 1, 'cc': 4}) == b'{\"bar\":\"more\",\"c\":3,\"foo\":1,\"cc\":4}'", "target": "def test_stateful_multiple_inputs(self):\n            @cv.gapi.kernel(GStatefulSum)\n            class GStatefulSumImpl:\n                @staticmethod\n                def setup(lhs_desc, rhs_desc):\n                    return SumState()\n                @staticmethod\n                def run(lhs, rhs, state):\n                    state.sum+= lhs + rhs\n                    return state.sum\n            g_in1 = cv.GOpaque.Int()\n            g_in2 = cv.GOpaque.Int()\n            g_out = GStatefulSum.on(g_in1, g_in2)\n            comp = cv.GComputation(cv.GIn(g_in1, g_in2), cv.GOut(g_out))\n            pkg  = cv.gapi.kernels(GStatefulSumImpl)\n            lhs_list = [1, 10, 15]\n            rhs_list = [2, 14, 32]\n            ref_out = 0\n            for lhs, rhs in zip(lhs_list, rhs_list):\n                ref_out += lhs + rhs\n                gapi_out = comp.apply(cv.gin(lhs, rhs), cv.gapi.compile_args(pkg))\n                self.assertEqual(ref_out, gapi_out)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "test", "example_id": "002768", "source": "def test_isinstance():\n    v = SchemaValidator(cs.int_schema())\n    assert v.validate_python(123) == 123\n    assert v.isinstance_python(123) is True\n    assert v.validate_python('123') == 123\n    assert v.isinstance_python('123') is True\n    with pytest.raises(ValidationError, match='Input should be a valid integer'):\n        v.validate_python('foo')\n    assert v.isinstance_python('foo') is False", "target": "def test_float_repr():\n    v = SchemaValidator(cs.float_schema())\n    assert (\n        plain_repr(v)\n        == 'SchemaValidator(title=\"float\",validator=Float(FloatValidator{strict:false,allow_inf_nan:true}),definitions=[],cache_strings=True)'\n    )\n    v = SchemaValidator(cs.float_schema(strict=True))\n    assert (\n        plain_repr(v)\n        == 'SchemaValidator(title=\"float\",validator=Float(FloatValidator{strict:true,allow_inf_nan:true}),definitions=[],cache_strings=True)'\n    )\n    v = SchemaValidator(cs.float_schema(multiple_of=7))\n    assert plain_repr(v).startswith('SchemaValidator(title=\"constrained-float\",validator=ConstrainedFloat(')", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|opencv/opencv", "data_split": "test", "example_id": "002769", "source": "def check_version(package: str) -> None:\n    release_version = os.getenv(\"RELEASE_VERSION\")\n    if release_version:\n        release_matrix = read_release_matrix()\n        stable_version = release_matrix[\"torch\"]\n    else:\n        stable_version = os.getenv(\"MATRIX_STABLE_VERSION\")\n    if channel == \"nightly\":\n        check_nightly_binaries_date(package)\n    elif stable_version is not None:\n        if not torch.__version__.startswith(stable_version):\n            raise RuntimeError(\n                f\"Torch version mismatch, expected {stable_version} for channel {channel}. But its {torch.__version__}\"\n            )\n        if release_version and package == \"all\":\n            for module in MODULES:\n                imported_module = importlib.import_module(module[\"name\"])\n                module_version = imported_module.__version__\n                if not module_version.startswith(release_matrix[module[\"name\"]]):\n                    raise RuntimeError(\n                        f\"{module['name']} version mismatch, expected: \\\n                            {release_matrix[module['name']]} for channel {channel}. But its {module_version}\"\n                    )\n                else:\n                    print(\n                        f\"{module['name']} version actual: {module_version} expected: \\\n                        {release_matrix[module['name']]} for channel {channel}.\"\n                    )\n    else:\n        print(f\"Skip version check for channel {channel} as stable version is None\")", "target": "def getOSIdentifier(self):\n        return \"Android\" + self.run([\"shell\", \"getprop ro.build.version.release\"], silent=True).strip()", "label": 0}
