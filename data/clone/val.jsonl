{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "validation", "example_id": "002216", "source": "def compile_pattern(pattern: PatternType) -> re.Pattern[PatternType]:\n    try:\n        return re.compile(pattern)\n    except re.error:\n        raise PydanticCustomError('pattern_regex', 'Input should be a valid regular expression')", "target": "def test_nested_typed_dict_field_serializers():\n    class Model(TypedDict):\n        x: Any\n    class OuterModel(TypedDict):\n        model: Model\n    schema = core_schema.typed_dict_schema(\n        {\n            'x': core_schema.typed_dict_field(\n                core_schema.any_schema(\n                    serialization=core_schema.wrap_serializer_function_ser_schema(\n                        lambda self, v, serializer: f'{list(self.keys())}',\n                        is_field_serializer=True,\n                        schema=core_schema.any_schema(),\n                    )\n                )\n            )\n        }\n    )\n    outer_schema = core_schema.typed_dict_schema({'model': core_schema.typed_dict_field(schema)})\n    s = SchemaSerializer(schema)\n    assert s.to_python(Model(x=None)) == {'x': \"['x']\"}\n    outer_s = SchemaSerializer(outer_schema)\n    assert outer_s.to_python(OuterModel(model=Model(x=None))) == {'model': {'x': \"['x']\"}}", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "validation", "example_id": "002217", "source": "def test_run_calls_clone_prepare_and_build(\n        self, mock_gen, mock_clone, mock_ensure, mock_run\n    ):\n        params = MagicMock()\n        params.output_dir = Path(\"shared\")\n        params.use_local_dockerfile = False\n        params.use_torch_whl = False\n        with patch(f\"{_VLLM_BUILD_MODULE}.VllmBuildParameters\", return_value=params):\n            runner = vllm_build.VllmBuildRunner()\n            runner.run()\n        mock_clone.assert_called_once()\n        mock_ensure.assert_called_once_with(Path(\"shared\"))\n        mock_gen.assert_called_once_with(params)\n        mock_run.assert_called_once()\n        _, kwargs = mock_run.call_args\n        assert kwargs.get(\"cwd\") == \"vllm\"", "target": "def test_is_instance_dict_not_str():\n    v = SchemaValidator(core_schema.dict_schema(keys_schema=core_schema.is_instance_schema(int)))\n    assert v.isinstance_python({1: 1}) is True\n    assert v.isinstance_python({'foo': 1}) is False", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "validation", "example_id": "002218", "source": "def premul_lstm_cell_no_bias(\n    igates: Tensor, hidden: tuple[Tensor, Tensor], w_hh: Tensor, b_hh: Tensor\n) -> tuple[Tensor, Tensor]:\n    hx, cx = hidden\n    gates = igates + torch.mm(hx, w_hh.t()) + b_hh\n    ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)\n    ingate = torch.sigmoid(ingate)\n    forgetgate = torch.sigmoid(forgetgate)\n    cellgate = torch.tanh(cellgate)\n    outgate = torch.sigmoid(outgate)\n    cy = (forgetgate * cx) + (ingate * cellgate)\n    hy = outgate * torch.tanh(cy)\n    return hy, cy", "target": "def permute_choices(choices: list[core_schema.CoreSchema]) -> list[list[core_schema.CoreSchema]]:\n    return [list(p) for p in permutations(choices)]", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "002219", "source": "def test_with_default_factory():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'x': core_schema.model_field(\n                    schema=core_schema.with_default_schema(\n                        schema=core_schema.str_schema(), default_factory=lambda: 'pikachu'\n                    )\n                )\n            }\n        )\n    )\n    assert v.validate_python({}) == ({'x': 'pikachu'}, None, set())\n    assert v.validate_python({'x': 'bulbi'}) == ({'x': 'bulbi'}, None, {'x'})", "target": "def test_with_default_factory():\n    v = SchemaValidator(\n        core_schema.typed_dict_schema(\n            fields={\n                'x': core_schema.typed_dict_field(\n                    schema=core_schema.with_default_schema(\n                        schema=core_schema.str_schema(), default_factory=lambda: 'pikachu'\n                    )\n                )\n            }\n        )\n    )\n    assert v.validate_python({}) == {'x': 'pikachu'}\n    assert v.validate_python({'x': 'bulbi'}) == {'x': 'bulbi'}", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "validation", "example_id": "002220", "source": "def description(self):\n        return \"information at https://github.com/pytorch/pytorch/issues/134133\"", "target": "def with_info_plain_validator_function(\n    function: WithInfoValidatorFunction,\n    *,\n    field_name: str | None = None,\n    ref: str | None = None,\n    json_schema_input_schema: CoreSchema | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> PlainValidatorFunctionSchema:\n    if field_name is not None:\n        warnings.warn(\n            'The `field_name` argument on `with_info_plain_validator_function` is deprecated, it will be passed to the function through `ValidationState` instead.',\n            DeprecationWarning,\n            stacklevel=2,\n        )\n    return _dict_not_none(\n        type='function-plain',\n        function=_dict_not_none(type='with-info', function=function, field_name=field_name),\n        ref=ref,\n        json_schema_input_schema=json_schema_input_schema,\n        metadata=metadata,\n        serialization=serialization,\n    )", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "validation", "example_id": "002221", "source": "def node_type(self) -> ASTNodeType:\n        return ASTNodeType.Function", "target": "def node_type(self) -> ASTNodeType:\n        return ASTNodeType.Class", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|opencv/opencv", "data_split": "validation", "example_id": "002222", "source": "def smoke_test_conv2d() -> None:\n    import torch.nn as nn\n    print(\"Testing smoke_test_conv2d\")\n    m = nn.Conv2d(16, 33, 3, stride=2)\n    m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))\n    assert m is not None\n    basic_conv = nn.Conv2d(\n        16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1)\n    )\n    input = torch.randn(20, 16, 50, 100)\n    output = basic_conv(input)\n    if is_cuda_system:\n        print(\"Testing smoke_test_conv2d with cuda\")\n        conv = nn.Conv2d(3, 3, 3).cuda()\n        x = torch.randn(1, 3, 24, 24, device=\"cuda\")\n        with torch.cuda.amp.autocast():\n            out = conv(x)\n        assert out is not None\n        supported_dtypes = [torch.float16, torch.float32, torch.float64]\n        for dtype in supported_dtypes:\n            print(f\"Testing smoke_test_conv2d with cuda for {dtype}\")\n            conv = basic_conv.to(dtype).cuda()\n            input = torch.randn(20, 16, 50, 100, device=\"cuda\").type(dtype)\n            output = conv(input)\n            assert output is not None", "target": "def listTests(self, short=False, main=False):\n        if len(self.tests) == 0:\n            raise Err(\"No tests found\")\n        for t in self.tests:\n            if short:\n                t = self.getAlias(t)\n            if not main or self.cache.isMainModule(t):\n                log.info(\"%s\", t)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "validation", "example_id": "002223", "source": "def test_wrong_function_signature() -> None:\n    def wrong_validator(value: Any) -> Any:\n        return value\n    v = SchemaValidator(core_schema.with_info_plain_validator_function(wrong_validator))\n    try:\n        v.validate_python(1)\n    except TypeError as exc:\n        assert 'takes 1 positional argument but 2 were given' in str(exc)\n    else:\n        raise AssertionError('v.validate_python(1) did not raise TypeError')", "target": "def test_error_details() -> None:\n    def act_on_error_details(_: ErrorDetails) -> None:\n        pass\n    v = SchemaValidator({'type': 'int'})\n    try:\n        v.validate_python('not an int')\n    except ValidationError as err:\n        for details in err.errors(include_url=False):\n            act_on_error_details(details)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "validation", "example_id": "002224", "source": "def test_union_time_respects_downcasts_correctly():\n    serialization_schema = core_schema.plain_serializer_function_ser_schema(lambda v: None)\n    json_validation_schema = core_schema.no_info_plain_validator_function(\n        function=lambda v: v, serialization=serialization_schema\n    )\n    test_custom_ser_schema = core_schema.json_schema(\n        schema=json_validation_schema,\n        serialization=serialization_schema,\n    )\n    s = SchemaSerializer(core_schema.union_schema(choices=[core_schema.time_schema(), test_custom_ser_schema]))\n    assert s.to_python('foo') is None", "target": "def test_function_plain_field_serializer_with_computed_field():\n    @dataclasses.dataclass\n    class Model:\n        x: int\n        @property\n        def computed_field_x(self) -> int:\n            return self.x + 200\n        def ser_func(self, v: Any, info: core_schema.FieldSerializationInfo) -> str:\n            return info.field_name + '_' + str(v * 2)\n    field_str_with_field_serializer = core_schema.str_schema(\n        serialization=core_schema.plain_serializer_function_ser_schema(\n            Model.ser_func,\n            is_field_serializer=True,\n            info_arg=True,\n            return_schema=core_schema.any_schema(),\n        )\n    )\n    s = SchemaSerializer(\n        core_schema.model_schema(\n            Model,\n            core_schema.model_fields_schema(\n                {'x': core_schema.model_field(field_str_with_field_serializer)},\n                computed_fields=[\n                    core_schema.computed_field('computed_field_x', field_str_with_field_serializer),\n                ],\n            ),\n        )\n    )\n    assert json.loads(s.to_json(Model(x=1000))) == {'x': 'x_2000', 'computed_field_x': 'computed_field_x_2400'}\n    assert s.to_python(Model(x=2000)) == {'x': 'x_4000', 'computed_field_x': 'computed_field_x_4400'}", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "002225", "source": "def test_custom_error(py_and_json: PyAndJson):\n    v = py_and_json(\n        core_schema.custom_error_schema(core_schema.int_schema(), 'foobar', custom_error_message='Hello there')\n    )\n    assert v.validate_test(1) == 1\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_test('foobar')\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'foobar', 'loc': (), 'msg': 'Hello there', 'input': 'foobar'}\n    ]", "target": "def test_custom_error():\n    v = SchemaValidator(\n        core_schema.union_schema(\n            choices=[core_schema.str_schema(), core_schema.bytes_schema()],\n            custom_error_type='my_error',\n            custom_error_message='Input should be a string or bytes',\n        )\n    )\n    assert v.validate_python('hello') == 'hello'\n    assert v.validate_python(b'hello') == b'hello'\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python(123)\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'my_error', 'loc': (), 'msg': 'Input should be a string or bytes', 'input': 123}\n    ]", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "002226", "source": "def with_config(*, config: ConfigDict) -> Callable[[_TypeT], _TypeT]: ...", "target": "def with_config(**config: Unpack[ConfigDict]) -> Callable[[_TypeT], _TypeT]: ...", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|opencv/opencv", "data_split": "validation", "example_id": "002227", "source": "def scriptAndSave(module, fileName):\n    print(\"-\" * 80)\n    script_module = torch.jit.script(module)\n    print(script_module.graph)\n    outputFileName = OUTPUT_DIR + fileName\n    script_module._save_for_lite_interpreter(outputFileName)\n    print(\"Saved to \" + outputFileName)\n    print(\"=\" * 80)", "target": "def test_fast(self):\n        fd = cv.FastFeatureDetector_create(30, True)\n        img = self.get_sample(\"samples/data/right02.jpg\", 0)\n        img = cv.medianBlur(img, 3)\n        keypoints = fd.detect(img)\n        self.assertTrue(600 <= len(keypoints) <= 700)\n        for kpt in keypoints:\n            self.assertNotEqual(kpt.response, 0)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "validation", "example_id": "002228", "source": "def children_types(self) -> Tuple[ASTNodeType, ...]:\n        return (ASTNodeType.Constant, )", "target": "def children_types(self) -> Tuple[ASTNodeType, ...]:\n        return (ASTNodeType.Class, ASTNodeType.Function,\n                ASTNodeType.Enumeration, ASTNodeType.Constant)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "002229", "source": "def test_only_allow_alias(py_and_json) -> None:\n    v = py_and_json(\n        cs.arguments_v3_schema(\n            [\n                cs.arguments_v3_parameter(\n                    name='a', schema=cs.str_schema(), alias='FieldA', mode='positional_or_keyword'\n                ),\n            ],\n            validate_by_name=False,\n            validate_by_alias=True,\n        )\n    )\n    assert v.validate_test(ArgsKwargs((), {'FieldA': 'hello'})) == ((), {'a': 'hello'})\n    assert v.validate_test({'FieldA': 'hello'}) == (('hello',), {})\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Missing required argument \\[type=missing_argument,'):\n        assert v.validate_test(ArgsKwargs((), {'a': 'hello'}))\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Missing required argument \\[type=missing_argument,'):\n        assert v.validate_test({'a': 'hello'})", "target": "def test_only_allow_alias(py_and_json) -> None:\n    schema = core_schema.arguments_schema(\n        [\n            core_schema.arguments_parameter(name='a', schema=core_schema.str_schema(), alias='FieldA'),\n        ],\n        validate_by_name=False,\n        validate_by_alias=True,\n    )\n    v = py_and_json(schema)\n    assert v.validate_test(ArgsKwargs((), {'FieldA': 'hello'})) == ((), {'a': 'hello'})\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Missing required argument \\[type=missing_argument,'):\n        assert v.validate_test(ArgsKwargs((), {'a': 'hello'}))", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "validation", "example_id": "002230", "source": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "target": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|scikit-learn/scikit-learn", "data_split": "validation", "example_id": "002231", "source": "def test_warn_on_missing_field() -> None:\n    class AModel(BasicModel): ...\n    class BModel(BasicModel): ...\n    s = SchemaSerializer(\n        core_schema.model_schema(\n            BasicModel,\n            core_schema.model_fields_schema(\n                {\n                    'root': core_schema.model_field(\n                        core_schema.tagged_union_schema(\n                            choices={\n                                'a': core_schema.model_schema(\n                                    AModel,\n                                    core_schema.model_fields_schema(\n                                        {\n                                            'type': core_schema.model_field(core_schema.literal_schema(['a'])),\n                                            'a': core_schema.model_field(core_schema.int_schema()),\n                                        }\n                                    ),\n                                ),\n                                'b': core_schema.model_schema(\n                                    BModel,\n                                    core_schema.model_fields_schema(\n                                        {\n                                            'type': core_schema.model_field(core_schema.literal_schema(['b'])),\n                                            'b': core_schema.model_field(core_schema.int_schema()),\n                                        }\n                                    ),\n                                ),\n                            },\n                            discriminator='type',\n                        )\n                    ),\n                }\n            ),\n        )\n    )\n    with pytest.warns(\n        UserWarning, match='Expected 2 fields but got 1: Expected `AModel` - serialized value may not be as expected .+'\n    ):\n        value = BasicModel(root=AModel(type='a'))\n        s.to_python(value)", "target": "def fit(est, data_train, target_train, libname, **fit_params):\n    print(f\"Fitting a {libname} model...\")\n    tic = time()\n    est.fit(data_train, target_train, **fit_params)\n    toc = time()\n    print(f\"fitted in {toc - tic:.3f}s\")", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "002232", "source": "def test_frozenset_no_validators_both(py_and_json: PyAndJson, input_value, expected):\n    v = py_and_json({'type': 'frozenset'})\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            v.validate_test(input_value)\n    else:\n        output = v.validate_test(input_value)\n        assert output == expected\n        assert isinstance(output, frozenset)", "target": "def test_frozenset_no_validators_both(py_and_json: PyAndJson, input_value, expected):\n    v = py_and_json({'type': 'set'})\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=expected.message):\n            v.validate_test(input_value)\n    else:\n        assert v.validate_test(input_value) == expected", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "validation", "example_id": "002233", "source": "def test_discriminated_union_ser_with_typed_dict() -> None:\n    v = SchemaSerializer(\n        core_schema.tagged_union_schema(\n            {\n                'a': core_schema.typed_dict_schema(\n                    {\n                        'type': core_schema.typed_dict_field(core_schema.literal_schema(['a'])),\n                        'a': core_schema.typed_dict_field(core_schema.int_schema()),\n                    }\n                ),\n                'b': core_schema.typed_dict_schema(\n                    {\n                        'type': core_schema.typed_dict_field(core_schema.literal_schema(['b'])),\n                        'b': core_schema.typed_dict_field(core_schema.str_schema()),\n                    }\n                ),\n            },\n            discriminator='type',\n        )\n    )\n    assert v.to_python({'type': 'a', 'a': 1}, warnings='error') == {'type': 'a', 'a': 1}\n    assert v.to_python({'type': 'b', 'b': 'foo'}, warnings='error') == {'type': 'b', 'b': 'foo'}", "target": "def deserialize(cls, data: JsonDict) -> PydanticModelClassVar:\n        data = data.copy()\n        return cls(**data)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "validation", "example_id": "002234", "source": "def sub(a, b):\n    return 3 * a - b", "target": "def huge_graph(x):\n    for _ in range(N):\n        x = x.sin()\n    return x", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "validation", "example_id": "002235", "source": "def test_too_short(py_and_json: PyAndJson):\n    v = py_and_json({'type': 'generator', 'items_schema': {'type': 'int'}, 'min_length': 2})\n    assert list(v.validate_test([1, 2, 3])) == [1, 2, 3]\n    assert list(v.validate_test([1, 2])) == [1, 2]\n    with pytest.raises(ValidationError) as exc_info:\n        list(v.validate_test([1]))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'too_short',\n            'loc': (),\n            'msg': 'Generator should have at least 2 items after validation, not 1',\n            'input': [1],\n            'ctx': {'field_type': 'Generator', 'min_length': 2, 'actual_length': 1},\n        }\n    ]", "target": "def test_url_pickle(value):\n    pickled = pickle.dumps(value)\n    unpickled = pickle.loads(pickled)\n    assert value == unpickled", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "002236", "source": "def constrain(self: _Pipeline[_InT, _NewOutGe], constraint: annotated_types.Ge) -> _Pipeline[_InT, _NewOutGe]: ...", "target": "def constrain(self: _Pipeline[_InT, _OutT], constraint: annotated_types.Predicate) -> _Pipeline[_InT, _OutT]: ...", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|scikit-learn/scikit-learn", "data_split": "validation", "example_id": "002237", "source": "def f(input_value, info: core_schema.ValidationInfo):\n        assert repr(info) == expected_repr\n        assert str(info) == expected_repr\n        return input_value", "target": "def get_data(dataset_name):\n    print(\"Getting dataset: %s\" % dataset_name)\n    if dataset_name == \"lfw_people\":\n        X = fetch_lfw_people().data\n    elif dataset_name == \"20newsgroups\":\n        X = fetch_20newsgroups_vectorized().data[:, :100000]\n    elif dataset_name == \"olivetti_faces\":\n        X = fetch_olivetti_faces().data\n    elif dataset_name == \"rcv1\":\n        X = fetch_rcv1().data\n    elif dataset_name == \"CIFAR\":\n        if handle_missing_dataset(CIFAR_FOLDER) == 0:\n            return\n        X1 = [unpickle(\"%sdata_batch_%d\" % (CIFAR_FOLDER, i + 1)) for i in range(5)]\n        X = np.vstack(X1)\n        del X1\n    elif dataset_name == \"SVHN\":\n        if handle_missing_dataset(SVHN_FOLDER) == 0:\n            return\n        X1 = sp.io.loadmat(\"%strain_32x32.mat\" % SVHN_FOLDER)[\"X\"]\n        X2 = [X1[:, :, :, i].reshape(32 * 32 * 3) for i in range(X1.shape[3])]\n        X = np.vstack(X2)\n        del X1\n        del X2\n    elif dataset_name == \"low rank matrix\":\n        X = make_low_rank_matrix(\n            n_samples=500,\n            n_features=int(1e4),\n            effective_rank=100,\n            tail_strength=0.5,\n            random_state=random_state,\n        )\n    elif dataset_name == \"uncorrelated matrix\":\n        X, _ = make_sparse_uncorrelated(\n            n_samples=500, n_features=10000, random_state=random_state\n        )\n    elif dataset_name == \"big sparse matrix\":\n        sparsity = int(1e6)\n        size = int(1e6)\n        small_size = int(1e4)\n        data = np.random.normal(0, 1, int(sparsity / 10))\n        data = np.repeat(data, 10)\n        row = np.random.uniform(0, small_size, sparsity)\n        col = np.random.uniform(0, small_size, sparsity)\n        X = sp.sparse.csr_matrix((data, (row, col)), shape=(size, small_size))\n        del data\n        del row\n        del col\n    else:\n        X = fetch_openml(dataset_name).data\n    return X", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "002238", "source": "def benchmark(self):\n        for M, N in self.get_shapes():\n            print(f\"\\n Tensor dimensions: [{M}, {N}]\")\n            torch_dtype = cutlass_torch.dtype(cutlass.BFloat16)\n            x = 0.1 * torch.randn(M, N, device=\"cuda\", dtype=torch_dtype)\n            target = torch.randint(0, N, (M,), device=\"cuda\", dtype=torch.int64)\n            self.benchmark_single_shape((x, target), setting=f\"shape: [{M}, {N}]\")", "target": "def benchmark(self):\n        for M, N in self.get_shapes():\n            print(f\"Tensor dimensions: [{M}, {N}]\")\n            torch_dtype = cutlass_torch.dtype(cutlass.BFloat16)\n            x = torch.randn(M, N, device=\"cuda\", dtype=torch_dtype, requires_grad=True)\n            w = torch.randn(N, device=\"cuda\", dtype=torch.float32, requires_grad=True)\n            dy = torch.randn(M, N, device=\"cuda\", dtype=torch_dtype)\n            self.benchmark_single_shape((x, w, dy), setting=f\"shape: [{M}, {N}]\")", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "002239", "source": "def area(self) -> int:\n            return self.width * self.height", "target": "def area(self) -> int:\n            raise ValueError('xxx')", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "002240", "source": "def area(self) -> int:\n            raise ValueError('xxx')", "target": "def area(self) -> None:\n            self.side = 0.0", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "validation", "example_id": "002241", "source": "def types_separator(self) -> str:\n        return \", \"", "target": "def types_separator(self) -> str:\n        return \", \"", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "validation", "example_id": "002242", "source": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "target": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|scikit-learn/scikit-learn", "data_split": "validation", "example_id": "002243", "source": "def test_non_model_field_wrap_validator_field_info() -> None:\n    class Model:\n        __slots__ = '__dict__', '__pydantic_fields_set__', '__pydantic_extra__', '__pydantic_private__'\n        x: str\n    def f(input_value: Any, val: core_schema.ValidatorFunctionWrapHandler, info: core_schema.ValidationInfo) -> Any:\n        assert info.field_name == 'x'\n        assert info.data == {}\n        return f'input: {val(input_value)}'\n    v = SchemaValidator(\n        core_schema.model_schema(\n            Model,\n            core_schema.model_fields_schema(\n                {\n                    'x': core_schema.model_field(\n                        core_schema.with_info_wrap_validator_function(f, core_schema.str_schema())\n                    )\n                }\n            ),\n        )\n    )\n    assert v.validate_python({'x': b'foo'}).x == 'input: foo'", "target": "def plot_test_scores(clfs):\n    plt.figure()\n    for name, _, _, _, _, test_scores, durations in clfs:\n        plt.plot(durations, test_scores, \"-o\", label=name)\n        plt.legend(loc=0)\n        plt.xlabel(\"seconds\")\n        plt.ylabel(\"test score\")\n        plt.ylim((0.92, 0.96))", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|scikit-learn/scikit-learn", "data_split": "validation", "example_id": "002244", "source": "def medium_sliced():\n    return (rand(32, 12, 64, 64)[..., ::2], rand(32, 12, 64, 64)[..., ::2])", "target": "def _classifier_has(attr):\n    return lambda estimator: (\n        hasattr(estimator.classifier_, attr)\n        if hasattr(estimator, \"classifier_\")\n        else hasattr(estimator.classifier, attr)\n    )", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "validation", "example_id": "002245", "source": "def typename(self) -> str:\n        return self.type_node.full_typename", "target": "def typename(self) -> str:\n        return self._typename", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "validation", "example_id": "002246", "source": "def lenient_isinstance(o: Any, class_or_tuple: type[Any] | tuple[type[Any], ...] | None) -> bool:\n    try:\n        return isinstance(o, class_or_tuple)\n    except TypeError:\n        return False", "target": "def arguments_v3_schema(\n    arguments: list[ArgumentsV3Parameter],\n    *,\n    validate_by_name: bool | None = None,\n    validate_by_alias: bool | None = None,\n    extra_behavior: Literal['forbid', 'ignore'] | None = None,\n    ref: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> ArgumentsV3Schema:\n    return _dict_not_none(\n        type='arguments-v3',\n        arguments_schema=arguments,\n        validate_by_name=validate_by_name,\n        validate_by_alias=validate_by_alias,\n        extra_behavior=extra_behavior,\n        ref=ref,\n        metadata=metadata,\n        serialization=serialization,\n    )", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "validation", "example_id": "002247", "source": "def types_separator(self) -> str:\n        return \"\"", "target": "def types_separator(self) -> str:\n        return \", \"", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "validation", "example_id": "002248", "source": "def f(input_value):\n        input_value.more = 'foobar'\n        return input_value", "target": "def test_function_after_config():\n    f_kwargs = None\n    def f(input_value, info):\n        nonlocal f_kwargs\n        f_kwargs = deepcopy_info(info)\n        return input_value + ' Changed'\n    v = SchemaValidator(\n        cs.typed_dict_schema(\n            fields={\n                'test_field': cs.typed_dict_field(\n                    schema={\n                        'type': 'function-after',\n                        'function': {'type': 'with-info', 'function': f, 'field_name': 'test_field'},\n                        'schema': cs.str_schema(),\n                    }\n                )\n            },\n            config=CoreConfig(allow_inf_nan=True),\n        )\n    )\n    assert v.validate_python({'test_field': b'321'}) == {'test_field': '321 Changed'}\n    assert f_kwargs == {'data': {}, 'config': {'allow_inf_nan': True}, 'context': None, 'field_name': 'test_field'}", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "validation", "example_id": "002249", "source": "def get_output(self, input_blob):\n        assert len(input_blob.shape) == 4\n        batch_tf = input_blob.transpose(0, 2, 3, 1)\n        out = self.sess.run(self.output,\n                       {self.in_blob_name+':0': batch_tf})\n        out = out[..., 1:1001]\n        return out", "target": "def get_output(self, input_blob):\n        return super(DnnTfInceptionModel, self).get_output(input_blob)[..., 1:1001]", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "validation", "example_id": "002250", "source": "def description(self):\n        return \"information at https://github.com/pytorch/pytorch/issues/134133\"", "target": "def test_extra():\n    class RootModel:\n        __slots__ = '__dict__', '__pydantic_fields_set__', '__pydantic_extra__', '__pydantic_private__'\n        root: int\n    v = SchemaValidator(core_schema.model_schema(RootModel, core_schema.int_schema(), root_model=True))\n    m = v.validate_python(1)\n    with pytest.raises(AttributeError):\n        m.__pydantic_extra__", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "validation", "example_id": "002251", "source": "def test_function_wrap_preserves_wrapped_serialization():\n    def f(value, handler, _info):\n        return handler(value)\n    s = SchemaSerializer(core_schema.with_info_wrap_validator_function(f, core_schema.int_schema()))\n    with pytest.warns(\n        UserWarning,\n        match=r\"Expected `int` - serialized value may not be as expected \\[input_value='abc', input_type=str\\]\",\n    ):\n        assert s.to_python('abc') == 'abc'", "target": "def ser_x(data: Model, v: Any) -> str:\n        assert data['x'] == 1_000\n        return f'{v:_}'", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "validation", "example_id": "002252", "source": "def test_schema_validator_containing_config():\n    v = SchemaValidator(\n        core_schema.model_fields_schema({'f': core_schema.model_field(core_schema.str_schema())}),\n        config=core_schema.CoreConfig(extra_fields_behavior='allow'),\n    )\n    v = pickle.loads(pickle.dumps(v))\n    m, model_extra, fields_set = v.validate_python({'f': 'x', 'extra_field': '123'})\n    assert m == {'f': 'x'}\n    assert model_extra == {'extra_field': '123'}\n    assert fields_set == {'f', 'extra_field'}\n    v.validate_assignment(m, 'f', 'y')\n    assert m == {'f': 'y'}", "target": "def typed_dict_schema(\n    fields: dict[str, TypedDictField],\n    *,\n    cls: type[Any] | None = None,\n    cls_name: str | None = None,\n    computed_fields: list[ComputedField] | None = None,\n    strict: bool | None = None,\n    extras_schema: CoreSchema | None = None,\n    extra_behavior: ExtraBehavior | None = None,\n    total: bool | None = None,\n    ref: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n    config: CoreConfig | None = None,\n) -> TypedDictSchema:\n    return _dict_not_none(\n        type='typed-dict',\n        fields=fields,\n        cls=cls,\n        cls_name=cls_name,\n        computed_fields=computed_fields,\n        strict=strict,\n        extras_schema=extras_schema,\n        extra_behavior=extra_behavior,\n        total=total,\n        ref=ref,\n        metadata=metadata,\n        serialization=serialization,\n        config=config,\n    )", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "002253", "source": "def iter_model_names(self, args):\n        model_names = sorted(TIMM_MODELS.keys())\n        start, end = self.get_benchmark_indices(len(model_names))\n        for index, model_name in enumerate(model_names):\n            if index < start or index >= end:\n                continue\n            if (\n                not re.search(\"|\".join(args.filter), model_name, re.IGNORECASE)\n                or re.search(\"|\".join(args.exclude), model_name, re.IGNORECASE)\n                or model_name in args.exclude_exact\n                or model_name in self.skip_models\n            ):\n                continue\n            yield model_name", "target": "def iter_model_names(self, args):\n        from torchbenchmark import _list_canary_model_paths, _list_model_paths\n        models = _list_model_paths()\n        models += [\n            f\n            for f in _list_canary_model_paths()\n            if os.path.basename(f) in self._config[\"canary_models\"]\n        ]\n        models.sort()\n        start, end = self.get_benchmark_indices(len(models))\n        for index, model_path in enumerate(models):\n            if index < start or index >= end:\n                continue\n            model_name = os.path.basename(model_path)\n            if (\n                not re.search(\"|\".join(args.filter), model_name, re.IGNORECASE)\n                or re.search(\"|\".join(args.exclude), model_name, re.IGNORECASE)\n                or model_name in args.exclude_exact\n                or model_name in self.skip_models\n            ):\n                continue\n            yield model_name", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "002254", "source": "def get_shapes(self) -> tuple[tuple[int, ...], ...]:\n        return (\n            (32768, 256),\n            (32768, 512),\n            (32768, 1024),\n            (32768, 2048),\n            (32768, 4096),\n            (32768, 8192),\n            (32768, 16384),\n            (32768, 32768),\n            (32768, 65536),\n            (16384, 131072),\n            (8192, 262144),\n        ) + extra_shapes_for_norm", "target": "def get_shapes(self) -> tuple[tuple[int, ...], ...]:\n        return (\n            (32768, 256),\n            (32768, 512),\n            (32768, 1024),\n            (32768, 2048),\n            (32768, 4096),\n            (32768, 8192),\n            (32768, 16384),\n            (32768, 32768),\n            (32768, 65536),\n        ) + extra_shapes_for_norm", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|scikit-learn/scikit-learn", "data_split": "validation", "example_id": "002255", "source": "def test_hide_input_in_errors(config, input_str):\n    v = SchemaValidator(\n        cs.model_schema(\n            cls=MyModel, schema=cs.model_fields_schema(fields={'f': cs.model_field(schema=cs.str_schema())})\n        ),\n        config=config,\n    )\n    with pytest.raises(ValidationError, match=re.escape(f'Input should be a valid string [{input_str}]')):\n        assert v.validate_python({'f': 123})", "target": "def make_paragraph_for_estimator_type(estimator_type):\n        intro = nodes.list_item()\n        intro += nodes.strong(text=\"Estimators that allow NaN values for type \")\n        intro += nodes.literal(text=f\"{estimator_type}\")\n        intro += nodes.strong(text=\":\\n\")\n        exists = False\n        lst = nodes.bullet_list()\n        for name, est_class in all_estimators(type_filter=estimator_type):\n            with suppress(SkipTest):\n                est = next(_construct_instances(est_class))\n                if est.__sklearn_tags__().input_tags.allow_nan:\n                    module_name = \".\".join(est_class.__module__.split(\".\")[:2])\n                    class_title = f\"{est_class.__name__}\"\n                    class_url = f\"./generated/{module_name}.{class_title}.html\"\n                    item = nodes.list_item()\n                    para = nodes.paragraph()\n                    para += nodes.reference(\n                        class_title, text=class_title, internal=False, refuri=class_url\n                    )\n                    exists = True\n                    item += para\n                    lst += item\n        intro += lst\n        return [intro] if exists else None", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "validation", "example_id": "002256", "source": "def compute_bench(samples_range, features_range):\n    it = 0\n    results = defaultdict(lambda: [])\n    max_it = len(samples_range) * len(features_range)\n    for n_samples in samples_range:\n        for n_features in features_range:\n            it += 1\n            print(\"==============================\")\n            print(\"Iteration %03d of %03d\" % (it, max_it))\n            print(\"n_samples %05d; n_features %02d\" % (n_samples, n_features))\n            print(\"==============================\")\n            print()\n            data = nr.randint(-50, 51, (n_samples, n_features))\n            for linkage in (\"single\", \"average\", \"complete\", \"ward\"):\n                print(linkage.capitalize())\n                tstart = time()\n                AgglomerativeClustering(linkage=linkage, n_clusters=10).fit(data)\n                delta = time() - tstart\n                print(\"Speed: %0.3fs\" % delta)\n                print()\n                results[linkage].append(delta)\n    return results", "target": "def compute_bench(samples_range, features_range):\n    it = 0\n    results = dict()\n    lars = np.empty((len(features_range), len(samples_range)))\n    lars_gram = lars.copy()\n    omp = lars.copy()\n    omp_gram = lars.copy()\n    max_it = len(samples_range) * len(features_range)\n    for i_s, n_samples in enumerate(samples_range):\n        for i_f, n_features in enumerate(features_range):\n            it += 1\n            n_informative = n_features // 10\n            print(\"====================\")\n            print(\"Iteration %03d of %03d\" % (it, max_it))\n            print(\"====================\")\n            dataset_kwargs = {\n                \"n_samples\": 1,\n                \"n_components\": n_features,\n                \"n_features\": n_samples,\n                \"n_nonzero_coefs\": n_informative,\n                \"random_state\": 0,\n            }\n            print(\"n_samples: %d\" % n_samples)\n            print(\"n_features: %d\" % n_features)\n            y, X, _ = make_sparse_coded_signal(**dataset_kwargs)\n            X = np.asfortranarray(X.T)\n            gc.collect()\n            print(\"benchmarking lars_path (with Gram):\", end=\"\")\n            sys.stdout.flush()\n            tstart = time()\n            G = np.dot(X.T, X)\n            Xy = np.dot(X.T, y)\n            lars_path_gram(Xy=Xy, Gram=G, n_samples=y.size, max_iter=n_informative)\n            delta = time() - tstart\n            print(\"%0.3fs\" % delta)\n            lars_gram[i_f, i_s] = delta\n            gc.collect()\n            print(\"benchmarking lars_path (without Gram):\", end=\"\")\n            sys.stdout.flush()\n            tstart = time()\n            lars_path(X, y, Gram=None, max_iter=n_informative)\n            delta = time() - tstart\n            print(\"%0.3fs\" % delta)\n            lars[i_f, i_s] = delta\n            gc.collect()\n            print(\"benchmarking orthogonal_mp (with Gram):\", end=\"\")\n            sys.stdout.flush()\n            tstart = time()\n            orthogonal_mp(X, y, precompute=True, n_nonzero_coefs=n_informative)\n            delta = time() - tstart\n            print(\"%0.3fs\" % delta)\n            omp_gram[i_f, i_s] = delta\n            gc.collect()\n            print(\"benchmarking orthogonal_mp (without Gram):\", end=\"\")\n            sys.stdout.flush()\n            tstart = time()\n            orthogonal_mp(X, y, precompute=False, n_nonzero_coefs=n_informative)\n            delta = time() - tstart\n            print(\"%0.3fs\" % delta)\n            omp[i_f, i_s] = delta\n    results[\"time(LARS) / time(OMP)\\n (w/ Gram)\"] = lars_gram / omp_gram\n    results[\"time(LARS) / time(OMP)\\n (w/o Gram)\"] = lars / omp\n    return results", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "validation", "example_id": "002257", "source": "def test_full(py_and_json: PyAndJson, input_value, expected) -> None:\n    v = py_and_json(\n        cs.arguments_v3_schema(\n            [\n                cs.arguments_v3_parameter(name='a', schema=cs.int_schema(), alias='aa', mode='positional_only'),\n                cs.arguments_v3_parameter(name='b', schema=cs.str_schema(), mode='positional_or_keyword'),\n                cs.arguments_v3_parameter(name='args', schema=cs.int_schema(), mode='var_args'),\n                cs.arguments_v3_parameter(name='c', schema=cs.bool_schema(), mode='keyword_only'),\n                cs.arguments_v3_parameter(name='kwargs', schema=cs.int_schema(), mode='var_kwargs_uniform'),\n            ]\n        )\n    )\n    assert v.validate_test(input_value) == expected", "target": "def replace_tag(filename) -> None:\n    with open(filename) as f:\n        lines = f.readlines()\n    for i, line in enumerate(lines):\n        if line.startswith(\"Tag:\"):\n            lines[i] = line.replace(\"-linux_\", \"-manylinux_2_28_\")\n            print(f\"Updated tag from {line} to {lines[i]}\")\n            break\n    with open(filename, \"w\") as f:\n        f.writelines(lines)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "validation", "example_id": "002258", "source": "def random_n(self) -> int:\n            return randint(0, 1_000)", "target": "def test_all_optional_fields():\n    v = SchemaValidator(\n        core_schema.typed_dict_schema(\n            total=False,\n            fields={\n                'x': core_schema.typed_dict_field(schema=core_schema.str_schema(strict=True)),\n                'y': core_schema.typed_dict_field(schema=core_schema.str_schema()),\n            },\n        )\n    )\n    assert v.validate_python({'x': 'pika', 'y': 'chu'}) == {'x': 'pika', 'y': 'chu'}\n    assert v.validate_python({'x': 'pika'}) == {'x': 'pika'}\n    assert v.validate_python({'y': 'chu'}) == {'y': 'chu'}\n    with pytest.raises(ValidationError) as exc_info:\n        assert v.validate_python({'x': 123})\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'string_type', 'loc': ('x',), 'msg': 'Input should be a valid string', 'input': 123}\n    ]", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "validation", "example_id": "002259", "source": "def required_modules(self) -> Tuple[str, ...]:\n        return self._required_modules", "target": "def required_modules(self) -> Tuple[str, ...]:\n        return (*self.positive_branch_type.required_modules,\n                *self.negative_branch_type.required_modules)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "validation", "example_id": "002260", "source": "def test_bad_iter(items_schema):\n    class BadIter:\n        def __init__(self, success: bool):\n            self._success = success\n            self._index = 0\n        def __iter__(self):\n            return self\n        def __len__(self):\n            return 2\n        def __next__(self):\n            self._index += 1\n            if self._index == 1:\n                return 1\n            elif self._success:\n                raise StopIteration()\n            else:\n                raise RuntimeError('broken')\n    v = SchemaValidator(cs.list_schema(items_schema={'type': items_schema}))\n    assert v.validate_python(BadIter(True)) == [1]\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python(BadIter(False))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'iteration_error',\n            'loc': (1,),\n            'msg': 'Error iterating over object, error: RuntimeError: broken',\n            'input': IsInstance(BadIter),\n            'ctx': {'error': 'RuntimeError: broken'},\n        }\n    ]", "target": "def test_parse_vector_double_convertible(self):\n        np.random.seed(1230965)\n        try_to_convert = partial(self._try_to_convert, cv.utils.dumpVectorOfDouble)\n        arr = np.random.randint(-20, 20, 40).astype(np.int32).reshape(10, 2, 2)\n        for convertible in ((1, 2.12, 3.5), [40, 50], tuple(),\n                            np.array([-10, 24], dtype=np.int32),\n                            np.array([-12.5, 1.4], dtype=np.double),\n                            np.array([10, 230, 12], dtype=np.float32), arr[:, 0, 1], ):\n            expected = \"[\" + \", \".join(map(lambda v: \"{:.2f}\".format(v), convertible)) + \"]\"\n            actual = try_to_convert(convertible)\n            self.assertEqual(expected, actual,\n                             msg=get_conversion_error_msg(convertible, expected, actual))", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "validation", "example_id": "002261", "source": "def compute_bench(samples_range, features_range, n_iter=3, rank=50):\n    it = 0\n    results = defaultdict(lambda: [])\n    max_it = len(samples_range) * len(features_range)\n    for n_samples in samples_range:\n        for n_features in features_range:\n            it += 1\n            print(\"====================\")\n            print(\"Iteration %03d of %03d\" % (it, max_it))\n            print(\"====================\")\n            X = make_low_rank_matrix(\n                n_samples, n_features, effective_rank=rank, tail_strength=0.2\n            )\n            gc.collect()\n            print(\"benchmarking scipy svd: \")\n            tstart = time()\n            svd(X, full_matrices=False)\n            results[\"scipy svd\"].append(time() - tstart)\n            gc.collect()\n            print(\"benchmarking scikit-learn randomized_svd: n_iter=0\")\n            tstart = time()\n            randomized_svd(X, rank, n_iter=0)\n            results[\"scikit-learn randomized_svd (n_iter=0)\"].append(time() - tstart)\n            gc.collect()\n            print(\"benchmarking scikit-learn randomized_svd: n_iter=%d \" % n_iter)\n            tstart = time()\n            randomized_svd(X, rank, n_iter=n_iter)\n            results[\"scikit-learn randomized_svd (n_iter=%d)\" % n_iter].append(\n                time() - tstart\n            )\n    return results", "target": "def compute_bench(samples_range, features_range):\n    it = 0\n    results = dict()\n    lars = np.empty((len(features_range), len(samples_range)))\n    lars_gram = lars.copy()\n    omp = lars.copy()\n    omp_gram = lars.copy()\n    max_it = len(samples_range) * len(features_range)\n    for i_s, n_samples in enumerate(samples_range):\n        for i_f, n_features in enumerate(features_range):\n            it += 1\n            n_informative = n_features // 10\n            print(\"====================\")\n            print(\"Iteration %03d of %03d\" % (it, max_it))\n            print(\"====================\")\n            dataset_kwargs = {\n                \"n_samples\": 1,\n                \"n_components\": n_features,\n                \"n_features\": n_samples,\n                \"n_nonzero_coefs\": n_informative,\n                \"random_state\": 0,\n            }\n            print(\"n_samples: %d\" % n_samples)\n            print(\"n_features: %d\" % n_features)\n            y, X, _ = make_sparse_coded_signal(**dataset_kwargs)\n            X = np.asfortranarray(X.T)\n            gc.collect()\n            print(\"benchmarking lars_path (with Gram):\", end=\"\")\n            sys.stdout.flush()\n            tstart = time()\n            G = np.dot(X.T, X)\n            Xy = np.dot(X.T, y)\n            lars_path_gram(Xy=Xy, Gram=G, n_samples=y.size, max_iter=n_informative)\n            delta = time() - tstart\n            print(\"%0.3fs\" % delta)\n            lars_gram[i_f, i_s] = delta\n            gc.collect()\n            print(\"benchmarking lars_path (without Gram):\", end=\"\")\n            sys.stdout.flush()\n            tstart = time()\n            lars_path(X, y, Gram=None, max_iter=n_informative)\n            delta = time() - tstart\n            print(\"%0.3fs\" % delta)\n            lars[i_f, i_s] = delta\n            gc.collect()\n            print(\"benchmarking orthogonal_mp (with Gram):\", end=\"\")\n            sys.stdout.flush()\n            tstart = time()\n            orthogonal_mp(X, y, precompute=True, n_nonzero_coefs=n_informative)\n            delta = time() - tstart\n            print(\"%0.3fs\" % delta)\n            omp_gram[i_f, i_s] = delta\n            gc.collect()\n            print(\"benchmarking orthogonal_mp (without Gram):\", end=\"\")\n            sys.stdout.flush()\n            tstart = time()\n            orthogonal_mp(X, y, precompute=False, n_nonzero_coefs=n_informative)\n            delta = time() - tstart\n            print(\"%0.3fs\" % delta)\n            omp[i_f, i_s] = delta\n    results[\"time(LARS) / time(OMP)\\n (w/ Gram)\"] = lars_gram / omp_gram\n    results[\"time(LARS) / time(OMP)\\n (w/o Gram)\"] = lars / omp\n    return results", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "validation", "example_id": "002262", "source": "def getInfoPlist(self, builddirs):\n        return os.path.join(builddirs[0], \"visionos\", \"Info.plist\")", "target": "def test_non_model_field_wrap_validator_field_info() -> None:\n    class Model:\n        __slots__ = '__dict__', '__pydantic_fields_set__', '__pydantic_extra__', '__pydantic_private__'\n        x: str\n    def f(input_value: Any, val: core_schema.ValidatorFunctionWrapHandler, info: core_schema.ValidationInfo) -> Any:\n        assert info.field_name == 'x'\n        assert info.data == {}\n        return f'input: {val(input_value)}'\n    v = SchemaValidator(\n        core_schema.model_schema(\n            Model,\n            core_schema.model_fields_schema(\n                {\n                    'x': core_schema.model_field(\n                        core_schema.with_info_wrap_validator_function(f, core_schema.str_schema())\n                    )\n                }\n            ),\n        )\n    )\n    assert v.validate_python({'x': b'foo'}).x == 'input: foo'", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|scikit-learn/scikit-learn", "data_split": "validation", "example_id": "002263", "source": "def required_definition_imports(self) -> Generator[str, None, None]:\n        return self.value.required_usage_imports", "target": "def make_estimator(self, params):\n        representation, precompute = params\n        estimator = Lasso(precompute=precompute, alpha=0.001, random_state=0)\n        return estimator", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "002264", "source": "def test_constraints_schema_validation() -> None:\n    val = SchemaValidator(cs.datetime_schema(gt='2020-01-01T00:00:00'))\n    with pytest.raises(ValidationError):\n        val.validate_python('2019-01-01T00:00:00')", "target": "def test_constraints_schema_validation() -> None:\n    val = SchemaValidator(cs.int_schema(gt='1'))\n    with pytest.raises(ValidationError):\n        val.validate_python('0')", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pytorch/pytorch", "data_split": "validation", "example_id": "002265", "source": "def test_fast(self):\n        fd = cv.FastFeatureDetector_create(30, True)\n        img = self.get_sample(\"samples/data/right02.jpg\", 0)\n        img = cv.medianBlur(img, 3)\n        keypoints = fd.detect(img)\n        self.assertTrue(600 <= len(keypoints) <= 700)\n        for kpt in keypoints:\n            self.assertNotEqual(kpt.response, 0)", "target": "def _init_device(self) -> None:\n        device_module.set_device(self.device)\n        symm_mem.set_backend(\"NVSHMEM\")", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "validation", "example_id": "002266", "source": "def pick_grad(self, name, is_training):\n        if is_training:\n            return torch.enable_grad()\n        else:\n            return torch.no_grad()", "target": "def test_decimal_multiple_of(py_and_json: PyAndJson, multiple_of: float, input_value: float, error: Err | None):\n    v = py_and_json({'type': 'decimal', 'multiple_of': Decimal(str(multiple_of))})\n    if error:\n        with pytest.raises(ValidationError, match=re.escape(error.message)):\n            v.validate_test(input_value)\n    else:\n        output = v.validate_test(input_value)\n        assert output == Decimal(str(input_value))\n        assert isinstance(output, Decimal)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|opencv/opencv", "data_split": "validation", "example_id": "002267", "source": "def broadcast_narrow_57611():\n    return (rand(1, 32, 32, 2), rand(1024, 1, 1, 2))", "target": "def test_comparison(self):\n            data = np.ones((10, 10, 3))\n            mat_wrapped = cv.Mat(data, wrap_channels=True)\n            mat_simple = cv.Mat(data)\n            np.testing.assert_equal(mat_wrapped, mat_simple)\n            np.testing.assert_equal(data, mat_simple)\n            np.testing.assert_equal(data, mat_wrapped)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "validation", "example_id": "002268", "source": "def quicklint(ctx, apply_patches, **kwargs):\n    ctx.invoke(lazy_setup_lint)\n    cmd = LINTRUNNER_BASE_CMD\n    if apply_patches:\n        cmd += [\"--apply-patches\"]\n    spin.util.run(cmd)", "target": "def apply_lints(filename):\n    patch = json.loads(subprocess.check_output([sys.executable, CSV_LINTER, filename]))\n    if patch.get(\"replacement\"):\n        with open(filename) as fd:\n            data = fd.read().replace(patch[\"original\"], patch[\"replacement\"])\n        with open(filename, \"w\") as fd:\n            fd.write(data)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "validation", "example_id": "002269", "source": "def required_definition_imports(self) -> Generator[str, None, None]:\n        for item in self:\n            yield from item.required_definition_imports", "target": "def required_definition_imports(self) -> Generator[str, None, None]:\n        yield \"import typing as _typing\"\n        yield from super().required_definition_imports", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|scikit-learn/scikit-learn", "data_split": "validation", "example_id": "002270", "source": "def unzip_columns(mat):\n        assert isinstance(mat, list)\n        assert isinstance(mat[0], list)\n        layers = len(mat)\n        columns = len(mat[0])\n        return [[mat[layer][col] for layer in range(layers)] for col in range(columns)]", "target": "def setup_loading_other_datasets():\n    try:\n        import pandas\n    except ImportError:\n        raise SkipTest(\"Skipping loading_other_datasets.rst, pandas not installed\")\n    run_network_tests = environ.get(\"SKLEARN_SKIP_NETWORK_TESTS\", \"1\") == \"0\"\n    if not run_network_tests:\n        raise SkipTest(\n            \"Skipping loading_other_datasets.rst, tests can be \"\n            \"enabled by setting SKLEARN_SKIP_NETWORK_TESTS=0\"\n        )", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "validation", "example_id": "002271", "source": "def test_format_error():\n    s = SchemaSerializer(core_schema.any_schema(serialization=core_schema.format_ser_schema('^5d')))\n    assert s.to_python(123) == 123\n    msg = \"Error calling `format(value, '^5d')`: ValueError:\"\n    with pytest.raises(PydanticSerializationError, match=re.escape(msg)):\n        s.to_python('x', mode='json')\n    with pytest.raises(PydanticSerializationError, match=re.escape(msg)):\n        s.to_json('x')", "target": "def outMeta(desc, max_corners, quality_lvl,\n                    min_distance, block_sz,\n                    use_harris_detector, k):\n            return cv.empty_array_desc()", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "002272", "source": "def get_memory_bytes(self, args, kwargs) -> int:\n        x, target, dloss = args\n        M, N = x.shape\n        return (\n            2 * M * N * x.dtype.itemsize\n            + M * target.dtype.itemsize\n            + M * dloss.dtype.itemsize\n        )", "target": "def get_memory_bytes(self, args, kwargs) -> int:\n        x, w = args\n        M, N = x.shape\n        return 2 * M * N * x.dtype.itemsize + N * w.dtype.itemsize", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "validation", "example_id": "002273", "source": "def test_simple(self):\n        img1 = self.get_sample('stitching/a1.png')\n        img2 = self.get_sample('stitching/a2.png')\n        stitcher = cv.Stitcher.create(cv.Stitcher_PANORAMA)\n        (_result, pano) = stitcher.stitch((img1, img2))\n        self.assertAlmostEqual(pano.shape[0], 685, delta=100, msg=\"rows: %r\" % list(pano.shape))\n        self.assertAlmostEqual(pano.shape[1], 1025, delta=100, msg=\"cols: %r\" % list(pano.shape))", "target": "def test_simple(self):\n        img1 = self.get_sample('stitching/a1.png')\n        img2 = self.get_sample('stitching/a2.png')\n        stitcher = cv.Stitcher.create(cv.Stitcher_PANORAMA)\n        stitcher.estimateTransform((img1, img2))\n        result, _ = stitcher.composePanorama((img1, img2))\n        assert result == 0", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "002274", "source": "def constrain(self: _Pipeline[_InT, _OutT], constraint: annotated_types.Predicate) -> _Pipeline[_InT, _OutT]: ...", "target": "def constrain(self: _Pipeline[_InT, _OutT], constraint: _Eq) -> _Pipeline[_InT, _OutT]: ...", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "validation", "example_id": "002275", "source": "def typename(self) -> str:\n            return self.type_node.full_typename", "target": "def typename(self) -> str:\n        return self._export_name", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "validation", "example_id": "002276", "source": "def outMeta(arr_desc0, arr_desc1, arr_desc2):\n        return cv.empty_array_desc(), cv.empty_array_desc(), \\\n               cv.empty_array_desc(), cv.empty_array_desc()", "target": "def outMeta(desc, max_corners, quality_lvl,\n                    min_distance, block_sz,\n                    use_harris_detector, k):\n            return cv.empty_array_desc()", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pytorch/pytorch", "data_split": "validation", "example_id": "002277", "source": "def rm_one(d):\n    d = str(d)\n    d = os.path.abspath(d)\n    if os.path.exists(d):\n        if os.path.isdir(d):\n            log.info(\"Removing dir: %s\", d)\n            shutil.rmtree(d)\n        elif os.path.isfile(d):\n            log.info(\"Removing file: %s\", d)\n            os.remove(d)", "target": "def main():\n    omp_max_threads = get_gomp_thread()\n    print(\n        f\"omp_max_threads after loading libgomp.so and libtorch_cpu.so: {omp_max_threads}\"\n    )\n    if omp_max_threads == 1:\n        raise RuntimeError(\n            \"omp_max_threads is 1. Check whether libgomp.so is loaded twice.\"\n        )", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "002278", "source": "def double_or_bust(input_value):\n    if input_value == 1:\n        raise RuntimeError('bust')\n    return input_value * 2", "target": "def double_or_bust(input_value, info):\n    if input_value == 1:\n        raise RuntimeError('bust')\n    return input_value * 2", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "validation", "example_id": "002279", "source": "def compute_bench(samples_range, features_range):\n    it = 0\n    results = defaultdict(lambda: [])\n    max_it = len(samples_range) * len(features_range)\n    for n_samples in samples_range:\n        for n_features in features_range:\n            it += 1\n            print(\"====================\")\n            print(\"Iteration %03d of %03d\" % (it, max_it))\n            print(\"====================\")\n            dataset_kwargs = {\n                \"n_samples\": n_samples,\n                \"n_features\": n_features,\n                \"n_informative\": n_features // 10,\n                \"effective_rank\": min(n_samples, n_features) / 10,\n                \"bias\": 0.0,\n            }\n            print(\"n_samples: %d\" % n_samples)\n            print(\"n_features: %d\" % n_features)\n            X, y = make_regression(**dataset_kwargs)\n            gc.collect()\n            print(\"benchmarking lars_path (with Gram):\", end=\"\")\n            sys.stdout.flush()\n            tstart = time()\n            G = np.dot(X.T, X)\n            Xy = np.dot(X.T, y)\n            lars_path_gram(Xy=Xy, Gram=G, n_samples=y.size, method=\"lasso\")\n            delta = time() - tstart\n            print(\"%0.3fs\" % delta)\n            results[\"lars_path (with Gram)\"].append(delta)\n            gc.collect()\n            print(\"benchmarking lars_path (without Gram):\", end=\"\")\n            sys.stdout.flush()\n            tstart = time()\n            lars_path(X, y, method=\"lasso\")\n            delta = time() - tstart\n            print(\"%0.3fs\" % delta)\n            results[\"lars_path (without Gram)\"].append(delta)\n            gc.collect()\n            print(\"benchmarking lasso_path (with Gram):\", end=\"\")\n            sys.stdout.flush()\n            tstart = time()\n            lasso_path(X, y, precompute=True)\n            delta = time() - tstart\n            print(\"%0.3fs\" % delta)\n            results[\"lasso_path (with Gram)\"].append(delta)\n            gc.collect()\n            print(\"benchmarking lasso_path (without Gram):\", end=\"\")\n            sys.stdout.flush()\n            tstart = time()\n            lasso_path(X, y, precompute=False)\n            delta = time() - tstart\n            print(\"%0.3fs\" % delta)\n            results[\"lasso_path (without Gram)\"].append(delta)\n    return results", "target": "def compute_bench(samples_range, features_range):\n    it = 0\n    results = dict()\n    lars = np.empty((len(features_range), len(samples_range)))\n    lars_gram = lars.copy()\n    omp = lars.copy()\n    omp_gram = lars.copy()\n    max_it = len(samples_range) * len(features_range)\n    for i_s, n_samples in enumerate(samples_range):\n        for i_f, n_features in enumerate(features_range):\n            it += 1\n            n_informative = n_features // 10\n            print(\"====================\")\n            print(\"Iteration %03d of %03d\" % (it, max_it))\n            print(\"====================\")\n            dataset_kwargs = {\n                \"n_samples\": 1,\n                \"n_components\": n_features,\n                \"n_features\": n_samples,\n                \"n_nonzero_coefs\": n_informative,\n                \"random_state\": 0,\n            }\n            print(\"n_samples: %d\" % n_samples)\n            print(\"n_features: %d\" % n_features)\n            y, X, _ = make_sparse_coded_signal(**dataset_kwargs)\n            X = np.asfortranarray(X.T)\n            gc.collect()\n            print(\"benchmarking lars_path (with Gram):\", end=\"\")\n            sys.stdout.flush()\n            tstart = time()\n            G = np.dot(X.T, X)\n            Xy = np.dot(X.T, y)\n            lars_path_gram(Xy=Xy, Gram=G, n_samples=y.size, max_iter=n_informative)\n            delta = time() - tstart\n            print(\"%0.3fs\" % delta)\n            lars_gram[i_f, i_s] = delta\n            gc.collect()\n            print(\"benchmarking lars_path (without Gram):\", end=\"\")\n            sys.stdout.flush()\n            tstart = time()\n            lars_path(X, y, Gram=None, max_iter=n_informative)\n            delta = time() - tstart\n            print(\"%0.3fs\" % delta)\n            lars[i_f, i_s] = delta\n            gc.collect()\n            print(\"benchmarking orthogonal_mp (with Gram):\", end=\"\")\n            sys.stdout.flush()\n            tstart = time()\n            orthogonal_mp(X, y, precompute=True, n_nonzero_coefs=n_informative)\n            delta = time() - tstart\n            print(\"%0.3fs\" % delta)\n            omp_gram[i_f, i_s] = delta\n            gc.collect()\n            print(\"benchmarking orthogonal_mp (without Gram):\", end=\"\")\n            sys.stdout.flush()\n            tstart = time()\n            orthogonal_mp(X, y, precompute=False, n_nonzero_coefs=n_informative)\n            delta = time() - tstart\n            print(\"%0.3fs\" % delta)\n            omp[i_f, i_s] = delta\n    results[\"time(LARS) / time(OMP)\\n (w/ Gram)\"] = lars_gram / omp_gram\n    results[\"time(LARS) / time(OMP)\\n (w/o Gram)\"] = lars / omp\n    return results", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "validation", "example_id": "002280", "source": "def test_function_before():\n    def f(input_value, _info):\n        assert isinstance(input_value, dict)\n        input_value['field_a'] += b' XX'\n        return input_value\n    v = SchemaValidator(\n        {\n            'type': 'function-before',\n            'function': {'type': 'with-info', 'function': f},\n            'schema': core_schema.model_schema(\n                cls=MyModel,\n                schema=core_schema.model_fields_schema(\n                    fields={\n                        'field_a': core_schema.model_field(schema=core_schema.str_schema()),\n                        'field_b': core_schema.model_field(schema=core_schema.int_schema()),\n                    }\n                ),\n            ),\n        }\n    )\n    m = v.validate_python({'field_a': b'321', 'field_b': '12'})\n    assert isinstance(m, MyModel)\n    assert m.field_a == '321 XX'\n    assert m.field_b == 12\n    m2 = MyModel()\n    v.validate_python({'field_a': b'321', 'field_b': '12'}, self_instance=m2)\n    assert m2.__dict__ == {'field_a': '321 XX', 'field_b': 12}\n    assert m2.__pydantic_fields_set__ == {'field_a', 'field_b'}", "target": "def config(self):\n        cmd = self.get_cmake_cmd()\n        cmd.append(self.opencv_dir)\n        execute(cmd)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "002281", "source": "def test_function_plain_field_serializer_to_python():\n    @dataclasses.dataclass\n    class Model:\n        x: int\n        def ser_x(self, v: Any, _) -> str:\n            assert self.x == 1_000\n            return f'{v:_}'\n    s = SchemaSerializer(\n        core_schema.model_schema(\n            Model,\n            core_schema.model_fields_schema(\n                {\n                    'x': core_schema.model_field(\n                        core_schema.int_schema(\n                            serialization=core_schema.plain_serializer_function_ser_schema(\n                                Model.ser_x, is_field_serializer=True, info_arg=True\n                            )\n                        )\n                    )\n                }\n            ),\n        )\n    )\n    assert s.to_python(Model(x=1000)) == {'x': '1_000'}", "target": "def test_function_plain_field_serializer_to_python():\n    class Model(TypedDict):\n        x: int\n    def ser_x(data: Model, v: Any, _) -> str:\n        assert data['x'] == 1_000\n        return f'{v:_}'\n    s = SchemaSerializer(\n        core_schema.typed_dict_schema(\n            {\n                'x': core_schema.typed_dict_field(\n                    core_schema.int_schema(\n                        serialization=core_schema.plain_serializer_function_ser_schema(\n                            ser_x, is_field_serializer=True, info_arg=True\n                        )\n                    )\n                )\n            }\n        )\n    )\n    assert s.to_python(Model(x=1000)) == {'x': '1_000'}", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "validation", "example_id": "002282", "source": "def test_dict_repeat():\n    v = SchemaValidator(\n        core_schema.definitions_schema(\n            core_schema.dict_schema(\n                core_schema.definition_reference_schema('foobar'), core_schema.definition_reference_schema('foobar')\n            ),\n            [core_schema.int_schema(ref='foobar')],\n        )\n    )\n    assert v.validate_python({'1': '2', 3: '4'}) == {1: 2, 3: 4}\n    assert v.validate_json(b'{\"1\": 2, \"3\": \"4\"}') == {1: 2, 3: 4}", "target": "def query_job_sha(repo, sha):\n    params = {\n        \"queryVariables\": {\"sha\": sha, \"repo\": repo},\n    }\n    KEY_ID = os.environ[\"CH_KEY_ID\"]\n    KEY_SECRET = os.environ[\"CH_KEY_SECRET\"]\n    r = requests.post(\n        url=ARTIFACTS_QUERY_URL,\n        data=json.dumps(params),\n        headers={\"Content-Type\": \"application/json\"},\n        auth=(KEY_ID, KEY_SECRET),\n    )\n    return r.json()[\"data\"]", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "002283", "source": "def area(self) -> bytes:\n            a = self.width * self.height\n            return b'%d' % a", "target": "def area(self) -> int:\n            return self.width * self.height", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "validation", "example_id": "002284", "source": "def general_wrap_validator_function(*args, **kwargs):\n    warnings.warn(\n        '`general_wrap_validator_function` is deprecated, use `with_info_wrap_validator_function` instead.',\n        DeprecationWarning,\n    )\n    return with_info_wrap_validator_function(*args, **kwargs)", "target": "def get_template_instantiation_type(typename: str) -> str:\n    _, args = replace_template_parameters_with_placeholders(typename)\n    if len(args) == 0:\n        raise ValueError(\n            \"typename ('{}') doesn't contain template instantiations\".format(typename)\n        )\n    if len(args) > 1:\n        raise ValueError(\n            \"typename ('{}') contains more than 1 template instantiation\".format(typename)\n        )\n    return args[0]", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "validation", "example_id": "002285", "source": "def test_numpy_writeable_flag_is_preserved(self):\n        array = np.zeros((10, 10, 1), dtype=np.uint8)\n        array.setflags(write=False)\n        with self.assertRaises(Exception):\n            cv.rectangle(array, (0, 0), (5, 5), (255), 2)", "target": "def ip_v6_address_validator(input_value: Any, /) -> IPv6Address:\n    if isinstance(input_value, IPv6Address):\n        return input_value\n    try:\n        return IPv6Address(input_value)\n    except ValueError:\n        raise PydanticCustomError('ip_v6_address', 'Input is not a valid IPv6 address')", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pytorch/pytorch", "data_split": "validation", "example_id": "002286", "source": "def create_type_node(typename: str,\n                     original_ctype_name: Optional[str] = None) -> TypeNode:\n    if original_ctype_name is None:\n        original_ctype_name = typename\n    typename = normalize_ctype_name(typename.strip())\n    type_node = PREDEFINED_TYPES.get(typename)\n    if type_node is not None:\n        type_node.ctype_name = original_ctype_name\n        return type_node\n    for alias in PREDEFINED_TYPES.values():\n        if alias.typename == typename:\n            return alias\n    if is_union_type(typename):\n        union_types = get_template_instantiation_type(typename)\n        return UnionTypeNode(\n            original_ctype_name,\n            items=create_type_nodes_from_template_arguments(union_types)\n        )\n    if is_sequence_type(typename):\n        if _is_template_instantiation(typename):\n            inner_sequence_type = create_type_node(\n                get_template_instantiation_type(typename)\n            )\n        else:\n            inner_sequence_type = create_type_node(typename.split(\"_\", 1)[-1])\n        return SequenceTypeNode(original_ctype_name, inner_sequence_type)\n    if is_tuple_type(typename):\n        tuple_types = get_template_instantiation_type(typename)\n        return TupleTypeNode(\n            original_ctype_name,\n            items=create_type_nodes_from_template_arguments(tuple_types)\n        )\n    return ASTNodeTypeNode(original_ctype_name, typename)", "target": "def filter_ciflow_tags(tags: set[str]) -> list[str]:\n    \"Return sorted list of ciflow tags\"\n    return sorted(\n        tag[:-2] for tag in tags if tag.startswith(\"ciflow/\") and tag.endswith(\"/*\")\n    )", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|scikit-learn/scikit-learn", "data_split": "validation", "example_id": "002287", "source": "def setInitialRect(self, rect):\n        self.initialRect = rect", "target": "def make_data(self, params):\n        data = _synth_classification_dataset(\n            n_samples=10000, n_features=100, n_classes=5\n        )\n        return data", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|opencv/opencv", "data_split": "validation", "example_id": "002288", "source": "def clear_tmp():\n    path = Path(__file__).resolve().parent / \"cache\" / \"tmp\"\n    for child in path.iterdir():\n        child.unlink()", "target": "def test_age_gender_infer_batch(self):\n            skip_if_openvino_not_available()\n            root_path  = '/omz_intel_models/intel/age-gender-recognition-retail-0013/FP32/age-gender-recognition-retail-0013'\n            model_path = self.find_file(root_path + '.xml',   [os.environ.get('OPENCV_DNN_TEST_DATA_PATH')], required=False)\n            bin_path   = self.find_file(root_path + '.bin',   [os.environ.get('OPENCV_DNN_TEST_DATA_PATH')], required=False)\n            device_id  = 'CPU'\n            img_path1 = self.find_file('cv/face/david1.jpg', [os.environ.get('OPENCV_TEST_DATA_PATH')])\n            img_path2 = self.find_file('cv/face/david2.jpg', [os.environ.get('OPENCV_TEST_DATA_PATH')])\n            img1 = cv.imread(img_path1)\n            img2 = cv.imread(img_path2)\n            batch_img = np.array([img1, img2])\n            def preproc(ppp):\n                ppp.input().model().set_layout(Layout(\"NCHW\"))\n                ppp.input().tensor().set_element_type(Type.u8)                              \\\n                                    .set_spatial_static_shape(img1.shape[0], img2.shape[1]) \\\n                                    .set_layout(Layout(\"NHWC\"))\n                ppp.input().preprocess().resize(ResizeAlgorithm.RESIZE_LINEAR)\n            ref = AgeGenderOV(model_path, bin_path, device_id)\n            ref.reshape(PartialShape([2, 3, 62, 62]))\n            ref.cfgPrePostProcessing(preproc)\n            ov_age, ov_gender = ref.apply(batch_img)\n            comp = AgeGenderGAPI(model_path, bin_path, device_id)\n            comp.pp.cfgReshape([2, 3, 62, 62])   \\\n                   .cfgInputModelLayout(\"NCHW\")  \\\n                   .cfgInputTensorLayout(\"NHWC\") \\\n                   .cfgResize(cv.INTER_LINEAR)\n            gapi_age, gapi_gender = comp.apply(batch_img)\n            self.assertEqual(0.0, cv.norm(ov_gender, gapi_gender, cv.NORM_INF))\n            self.assertEqual(0.0, cv.norm(ov_age, gapi_age, cv.NORM_INF))", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "validation", "example_id": "002289", "source": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "target": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "002290", "source": "def test_function_wrap_field_serializer_to_json():\n    class Model(RootModel):\n        def ser_root(self, v: Any, serializer: core_schema.SerializerFunctionWrapHandler, _) -> str:\n            assert self.root == 1_000\n            root = serializer(v)\n            return f'{root:_}'\n    s = SchemaSerializer(\n        core_schema.model_schema(\n            Model,\n            core_schema.int_schema(\n                serialization=core_schema.wrap_serializer_function_ser_schema(\n                    Model.ser_root, is_field_serializer=True, info_arg=True, schema=core_schema.any_schema()\n                )\n            ),\n            root_model=True,\n        )\n    )\n    assert json.loads(s.to_json(Model(1000))) == '1_000'", "target": "def test_function_wrap_field_serializer_to_json():\n    class Model(TypedDict):\n        x: int\n    def ser_x(\n        data: Model,\n        v: Any,\n        serializer: core_schema.SerializerFunctionWrapHandler,\n        info: core_schema.FieldSerializationInfo,\n    ) -> str:\n        assert data['x'] == 1_000\n        x = serializer(v)\n        return f'{x:_}-{info.field_name}'\n    s = SchemaSerializer(\n        core_schema.typed_dict_schema(\n            {\n                'x': core_schema.typed_dict_field(\n                    core_schema.int_schema(\n                        serialization=core_schema.wrap_serializer_function_ser_schema(\n                            ser_x, is_field_serializer=True, info_arg=True, schema=core_schema.any_schema()\n                        )\n                    )\n                )\n            }\n        )\n    )\n    assert json.loads(s.to_json(Model(x=1000))) == {'x': '1_000-x'}", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "validation", "example_id": "002291", "source": "def test_long_python_inequality():\n    v = SchemaValidator(cs.int_schema(gt=0, lt=int('1' * 4_300) - 5))\n    s = str(int('1' * 4_300) - 6)\n    s = v.validate_python(s)\n    assert s == int('1' * 4_300) - 6\n    s = str(int('1' * 4_300) - 5)\n    with pytest.raises(ValidationError, match='Input should be less than 1'):\n        v.validate_python(s)", "target": "def with_params_help(params_cls: type, title: str = \"Parameter defaults\"):\n    if not is_dataclass(params_cls):\n        raise TypeError(f\"{params_cls} must be a dataclass\")\n    def _decorator(cls: type) -> type:\n        block = generate_dataclass_help(params_cls)\n        cls.__doc__ = (cls.__doc__ or \"\") + f\"\\n\\n{title}:\\n{block}\"\n        return cls\n    return _decorator", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "validation", "example_id": "002292", "source": "def wrapper_function(*args, **kwargs):\n            return wrapper(*args, **kwargs)", "target": "def filter_requires_grad(tensors):\n    return [t for t in tensors if t.requires_grad]", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|opencv/opencv", "data_split": "validation", "example_id": "002293", "source": "def fetch_jobs(url: str, headers: dict[str, str]) -> list[dict[str, str]]:\n    response, links = fetch_url(url, headers=headers, reader=parse_json_and_links)\n    jobs = response[\"jobs\"]\n    assert type(jobs) is list\n    while \"next\" in links.keys():\n        response, links = fetch_url(\n            links[\"next\"][\"url\"], headers=headers, reader=parse_json_and_links\n        )\n        jobs.extend(response[\"jobs\"])\n    return jobs", "target": "def getNextFrame(self):\n        img = self.sceneBg.copy()\n        if self.foreground is not None:\n            self.currentCenter = (self.center[0] + self.getXOffset(self.time), self.center[1] + self.getYOffset(self.time))\n            img[self.currentCenter[0]:self.currentCenter[0]+self.foreground.shape[0],\n             self.currentCenter[1]:self.currentCenter[1]+self.foreground.shape[1]] = self.foreground\n        else:\n            self.currentRect = self.initialRect + int( 30*cos(self.time) + 50*sin(self.time/3))\n            if self.deformation:\n                self.currentRect[1:3] += int(self.h/20*cos(self.time))\n            cv.fillConvexPoly(img, self.currentRect, (0, 0, 255))\n        self.time += self.timeStep\n        if self.noise:\n            noise = np.zeros(self.sceneBg.shape, np.int8)\n            cv.randn(noise, np.zeros(3), np.ones(3)*255*self.noise)\n            img = cv.add(img, noise, dtype=cv.CV_8UC3)\n        return img", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "validation", "example_id": "002294", "source": "def listBoolConjunction(self, input: list[bool]) -> bool:\n        res = True\n        for x in input:\n            res = res and x\n        return res", "target": "def test_custom_ser():\n    s = SchemaSerializer(\n        core_schema.definitions_schema(\n            core_schema.definition_reference_schema('Branch'),\n            [\n                core_schema.typed_dict_schema(\n                    {\n                        'name': core_schema.typed_dict_field(core_schema.str_schema()),\n                        'sub_branch': core_schema.typed_dict_field(\n                            core_schema.nullable_schema(\n                                core_schema.definition_reference_schema(\n                                    'Branch', serialization=core_schema.to_string_ser_schema(when_used='always')\n                                )\n                            )\n                        ),\n                    },\n                    ref='Branch',\n                )\n            ],\n        )\n    )\n    assert s.to_python({'name': 'root', 'sub_branch': {'name': 'branch', 'sub_branch': None}}) == {\n        'name': 'root',\n        'sub_branch': \"{'name': 'branch', 'sub_branch': None}\",\n    }", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "validation", "example_id": "002295", "source": "def required_usage_imports(self) -> Generator[str, None, None]:\n        yield \"import typing as _typing\"", "target": "def required_usage_imports(self) -> Generator[str, None, None]:\n        yield \"import typing as _typing\"\n        yield from super().required_usage_imports", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "validation", "example_id": "002296", "source": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "target": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "validation", "example_id": "002297", "source": "def f(input_value, info):\n        nonlocal f_kwargs\n        f_kwargs = deepcopy_info(info)\n        return input_value + ' Changed'", "target": "def create_class_node(root: NamespaceNode, class_info,\n                      namespaces: Sequence[str]) -> ClassNode:\n    symbol_name = SymbolName.parse(class_info.full_original_name, namespaces)\n    scope = find_scope(root, symbol_name)\n    return create_class_node_in_scope(scope, symbol_name, class_info)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "validation", "example_id": "002298", "source": "def type_format(self) -> str:\n        return \"_typing.Sequence[{}]\"", "target": "def type_format(self) -> str:\n        if TypeNode.compatible_to_runtime_usage:\n            return \"_typing.Union[{}]\"\n        return \"{}\"", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "validation", "example_id": "002299", "source": "def description(self):\n        return \"partitioner benchmark 1 input and 100 weights, mix of recompute and non-recompute ops\"", "target": "def f(input_value):\n        return input_value * 2", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "002300", "source": "def test_dict_keys():\n    def fmt(value, _info):\n        return f'<{value}>'\n    s = SchemaSerializer(\n        core_schema.dict_schema(\n            core_schema.int_schema(serialization=core_schema.plain_serializer_function_ser_schema(fmt, info_arg=True))\n        )\n    )\n    assert s.to_python({1: True}) == {'<1>': True}", "target": "def test_dict_keys():\n    s = SchemaSerializer(\n        core_schema.dict_schema(core_schema.float_schema(serialization=core_schema.format_ser_schema('0.4f')))\n    )\n    assert s.to_python({1: True}, mode='json') == {'1.0000': True}", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "validation", "example_id": "002301", "source": "def types_separator(self) -> str:\n        if TypeNode.compatible_to_runtime_usage:\n            return \", \"\n        return \" | \"", "target": "def types_separator(self) -> str:\n        return \", \"", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "validation", "example_id": "002302", "source": "def execute(cmd, cwd = None, output = None):\n    if not output:\n        print(\"Executing: %s in %s\" % (cmd, cwd), file=sys.stderr)\n        print('Executing: ' + ' '.join(cmd))\n        retcode = check_call(cmd, cwd = cwd)\n        if retcode != 0:\n            raise Exception(\"Child returned:\", retcode)\n    else:\n        with open(output, \"a\") as f:\n            f.flush()\n            p = Popen(cmd, cwd = cwd, stdout = f)\n            os.waitpid(p.pid, 0)", "target": "def execute(cmd, cwd=None, shell=False):\n    try:\n        log.debug(\"Executing: %s\" % cmd)\n        log.info('Executing: ' + ' '.join(cmd))\n        if cwd:\n            log.info(\"    in: %s\" % cwd)\n        retcode = subprocess.call(cmd, shell=shell, cwd=str(cwd) if cwd else None)\n        if retcode < 0:\n            raise Fail(\"Child was terminated by signal: %s\" % -retcode)\n        elif retcode > 0:\n            raise Fail(\"Child returned: %s\" % retcode)\n    except OSError as e:\n        raise Fail(\"Execution failed: %d / %s\" % (e.errno, e.strerror))", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "002303", "source": "def multiple_of(self: _Pipeline[_InT, _NewOutMod], multiple_of: _NewOutMod) -> _Pipeline[_InT, _NewOutMod]: ...", "target": "def multiple_of(self: _Pipeline[_InT, Any], multiple_of: Any) -> _Pipeline[_InT, Any]:\n        return self.constrain(annotated_types.MultipleOf(multiple_of))", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "validation", "example_id": "002304", "source": "def type_format(self) -> str:\n        return \"_typing.Type[{}]\"", "target": "def git_revision(dir: Path) -> str:\n    return subprocess.check_output(['git', 'rev-parse', '--short', 'HEAD'], cwd=dir).decode('utf-8').strip()", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "validation", "example_id": "002305", "source": "def test_downcast_error():\n    v = SchemaValidator(\n        core_schema.tagged_union_schema(discriminator=lambda x: 123, choices={'str': core_schema.str_schema()})\n    )\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python('x')\n        assert exc_info.value.errors(include_url=False) == [\n            {\n                'type': 'union_tag_invalid',\n                'loc': (),\n                'msg': \"Input tag '123' found using <lambda>() does not match any of the expected tags: 'str'\",\n                'input': 'x',\n            }\n        ]", "target": "def getSourceKitten(self):\n        ret = check_output([\"gem\", \"which\", \"jazzy\"])\n        if ret.find('ERROR:') == 0:\n            raise Exception(\"Failed to find jazzy\")\n        else:\n            return os.path.join(ret[0:ret.rfind('/')], '../bin/sourcekitten')", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "002306", "source": "def test_generator_error():\n    def gen(error: bool):\n        yield 1\n        yield 2\n        if error:\n            raise RuntimeError('error')\n        yield 3\n    v = SchemaValidator(cs.list_schema(items_schema=cs.int_schema()))\n    assert v.validate_python(gen(False)) == [1, 2, 3]\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python(gen(True))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'iteration_error',\n            'loc': (2,),\n            'msg': 'Error iterating over object, error: RuntimeError: error',\n            'input': HasRepr(IsStr(regex='<generator object test_generator_error.<locals>.gen at 0x[0-9a-fA-F]+>')),\n            'ctx': {'error': 'RuntimeError: error'},\n        }\n    ]", "target": "def test_generator_error():\n    def gen(error: bool):\n        yield 1\n        yield 2\n        if error:\n            raise RuntimeError('error')\n        yield 3\n    v = SchemaValidator(core_schema.tuple_schema(items_schema=[core_schema.int_schema()], variadic_item_index=0))\n    assert v.validate_python(gen(False)) == (1, 2, 3)\n    msg = r'Error iterating over object, error: RuntimeError: error \\[type=iteration_error,'\n    with pytest.raises(ValidationError, match=msg):\n        v.validate_python(gen(True))", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "002307", "source": "def test_isinstance():\n    v = SchemaValidator(cs.int_schema())\n    assert v.validate_python(123) == 123\n    assert v.isinstance_python(123) is True\n    assert v.validate_python('123') == 123\n    assert v.isinstance_python('123') is True\n    with pytest.raises(ValidationError, match='Input should be a valid integer'):\n        v.validate_python('foo')\n    assert v.isinstance_python('foo') is False", "target": "def test_isinstance(py_and_json: PyAndJson):\n    def f(input_value, validator, info):\n        if 'error' in info.context:\n            raise ValueError('wrong')\n        return validator(input_value)\n    v = py_and_json(core_schema.with_info_wrap_validator_function(f, core_schema.str_schema()))\n    assert v.validate_python('foobar', None, {}) == 'foobar'\n    with pytest.raises(TypeError):\n        v.validate_test('foobar')\n    with pytest.raises(TypeError):\n        v.isinstance_test('foobar')\n    with pytest.raises(ValidationError, match=r'Value error, wrong \\[type=value_error,'):\n        v.validate_test('foobar', None, {'error'})\n    assert v.isinstance_test('foobar', None, {}) is True\n    with pytest.raises(TypeError):\n        v.isinstance_test('foobar')\n    assert v.isinstance_test('foobar', None, {'error'}) is False", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "validation", "example_id": "002308", "source": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "target": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "002309", "source": "def dataclass(\n        *,\n        init: Literal[False] = False,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool = False,\n        config: ConfigDict | type[object] | None = None,\n        validate_on_init: bool | None = None,\n        kw_only: bool = ...,\n        slots: bool = ...,\n    ) -> Callable[[type[_T]], type[PydanticDataclass]]:\n        ...", "target": "def dataclass(\n        *,\n        init: Literal[False] = False,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool | None = None,\n        config: ConfigDict | type[object] | None = None,\n        validate_on_init: bool | None = None,\n    ) -> Callable[[type[_T]], type[PydanticDataclass]]:\n        ...", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "validation", "example_id": "002310", "source": "def test_positional_empty(py_and_json: PyAndJson):\n    v = py_and_json({'type': 'tuple', 'items_schema': []})\n    assert v.validate_test([]) == ()\n    assert v.validate_python(()) == ()\n    with pytest.raises(ValidationError, match='type=too_long,'):\n        v.validate_test([1])", "target": "def fallback_func(obj):\n        nonlocal v\n        v += 1\n        return FoobarCount(v)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "validation", "example_id": "002311", "source": "def exec_file_wrapper(fpath, g_vars, l_vars):\n        with open(fpath) as f:\n            code = compile(f.read(), os.path.basename(fpath), 'exec')\n            exec(code, g_vars, l_vars)", "target": "def exec_file_wrapper(fpath, g_vars, l_vars):\n        execfile(fpath, g_vars, l_vars)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "002312", "source": "def _work(self):\n        @torch.compile(\n            backend=self.backend(),\n            fullgraph=True,\n            dynamic=self.is_dynamic(),\n        )\n        def f(a, b):\n            result = a.clone()\n            for i in range(1000):\n                if i % 3 == 0:\n                    result = result + b\n                elif i % 3 == 1:\n                    result = result + 8 * b\n                else:\n                    result = result.sin()\n            return result\n        with fresh_cache():\n            f(self.a, self.b)", "target": "def _work(self):\n        @torch.compile(backend=self.backend(), fullgraph=True)\n        def f(x):\n            tmps = [x + i for i in range(16)]\n            tmps = [x + tmp for tmp in tmps]\n            for i in range(len(tmps) - 4):\n                tmps[i] = tmps[i].sin().mul(tmps[i])\n                tmps[i + 1] -= tmps[i]\n                tmps[i + 2] -= tmps[i]\n                tmps[i + 3] -= tmps[i]\n            return sum(tmps)\n        f(self.x)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|opencv/opencv", "data_split": "validation", "example_id": "002313", "source": "def test_ghstack_branches_in_sync(self) -> None:\n        head_ref = \"gh/SS-JIA/206/head\"\n        self._skip_if_ref_does_not_exist(head_ref)\n        self.assertTrue(are_ghstack_branches_in_sync(self.repo, head_ref))", "target": "def resolve_enum_scopes(root: NamespaceNode,\n                        enums: Dict[SymbolName, EnumerationNode]):\n    for symbol_name, enum_node in enums.items():\n        if symbol_name.classes:\n            try:\n                scope = find_scope(root, symbol_name)\n            except ScopeNotFoundError:\n                for i, class_name in enumerate(symbol_name.classes):\n                    scope = find_scope(root,\n                                       SymbolName(symbol_name.namespaces,\n                                                  classes=symbol_name.classes[:i],\n                                                  name=class_name))\n                    if class_name in scope.classes:\n                        continue\n                    class_node = scope.add_class(class_name)\n                    class_node.is_exported = False\n                scope = find_scope(root, symbol_name)\n        else:\n            scope = find_scope(root, symbol_name)\n        enum_node.parent = scope", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "validation", "example_id": "002314", "source": "def _merge_lastfailed_files(source_pytest_cache: Path, dest_pytest_cache: Path) -> None:\n    source_lastfailed_file = source_pytest_cache / LASTFAILED_FILE_PATH\n    dest_lastfailed_file = dest_pytest_cache / LASTFAILED_FILE_PATH\n    if not source_lastfailed_file.exists():\n        return\n    if not dest_lastfailed_file.exists():\n        copy_file(source_lastfailed_file, dest_lastfailed_file)\n        return\n    from_lastfailed = load_json_file(source_lastfailed_file)\n    to_lastfailed = load_json_file(dest_lastfailed_file)\n    merged_content = _merged_lastfailed_content(from_lastfailed, to_lastfailed)\n    write_json_file(dest_lastfailed_file, merged_content)", "target": "def test_function_call_partial():\n    def my_function(a, b, c):\n        return a + b + c\n    v = SchemaValidator(\n        cs.call_schema(\n            function=partial(my_function, c=3),\n            arguments=cs.arguments_schema(\n                arguments=[\n                    {'name': 'a', 'mode': 'positional_or_keyword', 'schema': cs.int_schema()},\n                    {'name': 'b', 'mode': 'positional_or_keyword', 'schema': cs.int_schema()},\n                ]\n            ),\n        )\n    )\n    assert 'name:\"call[my_function]\"' in plain_repr(v)\n    assert v.validate_python((1, 2)) == 6\n    assert v.validate_python((1, '2')) == 6", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "002315", "source": "def constrain(\n        self: _Pipeline[_InT, _NewOutInterval], constraint: annotated_types.Interval\n    ) -> _Pipeline[_InT, _NewOutInterval]: ...", "target": "def constrain(self: _Pipeline[_InT, _OutT], constraint: _In) -> _Pipeline[_InT, _OutT]: ...", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "validation", "example_id": "002316", "source": "def get_estimator_and_data():\n    if args.problem == \"classification\":\n        X, y = make_classification(\n            args.n_samples * 2,\n            n_features=args.n_features,\n            n_classes=args.n_classes,\n            n_clusters_per_class=1,\n            n_informative=args.n_features // 2,\n            random_state=0,\n        )\n        return X, y, HistGradientBoostingClassifier\n    elif args.problem == \"regression\":\n        X, y = make_regression(\n            args.n_samples_max * 2, n_features=args.n_features, random_state=0\n        )\n        return X, y, HistGradientBoostingRegressor", "target": "def get_estimator_and_data():\n    if args.problem == \"classification\":\n        X, y = make_classification(\n            args.n_samples_max * 2,\n            n_features=args.n_features,\n            n_classes=args.n_classes,\n            n_clusters_per_class=1,\n            n_informative=args.n_classes,\n            random_state=0,\n        )\n        return X, y, HistGradientBoostingClassifier\n    elif args.problem == \"regression\":\n        X, y = make_regression(\n            args.n_samples_max * 2, n_features=args.n_features, random_state=0\n        )\n        return X, y, HistGradientBoostingRegressor", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "002317", "source": "def compiled(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, target = args\n        torch._dynamo.mark_dynamic(x, 0)\n        torch._dynamo.mark_dynamic(target, 0)\n        compiled_cross_entropy = torch.compile(\n            lambda x, target: F.cross_entropy(x, target, reduction=\"none\"),\n            mode=self.compile_mode,\n            fullgraph=True,\n        )\n        return lambda: compiled_cross_entropy(x, target)", "target": "def compiled(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, w = args\n        torch._dynamo.mark_dynamic(x, 0)\n        compiled_layernorm = torch.compile(\n            self.layernorm_ref, mode=self.compile_mode, fullgraph=True\n        )\n        return lambda: compiled_layernorm(x, w, eps=1e-6)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "validation", "example_id": "002318", "source": "def test_uuid_json(value, expected):\n    v = SchemaSerializer(core_schema.uuid_schema())\n    assert v.to_python(value, mode='json') == expected\n    assert v.to_json(value).decode() == f'\"{expected}\"'", "target": "def print_error(text):\n    print(\"=\"*60, file=sys.stderr)\n    print(\"ERROR: %s\" % text, file=sys.stderr)\n    print(\"=\"*60, file=sys.stderr)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "002319", "source": "def compiled(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, target = args\n        torch._dynamo.mark_dynamic(x, 0)\n        torch._dynamo.mark_dynamic(target, 0)\n        compiled_cross_entropy = torch.compile(\n            lambda x, target: F.cross_entropy(x, target, reduction=\"none\"),\n            mode=self.compile_mode,\n            fullgraph=True,\n        )\n        return lambda: compiled_cross_entropy(x, target)", "target": "def compiled(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, dy = args\n        compiled_softmax = torch.compile(\n            lambda x: F.softmax(x, dim=-1), mode=self.compile_mode, fullgraph=True\n        )\n        y = compiled_softmax(x)\n        return lambda: torch.autograd.grad(y, x, grad_outputs=dy, retain_graph=True)", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|opencv/opencv", "data_split": "validation", "example_id": "002320", "source": "def getTest(stests, x, y, row, col):\n    for pair in stests:\n        if pair[1][x] == row and pair[1][y] == col:\n            return pair[0]\n    return None", "target": "def cvt_nv12_to_yuv(self, y, uv):\n            h,w,_ = uv.shape\n            upsample_uv = cv.resize(uv, (h * 2, w * 2))\n            return cv.merge([y, upsample_uv])", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "validation", "example_id": "002321", "source": "def test_validate_strings_forbid_extra_fn_override():\n    v = SchemaValidator(\n        core_schema.typed_dict_schema(\n            {\n                'f': core_schema.typed_dict_field(core_schema.int_schema()),\n            }\n        )\n    )\n    with pytest.raises(ValidationError, match='Extra inputs are not permitted'):\n        v.validate_strings({'f': '1', 'extra_field': '123'}, extra='forbid')", "target": "def forward(\n        self, x: Tensor, input_pos: Tensor, freqs_cis: Tensor, mask: Tensor\n    ) -> Tensor:\n        h = x + self.attention(self.attention_norm(x), freqs_cis, mask, input_pos)\n        out = h + self.feed_forward(self.ffn_norm(h))\n        return out", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "002322", "source": "def attempt_rebuild_fn(attr_fn: Callable[[TypeAdapter], T]) -> Callable[[], T | None]:\n        def handler() -> T | None:\n            if adapter.rebuild(raise_errors=False, _parent_namespace_depth=5) is not False:\n                return attr_fn(adapter)\n            return None\n        return handler", "target": "def attempt_rebuild_fn(attr_fn: Callable[[type[PydanticDataclass]], T]) -> Callable[[], T | None]:\n        def handler() -> T | None:\n            if rebuild_dataclass(cls, raise_errors=False, _parent_namespace_depth=5) is not False:\n                return attr_fn(cls)\n            return None\n        return handler", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "validation", "example_id": "002323", "source": "def generate_experiment_groups(\n    op_names: list[str],\n    shapes: list[tuple[int, int, int]],\n    dtypes: list[torch.dtype],\n    enable_persistent_tma_matmuls: list[bool],\n    cutlass_instantiation_levels: list[str],\n    batch_sizes: list[int],\n) -> list[ExperimentGroupConfig]:\n    groups = []\n    for (\n        op_name,\n        shape,\n        dtype,\n        batch_size,\n    ) in itertools.product(op_names, shapes, dtypes, batch_sizes):\n        group = ExperimentGroupConfig(\n            op_name=op_name,\n            shape=shape,\n            dtype=dtype,\n            batch_size=batch_size,\n        )\n        experiments = generate_experiment_configs(\n            enable_persistent_tma_matmuls, cutlass_instantiation_levels\n        )\n        group.experiments.extend(experiments)\n        groups.append(group)\n    return groups", "target": "def with_params_help(params_cls: type, title: str = \"Parameter defaults\"):\n    if not is_dataclass(params_cls):\n        raise TypeError(f\"{params_cls} must be a dataclass\")\n    def _decorator(cls: type) -> type:\n        block = generate_dataclass_help(params_cls)\n        cls.__doc__ = (cls.__doc__ or \"\") + f\"\\n\\n{title}:\\n{block}\"\n        return cls\n    return _decorator", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "validation", "example_id": "002324", "source": "def peek(self) -> Optional[str]:\n        if self._idx + 1 >= len(self._val):\n            return None\n        return self._val[self._idx + 1]", "target": "def test_constraints_schema_validation() -> None:\n    val = SchemaValidator(core_schema.timedelta_schema(gt=3))\n    with pytest.raises(ValidationError):\n        val.validate_python(1)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "validation", "example_id": "002325", "source": "def test_dataclass_slots_field_after_validator():\n    @dataclasses.dataclass(slots=True)\n    class Foo:\n        a: int\n        b: str\n        @classmethod\n        def validate_b(cls, v: str, info: core_schema.ValidationInfo) -> str:\n            assert v == 'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return 'hello world!'\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(\n                    name='b',\n                    schema=core_schema.with_info_after_validator_function(Foo.validate_b, core_schema.str_schema()),\n                ),\n            ],\n        ),\n        ['a', 'b'],\n        slots=True,\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 1, 'b': b'hello'})\n    assert dataclasses.asdict(foo) == {'a': 1, 'b': 'hello world!'}", "target": "def c(self) -> str:\n            return f'{self.a} {self.b.decode()}'", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "002326", "source": "def compiled(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        (x,) = args\n        torch._dynamo.mark_dynamic(x, 0)\n        compiled_softmax = torch.compile(\n            lambda x: F.softmax(x, dim=-1), mode=self.compile_mode, fullgraph=True\n        )\n        return lambda: compiled_softmax(x)", "target": "def compiled(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, dy = args\n        compiled_softmax = torch.compile(\n            lambda x: F.softmax(x, dim=-1), mode=self.compile_mode, fullgraph=True\n        )\n        y = compiled_softmax(x)\n        return lambda: torch.autograd.grad(y, x, grad_outputs=dy, retain_graph=True)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "validation", "example_id": "002327", "source": "def main() -> None:\n    creds_dict = boto3.Session().get_credentials().get_frozen_credentials()._asdict()\n    print(f\"export AWS_ACCESS_KEY_ID={creds_dict['access_key']}\")\n    print(f\"export AWS_SECRET_ACCESS_KEY={creds_dict['secret_key']}\")\n    print(f\"export AWS_SESSION_TOKEN={creds_dict['token']}\")", "target": "def validate_python(self, py_input, strict: bool | None = None, context: Any = None):\n        return self.validator.validate_python(py_input, strict=strict, context=context)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "002328", "source": "def name(self) -> str:\n        prefix = f\"{self.category()}\"\n        return prefix", "target": "def name(self) -> str:\n        prefix = f\"{self.category()}_{self.backend()}\"\n        if self.is_dynamic():\n            prefix += \"_dynamic\"\n        if self.device() == \"cuda\":\n            prefix += \"_gpu\"\n        return prefix", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "002329", "source": "def eager(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        (x,) = args\n        return lambda: F.softmax(x, dim=-1)", "target": "def eager(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, w = args\n        return lambda: self.layernorm_ref(x, w)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "validation", "example_id": "002330", "source": "def test_recursive_function_deeper_ref():\n    s = SchemaSerializer(\n        core_schema.typed_dict_schema(\n            {\n                'a': core_schema.typed_dict_field(\n                    core_schema.definitions_schema(\n                        core_schema.definition_reference_schema('my_ref'),\n                        [\n                            core_schema.typed_dict_schema(\n                                {'b': core_schema.typed_dict_field(core_schema.definition_reference_schema('my_ref'))},\n                                ref='my_ref',\n                            )\n                        ],\n                    )\n                )\n            },\n            serialization=core_schema.wrap_serializer_function_ser_schema(\n                function=lambda x, _handler: x, is_field_serializer=False\n            ),\n        )\n    )\n    assert s.to_python({'a': {'b': {'b': {}}}}) == {'a': {'b': {'b': {}}}}", "target": "def test_fields_required_by_default():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'x': core_schema.model_field(schema=core_schema.str_schema()),\n                'y': core_schema.model_field(schema=core_schema.str_schema()),\n            }\n        )\n    )\n    assert v.validate_python({'x': 'pika', 'y': 'chu'}) == ({'x': 'pika', 'y': 'chu'}, None, {'x', 'y'})\n    with pytest.raises(ValidationError) as exc_info:\n        assert v.validate_python({'x': 'pika'})\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'missing', 'loc': ('y',), 'msg': 'Field required', 'input': {'x': 'pika'}}\n    ]", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "validation", "example_id": "002331", "source": "def test_uuid_key():\n    v = SchemaSerializer(core_schema.dict_schema(core_schema.uuid_schema(), core_schema.uuid_schema()))\n    assert v.to_python(\n        {UUID('12345678-1234-5678-1234-567812345678'): UUID('12345678-1234-5678-1234-567812345678')}\n    ) == {UUID('12345678-1234-5678-1234-567812345678'): UUID('12345678-1234-5678-1234-567812345678')}\n    assert v.to_python(\n        {UUID('12345678-1234-5678-1234-567812345678'): UUID('12345678-1234-5678-1234-567812345678')}, mode='json'\n    ) == {'12345678-1234-5678-1234-567812345678': '12345678-1234-5678-1234-567812345678'}\n    assert (\n        v.to_json({UUID('12345678-1234-5678-1234-567812345678'): UUID('12345678-1234-5678-1234-567812345678')})\n        == b'{\"12345678-1234-5678-1234-567812345678\":\"12345678-1234-5678-1234-567812345678\"}'\n    )", "target": "def parseIntMetric(self, xmlnode, name, default = 0):\n        if name in self.properties:\n            self.metrix[name] = int(self.properties[name])\n        elif xmlnode.hasAttribute(name):\n            self.metrix[name] = int(xmlnode.getAttribute(name))\n        else:\n            self.metrix[name] = default", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "002332", "source": "def ser_x(self, v: Any, _) -> str:\n            assert self.x == 1_000\n            return f'{v:_}'", "target": "def ser_x(\n        data: Model,\n        v: Any,\n        serializer: core_schema.SerializerFunctionWrapHandler,\n        info: core_schema.FieldSerializationInfo,\n    ) -> str:\n        assert data['x'] == 1_000\n        x = serializer(v)\n        return f'{x:_}-{info.field_name}'", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "002333", "source": "def dataclass(\n        _cls: type[_T],\n        *,\n        init: Literal[False] = False,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool | None = None,\n        config: ConfigDict | type[object] | None = None,\n        validate_on_init: bool | None = None,\n        kw_only: bool = ...,\n        slots: bool = ...,\n    ) -> type[PydanticDataclass]: ...", "target": "def dataclass(\n        _cls: type[_T],\n        *,\n        init: Literal[False] = False,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool | None = None,\n        config: ConfigDict | type[object] | None = None,\n        validate_on_init: bool | None = None,\n    ) -> type[PydanticDataclass]: ...", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "validation", "example_id": "002334", "source": "def typename(self) -> str:\n        return \"\"", "target": "def typename(self) -> str:\n        return self._export_name", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "validation", "example_id": "002335", "source": "def apply_fsdp(args, model, use_checkpointing=False, use_wrap_policy=True):\n    wrap_policy = None\n    blocks = MODEL_FSDP_WRAP[\n        \"toy_model\" if model.__class__ is ToyModel else args.torchbench_model\n    ]\n    if use_wrap_policy:\n        wrap_policy = ModuleWrapPolicy(blocks)\n    model = FSDP(model, auto_wrap_policy=wrap_policy, use_orig_params=True)\n    if use_checkpointing:\n        fsdp_checkpointing_base(model, blocks)\n    return model", "target": "def test_dataclass_wrap_json():\n    schema = core_schema.no_info_wrap_validator_function(\n        lambda v, handler: handler(v),\n        core_schema.dataclass_schema(\n            FooDataclass,\n            core_schema.dataclass_args_schema(\n                'FooDataclass',\n                [\n                    core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                    core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n                ],\n            ),\n            ['a', 'b'],\n        ),\n    )\n    v = SchemaValidator(schema)\n    assert v.validate_json('{\"a\": \"hello\", \"b\": true}') == FooDataclass(a='hello', b=True)\n    assert v.validate_json('{\"a\": \"hello\", \"b\": true}', strict=True) == FooDataclass(a='hello', b=True)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "validation", "example_id": "002336", "source": "def test_empty_string_field_name(py_and_json: PyAndJson):\n    v = py_and_json({'type': 'model-fields', 'fields': {'': {'type': 'model-field', 'schema': {'type': 'int'}}}})\n    assert v.validate_test({'': 123}) == ({'': 123}, None, {''})", "target": "def update_not_none(mapping: dict[Any, Any], **update: Any) -> None:\n    mapping.update({k: v for k, v in update.items() if v is not None})", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "validation", "example_id": "002337", "source": "def test_negative(schema_func, seq_f):\n    v = SchemaSerializer(schema_func(core_schema.any_schema()))\n    assert v.to_python(seq_f('a', 'b', 'c', 'd', 'e')) == seq_f('a', 'b', 'c', 'd', 'e')\n    assert v.to_python(seq_f('a', 'b', 'c', 'd', 'e'), include={-1, -2}) == seq_f('d', 'e')\n    assert v.to_python(seq_f('a', 'b', 'c', 'd', 'e'), include={-1: None, -2: None}) == seq_f('d', 'e')\n    assert v.to_python(seq_f('a', 'b', 'c', 'd', 'e'), include={-1, -2}, mode='json') == ['d', 'e']\n    assert v.to_python(seq_f('a', 'b', 'c', 'd', 'e'), include={-1: None, -2: None}, mode='json') == ['d', 'e']\n    assert v.to_json(seq_f('a', 'b', 'c', 'd', 'e'), include={-1, -2}) == b'[\"d\",\"e\"]'", "target": "def milstm_cell(x, hx, cx, w_ih, w_hh, alpha, beta_i, beta_h, bias):\n    Wx = x.mm(w_ih.t())\n    Uz = hx.mm(w_hh.t())\n    gates = alpha * Wx * Uz + beta_i * Wx + beta_h * Uz + bias\n    ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)\n    ingate = ingate.sigmoid()\n    forgetgate = forgetgate.sigmoid()\n    cellgate = cellgate.tanh()\n    outgate = outgate.sigmoid()\n    cy = (forgetgate * cx) + (ingate * cellgate)\n    hy = outgate * cy.tanh()\n    return hy, cy", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "validation", "example_id": "002338", "source": "def is_pydantic_dataclass(class_: type[Any], /) -> TypeGuard[type[PydanticDataclass]]:\n    try:\n        return '__is_pydantic_dataclass__' in class_.__dict__ and dataclasses.is_dataclass(class_)\n    except AttributeError:\n        return False", "target": "def test_only_allow_alias(py_and_json) -> None:\n    schema = core_schema.dataclass_schema(\n        BasicDataclass,\n        core_schema.dataclass_args_schema(\n            'BasicDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), validation_alias='FieldA'),\n            ],\n        ),\n        ['a'],\n        config=core_schema.CoreConfig(validate_by_name=False, validate_by_alias=True),\n    )\n    v = py_and_json(schema)\n    assert v.validate_test({'FieldA': 'hello'}) == BasicDataclass(a='hello')\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'a': 'hello'})", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "002339", "source": "def multiple_of(self: _Pipeline[_InT, _NewOutDiv], multiple_of: _NewOutDiv) -> _Pipeline[_InT, _NewOutDiv]: ...", "target": "def multiple_of(self: _Pipeline[_InT, _NewOutMod], multiple_of: _NewOutMod) -> _Pipeline[_InT, _NewOutMod]: ...", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|opencv/opencv", "data_split": "validation", "example_id": "002340", "source": "def can_document_member(cls, member, membername, isattr, parent):\n        return True", "target": "def run(in_ys, in_ps, in_rs):\n        return [np.array([ys[0], ps[0], rs[0]]).T for ys, ps, rs in zip(in_ys, in_ps, in_rs)]", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "002341", "source": "def test_total_time(shapes):\n    print(\"shape; torch bmm; inductor aten bmm; inductor triton bmm\")\n    for i in range(len(shapes)):\n        a_shape, b_shape = shapes[i]\n        print(a_shape, \"x\", b_shape, end=\"; \")\n        a = torch.randn(a_shape, device=\"cuda\", dtype=torch.float16)\n        b = torch.randn(b_shape, device=\"cuda\", dtype=a.dtype)\n        config.triton.use_bmm = False\n        inductor_aten_bmm(a, b)\n        config.triton.use_bmm = True\n        inductor_triton_bmm(a, b)\n        torch_ms = time_with_torch_timer(torch_bmm, (a, b)).mean * 1000\n        config.triton.use_bmm = False\n        ind_aten_ms = time_with_torch_timer(inductor_aten_bmm, (a, b)).mean * 1000\n        config.triton.use_bmm = True\n        ind_triton_ms = time_with_torch_timer(inductor_triton_bmm, (a, b)).mean * 1000\n        print(torch_ms, ind_aten_ms, ind_triton_ms, sep=\"; \")", "target": "def test_total_time(shapes):\n    print(\"shape; torch mm; triton mm; inductor aten mm; inductor triton mm\")\n    for i in range(len(shapes)):\n        a_shape, b_shape = shapes[i]\n        print(a_shape, \"x\", b_shape, end=\"; \")\n        a = torch.randn(a_shape, device=\"cuda\", dtype=torch.float16)\n        b = torch.randn(b_shape, device=\"cuda\", dtype=a.dtype)\n        config.triton.mm = \"aten\"\n        inductor_aten_mm(a, b)\n        config.triton.mm = \"triton\"\n        inductor_triton_mm(a, b)\n        torch_ms = time_with_torch_timer(torch_mm, (a, b)).mean * 1000\n        triton_ms = time_with_torch_timer(triton_mm, (a, b)).mean * 1000\n        config.triton.mm = \"aten\"\n        ind_aten_ms = time_with_torch_timer(inductor_aten_mm, (a, b)).mean * 1000\n        config.triton.mm = \"triton\"\n        ind_triton_ms = time_with_torch_timer(inductor_triton_mm, (a, b)).mean * 1000\n        print(torch_ms, triton_ms, ind_aten_ms, ind_triton_ms, sep=\"; \")\n        torch._dynamo.reset()", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "validation", "example_id": "002342", "source": "def test_function_validator_wrapping_args_schema_before() -> None:\n    calls: list[Any] = []\n    def func(*args: Any) -> Any:\n        calls.append(args)\n        return args[0]\n    @dataclasses.dataclass\n    class Model:\n        number: int = 1\n    cs = core_schema.dataclass_schema(\n        Model,\n        core_schema.no_info_before_validator_function(\n            func,\n            core_schema.dataclass_args_schema(\n                'Model', [core_schema.dataclass_field('number', core_schema.int_schema())]\n            ),\n        ),\n        ['number'],\n    )\n    v = SchemaValidator(cs)\n    instance: Model = v.validate_python({'number': 1})\n    assert instance.number == 1\n    assert calls == [({'number': 1},)]\n    v.validate_assignment(instance, 'number', 2)\n    assert instance.number == 2\n    assert calls == [({'number': 1},), ({'number': 2},)]", "target": "def filter_requires_grad(tensors):\n    return [t for t in tensors if t.requires_grad]", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "validation", "example_id": "002343", "source": "def run_llama2_7b_int8(device: str = \"cuda\"):\n    model = GPTModelConfig(\n        \"Llama-2-7b-chat-hf\",\n        LLaMA,\n        \"int8\",\n        LLaMAWeightOnlyInt8QuantHandler,\n        144,\n        957,\n        136,\n    )\n    token_per_sec, memory_bandwidth, compilation_time = run_experiment(\n        model, device=device\n    )\n    return [\n        Experiment(\n            model.name,\n            \"token_per_sec\",\n            model.token_per_sec,\n            f\"{token_per_sec:.02f}\",\n            model.mode,\n            device,\n            get_arch_name(),\n            True,\n        ),\n        Experiment(\n            model.name,\n            \"memory_bandwidth(GB/s)\",\n            model.memory_bandwidth,\n            f\"{memory_bandwidth:.02f}\",\n            model.mode,\n            device,\n            get_arch_name(),\n            True,\n        ),\n        Experiment(\n            model.name,\n            \"compilation_time(s)\",\n            model.compilation_time,\n            f\"{compilation_time:.02f}\",\n            model.mode,\n            device,\n            get_arch_name(),\n            True,\n        ),\n    ]", "target": "def args(*args, **kwargs):\n    return args, kwargs", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "002344", "source": "def f(value, serializer, _info):\n        return f'result={serializer(len(value))}'", "target": "def f(value, serializer, _info):\n        return f'result={serializer(value)}'", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "validation", "example_id": "002345", "source": "def test_model_init_nested():\n    class MyModel:\n        __slots__ = '__dict__', '__pydantic_fields_set__', '__pydantic_extra__', '__pydantic_private__'\n    v = SchemaValidator(\n        core_schema.model_schema(\n            cls=MyModel,\n            schema=core_schema.model_fields_schema(\n                fields={\n                    'field_a': core_schema.model_field(schema=core_schema.str_schema()),\n                    'field_b': core_schema.model_field(\n                        schema=core_schema.model_schema(\n                            cls=MyModel,\n                            schema=core_schema.model_fields_schema(\n                                fields={\n                                    'x_a': core_schema.model_field(schema=core_schema.str_schema()),\n                                    'x_b': core_schema.model_field(schema=core_schema.int_schema()),\n                                }\n                            ),\n                        )\n                    ),\n                }\n            ),\n        )\n    )\n    m = v.validate_python({'field_a': 'test', 'field_b': {'x_a': 'foo', 'x_b': 12}})\n    assert isinstance(m, MyModel)\n    assert m.field_a == 'test'\n    assert isinstance(m.field_b, MyModel)\n    assert m.field_b.x_a == 'foo'\n    assert m.field_b.x_b == 12\n    m2 = MyModel()\n    v.validate_python({'field_a': 'test', 'field_b': {'x_a': 'foo', 'x_b': 12}}, self_instance=m2)\n    assert m2.field_a == 'test'\n    assert isinstance(m2.field_b, MyModel)\n    assert m2.field_b.x_a == 'foo'\n    assert m2.field_b.x_b == 12\n    assert m2.__pydantic_fields_set__ == {'field_a', 'field_b'}", "target": "def parseLongMetric(self, xmlnode, name, default = 0):\n        if name in self.properties:\n            self.metrix[name] = long(self.properties[name])\n        elif xmlnode.hasAttribute(name):\n            self.metrix[name] = long(xmlnode.getAttribute(name))\n        else:\n            self.metrix[name] = default", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "002346", "source": "def benchmark(self):\n        for M, N in self.get_shapes():\n            print(f\"Tensor dimensions: [{M}, {N}]\")\n            torch_dtype = cutlass_torch.dtype(cutlass.BFloat16)\n            x = torch.randn(M, N, device=\"cuda\", dtype=torch_dtype)\n            w = torch.randn(N, device=\"cuda\", dtype=torch.float32)\n            self.benchmark_single_shape((x, w), setting=f\"shape: [{M}, {N}]\")", "target": "def benchmark(self):\n        for M, N in self.get_shapes():\n            print(f\"Tensor dimensions: [{M}, {N}]\")\n            torch_dtype = cutlass_torch.dtype(cutlass.BFloat16)\n            x = torch.randn(M, N, device=\"cuda\", dtype=torch_dtype, requires_grad=True)\n            w = torch.randn(N, device=\"cuda\", dtype=torch.float32, requires_grad=True)\n            dy = torch.randn(M, N, device=\"cuda\", dtype=torch_dtype)\n            self.benchmark_single_shape((x, w, dy), setting=f\"shape: [{M}, {N}]\")", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "002347", "source": "def test_union_bool_int(input_value, expected_value, bool_case_label, int_case_label):\n    bool_case = core_schema.bool_schema() if not bool_case_label else (core_schema.bool_schema(), 'my_bool_label')\n    int_case = core_schema.int_schema() if not int_case_label else (core_schema.int_schema(), 'my_int_label')\n    s = SchemaSerializer(core_schema.union_schema([bool_case, int_case]))\n    assert s.to_python(input_value) == expected_value\n    assert s.to_python(input_value, mode='json') == expected_value\n    assert s.to_json(input_value) == json.dumps(expected_value).encode()", "target": "def test_union_bool_int(input_value, expected_value):\n    v = SchemaValidator(core_schema.union_schema(choices=[core_schema.bool_schema(), core_schema.int_schema()]))\n    assert v.validate_python(input_value) == expected_value", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "002348", "source": "def liger(self, args, kwargs) -> Any:\n        from liger_kernel.transformers.layer_norm import LigerLayerNorm\n        x, w = args\n        M, N = x.shape\n        liger_layernorm = LigerLayerNorm(hidden_size=N, eps=1e-6).cuda()\n        liger_layernorm.weight.data.copy_(w)\n        liger_layernorm.bias.data.copy_(\n            torch.zeros(N, device=\"cuda\", dtype=torch.float32)\n        )\n        return lambda: liger_layernorm(x)", "target": "def liger(self, args, kwargs) -> Any:\n        from liger_kernel.ops.layer_norm import layer_norm_backward\n        x, w, dy = args\n        eps = 1e-6\n        mean, rstd = self.compute_mean_rstd(x, eps)\n        M, N = x.shape\n        return lambda: layer_norm_backward(dy, x, w, None, mean, rstd)[0:2]", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|pytorch/pytorch", "data_split": "validation", "example_id": "002349", "source": "def predict(est, data_test, target_test):\n    if args.no_predict:\n        return\n    tic = time()\n    predicted_test = est.predict(data_test)\n    predicted_proba_test = est.predict_proba(data_test)\n    toc = time()\n    roc_auc = roc_auc_score(target_test, predicted_proba_test[:, 1])\n    acc = accuracy_score(target_test, predicted_test)\n    print(f\"predicted in {toc - tic:.3f}s, ROC AUC: {roc_auc:.4f}, ACC: {acc:.4f}\")", "target": "def main():\n    result_path = sys.argv[1]\n    all = [\n        Benchmark(),\n    ]\n    for benchmark in all:\n        benchmark.enable_compile_time_instruction_count().collect_all().append_results(\n            result_path\n        )", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "002350", "source": "def quack(self, args, kwargs=None) -> Any:\n        from quack.cross_entropy import cross_entropy\n        assert kwargs is None\n        x, target, dloss = args\n        loss = cross_entropy(x, target)\n        return lambda: torch.autograd.grad(\n            loss, x, grad_outputs=dloss, retain_graph=True\n        )", "target": "def quack(self, args, kwargs=None) -> Any:\n        from quack.softmax import softmax\n        assert kwargs is None\n        (x,) = args\n        return lambda: softmax(x)", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "validation", "example_id": "002351", "source": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "target": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "validation", "example_id": "002352", "source": "def dec(f: ModelSerializer) -> _decorators.PydanticDescriptorProxy[Any]:\n        dec_info = _decorators.ModelSerializerDecoratorInfo(mode=mode, return_type=return_type, when_used=when_used)\n        return _decorators.PydanticDescriptorProxy(f, dec_info)", "target": "def test_complex_recursive_type() -> None:\n    schema = core_schema.definitions_schema(\n        core_schema.definition_reference_schema('JsonType'),\n        [\n            core_schema.nullable_schema(\n                core_schema.union_schema(\n                    [\n                        core_schema.list_schema(core_schema.definition_reference_schema('JsonType')),\n                        core_schema.dict_schema(\n                            core_schema.str_schema(), core_schema.definition_reference_schema('JsonType')\n                        ),\n                        core_schema.str_schema(),\n                        core_schema.int_schema(),\n                        core_schema.float_schema(),\n                        core_schema.bool_schema(),\n                    ]\n                ),\n                ref='JsonType',\n            )\n        ],\n    )\n    validator = SchemaValidator(schema)\n    with pytest.raises(ValidationError) as exc_info:\n        validator.validate_python({'a': datetime.date(year=1992, month=12, day=11)})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'list_type',\n            'loc': ('list[nullable[union[list[...],dict[str,...],str,int,float,bool]]]',),\n            'msg': 'Input should be a valid list',\n            'input': {'a': datetime.date(1992, 12, 11)},\n        },\n        {\n            'type': 'list_type',\n            'loc': ('dict[str,...]', 'a', 'list[nullable[union[list[...],dict[str,...],str,int,float,bool]]]'),\n            'msg': 'Input should be a valid list',\n            'input': datetime.date(1992, 12, 11),\n        },\n        {\n            'type': 'dict_type',\n            'loc': ('dict[str,...]', 'a', 'dict[str,...]'),\n            'msg': 'Input should be a valid dictionary',\n            'input': datetime.date(1992, 12, 11),\n        },\n        {\n            'type': 'string_type',\n            'loc': ('dict[str,...]', 'a', 'str'),\n            'msg': 'Input should be a valid string',\n            'input': datetime.date(1992, 12, 11),\n        },\n        {\n            'type': 'int_type',\n            'loc': ('dict[str,...]', 'a', 'int'),\n            'msg': 'Input should be a valid integer',\n            'input': datetime.date(1992, 12, 11),\n        },\n        {\n            'type': 'float_type',\n            'loc': ('dict[str,...]', 'a', 'float'),\n            'msg': 'Input should be a valid number',\n            'input': datetime.date(1992, 12, 11),\n        },\n        {\n            'type': 'bool_type',\n            'loc': ('dict[str,...]', 'a', 'bool'),\n            'msg': 'Input should be a valid boolean',\n            'input': datetime.date(1992, 12, 11),\n        },\n        {\n            'type': 'string_type',\n            'loc': ('str',),\n            'msg': 'Input should be a valid string',\n            'input': {'a': datetime.date(1992, 12, 11)},\n        },\n        {\n            'type': 'int_type',\n            'loc': ('int',),\n            'msg': 'Input should be a valid integer',\n            'input': {'a': datetime.date(1992, 12, 11)},\n        },\n        {\n            'type': 'float_type',\n            'loc': ('float',),\n            'msg': 'Input should be a valid number',\n            'input': {'a': datetime.date(1992, 12, 11)},\n        },\n        {\n            'type': 'bool_type',\n            'loc': ('bool',),\n            'msg': 'Input should be a valid boolean',\n            'input': {'a': datetime.date(1992, 12, 11)},\n        },\n    ]", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "validation", "example_id": "002353", "source": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "target": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "002354", "source": "def to_options(self) -> dict[str, Any]:\n        return {\n            **super().to_options(),\n            \"cuda.cutlass_instantiation_level\": self.cutlass_instantiation_level,\n        }", "target": "def to_options(self) -> dict[str, Any]:\n        return {\n            **super().to_options(),\n            \"triton.enable_persistent_tma_matmul\": self.enable_persistent_tma_matmul,\n        }", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "validation", "example_id": "002355", "source": "def test_model_class():\n    class Branch:\n        __slots__ = '__dict__', '__pydantic_fields_set__', '__pydantic_extra__', '__pydantic_private__'\n        width: int\n        branch: Optional['Branch']\n    v = SchemaValidator(\n        core_schema.definitions_schema(\n            core_schema.definition_reference_schema('Branch'),\n            [\n                core_schema.model_schema(\n                    Branch,\n                    core_schema.model_fields_schema(\n                        {\n                            'width': core_schema.model_field(core_schema.int_schema()),\n                            'branch': core_schema.model_field(\n                                core_schema.with_default_schema(\n                                    core_schema.union_schema(\n                                        [core_schema.none_schema(), core_schema.definition_reference_schema('Branch')]\n                                    ),\n                                    default=None,\n                                )\n                            ),\n                        }\n                    ),\n                    ref='Branch',\n                )\n            ],\n        )\n    )\n    m1: Branch = v.validate_python({'width': '1'})\n    assert isinstance(m1, Branch)\n    assert m1.__pydantic_fields_set__ == {'width'}\n    assert m1.__dict__ == {'width': 1, 'branch': None}\n    assert m1.width == 1\n    assert m1.branch is None\n    m2: Branch = v.validate_python({'width': '10', 'branch': {'width': 20}})\n    assert isinstance(m2, Branch)\n    assert m2.__pydantic_fields_set__ == {'width', 'branch'}\n    assert m2.width == 10\n    assert isinstance(m2.branch, Branch)\n    assert m2.branch.width == 20\n    assert m2.branch.branch is None", "target": "def test_bytes_base64():\n    s = SchemaSerializer(core_schema.bytes_schema(), {'ser_json_bytes': 'base64'})\n    assert s.to_python(b'foobar') == b'foobar'\n    assert s.to_json(b'foobar') == b'\"Zm9vYmFy\"'\n    assert s.to_python(b'foobar', mode='json') == 'Zm9vYmFy'\n    assert base64.b64decode(s.to_python(b'foobar', mode='json').encode()) == b'foobar'\n    assert s.to_json(b'foo bar') == b'\"Zm9vIGJhcg==\"'\n    assert s.to_python(b'foo bar', mode='json') == 'Zm9vIGJhcg=='\n    assert base64.b64decode(s.to_python(b'foo bar', mode='json').encode()) == b'foo bar'", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "validation", "example_id": "002356", "source": "def multinomial_sample_one_no_sync(\n    probs_sort,\n):\n    q = torch.empty_like(probs_sort).exponential_(1)\n    return torch.argmax(probs_sort / q, dim=-1, keepdim=True).to(dtype=torch.int)", "target": "def greater_than_validator(x: Any, gt: Any) -> Any:\n    try:\n        if not (x > gt):\n            raise PydanticKnownError('greater_than', {'gt': _safe_repr(gt)})\n        return x\n    except TypeError:\n        raise TypeError(f\"Unable to apply constraint 'gt' to supplied value {x}\")", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "002357", "source": "def constrain(self: _Pipeline[_InT, _NewOutLe], constraint: annotated_types.Le) -> _Pipeline[_InT, _NewOutLe]: ...", "target": "def constrain(\n        self: _Pipeline[_InT, _NewOutT], constraint: annotated_types.MultipleOf\n    ) -> _Pipeline[_InT, _NewOutT]: ...", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "002358", "source": "def ser_x(data: Model, v: Any, _) -> str:\n        assert data['x'] == 1_000\n        return f'{v:_}'", "target": "def ser_x(data: Model, v: Any, serializer: core_schema.SerializerFunctionWrapHandler, _) -> str:\n        x = serializer(v)\n        assert data['x'] == 1_000\n        return f'{x:_}'", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "validation", "example_id": "002359", "source": "def full_typename(self) -> str:\n        return \"cv2.typing.\" + self.typename", "target": "def full_typename(self) -> str:\n        return '_typing.Callable[[{}], {}]'.format(\n            ', '.join(arg.full_typename for arg in self.arg_types),\n            self.ret_type.full_typename\n        )", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "002360", "source": "def benchmark(self):\n        for M, N in self.get_shapes():\n            print(f\"Tensor dimensions: [{M}, {N}]\")\n            torch_dtype = cutlass_torch.dtype(cutlass.BFloat16)\n            x = 0.1 * torch.randn(\n                M, N, device=\"cuda\", dtype=torch_dtype, requires_grad=True\n            )\n            dy = torch.randn(M, N, device=\"cuda\", dtype=torch_dtype)\n            self.benchmark_single_shape((x, dy), setting=f\"shape: [{M}, {N}]\")", "target": "def benchmark(self):\n        for M, N in self.get_shapes():\n            print(f\"Tensor dimensions: [{M}, {N}]\")\n            torch_dtype = cutlass_torch.dtype(cutlass.BFloat16)\n            x = torch.randn(M, N, device=\"cuda\", dtype=torch_dtype, requires_grad=True)\n            w = torch.randn(N, device=\"cuda\", dtype=torch.float32, requires_grad=True)\n            dy = torch.randn(M, N, device=\"cuda\", dtype=torch_dtype)\n            self.benchmark_single_shape((x, w, dy), setting=f\"shape: [{M}, {N}]\")", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|pydantic/pydantic", "data_split": "validation", "example_id": "002361", "source": "def fixed_batch_size_comparison(data):\n    all_features = [\n        i.astype(int) for i in np.linspace(data.shape[1] // 10, data.shape[1], num=5)\n    ]\n    batch_size = 1000\n    all_times = defaultdict(list)\n    all_errors = defaultdict(list)\n    for n_components in all_features:\n        pca = PCA(n_components=n_components)\n        ipca = IncrementalPCA(n_components=n_components, batch_size=batch_size)\n        results_dict = {\n            k: benchmark(est, data) for k, est in [(\"pca\", pca), (\"ipca\", ipca)]\n        }\n        for k in sorted(results_dict.keys()):\n            all_times[k].append(results_dict[k][\"time\"])\n            all_errors[k].append(results_dict[k][\"error\"])\n    plot_feature_times(all_times, batch_size, all_features, data)\n    plot_feature_errors(all_errors, batch_size, all_features, data)", "target": "def input_data_wrong():\n    return {\n        'field_str': ['fo'],\n        'field_str_con': 'f',\n        'field_int': 1.5,\n        'field_int_con': 11,\n        'field_float': False,\n        'field_float_con': 10.1,\n        'field_decimal': 'wrong',\n        'field_bool': 4,\n        'field_bytes': 42,\n        'field_bytes_con': b'foo',\n        'field_date': 'wrong',\n        'field_date_con': '2000-01-01',\n        'field_time': 'boom',\n        'field_time_con': '23:00:00',\n        'field_datetime': b'smash',\n        'field_datetime_con': '1900-01-01T00:00:00',\n        'field_uuid': '12345678-1234-5678-1234-567812345678',\n        'field_list_any': {1: 2, 3: 4},\n        'field_list_str': [(i,) for i in range(100)],\n        'field_list_str_con': ['a', 'b'],\n        'field_set_any': {'a': b'b', True: 1.0, None: 5},\n        'field_set_int': {f'x{i}' for i in range(100)},\n        'field_set_int_con': {i for i in range(40)},\n        'field_frozenset_any': 'wrong',\n        'field_frozenset_bytes': frozenset([i for i in range(100)]),\n        'field_frozenset_bytes_con': frozenset({b'a', b'b'}),\n        'field_tuple_var_len_any': b'wrong',\n        'field_tuple_var_len_float': tuple(f'x{i}' for i in range(100)),\n        'field_tuple_var_len_float_con': (1.0, 2.0),\n        'field_tuple_fix_len': ('a', 1, 1.0, True, 'more'),\n        'field_dict_any': {'a', 'b', 1, True, 1.0, 2.0},\n        'field_dict_str_float': {(i,): f'x{i}' for i in range(100)},\n        'field_literal_1_int': 2,\n        'field_literal_1_str': 'bat',\n        'field_literal_mult_int': 42,\n        'field_literal_mult_str': 'wrong',\n        'field_literal_assorted': 'wrong',\n        'field_list_nullable_int': [f'x{i}' for i in range(100)],\n        'field_union': {'field_str': ('foo',), 'field_int': 'x', 'field_float': b'y'},\n        'field_functions_model': {'field_before': 1, 'field_after': 1, 'field_wrap': 1, 'field_plain': 1},\n        'field_recursive': {'name': 'foo', 'sub_branch': {'name': 'bar', 'sub_branch': {}}},\n    }", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "002362", "source": "def test_alias_path(py_and_json: PyAndJson, input_value, expected):\n    v = py_and_json(\n        {\n            'type': 'model-fields',\n            'fields': {\n                'field_a': {'validation_alias': ['foo', 'bar'], 'type': 'model-field', 'schema': {'type': 'int'}}\n            },\n        }\n    )\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=expected.message):\n            v.validate_test(input_value)\n    else:\n        output = v.validate_test(input_value)\n        assert output == expected", "target": "def test_alias_path(py_and_json: PyAndJson, input_value, expected):\n    v = py_and_json(\n        {\n            'type': 'typed-dict',\n            'fields': {\n                'field_a': {'validation_alias': ['foo', 'bar'], 'type': 'typed-dict-field', 'schema': {'type': 'int'}}\n            },\n        }\n    )\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=expected.message):\n            v.validate_test(input_value)\n    else:\n        output = v.validate_test(input_value)\n        assert output == expected", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "002363", "source": "def _get_types_namespace(self) -> NamespacesTuple:\n        return self._generate_schema._types_namespace", "target": "def _get_types_namespace(self) -> NamespacesTuple:\n        raise NotImplementedError", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "002364", "source": "def eager(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, w = args\n        return lambda: self.rms_norm_ref(x, w)", "target": "def eager(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, w, dy = args\n        y = self.rms_norm_ref(x, w)\n        return lambda: torch.autograd.grad(\n            y, [x, w], grad_outputs=dy, retain_graph=True\n        )", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "validation", "example_id": "002365", "source": "def area(self) -> int:\n            return self.width * self.height", "target": "def test_decimal_key(py_and_json: PyAndJson):\n    v = py_and_json({'type': 'dict', 'keys_schema': {'type': 'decimal'}, 'values_schema': {'type': 'int'}})\n    assert v.validate_test({'1': 1, '2': 2}) == {Decimal('1'): 1, Decimal('2'): 2}\n    assert v.validate_test({'1.5': 1, '2.4': 2}) == {Decimal('1.5'): 1, Decimal('2.4'): 2}\n    if v.validator_type == 'python':\n        with pytest.raises(ValidationError, match='Input should be an instance of Decimal'):\n            v.validate_test({'1.5': 1, '2.5': 2}, strict=True)\n    else:\n        assert v.validate_test({'1.5': 1, '2.4': 2}, strict=True) == {Decimal('1.5'): 1, Decimal('2.4'): 2}", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|opencv/opencv", "data_split": "validation", "example_id": "002366", "source": "def test_fn():\n        return torch.convolution(x_chan, weight_chan, bias=None, **kwargs)", "target": "def get_preprocessed_img(self, img_path):\n        image_data = read_rgb_img(img_path, self.bgr_to_rgb)\n        image_data = self.preprocess(image_data)\n        return self.reshape_img(image_data)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|opencv/opencv", "data_split": "validation", "example_id": "002367", "source": "def process_fn(soup, anchor, python_signature, symbol):\n    try:\n        r = anchor.find_next_sibling(class_='memitem').find(class_='memproto').find('table')\n        insert_python_fn_signature(soup, r, python_signature, symbol)\n    except:\n        logging.error(\"Can't process: %s\" % symbol)\n        traceback.print_exc()\n        pprint(anchor)", "target": "def test_parse_to_size_t_not_convertible(self):\n        min_long, _ = get_limits(ctypes.c_long)\n        for not_convertible in (1.2, True, False, np.bool_(True), np.float32(4), float(3),\n                                np.double(45), 's', 'str', np.array([1, 2]), (1,), [1, 2],\n                                np.float64(6), complex(1, 1), complex(imag=2), complex(1.1),\n                                -1, min_long, np.int8(-35)):\n            with self.assertRaises((TypeError, OverflowError),\n                                   msg=get_no_exception_msg(not_convertible)):\n                _ = cv.utils.dumpSizeT(not_convertible)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "validation", "example_id": "002368", "source": "def get_output(self, input_blob):\n        if self.need_reshape:\n            self.net.blobs[self.in_blob_name].reshape(*input_blob.shape)\n        return self.net.forward_all(**{self.in_blob_name: input_blob})[self.out_blob_name]", "target": "def test_callable_cases(input_value, expected):\n    v = SchemaValidator(cs.callable_schema())\n    assert v.isinstance_python(input_value) == expected", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "002369", "source": "def test_constraints_schema_validation() -> None:\n    val = SchemaValidator(cs.date_schema(gt='2020-01-01'))\n    with pytest.raises(ValidationError):\n        val.validate_python('2019-01-01')", "target": "def test_constraints_schema_validation() -> None:\n    val = SchemaValidator(core_schema.timedelta_schema(gt=3))\n    with pytest.raises(ValidationError):\n        val.validate_python(1)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "002370", "source": "def test_constraints_schema_validation_error(constraint: str) -> None:\n    with pytest.raises(SchemaError, match=f\"'{constraint}' must be coercible to a date instance\"):\n        SchemaValidator(cs.date_schema(**{constraint: 'bad_value'}))", "target": "def test_constraints_schema_validation_error(constraint: str) -> None:\n    with pytest.raises(SchemaError, match=f\"'{constraint}' must be coercible to an integer\"):\n        SchemaValidator(cs.int_schema(**{constraint: 'bad_value'}))", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "validation", "example_id": "002371", "source": "def get_limits(dtype):\n    if not is_numeric(dtype):\n        return None, None\n    if np.issubdtype(dtype, np.integer):\n        info = np.iinfo(dtype)\n    else:\n        info = np.finfo(dtype)\n    return info.min, info.max", "target": "def test_alias_build_error(alias_schema, error):\n    with pytest.raises(SchemaError, match=error):\n        SchemaValidator(\n            schema={\n                'type': 'typed-dict',\n                'fields': {'field_a': {'type': 'typed-dict-field', 'schema': {'type': 'int'}, **alias_schema}},\n            }\n        )", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "validation", "example_id": "002372", "source": "def bytes_schema(\n    *,\n    max_length: int | None = None,\n    min_length: int | None = None,\n    strict: bool | None = None,\n    ref: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> BytesSchema:\n    return _dict_not_none(\n        type='bytes',\n        max_length=max_length,\n        min_length=min_length,\n        strict=strict,\n        ref=ref,\n        metadata=metadata,\n        serialization=serialization,\n    )", "target": "def constrain(\n        self: _Pipeline[_InT, _NewOutT], constraint: annotated_types.MultipleOf\n    ) -> _Pipeline[_InT, _NewOutT]: ...", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "002373", "source": "def f1(input_value, info):\n        info.context['f1'] = input_value\n        return input_value + f'| context: {info.context}'", "target": "def f1(input_value, info):\n        info.context['f1'] = input_value\n        return input_value + f'| context: {info.context}'", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "validation", "example_id": "002374", "source": "def test_dataclass_self_init_post_init():\n    calls = []\n    @dataclasses.dataclass(init=False)\n    class Foo:\n        a: str\n        b: bool\n        c: dataclasses.InitVar[int]\n        def __init__(self, *args, **kwargs):\n            v.validate_python(ArgsKwargs(args, kwargs), self_instance=self)\n        def __post_init__(self, c):\n            calls.append(c)\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), kw_only=False),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), kw_only=False),\n                core_schema.dataclass_field(name='c', schema=core_schema.int_schema(), init_only=True),\n            ],\n            collect_init_only=True,\n        ),\n        ['a', 'b', 'c'],\n        post_init=True,\n    )\n    v = SchemaValidator(schema)\n    foo = Foo(b'hello', 'True', c='123')\n    assert dataclasses.is_dataclass(foo)\n    assert dataclasses.asdict(foo) == {'a': 'hello', 'b': True}\n    assert calls == [123]", "target": "def serialize(self) -> JsonDict:\n        assert self.type\n        return {\n            'name': self.name,\n            'alias': self.alias,\n            'is_frozen': self.is_frozen,\n            'has_dynamic_alias': self.has_dynamic_alias,\n            'has_default': self.has_default,\n            'strict': self.strict,\n            'line': self.line,\n            'column': self.column,\n            'type': self.type.serialize(),\n        }", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "validation", "example_id": "002375", "source": "def type_format(self) -> str:\n        return \"\"", "target": "def type_format(self) -> str:\n        if TypeNode.compatible_to_runtime_usage:\n            return \"_typing.Union[{}]\"\n        return \"{}\"", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "002376", "source": "def get_memory_bytes(self, args, kwargs) -> int:\n        x, target = args\n        M, N = x.shape\n        dtype = x.dtype\n        return (M * N + M + M) * dtype.itemsize", "target": "def get_memory_bytes(self, args, kwargs) -> int:\n        x, w = args\n        M, N = x.shape\n        return 2 * M * N * x.dtype.itemsize + N * w.dtype.itemsize", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "validation", "example_id": "002377", "source": "def types_separator(self) -> str:\n        return \", \"", "target": "def types_separator(self) -> str:\n        if TypeNode.compatible_to_runtime_usage:\n            return \", \"\n        return \" | \"", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|opencv/opencv", "data_split": "validation", "example_id": "002378", "source": "def getTestList(self, white, black):\n        res = [t for t in white or self.tests if self.getAlias(t) not in black]\n        if len(res) == 0:\n            raise Err(\"No tests found\")\n        return set(res)", "target": "def evaluate_model(model, digits, samples, labels):\n    resp = model.predict(samples)\n    err = (labels != resp).mean()\n    confusion = np.zeros((10, 10), np.int32)\n    for i, j in zip(labels, resp):\n        confusion[int(i), int(j)] += 1\n    return err, confusion", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "validation", "example_id": "002379", "source": "def get_output(self, input_blob):\n        self.net.setInput(input_blob, self.in_blob_name)\n        return self.net.forward(self.out_blob_name)", "target": "def get_output(self, input_blob):\n        self.net.setInput(input_blob, self.in_blob_name)\n        return self.net.forward(self.out_blob_name)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "002380", "source": "def get_shapes(self) -> tuple[tuple[int, ...], ...]:\n        return (\n            (32768, 256),\n            (32768, 512),\n            (32768, 1024),\n            (32768, 2048),\n            (32768, 4096),\n            (32768, 8192),\n            (32768, 16384),\n            (32768, 32768),\n            (32768, 65536),\n            (16384, 131072),\n            (8192, 262144),\n        )", "target": "def get_shapes(self) -> tuple[tuple[int, ...], ...]:\n        return (\n            (32768, 256),\n            (32768, 512),\n            (32768, 1024),\n            (32768, 2048),\n            (32768, 4096),\n            (32768, 8192),\n            (32768, 16384),\n            (32768, 32768),\n            (32768, 65536),\n            (16384, 131072),\n            (8192, 262144),\n        ) + extra_shapes_for_norm", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|opencv/opencv", "data_split": "validation", "example_id": "002381", "source": "def sample(logits, temperature: float = 1.0, top_k: Optional[int] = None):\n    probs = logits_to_probs(logits[0, -1], temperature, top_k)\n    idx_next = multinomial_sample_one_no_sync(probs)\n    return idx_next, probs", "target": "def check_close_boxes(self, a, b, delta, angle_delta):\n        self.check_close_pairs(a[0], b[0], delta)\n        self.check_close_pairs(a[1], b[1], delta)\n        self.check_close_angles(a[2], b[2], angle_delta)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "validation", "example_id": "002382", "source": "def resolve_ref_schema(self, maybe_ref_schema: core_schema.CoreSchema, /) -> core_schema.CoreSchema:\n        raise NotImplementedError", "target": "def test_union_timedelta_respects_instanceof_check():\n    serialization_schema = core_schema.plain_serializer_function_ser_schema(lambda v: None)\n    json_validation_schema = core_schema.no_info_plain_validator_function(\n        function=lambda v: v, serialization=serialization_schema\n    )\n    test_custom_ser_schema = core_schema.json_schema(\n        schema=json_validation_schema,\n        serialization=serialization_schema,\n    )\n    s = SchemaSerializer(core_schema.union_schema(choices=[core_schema.timedelta_schema(), test_custom_ser_schema]))\n    assert s.to_python('foo') is None", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "validation", "example_id": "002383", "source": "def predict(self, samples):\n        _ret, resp = self.model.predict(samples)\n        return resp.ravel()", "target": "def test_float_kwargs(py_and_json: PyAndJson, kwargs: dict[str, Any], input_value, expected):\n    v = py_and_json({'type': 'float', **kwargs})\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            v.validate_test(input_value)\n    else:\n        output = v.validate_test(input_value)\n        assert output == expected\n        assert isinstance(output, float)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "002384", "source": "def forward(self, inputs):\n        output = self.pool(F.relu(self.conv(inputs)))\n        output = output.view(1)\n        return output", "target": "def forward(self, x):\n        for i, l in enumerate(self.linears):\n            x = self.linears[i // 2](x) + l(x)\n        return x", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "002385", "source": "def compiled(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, target, dloss = args\n        compiled_cross_entropy = torch.compile(\n            lambda x, target: F.cross_entropy(x, target, reduction=\"none\"),\n            mode=self.compile_mode,\n            fullgraph=True,\n        )\n        loss = compiled_cross_entropy(x, target)\n        return lambda: torch.autograd.grad(\n            loss, x, grad_outputs=dloss, retain_graph=True\n        )", "target": "def compiled(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, w = args\n        torch._dynamo.mark_dynamic(x, 0)\n        compiled_rms_norm = torch.compile(\n            self.rms_norm_ref, mode=self.compile_mode, fullgraph=True\n        )\n        return lambda: compiled_rms_norm(x, w)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "002386", "source": "def update(self, input_pos, k_val, v_val):\n        assert input_pos.shape[0] == k_val.shape[2]\n        k_out = self.k_cache\n        v_out = self.v_cache\n        k_out[:, :, input_pos] = k_val\n        v_out[:, :, input_pos] = v_val\n        return k_out, v_out", "target": "def update(self, input_pos, k_val, v_val):\n        assert input_pos.shape[0] == k_val.shape[2]\n        k_out = self.k_cache\n        v_out = self.v_cache\n        k_out[:, :, input_pos] = k_val\n        v_out[:, :, input_pos] = v_val\n        return k_out, v_out", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "validation", "example_id": "002387", "source": "def f_w(v: Any, handler: core_schema.ValidatorFunctionWrapHandler, info: core_schema.ValidationInfo) -> Any:\n        calls.append(info.mode)\n        return handler(v)", "target": "def jacfwd(model, inp, v=None, strict=None):\n        argnums = tuple(range(len(inp)))\n        return ft.jacfwd(model, argnums)(*inp)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "002388", "source": "def f(value, serializer):\n        if value == 42:\n            return 42\n        return f'result={serializer(value)}'", "target": "def f(value, serializer, _info):\n        return f'result={serializer(value)}'", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "validation", "example_id": "002389", "source": "def test_GPU_time(shapes):\n    print(\"shape; torch mm; triton mm; inductor aten mm; inductor triton mm\")\n    for i in range(len(shapes)):\n        a_shape, b_shape = shapes[i]\n        print(a_shape, \"x\", b_shape, end=\"; \")\n        a = torch.randn(a_shape, device=\"cuda\", dtype=torch.float16)\n        b = torch.randn(b_shape, device=\"cuda\", dtype=a.dtype)\n        config.triton.mm = \"aten\"\n        inductor_aten_mm(a, b)\n        config.triton.mm = \"triton\"\n        inductor_triton_mm(a, b)\n        torch_ms, _, _ = benchmarker.benchmark_gpu(lambda: torch_mm(a, b))\n        triton_ms, _, _ = benchmarker.benchmark_gpu(lambda: triton_mm(a, b))\n        ind_aten_ms, _, _ = benchmarker.benchmark_gpu(lambda: inductor_aten_mm(a, b))\n        ind_triton_ms, _, _ = benchmarker.benchmark_gpu(\n            lambda: inductor_triton_mm(a, b)\n        )\n        print(torch_ms, triton_ms, ind_aten_ms, ind_triton_ms, sep=\"; \")\n        torch._dynamo.reset()", "target": "def test_complete_core_strict(benchmark):\n    v = SchemaValidator(schema(strict=True))\n    benchmark(v.validate_python, input_data_strict())", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "validation", "example_id": "002390", "source": "def execute(cmd, cwd = None):\n    print(\"Executing: %s in %s\" % (cmd, cwd), file=sys.stderr)\n    print('Executing: ' + ' '.join(cmd))\n    retcode = check_call(cmd, cwd = cwd)\n    if retcode != 0:\n        raise Exception(\"Child returned:\", retcode)", "target": "def execute(cmd, cwd=None, shell=False):\n    try:\n        log.debug(\"Executing: %s\" % cmd)\n        log.info('Executing: ' + ' '.join(cmd))\n        if cwd:\n            log.info(\"    in: %s\" % cwd)\n        retcode = subprocess.call(cmd, shell=shell, cwd=str(cwd) if cwd else None)\n        if retcode < 0:\n            raise Fail(\"Child was terminated by signal: %s\" % -retcode)\n        elif retcode > 0:\n            raise Fail(\"Child returned: %s\" % retcode)\n    except OSError as e:\n        raise Fail(\"Execution failed: %d / %s\" % (e.errno, e.strerror))", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "validation", "example_id": "002391", "source": "def test_generator_too_long():\n    v = SchemaValidator(cs.generator_schema(items_schema=cs.int_schema(), max_length=2))\n    validating_iterator = v.validate_python(gen())\n    assert next(validating_iterator) == 1\n    assert next(validating_iterator) == 2\n    with pytest.raises(ValidationError) as exc_info:\n        next(validating_iterator)\n    errors = exc_info.value.errors(include_url=False)\n    assert errors == [\n        {\n            'type': 'too_long',\n            'loc': (),\n            'input': HasRepr(IsStr(regex='<generator object gen at .+>')),\n            'msg': 'Generator should have at most 2 items after validation, not more',\n            'ctx': {'field_type': 'Generator', 'max_length': 2, 'actual_length': None},\n        }\n    ]", "target": "def f(input_value, validator, info):\n        try:\n            return validator(input_value) * 2\n        except ValidationError as e:\n            assert e.title == 'ValidatorCallable'\n            assert str(e).startswith('1 validation error for ValidatorCallable\\n')\n            raise e", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "002392", "source": "def validate_arguments(\n    func: None = None, *, config: 'ConfigType' = None\n) -> Callable[['AnyCallableT'], 'AnyCallableT']: ...", "target": "def validate_arguments(func: 'AnyCallableT') -> 'AnyCallableT': ...", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "validation", "example_id": "002393", "source": "def my_validator(value: Any, info: Any) -> str:\n        return str(value)", "target": "def extract_weights(mod: nn.Module) -> tuple[tuple[Tensor, ...], list[str]]:\n    orig_params = tuple(mod.parameters())\n    names = []\n    for name, p in list(mod.named_parameters()):\n        _del_nested_attr(mod, name.split(\".\"))\n        names.append(name)\n    params = tuple(p.detach().requires_grad_() for p in orig_params)\n    return params, names", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "002394", "source": "def gen(error: bool):\n        yield 1\n        yield 2\n        if error:\n            raise RuntimeError('error')\n        yield 3", "target": "def gen(error: bool):\n        yield 1\n        yield 2\n        if error:\n            raise RuntimeError('error')\n        yield 3", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "002395", "source": "def eager(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        (x,) = args\n        return lambda: F.softmax(x, dim=-1)", "target": "def eager(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, w, dy = args\n        y = self.layernorm_ref(x, w)\n        return lambda: torch.autograd.grad(\n            y, [x, w], grad_outputs=dy, retain_graph=True\n        )", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "validation", "example_id": "002396", "source": "def test_simple_any_ser_schema():\n    import operator\n    class MyEnum(Enum):\n        A = (1,)\n        B = (2,)\n    v = SchemaSerializer(\n        core_schema.no_info_after_validator_function(\n            operator.attrgetter('value'),\n            core_schema.enum_schema(MyEnum, list(MyEnum.__members__.values())),\n            serialization=core_schema.simple_ser_schema('any'),\n        ),\n    )\n    assert v.to_python({MyEnum.A: 'x'}) == {MyEnum.A: 'x'}\n    assert v.to_python({MyEnum.A: 'x'}, mode='json') == {'1': 'x'}\n    assert v.to_json({MyEnum.A: 'x'}) == b'{\"1\":\"x\"}'\n    assert v.to_python(1) == 1\n    assert v.to_json(1) == b'1'", "target": "def test_exact_check(self, schema_validator: SchemaValidator):\n        m_b = schema_validator.validate_python({'c': 2, 'd': 'again'})\n        assert isinstance(m_b, self.ModelB)\n        m_b2 = schema_validator.validate_python(m_b)\n        assert m_b2 is m_b", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "002397", "source": "def constrain(self: _Pipeline[_InT, _NewOutGe], constraint: annotated_types.Ge) -> _Pipeline[_InT, _NewOutGe]: ...", "target": "def constrain(\n        self: _Pipeline[_InT, _NewOutDatetime], constraint: annotated_types.Timezone\n    ) -> _Pipeline[_InT, _NewOutDatetime]: ...", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "validation", "example_id": "002398", "source": "def outMeta(arr_desc0, arr_desc1, arr_desc2):\n        return cv.empty_array_desc()", "target": "def outMeta(desc):\n            out_desc = desc.withType(desc.depth, 1)\n            return out_desc, out_desc, out_desc", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "validation", "example_id": "002399", "source": "def ser_root(self, v: Any, _) -> str:\n            assert self.root == 1_000\n            return f'{v:_}'", "target": "def get_metaclass_hook(self, fullname: str) -> Callable[[ClassDefContext], None] | None:\n        if fullname == MODEL_METACLASS_FULLNAME:\n            return self._pydantic_model_metaclass_marker_callback\n        return None", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "validation", "example_id": "002400", "source": "def forward(\n        nu_value: Tensor, sigma_unconstrained_value: Tensor, beta_value: Tensor\n    ) -> Tensor:\n        sigma_constrained_value = sigma_unconstrained_value.exp()\n        mu = X.mm(beta_value)\n        nu_score = dist.StudentT(nu_value, mu, sigma_constrained_value).log_prob(\n            Y\n        ).sum() + nu.log_prob(nu_value)\n        sigma_score = (\n            dist.StudentT(nu_value, mu, sigma_constrained_value).log_prob(Y).sum()\n            + sigma.log_prob(sigma_constrained_value)\n            + sigma_unconstrained_value\n        )\n        beta_score = dist.StudentT(nu_value, mu, sigma_constrained_value).log_prob(\n            Y\n        ).sum() + beta.log_prob(beta_value)\n        return nu_score.sum() + sigma_score.sum() + beta_score.sum()", "target": "def test_complex_inference() -> None:\n    s = SchemaSerializer(core_schema.any_schema())\n    assert s.to_python(1 + 2j) == 1 + 2j\n    assert s.to_json(1 + 2j) == b'\"1+2j\"'", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "002401", "source": "def eager(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, target = args\n        return lambda: F.cross_entropy(x, target, reduction=\"none\")", "target": "def eager(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, target, dloss = args\n        loss = F.cross_entropy(x, target, reduction=\"none\")\n        return lambda: torch.autograd.grad(\n            loss, x, grad_outputs=dloss, retain_graph=True\n        )", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "002402", "source": "def test_dict(py_and_json: PyAndJson):\n    v = py_and_json({'type': 'dict', 'keys_schema': {'type': 'int'}, 'values_schema': {'type': 'int'}})\n    assert v.validate_test({'1': 2, '3': 4}) == {1: 2, 3: 4}\n    v = py_and_json({'type': 'dict', 'strict': True, 'keys_schema': {'type': 'int'}, 'values_schema': {'type': 'int'}})\n    assert v.validate_test({'1': 2, '3': 4}) == {1: 2, 3: 4}\n    assert v.validate_test({}) == {}\n    with pytest.raises(ValidationError, match=re.escape('[type=dict_type, input_value=[], input_type=list]')):\n        v.validate_test([])", "target": "def test_dict(py_and_json: PyAndJson):\n    v = py_and_json({'type': 'dict', 'keys_schema': {'type': 'time'}, 'values_schema': {'type': 'int'}})\n    assert v.validate_test({'12:01:01': 2, '12:01:02': 4}) == {time(12, 1, 1): 2, time(12, 1, 2): 4}", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "validation", "example_id": "002403", "source": "def test_cycle_same():\n    def fallback_func_passthrough(obj):\n        return obj\n    f = Foobar()\n    with pytest.raises(ValueError, match=r'Circular reference detected \\(id repeated\\)'):\n        to_jsonable_python(f, fallback=fallback_func_passthrough)\n    with pytest.raises(ValueError, match=r'Circular reference detected \\(id repeated\\)'):\n        to_json(f, fallback=fallback_func_passthrough)", "target": "def cfgPrePostProcessing(self, pp_callback):\n            ppp = PrePostProcessor(self.model)\n            pp_callback(ppp)\n            self.model = ppp.build()", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|pytorch/pytorch", "data_split": "validation", "example_id": "002404", "source": "def scatter_time_vs_s(time, norm, point_labels, title):\n    plt.figure()\n    size = 100\n    for i, l in enumerate(sorted(norm.keys())):\n        if l != \"fbpca\":\n            plt.scatter(time[l], norm[l], label=l, marker=\"o\", c=\"b\", s=size)\n            for label, x, y in zip(point_labels, list(time[l]), list(norm[l])):\n                plt.annotate(\n                    label,\n                    xy=(x, y),\n                    xytext=(0, -80),\n                    textcoords=\"offset points\",\n                    ha=\"right\",\n                    arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3\"),\n                    va=\"bottom\",\n                    size=11,\n                    rotation=90,\n                )\n        else:\n            plt.scatter(time[l], norm[l], label=l, marker=\"^\", c=\"red\", s=size)\n            for label, x, y in zip(point_labels, list(time[l]), list(norm[l])):\n                plt.annotate(\n                    label,\n                    xy=(x, y),\n                    xytext=(0, 30),\n                    textcoords=\"offset points\",\n                    ha=\"right\",\n                    arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3\"),\n                    va=\"bottom\",\n                    size=11,\n                    rotation=90,\n                )\n    plt.legend(loc=\"best\")\n    plt.suptitle(title)\n    plt.ylabel(\"norm discrepancy\")\n    plt.xlabel(\"running time [s]\")", "target": "def dynamically_quantize_per_channel(x, quant_min, quant_max, target_dtype):\n    eps = torch.finfo(torch.float32).eps\n    min_val, max_val = torch.aminmax(x, dim=1)\n    min_val_neg = torch.min(min_val, torch.zeros_like(min_val))\n    max_val_pos = torch.max(max_val, torch.zeros_like(max_val))\n    device = min_val_neg.device\n    max_val_pos = torch.max(-min_val_neg, max_val_pos)\n    scales = max_val_pos / (float(quant_max - quant_min) / 2)\n    scales = torch.clamp(scales, min=eps).to(x.dtype)\n    zero_points = torch.zeros(min_val_neg.size(), dtype=torch.int64, device=device)\n    x_div = x / scales.unsqueeze(-1)\n    x_round = torch.round(x_div)\n    x_zp = x_round + zero_points.unsqueeze(-1)\n    quant = torch.clamp(x_zp, quant_min, quant_max).to(target_dtype)\n    return quant, scales, zero_points", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "002405", "source": "def compiled(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, dy = args\n        compiled_softmax = torch.compile(\n            lambda x: F.softmax(x, dim=-1), mode=self.compile_mode, fullgraph=True\n        )\n        y = compiled_softmax(x)\n        return lambda: torch.autograd.grad(y, x, grad_outputs=dy, retain_graph=True)", "target": "def compiled(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, w = args\n        torch._dynamo.mark_dynamic(x, 0)\n        compiled_rms_norm = torch.compile(\n            self.rms_norm_ref, mode=self.compile_mode, fullgraph=True\n        )\n        return lambda: compiled_rms_norm(x, w)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|scikit-learn/scikit-learn", "data_split": "validation", "example_id": "002406", "source": "def fake_temp_env(map: dict[str, str]):\n        temp_calls.append(map)\n        return nullcontext()", "target": "def make_pca_scorers(caller):\n    caller.train_scorer = lambda _, __: caller.estimator.explained_variance_ratio_.sum()\n    caller.test_scorer = lambda _, __: (\n        explained_variance_ratio(caller.estimator.transform(caller.X_val), caller.X_val)\n    )", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "validation", "example_id": "002407", "source": "def type_format(self) -> str:\n        return \"_typing.Sequence[{}]\"", "target": "def type_format(self) -> str:\n        return \"_typing.Type[{}]\"", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|scikit-learn/scikit-learn", "data_split": "validation", "example_id": "002408", "source": "def test_function_wrap_custom_schema():\n    def f(value, serializer, _info):\n        return f'result={serializer(len(value))} repr={serializer!r}'\n    s = SchemaSerializer(\n        core_schema.any_schema(\n            serialization=core_schema.wrap_serializer_function_ser_schema(\n                f, info_arg=True, schema=core_schema.int_schema()\n            )\n        )\n    )\n    assert s.to_python('foo') == 'result=3 repr=SerializationCallable(serializer=int)'\n    assert s.to_python('foo', mode='json') == 'result=3 repr=SerializationCallable(serializer=int)'\n    assert s.to_json('foo') == b'\"result=3 repr=SerializationCallable(serializer=int)\"'", "target": "def make_estimator(self, params):\n        (kernel,) = params\n        estimator = SVC(\n            max_iter=100, tol=1e-16, kernel=kernel, random_state=0, gamma=\"scale\"\n        )\n        return estimator", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "validation", "example_id": "002409", "source": "def train(self, samples, responses):\n        _sample_n, var_n = samples.shape\n        new_samples = self.unroll_samples(samples)\n        new_responses = self.unroll_responses(responses)\n        var_types = np.array([cv.ml.VAR_NUMERICAL] * var_n + [cv.ml.VAR_CATEGORICAL, cv.ml.VAR_CATEGORICAL], np.uint8)\n        self.model.setWeakCount(15)\n        self.model.setMaxDepth(10)\n        self.model.train(cv.ml.TrainData_create(new_samples, cv.ml.ROW_SAMPLE, new_responses.astype(int), varType = var_types))", "target": "def train(self, samples, responses):\n        _sample_n, var_n = samples.shape\n        new_responses = self.unroll_responses(responses).reshape(-1, self.class_n)\n        layer_sizes = np.int32([var_n, 100, 100, self.class_n])\n        self.model.setLayerSizes(layer_sizes)\n        self.model.setTrainMethod(cv.ml.ANN_MLP_BACKPROP)\n        self.model.setBackpropMomentumScale(0)\n        self.model.setBackpropWeightScale(0.001)\n        self.model.setTermCriteria((cv.TERM_CRITERIA_COUNT, 20, 0.01))\n        self.model.setActivationFunction(cv.ml.ANN_MLP_SIGMOID_SYM, 2, 1)\n        self.model.train(samples, cv.ml.ROW_SAMPLE, np.float32(new_responses))", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "validation", "example_id": "002410", "source": "def is_resolved(self) -> bool:\n        return True", "target": "def is_resolved(self) -> bool:\n        return self.value.is_resolved", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "validation", "example_id": "002411", "source": "def convert_returned_scalar_to_tuple(root: NamespaceNode) -> None:\n    float_4_tuple_node = TupleTypeNode(\n        \"ScalarOutput\",\n        items=(PrimitiveTypeNode.float_(),) * 4\n    )\n    def fix_scalar_return_type(fn: FunctionNode.Overload):\n        if fn.return_type is None:\n            return\n        if fn.return_type.type_node.typename == \"Scalar\":\n            fn.return_type.type_node = float_4_tuple_node\n    for overload in for_each_function_overload(root):\n        fix_scalar_return_type(overload)\n    for ns in root.namespaces.values():\n        for overload in for_each_function_overload(ns):\n            fix_scalar_return_type(overload)", "target": "def test_positional_only(py_and_json: PyAndJson, input_value) -> None:\n    v = py_and_json(\n        cs.arguments_v3_schema(\n            [\n                cs.arguments_v3_parameter(name='a', schema=cs.int_schema(), mode='positional_only'),\n                cs.arguments_v3_parameter(\n                    name='b', schema=cs.with_default_schema(cs.bool_schema(), default=True), mode='positional_only'\n                ),\n            ]\n        )\n    )\n    assert v.validate_test(input_value) == ((1, True), {})", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "validation", "example_id": "002412", "source": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "target": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|opencv/opencv", "data_split": "validation", "example_id": "002413", "source": "def file_digest(file, algorithm: str):\n    try:\n        return hashlib.file_digest(file, algorithm)\n    except AttributeError:\n        pass\n    hash = hashlib.new(algorithm)\n    while chunk := file.read(8192):\n        hash.update(chunk)\n    return hash", "target": "def types_separator(self) -> str:\n        return \", \"", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "validation", "example_id": "002414", "source": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "target": "def test_alias_extra(py_and_json: PyAndJson):\n    v = py_and_json(\n        {\n            'type': 'typed-dict',\n            'extra_behavior': 'allow',\n            'fields': {\n                'field_a': {\n                    'validation_alias': [['FieldA'], ['foo', 2]],\n                    'type': 'typed-dict-field',\n                    'schema': {'type': 'int'},\n                }\n            },\n            'config': {'loc_by_alias': False},\n        }\n    )\n    assert v.validate_test({'FieldA': 1}) == {'field_a': 1}\n    assert v.validate_test({'foo': [1, 2, 3]}) == {'field_a': 3}\n    with pytest.raises(ValidationError) as exc_info:\n        assert v.validate_test({'FieldA': '...'}) == {'field_a': 1}\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('field_a',),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': '...',\n        }\n    ]", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|opencv/opencv", "data_split": "validation", "example_id": "002415", "source": "def test_registry(self):\n        self.check_name(cv.videoio_registry.getBackendName(cv.CAP_ANY));\n        self.check_name(cv.videoio_registry.getBackendName(cv.CAP_FFMPEG))\n        self.check_name(cv.videoio_registry.getBackendName(cv.CAP_OPENCV_MJPEG))\n        backends = cv.videoio_registry.getBackends()\n        for backend in backends:\n            self.check_name(cv.videoio_registry.getBackendName(backend))", "target": "def test_kmeans_3d(self):\n            count     = 100\n            sz        = (count, 3)\n            amount    = sz[0]\n            K         = 5\n            flags     = cv.KMEANS_RANDOM_CENTERS\n            attempts  = 1\n            criteria  = (cv.TERM_CRITERIA_MAX_ITER + cv.TERM_CRITERIA_EPS, 30, 0)\n            in_vector = self.generate_random_points(sz)\n            in_labels = []\n            data        = cv.GArrayT(cv.gapi.CV_POINT3F)\n            best_labels = cv.GArrayT(cv.gapi.CV_INT)\n            compactness, out_labels, centers = cv.gapi.kmeans(data, K, best_labels, criteria, attempts, flags)\n            comp = cv.GComputation(cv.GIn(data, best_labels), cv.GOut(compactness, out_labels, centers))\n            compact, labels, centers = comp.apply(cv.gin(in_vector, in_labels))\n            self.assertTrue(compact >= 0)\n            self.assertEqual(amount, len(labels))\n            self.assertEqual(K, len(centers))", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "validation", "example_id": "002416", "source": "def import_string(value: Any) -> Any:\n    if isinstance(value, str):\n        try:\n            return _import_string_logic(value)\n        except ImportError as e:\n            raise PydanticCustomError('import_error', 'Invalid python path: {error}', {'error': str(e)}) from e\n    else:\n        return value", "target": "def search_down_symbol(scope: Optional[ASTNode],\n                           scope_sep: str) -> Optional[ASTNode]:\n        parts = full_symbol_name.split(scope_sep, maxsplit=1)\n        while len(parts) == 2:\n            scope = _resolve_symbol(scope, parts[0])\n            if scope is None:\n                return None\n            node = _resolve_symbol(scope, parts[1])\n            if node is not None:\n                return node\n            parts = parts[1].split(scope_sep, maxsplit=1)\n        return None", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "002417", "source": "def name(self):\n        prefix = f\"{self.category()}_{self.backend()}\"\n        if self.is_dynamic():\n            prefix += \"_dynamic\"\n        if self.device() == \"cuda\":\n            prefix += \"_gpu\"\n        return prefix", "target": "def name(self):\n        prefix = f\"{self.category()}_{self._name}_{self.backend()}\"\n        if self.is_dynamic():\n            prefix += \"_dynamic\"\n        if self._is_gpu:\n            prefix += \"_gpu\"\n        if self._force_shape_pad:\n            prefix += \"_force_shape_pad\"\n        return prefix", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "validation", "example_id": "002418", "source": "def getCMakeArgs(self):\n        args = TestRunner.getCMakeArgs(self)\n        args = args + [\n            \"-DIOS_ARCH=%s\" % self.arch,\n            \"-DIPHONEOS_DEPLOYMENT_TARGET=%s\" % os.environ['IPHONEOS_DEPLOYMENT_TARGET'],\n        ]\n        return args", "target": "def getCMakeArgs(self):\n        args = TestRunner.getCMakeArgs(self)\n        args = args + [\n            '-DMACOSX_DEPLOYMENT_TARGET=%s' % os.environ['MACOSX_DEPLOYMENT_TARGET']\n        ]\n        return args", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "validation", "example_id": "002419", "source": "def _work(self):\n        @torch.compile(\n            backend=self.backend(),\n            fullgraph=True,\n            dynamic=self.is_dynamic(),\n        )\n        def f(a, b):\n            result = a.clone()\n            for i in range(1000):\n                if i % 3 == 0:\n                    result = result + b\n                elif i % 3 == 1:\n                    result = result + 8 * b\n                else:\n                    result = result.sin()\n            return result\n        with fresh_cache():\n            f(self.a, self.b)", "target": "def test_on_error_raise_explicit(self, py_and_json: PyAndJson):\n        v = py_and_json(\n            {\n                'type': 'typed-dict',\n                'fields': {\n                    'x': {\n                        'type': 'typed-dict-field',\n                        'schema': {'type': 'default', 'schema': {'type': 'str'}, 'on_error': 'raise'},\n                    }\n                },\n            }\n        )\n        assert v.validate_test({'x': 'foo'}) == {'x': 'foo'}\n        with pytest.raises(ValidationError) as exc_info:\n            v.validate_test({'x': ['foo']})\n        assert exc_info.value.errors(include_url=False) == [\n            {'input': ['foo'], 'type': 'string_type', 'loc': ('x',), 'msg': 'Input should be a valid string'}\n        ]", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "validation", "example_id": "002420", "source": "def float_(cls, ctype_name: Optional[str] = None,\n               required_modules: Tuple[str, ...] = ()):\n        if ctype_name is None:\n            ctype_name = \"float\"\n        return PrimitiveTypeNode(ctype_name, typename=\"float\", required_modules=required_modules)", "target": "def float_(cls, ctype_name: str, export_name: Optional[str] = None,\n               doc: Optional[str] = None, required_modules: Tuple[str, ...] = ()):\n        return cls(ctype_name, PrimitiveTypeNode.float_(), export_name, doc, required_modules)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "002421", "source": "def area(self) -> int:\n            return self.width * self.height", "target": "def area(self, area: float) -> None:\n            self.side = area**0.5", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "validation", "example_id": "002422", "source": "def relative_typename(self, full_node_name: str) -> str:\n        return self.type_node.relative_typename(full_node_name)", "target": "def relative_typename(self, module: str) -> str:\n        return self.type_format.format(self.types_separator.join(\n            item.relative_typename(module) for item in self\n        ))", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "validation", "example_id": "002423", "source": "def test_simple(self):\n        img1 = self.get_sample('stitching/a1.png')\n        img2 = self.get_sample('stitching/a2.png')\n        stitcher = cv.Stitcher.create(cv.Stitcher_PANORAMA)\n        stitcher.estimateTransform((img1, img2))\n        result, _ = stitcher.composePanorama()\n        assert result == 0", "target": "def test_simple(self):\n        img1 = self.get_sample('stitching/a1.png')\n        img2 = self.get_sample('stitching/a2.png')\n        stitcher = cv.Stitcher.create(cv.Stitcher_PANORAMA)\n        stitcher.estimateTransform((img1, img2))\n        result, _ = stitcher.composePanorama((img1, img2))\n        assert result == 0", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "validation", "example_id": "002424", "source": "def children_types(self) -> Tuple[ASTNodeType, ...]:\n        return ()", "target": "def children_types(self) -> Tuple[ASTNodeType, ...]:\n        return (ASTNodeType.Class, ASTNodeType.Function,\n                ASTNodeType.Enumeration, ASTNodeType.Constant)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "002425", "source": "def compiled(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, target = args\n        torch._dynamo.mark_dynamic(x, 0)\n        torch._dynamo.mark_dynamic(target, 0)\n        compiled_cross_entropy = torch.compile(\n            lambda x, target: F.cross_entropy(x, target, reduction=\"none\"),\n            mode=self.compile_mode,\n            fullgraph=True,\n        )\n        return lambda: compiled_cross_entropy(x, target)", "target": "def compiled(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, w, dy = args\n        y = torch.compile(self.rms_norm_ref, mode=self.compile_mode, fullgraph=True)(\n            x, w\n        )\n        return lambda: torch.autograd.grad(\n            y, [x, w], grad_outputs=dy, retain_graph=True\n        )", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|pydantic/pydantic", "data_split": "validation", "example_id": "002426", "source": "def bhtsne(X):\n            n_iter = -1\n            return (\n                run_bh_tsne(\n                    X,\n                    use_pca=False,\n                    perplexity=args.perplexity,\n                    verbose=args.verbose > 0,\n                ),\n                n_iter,\n            )", "target": "def test_length_constraints_omit(input_value, expected):\n    v = SchemaValidator(\n        core_schema.tuple_schema(\n            items_schema=[core_schema.with_default_schema(schema=core_schema.int_schema(), on_error='omit')],\n            variadic_item_index=0,\n            max_length=4,\n        )\n    )\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            v.validate_python(input_value)\n    else:\n        assert v.validate_python(input_value) == expected", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "validation", "example_id": "002427", "source": "def test_union_set_int_set_str(input_value, expected):\n    v = SchemaValidator(\n        cs.union_schema(\n            choices=[\n                cs.set_schema(items_schema=cs.int_schema(strict=True)),\n                cs.set_schema(items_schema=cs.str_schema(strict=True)),\n            ]\n        )\n    )\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)) as exc_info:\n            v.validate_python(input_value)\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        assert v.validate_python(input_value) == expected", "target": "def forward(*new_params: Tensor) -> Tensor:\n        load_weights(model, names, new_params)\n        out = model(inputs)\n        loss = criterion(\n            out.reshape(N * seq_length, ntoken), targets.reshape(N * seq_length)\n        )\n        return loss", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pytorch/pytorch", "data_split": "validation", "example_id": "002428", "source": "def clean_build_dir(self):\n        for d in [\"CMakeCache.txt\", \"CMakeFiles/\", \"bin/\", \"libs/\", \"lib/\", \"modules\"]:\n            rm_one(d)", "target": "def update(self, input_pos, k_val, v_val):\n        assert input_pos.shape[0] == k_val.shape[2]\n        k_out = self.k_cache\n        v_out = self.v_cache\n        k_out[:, :, input_pos] = k_val\n        v_out[:, :, input_pos] = v_val\n        return k_out, v_out", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pytorch/pytorch", "data_split": "validation", "example_id": "002429", "source": "def add_enumeration(self, name: str) -> EnumerationNode:\n        return self._add_child(EnumerationNode, name)", "target": "def iter_model_names(self, args):\n        from torchbenchmark import _list_canary_model_paths, _list_model_paths\n        models = _list_model_paths()\n        models += [\n            f\n            for f in _list_canary_model_paths()\n            if os.path.basename(f) in self._config[\"canary_models\"]\n        ]\n        models.sort()\n        start, end = self.get_benchmark_indices(len(models))\n        for index, model_path in enumerate(models):\n            if index < start or index >= end:\n                continue\n            model_name = os.path.basename(model_path)\n            if (\n                not re.search(\"|\".join(args.filter), model_name, re.IGNORECASE)\n                or re.search(\"|\".join(args.exclude), model_name, re.IGNORECASE)\n                or model_name in args.exclude_exact\n                or model_name in self.skip_models\n            ):\n                continue\n            yield model_name", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|scikit-learn/scikit-learn", "data_split": "validation", "example_id": "002430", "source": "def setup_preprocessing():\n    try:\n        import pandas\n    except ImportError:\n        raise SkipTest(\"Skipping preprocessing.rst, pandas not installed\")", "target": "def plot_train_losses(clfs):\n    plt.figure()\n    for name, _, _, train_losses, _, _, durations in clfs:\n        plt.plot(durations, train_losses, \"-o\", label=name)\n        plt.legend(loc=0)\n        plt.xlabel(\"seconds\")\n        plt.ylabel(\"train loss\")", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "002431", "source": "def my_function(a, b, c):\n        return a + b + c", "target": "def my_function(input_value, info):\n        return input_value + 'x'", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "validation", "example_id": "002432", "source": "def required_usage_imports(self) -> Generator[str, None, None]:\n        yield \"import cv2.typing\"", "target": "def required_usage_imports(self) -> Generator[str, None, None]:\n        for item in self:\n            yield from item.required_usage_imports", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "002433", "source": "def test_filter_args(params):\n    s = SchemaSerializer(core_schema.dict_schema())\n    include, exclude, expected = params['include'], params['exclude'], IsStrictDict(params['expected'])\n    value = {'0': 0, '1': 1, '2': 2, '3': 3}\n    assert s.to_python(value, include=include, exclude=exclude) == expected\n    assert s.to_python(value, mode='json', include=include, exclude=exclude) == expected\n    assert json.loads(s.to_json(value, include=include, exclude=exclude)) == expected", "target": "def test_filter_args(params):\n    s = SchemaSerializer(core_schema.list_schema())\n    include, exclude, expected = params['include'], params['exclude'], params['expected']\n    value = ['0', '1', '2', '3']\n    assert s.to_python(value, include=include, exclude=exclude) == expected\n    assert s.to_python(value, mode='json', include=include, exclude=exclude) == expected\n    assert json.loads(s.to_json(value, include=include, exclude=exclude)) == expected", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "validation", "example_id": "002434", "source": "def rnn_tanh_cell(input, hidden, w_ih, w_hh, b_ih, b_hh):\n    igates = torch.mm(input, w_ih.t()) + b_ih\n    hgates = torch.mm(hidden, w_hh.t()) + b_hh\n    return torch.tanh(igates + hgates)", "target": "def test_var_args(py_and_json: PyAndJson, input_value, expected) -> None:\n    v = py_and_json(\n        cs.arguments_v3_schema(\n            [\n                cs.arguments_v3_parameter(name='args', schema=cs.int_schema(), mode='var_args'),\n            ]\n        )\n    )\n    assert v.validate_test(input_value) == expected", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "002435", "source": "def test_allow_extra_invalid():\n    with pytest.raises(SchemaError, match='extras_schema can only be used if extra_behavior=allow'):\n        SchemaValidator(\n            schema=core_schema.model_fields_schema(\n                fields={}, extras_schema=core_schema.int_schema(), extra_behavior='ignore'\n            )\n        )\n    with pytest.raises(SchemaError, match='extras_keys_schema can only be used if extra_behavior=allow'):\n        SchemaValidator(\n            schema=core_schema.model_fields_schema(\n                fields={}, extras_keys_schema=core_schema.int_schema(), extra_behavior='ignore'\n            )\n        )", "target": "def test_allow_extra_invalid():\n    with pytest.raises(SchemaError, match='extras_schema can only be used if extra_behavior=allow'):\n        SchemaValidator(\n            schema=core_schema.typed_dict_schema(\n                fields={}, extras_schema=core_schema.int_schema(), extra_behavior='ignore'\n            )\n        )", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "validation", "example_id": "002436", "source": "def read_colors(colors):\n        result = []\n        for color in colors:\n            result.append(DatasetImageFetch.pix_to_c(color))\n        return result", "target": "def _dict_not_none(**kwargs: Any) -> Any:\n    return {k: v for k, v in kwargs.items() if v is not None}", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|opencv/opencv", "data_split": "validation", "example_id": "002437", "source": "def add_overload(self, arguments: Sequence[\"FunctionNode.Arg\"] = (),\n                     return_type: Optional[\"FunctionNode.RetType\"] = None):\n        self.overloads.append(FunctionNode.Overload(arguments, return_type))", "target": "def test_parse_to_int64_convertible(self):\n        try_to_convert = partial(self._try_to_convert, cv.utils.dumpInt64)\n        min_int64, max_int64 = get_limits(ctypes.c_longlong)\n        for convertible in (-10, -1, 2, int(43.2), np.uint8(15), np.int8(33), np.int16(-13),\n                            np.int32(4), np.int64(345), (23), min_int64, max_int64, np.int_(33)):\n            expected = 'int64: {0:d}'.format(convertible)\n            actual = try_to_convert(convertible)\n            self.assertEqual(expected, actual,\n                             msg=get_conversion_error_msg(convertible, expected, actual))", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "validation", "example_id": "002438", "source": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "target": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|opencv/opencv", "data_split": "validation", "example_id": "002439", "source": "def pytest_addoption(parser):\n    parser.addoption(\"--fuser\", default=\"old\", help=\"fuser to use for benchmarks\")\n    parser.addoption(\n        \"--executor\", default=\"legacy\", help=\"executor to use for benchmarks\"\n    )", "target": "def test_parse_to_term_criteria_not_convertible(self):\n        for not_convertible in ([], (), np.array([]), [1, 4], (10,), (1.5, 34, 0.1),\n                                {1: 5, 3: 5, 10: 10}, '145'):\n            with self.assertRaises((TypeError), msg=get_no_exception_msg(not_convertible)):\n                _ = cv.utils.dumpTermCriteria(not_convertible)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "002440", "source": "def root_validator(\n    *,\n    pre: Literal[True],\n    allow_reuse: bool = ...,\n) -> Callable[\n    [_V1RootValidatorFunctionType],\n    _V1RootValidatorFunctionType,\n]: ...", "target": "def root_validator(\n    *,\n    pre: Literal[False],\n    skip_on_failure: Literal[True],\n    allow_reuse: bool = ...,\n) -> Callable[\n    [_V1RootValidatorFunctionType],\n    _V1RootValidatorFunctionType,\n]: ...", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "validation", "example_id": "002441", "source": "def checkCMakeVersion(self):\n        assert get_cmake_version() >= (3, 17), \"CMake 3.17 or later is required. Current version is {}\".format(get_cmake_version())", "target": "def checkCMakeVersion(self):\n        assert get_cmake_version() >= (3, 17), \"CMake 3.17 or later is required. Current version is {}\".format(get_cmake_version())", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "validation", "example_id": "002442", "source": "def test_copy_dir_to_new_dir(self):\n        src = self.tmp_path / \"srcdir\"\n        (src / \"a\").mkdir(parents=True)\n        (src / \"a\" / \"f.txt\").write_text(\"content\")\n        dst = self.tmp_path / \"destdir\"\n        copy(src, dst)\n        self.assertEqual((dst / \"a\" / \"f.txt\").read_text(), \"content\")", "target": "def description(self) -> str:\n        return \"a mm 100 times in a loop with max auto tune on\"", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "002443", "source": "def get_shapes(self) -> tuple[tuple[int, ...], ...]:\n        return (\n            (32768, 256),\n            (32768, 512),\n            (32768, 1024),\n            (32768, 2048),\n            (32768, 4096),\n            (32768, 8192),\n            (32768, 16384),\n            (32768, 32768),\n            (32768, 65536),\n        ) + extra_shapes_for_norm", "target": "def get_shapes(self) -> tuple[tuple[int, ...], ...]:\n        return (\n            (32768, 256),\n            (32768, 512),\n            (32768, 1024),\n            (32768, 2048),\n            (32768, 4096),\n            (32768, 8192),\n            (32768, 16384),\n            (32768, 32768),\n            (32768, 65536),\n        ) + extra_shapes_for_norm", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "validation", "example_id": "002444", "source": "def make_estimator(self, params):\n        (n_jobs,) = params\n        clf = RandomForestClassifier(random_state=0)\n        if Benchmark.data_size == \"large\":\n            n_estimators_list = [10, 25, 50, 100, 500]\n            max_depth_list = [5, 10, None]\n            max_features_list = [0.1, 0.4, 0.8, 1.0]\n        else:\n            n_estimators_list = [10, 25, 50]\n            max_depth_list = [5, 10]\n            max_features_list = [0.1, 0.4, 0.8]\n        param_grid = {\n            \"n_estimators\": n_estimators_list,\n            \"max_depth\": max_depth_list,\n            \"max_features\": max_features_list,\n        }\n        estimator = GridSearchCV(clf, param_grid, n_jobs=n_jobs, cv=4)\n        return estimator", "target": "def make_estimator(self, params):\n        representation, n_jobs = params\n        n_estimators = 500 if Benchmark.data_size == \"large\" else 100\n        estimator = RandomForestClassifier(\n            n_estimators=n_estimators,\n            min_samples_split=10,\n            max_features=\"log2\",\n            n_jobs=n_jobs,\n            random_state=0,\n        )\n        return estimator", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "validation", "example_id": "002445", "source": "def getToolchain(self):\n        toolchain = os.path.join(self.script_dir, \"cmake\", \"Toolchains\", \"Toolchain-%s_Xcode.cmake\" % self.target)\n        return toolchain", "target": "def getToolchain(self, arch, target):\n        return None", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "validation", "example_id": "002446", "source": "def test_in_union():\n    def my_function(a):\n        return a\n    v = SchemaValidator(\n        cs.union_schema(\n            choices=[\n                cs.call_schema(\n                    function=my_function,\n                    arguments=cs.arguments_schema(\n                        arguments=[{'name': 'a', 'mode': 'positional_or_keyword', 'schema': cs.int_schema()}]\n                    ),\n                ),\n                cs.int_schema(),\n            ]\n        )\n    )\n    assert v.validate_python((1,)) == 1\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python((1, 2))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'unexpected_positional_argument',\n            'loc': ('call[my_function]', 1),\n            'msg': 'Unexpected positional argument',\n            'input': 2,\n        },\n        {'type': 'int_type', 'loc': ('int',), 'msg': 'Input should be a valid integer', 'input': (1, 2)},\n    ]", "target": "def get_first_not_none(a: Any, b: Any) -> Any:\n    return a if a is not None else b", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "002447", "source": "def test_constraints_schema_validation() -> None:\n    val = SchemaValidator(cs.decimal_schema(gt='1'))\n    with pytest.raises(ValidationError):\n        val.validate_python('0')", "target": "def test_constraints_schema_validation() -> None:\n    val = SchemaValidator(cs.date_schema(gt='2020-01-01'))\n    with pytest.raises(ValidationError):\n        val.validate_python('2019-01-01')", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "validation", "example_id": "002448", "source": "def required_usage_imports(self) -> Generator[str, None, None]:\n        yield from ()", "target": "def required_usage_imports(self) -> Generator[str, None, None]:\n        if TypeNode.compatible_to_runtime_usage:\n            yield \"import typing as _typing\"\n        yield from super().required_usage_imports", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|scikit-learn/scikit-learn", "data_split": "validation", "example_id": "002449", "source": "def gen(error: bool):\n        yield 1\n        yield 2\n        if error:\n            raise RuntimeError('my error')\n        yield 3", "target": "def fixed_batch_size_comparison(data):\n    all_features = [\n        i.astype(int) for i in np.linspace(data.shape[1] // 10, data.shape[1], num=5)\n    ]\n    batch_size = 1000\n    all_times = defaultdict(list)\n    all_errors = defaultdict(list)\n    for n_components in all_features:\n        pca = PCA(n_components=n_components)\n        ipca = IncrementalPCA(n_components=n_components, batch_size=batch_size)\n        results_dict = {\n            k: benchmark(est, data) for k, est in [(\"pca\", pca), (\"ipca\", ipca)]\n        }\n        for k in sorted(results_dict.keys()):\n            all_times[k].append(results_dict[k][\"time\"])\n            all_errors[k].append(results_dict[k][\"error\"])\n    plot_feature_times(all_times, batch_size, all_features, data)\n    plot_feature_errors(all_errors, batch_size, all_features, data)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "002450", "source": "def quack(self, args, kwargs=None) -> Any:\n        from quack.rmsnorm import _rmsnorm_fwd\n        x, w = args\n        y = torch.empty_like(x)\n        def quack_fwd():\n            _rmsnorm_fwd(\n                x,\n                w,\n                out=y,\n                bias=None,\n                rstd=None,\n                residual=None,\n                residual_out=None,\n                eps=1e-6,\n            )\n            return y\n        return quack_fwd", "target": "def quack(self, args, kwargs=None) -> Any:\n        from quack.rmsnorm import _get_sm_count, _rmsnorm_bwd\n        (\n            x,\n            w,\n            dy,\n        ) = args\n        M, N = x.shape\n        rstd = self.compute_rstd(x, eps=1e-6)\n        dx = torch.empty_like(x)\n        sm_count = _get_sm_count(x.size(1), x.device)\n        dw_partial = torch.empty(\n            sm_count, x.size(1), device=x.device, dtype=torch.float32\n        )\n        def quack_bwd():\n            _rmsnorm_bwd(\n                x,\n                w,\n                dy,\n                rstd,\n                dx,\n                dw_partial,\n                db_partial=None,\n                dresidual_out=None,\n                dresidual=None,\n                sm_count=sm_count,\n            )\n            dw = dw_partial.sum(dim=0).to(w.dtype)\n            return dx, dw\n        return quack_bwd", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "002451", "source": "def precompute_freqs_cis(seq_len: int, n_elem: int, base: int = 10000) -> Tensor:\n    freqs = 1.0 / (\n        base ** (torch.arange(0, n_elem, 2)[: (n_elem // 2)].float() / n_elem)\n    )\n    t = torch.arange(seq_len, device=freqs.device)\n    freqs = torch.outer(t, freqs)\n    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n    cache = torch.stack([freqs_cis.real, freqs_cis.imag], dim=-1)\n    return cache.to(dtype=torch.bfloat16)", "target": "def precompute_freqs_cis(seq_len: int, n_elem: int, base: int = 10000) -> Tensor:\n    freqs = 1.0 / (\n        base ** (torch.arange(0, n_elem, 2)[: (n_elem // 2)].float() / n_elem)\n    )\n    t = torch.arange(seq_len, device=freqs.device)\n    freqs = torch.outer(t, freqs)\n    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n    cache = torch.stack([freqs_cis.real, freqs_cis.imag], dim=-1)\n    return cache.to(dtype=torch.bfloat16)", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|opencv/opencv", "data_split": "validation", "example_id": "002452", "source": "def create_class_node_in_scope(scope: Union[NamespaceNode, ClassNode],\n                               symbol_name: SymbolName,\n                               class_info) -> ClassNode:\n    properties = []\n    for property in class_info.props:\n        export_property_name = property.name\n        if keyword.iskeyword(export_property_name):\n            export_property_name += \"_\"\n        properties.append(\n            ClassProperty(\n                name=export_property_name,\n                type_node=create_type_node(property.tp),\n                is_readonly=property.readonly\n            )\n        )\n    class_node = scope.add_class(symbol_name.name,\n                                 properties=properties)\n    class_node.export_name = class_info.export_name\n    if class_info.constructor is not None:\n        create_function_node_in_scope(class_node, class_info.constructor)\n    for method in class_info.methods.values():\n        create_function_node_in_scope(class_node, method)\n    return class_node", "target": "def test_parse_to_string_convertible(self):\n        try_to_convert = partial(self._try_to_convert, cv.utils.dumpString)\n        for convertible in (None, '', 's', 'str', str(123), np.str_('test2')):\n            expected = 'string: ' + (convertible if convertible else '')\n            actual = try_to_convert(convertible)\n            self.assertEqual(expected, actual,\n                             msg=get_conversion_error_msg(convertible, expected, actual))", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "validation", "example_id": "002453", "source": "def ge(self: _Pipeline[_InT, _NewOutGe], ge: _NewOutGe) -> _Pipeline[_InT, _NewOutGe]:\n        return self.constrain(annotated_types.Ge(ge))", "target": "def test_function_before():\n    def f(input_value, _info):\n        assert isinstance(input_value, dict)\n        input_value['field_a'] += b' XX'\n        return input_value\n    v = SchemaValidator(\n        {\n            'type': 'function-before',\n            'function': {'type': 'with-info', 'function': f},\n            'schema': core_schema.model_schema(\n                cls=MyModel,\n                schema=core_schema.model_fields_schema(\n                    fields={\n                        'field_a': core_schema.model_field(schema=core_schema.str_schema()),\n                        'field_b': core_schema.model_field(schema=core_schema.int_schema()),\n                    }\n                ),\n            ),\n        }\n    )\n    m = v.validate_python({'field_a': b'321', 'field_b': '12'})\n    assert isinstance(m, MyModel)\n    assert m.field_a == '321 XX'\n    assert m.field_b == 12\n    m2 = MyModel()\n    v.validate_python({'field_a': b'321', 'field_b': '12'}, self_instance=m2)\n    assert m2.__dict__ == {'field_a': '321 XX', 'field_b': 12}\n    assert m2.__pydantic_fields_set__ == {'field_a', 'field_b'}", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "validation", "example_id": "002454", "source": "def execute(cmd, cwd = None):\n    print(\"Executing: %s in %s\" % (cmd, cwd), file=sys.stderr)\n    print('Executing: ' + ' '.join(cmd))\n    retcode = check_call(cmd, cwd = cwd)\n    if retcode != 0:\n        raise Exception(\"Child returned:\", retcode)", "target": "def execute(cmd, cwd = None):\n    print(\"Executing: %s in %s\" % (cmd, cwd), file=sys.stderr)\n    print('Executing: ' + ' '.join(cmd))\n    retcode = check_call(cmd, cwd = cwd)\n    if retcode != 0:\n        raise Exception(\"Child returned:\", retcode)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "002455", "source": "def func(*args: Any) -> Any:\n        calls.append(args)\n        return args[0]", "target": "def func(*args: Any) -> Any:\n        calls.append(args)\n        return args[0]", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|opencv/opencv", "data_split": "validation", "example_id": "002456", "source": "def get_shapes(self) -> tuple[tuple[int, ...], ...]:\n        return (\n            (32768, 256),\n            (32768, 512),\n            (32768, 1024),\n            (32768, 2048),\n            (32768, 4096),\n            (32768, 8192),\n            (32768, 16384),\n            (32768, 32768),\n            (32768, 65536),\n            (16384, 131072),\n            (8192, 262144),\n        )", "target": "def check_close_angles(self, a, b, angle_delta):\n        self.assertTrue(abs(a - b) <= angle_delta or\n                        abs(360 - abs(a - b)) <= angle_delta)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|scikit-learn/scikit-learn", "data_split": "validation", "example_id": "002457", "source": "def test_config_time(\n    t: date, expected_to_python, expected_to_json, expected_to_python_dict, expected_to_json_dict, mode\n):\n    s = SchemaSerializer(core_schema.time_schema(), config={'ser_json_temporal': mode})\n    assert s.to_python(t) == t\n    assert s.to_python(t, mode='json') == expected_to_python\n    assert s.to_json(t) == expected_to_json\n    with pytest.warns(\n        UserWarning,\n        match=(\n            r'Expected `time` - serialized value may not be as expected '\n            r\"\\[input_value=\\{datetime\\.time\\([^)]*\\): 'foo'\\}, input_type=dict\\]\"\n        ),\n    ):\n        assert s.to_python({t: 'foo'}) == {t: 'foo'}\n    with pytest.warns(\n        UserWarning,\n        match=(\n            r'Expected `time` - serialized value may not be as expected '\n            r\"\\[input_value=\\{datetime\\.time\\([^)]*\\): 'foo'\\}, input_type=dict\\]\"\n        ),\n    ):\n        assert s.to_python({t: 'foo'}, mode='json') == expected_to_python_dict\n    with pytest.warns(\n        UserWarning,\n        match=(\n            r'Expected `time` - serialized value may not be as expected '\n            r\"\\[input_value=\\{datetime\\.time\\([^)]*\\): 'foo'\\}, input_type=dict\\]\"\n        ),\n    ):\n        assert s.to_json({t: 'foo'}) == expected_to_json_dict", "target": "def bench(args):\n    results_dir = Path(args.bench_results)\n    branch = args.branch\n    random_state = 1\n    results = defaultdict(list)\n    n_samples_train = 1000\n    for n_samples_test in [\n        1000,\n        10000,\n        50000,\n    ]:\n        for n_features in [10, 100, 1000]:\n            for contamination in [0.01, 0.1, 0.5]:\n                for n_jobs in [1, 2, 3, 4]:\n                    X_train, X_test = get_data(\n                        n_samples_train,\n                        n_samples_test,\n                        n_features,\n                        contamination,\n                        random_state,\n                    )\n                    print(\"--- Fitting the IsolationForest estimator...\")\n                    model = IsolationForest(n_jobs=-1, random_state=random_state)\n                    tstart = time()\n                    model.fit(X_train)\n                    fit_time = time() - tstart\n                    for _ in range(1000):\n                        1 + 1\n                    with parallel_config(\"threading\", n_jobs=n_jobs):\n                        tstart = time()\n                        model.decision_function(X_test)\n                        predict_time = time() - tstart\n                    results[\"predict_time\"].append(predict_time)\n                    results[\"fit_time\"].append(fit_time)\n                    results[\"n_samples_train\"].append(n_samples_train)\n                    results[\"n_samples_test\"].append(n_samples_test)\n                    results[\"n_features\"].append(n_features)\n                    results[\"contamination\"].append(contamination)\n                    results[\"n_jobs\"].append(n_jobs)\n    df = pd.DataFrame(results)\n    df.to_csv(results_dir / f\"{branch}.csv\", index=False)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "validation", "example_id": "002458", "source": "def create_type_node(typename: str,\n                     original_ctype_name: Optional[str] = None) -> TypeNode:\n    if original_ctype_name is None:\n        original_ctype_name = typename\n    typename = normalize_ctype_name(typename.strip())\n    type_node = PREDEFINED_TYPES.get(typename)\n    if type_node is not None:\n        type_node.ctype_name = original_ctype_name\n        return type_node\n    for alias in PREDEFINED_TYPES.values():\n        if alias.typename == typename:\n            return alias\n    if is_union_type(typename):\n        union_types = get_template_instantiation_type(typename)\n        return UnionTypeNode(\n            original_ctype_name,\n            items=create_type_nodes_from_template_arguments(union_types)\n        )\n    if is_sequence_type(typename):\n        if _is_template_instantiation(typename):\n            inner_sequence_type = create_type_node(\n                get_template_instantiation_type(typename)\n            )\n        else:\n            inner_sequence_type = create_type_node(typename.split(\"_\", 1)[-1])\n        return SequenceTypeNode(original_ctype_name, inner_sequence_type)\n    if is_tuple_type(typename):\n        tuple_types = get_template_instantiation_type(typename)\n        return TupleTypeNode(\n            original_ctype_name,\n            items=create_type_nodes_from_template_arguments(tuple_types)\n        )\n    return ASTNodeTypeNode(original_ctype_name, typename)", "target": "def test_slots_mixed(any_serializer):\n    @dataclasses.dataclass(slots=True)\n    class Model:\n        x: int\n        y: dataclasses.InitVar[str]\n        z: ClassVar[str] = 'z-classvar'\n    @dataclasses.dataclass\n    class SubModel(Model):\n        x2: int\n        y2: dataclasses.InitVar[str]\n        z2: ClassVar[str] = 'z2-classvar'\n    dc = SubModel(x=1, y='a', x2=2, y2='b')\n    assert dataclasses.asdict(dc) == {'x': 1, 'x2': 2}\n    assert any_serializer.to_python(dc) == {'x': 1, 'x2': 2}\n    assert any_serializer.to_json(dc) == b'{\"x\":1,\"x2\":2}'", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "validation", "example_id": "002459", "source": "def types_separator(self) -> str:\n        return \"\"", "target": "def types_separator(self) -> str:\n        if TypeNode.compatible_to_runtime_usage:\n            return \", \"\n        return \" | \"", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "validation", "example_id": "002460", "source": "def test_json_bytes_hex_invalid():\n    v = SchemaValidator(core_schema.bytes_schema(), config=CoreConfig(val_json_bytes='hex'))\n    wrong_input = 'a'\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_json(json.dumps(wrong_input))\n    assert exc_info.value.errors(include_url=False, include_context=False) == [\n        {\n            'type': 'bytes_invalid_encoding',\n            'loc': (),\n            'msg': 'Data should be valid hex: Odd number of digits',\n            'input': wrong_input,\n        }\n    ]\n    wrong_input = 'ag'\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_json(json.dumps(wrong_input))\n    assert exc_info.value.errors(include_url=False, include_context=False) == [\n        {\n            'type': 'bytes_invalid_encoding',\n            'loc': (),\n            'msg': \"Data should be valid hex: Invalid character 'g' at position 1\",\n            'input': wrong_input,\n        }\n    ]", "target": "def test_union_of_unions_of_models_with_tagged_union_json_serialization(\n    input: dict[str, bool | int | float | str], expected: bytes\n) -> None:\n    s = SchemaSerializer(\n        core_schema.dict_schema(\n            keys_schema=core_schema.str_schema(),\n            values_schema=core_schema.union_schema(\n                [\n                    core_schema.union_schema([core_schema.bool_schema(), core_schema.int_schema()]),\n                    core_schema.union_schema([core_schema.float_schema(), core_schema.str_schema()]),\n                ]\n            ),\n        )\n    )\n    assert s.to_json(input, warnings='error') == expected", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "002461", "source": "def test_invalid_schema():\n    with pytest.raises(SchemaError, match=\"'default' and 'default_factory' cannot be used together\"):\n        SchemaValidator(\n            schema=cs.arguments_schema(\n                arguments=[\n                    {\n                        'name': 'a',\n                        'mode': 'positional_or_keyword',\n                        'schema': cs.with_default_schema(schema=cs.int_schema(), default=1, default_factory=lambda: 2),\n                    }\n                ]\n            )\n        )", "target": "def test_invalid_schema():\n    with pytest.raises(SchemaError, match='Definitions error: definition `Branch` was never filled'):\n        SchemaValidator(\n            schema=cs.list_schema(\n                items_schema=cs.typed_dict_schema(\n                    fields={\n                        'width': cs.typed_dict_field(schema=cs.int_schema()),\n                        'branch': cs.typed_dict_field(\n                            schema=cs.with_default_schema(\n                                schema=cs.nullable_schema(schema=cs.definition_reference_schema(schema_ref='Branch')),\n                                default=None,\n                            )\n                        ),\n                    }\n                )\n            )\n        )", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "validation", "example_id": "002462", "source": "def testOverwriteNativeMethod(arg):\n    return NativeMethodPatchedResult(\n        arg + 1,\n        cv2.utils._native.testOverwriteNativeMethod(arg)\n    )", "target": "def push(self, typ: type[Any] | TypeAliasType, /) -> Generator[None]:\n        self._types_stack.append(typ)\n        self.__dict__.pop('types_namespace', None)\n        try:\n            yield\n        finally:\n            self._types_stack.pop()\n            self.__dict__.pop('types_namespace', None)", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|pytorch/pytorch", "data_split": "validation", "example_id": "002463", "source": "def _20newsgroups_highdim_dataset(n_samples=None, ngrams=(1, 1), dtype=np.float32):\n    newsgroups = fetch_20newsgroups(random_state=0)\n    vectorizer = TfidfVectorizer(ngram_range=ngrams, dtype=dtype)\n    X = vectorizer.fit_transform(newsgroups.data[:n_samples])\n    y = newsgroups.target[:n_samples]\n    X, X_val, y, y_val = train_test_split(X, y, test_size=0.1, random_state=0)\n    return X, X_val, y, y_val", "target": "def inductor_triton_mm(a, b):\n    return torch.mm(a, b)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "validation", "example_id": "002464", "source": "def outMeta(arr_desc0, arr_desc1):\n        return cv.empty_array_desc(), cv.empty_array_desc()", "target": "def outMeta(desc, max_corners, quality_lvl,\n                    min_distance, block_sz,\n                    use_harris_detector, k):\n            return cv.empty_array_desc()", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "validation", "example_id": "002465", "source": "def make_estimator(self, params):\n        representation, solver = params\n        estimator = Ridge(solver=solver, fit_intercept=False, random_state=0)\n        return estimator", "target": "def make_estimator(self, params):\n        (kernel,) = params\n        estimator = SVC(\n            max_iter=100, tol=1e-16, kernel=kernel, random_state=0, gamma=\"scale\"\n        )\n        return estimator", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "validation", "example_id": "002466", "source": "def test_constraints_schema_validation_error(constraint: str) -> None:\n    with pytest.raises(SchemaError, match=f\"'{constraint}' must be coercible to a Decimal instance\"):\n        SchemaValidator(cs.decimal_schema(**{constraint: 'bad_value'}))", "target": "def _work(self):\n        if self._inference_mode:\n            with inference_mode():\n                self._add1(self.a)\n        elif self._backward:\n            self.forward_val.backward()\n        else:\n            self._add1(self.a)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "002467", "source": "def f(value, serializer, _info):\n        return f'result={serializer(len(value))}'", "target": "def f(value, serializer, _info):\n        return f'result={serializer(len(value))} repr={serializer!r}'", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "validation", "example_id": "002468", "source": "def test_person_detection_retail_0013(self):\n            if not cv.dnn.DNN_TARGET_CPU in cv.dnn.getAvailableTargets(cv.dnn.DNN_BACKEND_INFERENCE_ENGINE):\n                return\n            root_path    = '/omz_intel_models/intel/person-detection-retail-0013/FP32/person-detection-retail-0013'\n            model_path   = self.find_file(root_path + '.xml',   [os.environ.get('OPENCV_DNN_TEST_DATA_PATH')], required=False)\n            weights_path = self.find_file(root_path + '.bin',   [os.environ.get('OPENCV_DNN_TEST_DATA_PATH')], required=False)\n            img_path     = self.find_file('gpu/lbpcascade/er.png', [os.environ.get('OPENCV_TEST_DATA_PATH')])\n            device_id    = 'CPU'\n            img          = cv.resize(cv.imread(img_path), (544, 320))\n            net = cv.dnn.readNetFromModelOptimizer(model_path, weights_path)\n            net.setPreferableBackend(cv.dnn.DNN_BACKEND_INFERENCE_ENGINE)\n            net.setPreferableTarget(cv.dnn.DNN_TARGET_CPU)\n            blob = cv.dnn.blobFromImage(img)\n            def parseSSD(detections, size):\n                h, w = size\n                bboxes = []\n                detections = detections.reshape(-1, 7)\n                for sample_id, class_id, confidence, xmin, ymin, xmax, ymax in detections:\n                    if confidence >= 0.5:\n                        x      = int(xmin * w)\n                        y      = int(ymin * h)\n                        width  = int(xmax * w - x)\n                        height = int(ymax * h - y)\n                        bboxes.append((x, y, width, height))\n                return bboxes\n            net.setInput(blob)\n            dnn_detections = net.forward()\n            dnn_boxes = parseSSD(np.array(dnn_detections), img.shape[:2])\n            g_in   = cv.GMat()\n            inputs = cv.GInferInputs()\n            inputs.setInput('data', g_in)\n            g_sz       = cv.gapi.streaming.size(g_in)\n            outputs    = cv.gapi.infer(\"net\", inputs)\n            detections = outputs.at(\"detection_out\")\n            bboxes     = cv.gapi.parseSSD(detections, g_sz, 0.5, False, False)\n            comp = cv.GComputation(cv.GIn(g_in), cv.GOut(bboxes))\n            pp = cv.gapi.ie.params(\"net\", model_path, weights_path, device_id)\n            gapi_boxes = comp.apply(cv.gin(img.astype(np.float32)),\n                                    args=cv.gapi.compile_args(cv.gapi.networks(pp)))\n            self.assertEqual(0.0, cv.norm(np.array(dnn_boxes).flatten(),\n                                          np.array(gapi_boxes).flatten(),\n                                          cv.NORM_INF))", "target": "def test_person_detection_retail_0013(self):\n            if not cv.dnn.DNN_TARGET_CPU in cv.dnn.getAvailableTargets(cv.dnn.DNN_BACKEND_INFERENCE_ENGINE):\n                return\n            root_path    = '/omz_intel_models/intel/person-detection-retail-0013/FP32/person-detection-retail-0013'\n            model_path   = self.find_file(root_path + '.xml',   [os.environ.get('OPENCV_DNN_TEST_DATA_PATH')], required=False)\n            weights_path = self.find_file(root_path + '.bin',   [os.environ.get('OPENCV_DNN_TEST_DATA_PATH')], required=False)\n            img_path     = self.find_file('gpu/lbpcascade/er.png', [os.environ.get('OPENCV_TEST_DATA_PATH')])\n            device_id    = 'CPU'\n            img          = cv.resize(cv.imread(img_path), (544, 320))\n            net = cv.dnn.readNetFromModelOptimizer(model_path, weights_path)\n            net.setPreferableBackend(cv.dnn.DNN_BACKEND_INFERENCE_ENGINE)\n            net.setPreferableTarget(cv.dnn.DNN_TARGET_CPU)\n            blob = cv.dnn.blobFromImage(img)\n            def parseSSD(detections, size):\n                h, w = size\n                bboxes = []\n                detections = detections.reshape(-1, 7)\n                for sample_id, class_id, confidence, xmin, ymin, xmax, ymax in detections:\n                    if confidence >= 0.5:\n                        x      = int(xmin * w)\n                        y      = int(ymin * h)\n                        width  = int(xmax * w - x)\n                        height = int(ymax * h - y)\n                        bboxes.append((x, y, width, height))\n                return bboxes\n            net.setInput(blob)\n            dnn_detections = net.forward()\n            dnn_boxes = parseSSD(np.array(dnn_detections), img.shape[:2])\n            g_in   = cv.GMat()\n            inputs = cv.GInferInputs()\n            inputs.setInput('data', g_in)\n            g_sz       = cv.gapi.streaming.size(g_in)\n            outputs    = cv.gapi.infer(\"net\", inputs)\n            detections = outputs.at(\"detection_out\")\n            bboxes     = cv.gapi.parseSSD(detections, g_sz, 0.5, False, False)\n            comp = cv.GComputation(cv.GIn(g_in), cv.GOut(bboxes))\n            pp = cv.gapi.ie.params(\"net\", model_path, weights_path, device_id)\n            gapi_boxes = comp.apply(cv.gin(img.astype(np.float32)),\n                                    args=cv.gapi.compile_args(cv.gapi.networks(pp)))\n            self.assertEqual(0.0, cv.norm(np.array(dnn_boxes).flatten(),\n                                          np.array(gapi_boxes).flatten(),\n                                          cv.NORM_INF))", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "validation", "example_id": "002469", "source": "def type_format(self) -> str:\n        if TypeNode.compatible_to_runtime_usage:\n            return \"_typing.Tuple[{}]\"\n        return \"tuple[{}]\"", "target": "def type_format(self) -> str:\n        if TypeNode.compatible_to_runtime_usage:\n            return \"_typing.Dict[{}]\"\n        return \"dict[{}]\"", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "validation", "example_id": "002470", "source": "def tokenize_function(examples):\n        return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)", "target": "def test_alias(py_and_json: PyAndJson):\n    v = py_and_json(\n        {\n            'type': 'model-fields',\n            'fields': {'field_a': {'validation_alias': 'FieldA', 'type': 'model-field', 'schema': {'type': 'int'}}},\n        }\n    )\n    assert v.validate_test({'FieldA': '123'}) == ({'field_a': 123}, None, {'field_a'})\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'foobar': '123'})\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'field_a': '123'})", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "validation", "example_id": "002471", "source": "def test_model_root_function_assignment(mode: str, calls1: Any, calls2: Any):\n    calls: list[Any] = []\n    class Model:\n        __slots__ = '__dict__', '__pydantic_fields_set__', '__pydantic_extra__', '__pydantic_private__'\n        x: str\n        y: int\n        def __init__(self, **kwargs: Any) -> None:\n            self.__dict__.update(kwargs)\n    def f(input_value: Any, *args: Any) -> Any:\n        if mode == 'wrap':\n            handler, _ = args\n            calls.append({'value': input_value})\n            return handler(input_value)\n        else:\n            calls.append({'value': input_value})\n            return input_value\n    v = SchemaValidator(\n        core_schema.model_schema(\n            Model,\n            {\n                'type': f'function-{mode}',\n                'function': {'type': 'with-info', 'function': f},\n                'schema': core_schema.model_fields_schema(\n                    {\n                        'x': core_schema.model_field(core_schema.str_schema()),\n                        'y': core_schema.model_field(core_schema.int_schema()),\n                    }\n                ),\n            },\n        )\n    )\n    m = Model()\n    v.validate_python({'x': b'input', 'y': '123'}, self_instance=m)\n    assert m.x == 'input'\n    assert m.y == 123\n    assert calls == [calls1]\n    v.validate_assignment(m, 'x', b'different')\n    assert calls == [calls1, calls2]", "target": "def build_perf(self):\n        execute([\"make\", \"-j\", str(multiprocessing.cpu_count()), \"opencv_js_perf\"])", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|pytorch/pytorch", "data_split": "validation", "example_id": "002472", "source": "def generate_synthetic_data():\n    rs = np.random.RandomState(0)\n    n_pts = 36\n    x, y = np.ogrid[0:l, 0:l]\n    mask_outer = (x - l / 2.0) ** 2 + (y - l / 2.0) ** 2 < (l / 2.0) ** 2\n    mask = np.zeros((l, l))\n    points = l * rs.rand(2, n_pts)\n    mask[(points[0]).astype(int), (points[1]).astype(int)] = 1\n    mask = ndimage.gaussian_filter(mask, sigma=l / n_pts)\n    res = np.logical_and(mask > mask.mean(), mask_outer)\n    return np.logical_xor(res, ndimage.binary_erosion(res))", "target": "def _skip_if_ref_does_not_exist(self, ref: str) -> None:\n        try:\n            self.repo.show_ref(ref)\n        except RuntimeError as e:\n            raise SkipTest(f\"Can't find head ref {ref} due to {str(e)}\") from e", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "validation", "example_id": "002473", "source": "def _get_caller_frame_info(depth: int = 2) -> tuple[str | None, bool]:\n    try:\n        previous_caller_frame = sys._getframe(depth)\n    except ValueError as e:\n        raise RuntimeError('This function must be used inside another function') from e\n    except AttributeError:\n        return None, False\n    frame_globals = previous_caller_frame.f_globals\n    return frame_globals.get('__name__'), previous_caller_frame.f_locals is frame_globals", "target": "def test_int_to_float_key():\n    s = SchemaSerializer(core_schema.dict_schema(core_schema.float_schema(), core_schema.float_schema()))\n    v_plain = s.to_python({1: 1})\n    assert v_plain == {1: 1}\n    assert type(list(v_plain.keys())[0]) == int\n    assert type(v_plain[1]) == int\n    v_json = s.to_python({1: 1}, mode='json')\n    assert v_json == {'1': 1.0}\n    assert type(v_json['1']) == float\n    assert s.to_json({1: 1}) == b'{\"1\":1.0}'", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "validation", "example_id": "002474", "source": "def state_dict_hook(module, destination, prefix, local_metadata):\n        for name, param in module.named_parameters():\n            if isinstance(destination[name], torch.Tensor) and not isinstance(\n                destination[name], torch.nn.Parameter\n            ):\n                destination[name] = torch.nn.Parameter(destination[name])", "target": "def validate_b(cls, v: str, info: core_schema.ValidationInfo) -> str:\n            assert v == 'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return 'hello world!'", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "validation", "example_id": "002475", "source": "def make_estimator(self, params):\n        (method,) = params\n        estimator = TSNE(random_state=0, method=method)\n        return estimator", "target": "def make_estimator(self, params):\n        representation, precompute = params\n        estimator = Lasso(precompute=precompute, alpha=0.001, random_state=0)\n        return estimator", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "validation", "example_id": "002476", "source": "def is_manylinux1_compatible():\n    from distutils.util import get_platform\n    if get_platform() not in [\"linux-x86_64\", \"linux-i686\", \"linux-s390x\"]:\n        return False\n    try:\n        import _manylinux\n        return bool(_manylinux.manylinux1_compatible)\n    except (ImportError, AttributeError):\n        pass\n    return have_compatible_glibc(2, 5)", "target": "def _apply_field_title_generator_to_field_info(\n    title_generator: Callable[[str, FieldInfo], str],\n    field_name: str,\n    field_info: FieldInfo,\n):\n    if field_info.title is None:\n        title = title_generator(field_name, field_info)\n        if not isinstance(title, str):\n            raise TypeError(f'field_title_generator {title_generator} must return str, not {title.__class__}')\n        field_info.title = title", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "validation", "example_id": "002477", "source": "def node_type(self) -> ASTNodeType:\n        return ASTNodeType.Namespace", "target": "def node_type(self) -> ASTNodeType:\n        return ASTNodeType.Class", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "002478", "source": "def f(value, serializer, _info):\n        return f'result={serializer(len(value))} repr={serializer!r}'", "target": "def f(prefix, value, _info):\n        return f'{prefix}{value}'", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "002479", "source": "def test_custom_serializer():\n    s = SchemaSerializer(core_schema.any_schema(serialization=core_schema.simple_ser_schema('generator')))\n    assert s.to_python(gen_ok(1, 2), mode='json') == [1, 2]\n    assert s.to_json(gen_ok(1, 2)) == b'[1,2]'", "target": "def test_custom_serializer():\n    s = SchemaSerializer(core_schema.any_schema(serialization=core_schema.simple_ser_schema('multi-host-url')))\n    multi_host_url = MultiHostUrl('https://ex.com,ex.org/path')\n    assert s.to_python(multi_host_url) == multi_host_url", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "002480", "source": "def liger(self, args, kwargs=None) -> Any:\n        from liger_kernel.transformers.softmax import LigerSoftmax\n        assert kwargs is None\n        x, dy = args\n        softmax = LigerSoftmax().to(\"cuda\")\n        y = softmax(x)\n        return lambda: torch.autograd.grad(y, x, grad_outputs=dy, retain_graph=True)", "target": "def liger(self, args, kwargs=None) -> Any:\n        from liger_kernel.transformers.rms_norm import LigerRMSNorm\n        x, w, dy = args\n        M, N = x.shape\n        liger_rmsnorm = LigerRMSNorm(\n            hidden_size=N, eps=1e-6, casting_mode=\"gemma\"\n        ).cuda()\n        liger_rmsnorm.weight.data.copy_(w)\n        y = liger_rmsnorm(x)\n        return lambda: torch.autograd.grad(\n            y, [x, liger_rmsnorm.weight], grad_outputs=dy, retain_graph=True\n        )", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "002481", "source": "def test_dataclass():\n    schema = core_schema.call_schema(\n        core_schema.arguments_schema(\n            [\n                core_schema.arguments_parameter('foo', core_schema.int_schema()),\n                core_schema.arguments_parameter('bar', core_schema.str_schema()),\n                core_schema.arguments_parameter('spam', core_schema.bytes_schema(), mode='keyword_only'),\n                core_schema.arguments_parameter('frog', core_schema.int_schema(), mode='keyword_only'),\n            ]\n        ),\n        DataClass,\n        serialization=core_schema.model_ser_schema(\n            DataClass,\n            core_schema.model_fields_schema(\n                {\n                    'foo': core_schema.model_field(core_schema.int_schema()),\n                    'bar': core_schema.model_field(core_schema.str_schema()),\n                    'spam': core_schema.model_field(core_schema.bytes_schema()),\n                }\n            ),\n        ),\n    )\n    v = SchemaValidator(schema)\n    dc = v.validate_python({'foo': 1, 'bar': 'bar-str', 'spam': 'bite', 'frog': 123})\n    assert dc == DataClass(foo=1, bar='bar-str', spam=b'bite', frog=123)\n    dc.class_var = 2\n    assert dataclasses.is_dataclass(dc)\n    s = SchemaSerializer(schema)\n    assert dataclasses.asdict(dc) == IsStrictDict(foo=1, bar='bar-str', spam=b'bite')\n    assert s.to_python(dc) == IsStrictDict(foo=1, bar='bar-str', spam=b'bite')\n    assert s.to_python(dc, mode='json') == {'foo': 1, 'bar': 'bar-str', 'spam': 'bite'}\n    assert json.loads(s.to_json(dc)) == {'foo': 1, 'bar': 'bar-str', 'spam': 'bite'}", "target": "def test_dataclass():\n    @dataclasses.dataclass\n    class my_dataclass:\n        a: int\n        b: str\n    v = SchemaValidator(\n        cs.call_schema(\n            function=my_dataclass,\n            arguments=cs.arguments_schema(\n                arguments=[\n                    {'name': 'a', 'mode': 'positional_or_keyword', 'schema': cs.int_schema()},\n                    {'name': 'b', 'mode': 'positional_or_keyword', 'schema': cs.str_schema()},\n                ]\n            ),\n        )\n    )\n    d = v.validate_python(('1', b'2'))\n    assert dataclasses.is_dataclass(d)\n    assert d.a == 1\n    assert d.b == '2'\n    d = v.validate_python({'a': 1, 'b': '2'})\n    assert dataclasses.is_dataclass(d)\n    assert d.a == 1\n    assert d.b == '2'\n    assert 'name:\"call[my_dataclass]\"' in plain_repr(v)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "validation", "example_id": "002482", "source": "def forward(*new_params: Tensor) -> Tensor:\n        load_weights(model, names, new_params)\n        out = model(inputs)[\"out\"]\n        loss = criterion(out, labels)\n        return loss", "target": "def test_json_bytes_hex_invalid():\n    v = SchemaValidator(core_schema.bytes_schema(), config=CoreConfig(val_json_bytes='hex'))\n    wrong_input = 'a'\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_json(json.dumps(wrong_input))\n    assert exc_info.value.errors(include_url=False, include_context=False) == [\n        {\n            'type': 'bytes_invalid_encoding',\n            'loc': (),\n            'msg': 'Data should be valid hex: Odd number of digits',\n            'input': wrong_input,\n        }\n    ]\n    wrong_input = 'ag'\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_json(json.dumps(wrong_input))\n    assert exc_info.value.errors(include_url=False, include_context=False) == [\n        {\n            'type': 'bytes_invalid_encoding',\n            'loc': (),\n            'msg': \"Data should be valid hex: Invalid character 'g' at position 1\",\n            'input': wrong_input,\n        }\n    ]", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "002483", "source": "def my_function(a, b, c):\n        return a + b + c", "target": "def my_function(input_value, info):\n        return input_value + 'x'", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "validation", "example_id": "002484", "source": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "target": "def check_eq(v: Any) -> bool:\n            return operator.__eq__(v, value)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "002485", "source": "def test_extra_behavior_ignore(\n    config: Union[core_schema.CoreConfig, None],\n    schema_extra_behavior_kw: dict[str, Any],\n    validate_fn_extra_kw: Union[ExtraBehavior, None],\n):\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            {'f': core_schema.model_field(core_schema.str_schema())}, **schema_extra_behavior_kw\n        ),\n        config=config,\n    )\n    m, model_extra, fields_set = v.validate_python({'f': 'x', 'extra_field': 123}, extra=validate_fn_extra_kw)\n    assert m == {'f': 'x'}\n    assert model_extra is None\n    assert fields_set == {'f'}\n    v.validate_assignment(m, 'f', 'y', extra=validate_fn_extra_kw)\n    assert m['f'] == 'y'\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_assignment(m, 'not_f', 'xyz', extra=validate_fn_extra_kw)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'no_such_attribute',\n            'loc': ('not_f',),\n            'msg': \"Object has no attribute 'not_f'\",\n            'input': 'xyz',\n            'ctx': {'attribute': 'not_f'},\n        }\n    ]\n    assert 'not_f' not in m", "target": "def test_extra_behavior_ignore(\n    config: Union[core_schema.CoreConfig, None],\n    schema_extra_behavior_kw: dict[str, Any],\n    validate_fn_extra_kw: Union[ExtraBehavior, None],\n):\n    v = SchemaValidator(\n        core_schema.typed_dict_schema(\n            {'f': core_schema.typed_dict_field(core_schema.str_schema())}, **schema_extra_behavior_kw\n        ),\n        config=config,\n    )\n    m: dict[str, Any] = v.validate_python({'f': 'x', 'extra_field': 123}, extra=validate_fn_extra_kw)\n    assert m == {'f': 'x'}", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "validation", "example_id": "002486", "source": "def run(img, sc, dtype):\n            return img + np.array(sc, dtype=np.uint8)[:-1]", "target": "def run(arr0, arr1):\n                    return arr0 + arr1", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "validation", "example_id": "002487", "source": "def test_float(input_value, expected):\n    v = SchemaValidator(core_schema.float_schema())\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            v.validate_json(input_value)\n    else:\n        assert v.validate_json(input_value) == expected", "target": "def test_float(py_and_json: PyAndJson, input_value, expected):\n    v = py_and_json({'type': 'float'})\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            v.validate_test(input_value)\n    else:\n        output = v.validate_test(input_value)\n        assert output == expected\n        assert isinstance(output, float)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "validation", "example_id": "002488", "source": "def test_installs_packages_when_present(monkeypatch, patch_module):\n    run_test_plan = patch_module.module.run_test_plan\n    tests_map = {\n        \"with_pkgs\": {\n            \"title\": \"Needs deps\",\n            \"package_install\": [\"timm==1.0.0\", \"flash-attn\"],\n            \"steps\": [\"pytest -q\"],\n        }\n    }\n    patch_module.run_command.return_value = 0\n    run_test_plan(\"with_pkgs\", \"gpu\", tests_map)\n    patch_module.pip_install_packages.assert_called_once_with(\n        packages=[\"timm==1.0.0\", \"flash-attn\"],\n        prefer_uv=True,\n    )", "target": "def test_dataclass():\n    schema = core_schema.call_schema(\n        core_schema.arguments_schema(\n            [\n                core_schema.arguments_parameter('foo', core_schema.int_schema()),\n                core_schema.arguments_parameter('bar', core_schema.str_schema()),\n                core_schema.arguments_parameter('spam', core_schema.bytes_schema(), mode='keyword_only'),\n                core_schema.arguments_parameter('frog', core_schema.int_schema(), mode='keyword_only'),\n            ]\n        ),\n        DataClass,\n        serialization=core_schema.model_ser_schema(\n            DataClass,\n            core_schema.model_fields_schema(\n                {\n                    'foo': core_schema.model_field(core_schema.int_schema()),\n                    'bar': core_schema.model_field(core_schema.str_schema()),\n                    'spam': core_schema.model_field(core_schema.bytes_schema()),\n                }\n            ),\n        ),\n    )\n    v = SchemaValidator(schema)\n    dc = v.validate_python({'foo': 1, 'bar': 'bar-str', 'spam': 'bite', 'frog': 123})\n    assert dc == DataClass(foo=1, bar='bar-str', spam=b'bite', frog=123)\n    dc.class_var = 2\n    assert dataclasses.is_dataclass(dc)\n    s = SchemaSerializer(schema)\n    assert dataclasses.asdict(dc) == IsStrictDict(foo=1, bar='bar-str', spam=b'bite')\n    assert s.to_python(dc) == IsStrictDict(foo=1, bar='bar-str', spam=b'bite')\n    assert s.to_python(dc, mode='json') == {'foo': 1, 'bar': 'bar-str', 'spam': 'bite'}\n    assert json.loads(s.to_json(dc)) == {'foo': 1, 'bar': 'bar-str', 'spam': 'bite'}", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|opencv/opencv", "data_split": "validation", "example_id": "002489", "source": "def build_argparser():\n    parser = argparse.ArgumentParser(description='This is an OpenCV-based version of Gaze Estimation example')\n    parser.add_argument('--input',\n                        help='Path to the input video file or camera device number')\n    parser.add_argument('--out',\n                        help='Path to the output video file')\n    parser.add_argument('--facem',\n                        default='face-detection-retail-0005.xml',\n                        help='Path to OpenVINO face detection model (.xml)')\n    parser.add_argument('--faced',\n                        default='CPU',\n                        help='Target device for the face detection' +\n                        '(e.g. CPU, GPU, VPU, ...)')\n    parser.add_argument('--headm',\n                        default='head-pose-estimation-adas-0001.xml',\n                        help='Path to OpenVINO head pose estimation model (.xml)')\n    parser.add_argument('--headd',\n                        default='CPU',\n                        help='Target device for the head pose estimation inference ' +\n                        '(e.g. CPU, GPU, VPU, ...)')\n    parser.add_argument('--landm',\n                        default='facial-landmarks-35-adas-0002.xml',\n                        help='Path to OpenVINO landmarks detector model (.xml)')\n    parser.add_argument('--landd',\n                        default='CPU',\n                        help='Target device for the landmarks detector (e.g. CPU, GPU, VPU, ...)')\n    parser.add_argument('--gazem',\n                        default='gaze-estimation-adas-0002.xml',\n                        help='Path to OpenVINO gaze vector estimaiton model (.xml)')\n    parser.add_argument('--gazed',\n                        default='CPU',\n                        help='Target device for the gaze vector estimation inference ' +\n                        '(e.g. CPU, GPU, VPU, ...)')\n    parser.add_argument('--eyem',\n                        default='open-closed-eye-0001.xml',\n                        help='Path to OpenVINO open closed eye model (.xml)')\n    parser.add_argument('--eyed',\n                        default='CPU',\n                        help='Target device for the eyes state inference (e.g. CPU, GPU, VPU, ...)')\n    return parser", "target": "def find_squares(img):\n    img = cv.GaussianBlur(img, (5, 5), 0)\n    squares = []\n    for gray in cv.split(img):\n        for thrs in xrange(0, 255, 26):\n            if thrs == 0:\n                bin = cv.Canny(gray, 0, 50, apertureSize=5)\n                bin = cv.dilate(bin, None)\n            else:\n                _retval, bin = cv.threshold(gray, thrs, 255, cv.THRESH_BINARY)\n            contours, _hierarchy = cv.findContours(bin, cv.RETR_LIST, cv.CHAIN_APPROX_SIMPLE)\n            for cnt in contours:\n                cnt_len = cv.arcLength(cnt, True)\n                cnt = cv.approxPolyDP(cnt, 0.02*cnt_len, True)\n                if len(cnt) == 4 and cv.contourArea(cnt) > 1000 and cv.isContourConvex(cnt):\n                    cnt = cnt.reshape(-1, 2)\n                    max_cos = np.max([angle_cos( cnt[i], cnt[(i+1) % 4], cnt[(i+2) % 4] ) for i in xrange(4)])\n                    if max_cos < 0.1 and filterSquares(squares, cnt):\n                        squares.append(cnt)\n    return squares", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "validation", "example_id": "002490", "source": "def test_smart_union_json_string_types_str_first(schema: core_schema.CoreSchema, input_value: str):\n    validator = SchemaValidator(core_schema.union_schema([core_schema.str_schema(), schema]))\n    assert validator.validate_json(f'\"{input_value}\"') == input_value\n    assert validator.validate_python(input_value) == input_value", "target": "def test_positional_empty_extra(py_and_json: PyAndJson):\n    v = py_and_json({'type': 'tuple', 'items_schema': [{'type': 'int'}], 'variadic_item_index': 0})\n    assert v.validate_test([]) == ()\n    assert v.validate_python(()) == ()\n    assert v.validate_test([1]) == (1,)\n    assert v.validate_test(list(range(100))) == tuple(range(100))", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "validation", "example_id": "002491", "source": "def get_memory_bytes(self, args, kwargs) -> int:\n        x, dy = args\n        M, N = x.shape\n        return 3 * M * N * x.dtype.itemsize", "target": "def get_memory_bytes(self, args, kwargs) -> int:\n        x, w = args\n        M, N = x.shape\n        return 2 * M * N * x.dtype.itemsize + N * w.dtype.itemsize", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "validation", "example_id": "002492", "source": "def collect_known_metadata(annotations: Iterable[Any]) -> tuple[dict[str, Any], list[Any]]:\n    annotations = expand_grouped_metadata(annotations)\n    res: dict[str, Any] = {}\n    remaining: list[Any] = []\n    for annotation in annotations:\n        if isinstance(annotation, PydanticMetadata):\n            res.update(annotation.__dict__)\n        elif (annotation_type := type(annotation)) in (at_to_constraint_map := _get_at_to_constraint_map()):\n            constraint = at_to_constraint_map[annotation_type]\n            res[constraint] = getattr(annotation, constraint)\n        elif isinstance(annotation, type) and issubclass(annotation, PydanticMetadata):\n            res.update({k: v for k, v in vars(annotation).items() if not k.startswith('_')})\n        else:\n            remaining.append(annotation)\n    res = {k: v for k, v in res.items() if v is not None}\n    return res, remaining", "target": "def _run(self):\n        if not os.path.isdir(self.build_dir):\n            os.makedirs(self.build_dir)\n        self.runTest()", "label": 0}
