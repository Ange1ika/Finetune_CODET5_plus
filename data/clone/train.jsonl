{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000000", "source": "def ser_x(self, v: Any, serializer: core_schema.SerializerFunctionWrapHandler, _) -> str:\n            x = serializer(v)\n            assert self.x == 1_000\n            return f'{x:_}'", "target": "def ser_x(data: Model, v: Any, _) -> str:\n        assert data['x'] == 1_000\n        return f'{v:_}'", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000001", "source": "def get_shapes(self) -> tuple[tuple[int, ...], ...]:\n        return (\n            (32768, 256),\n            (32768, 512),\n            (32768, 1024),\n            (32768, 2048),\n            (32768, 4096),\n            (32768, 8192),\n            (32768, 16384),\n            (32768, 32768),\n            (32768, 65536),\n            (16384, 131072),\n            (8192, 262144),\n        )", "target": "def get_shapes(self) -> tuple[tuple[int, ...], ...]:\n        return (\n            (32768, 256),\n            (32768, 512),\n            (32768, 1024),\n            (32768, 2048),\n            (32768, 4096),\n            (32768, 8192),\n            (32768, 16384),\n            (32768, 32768),\n            (32768, 65536),\n        ) + extra_shapes_for_norm", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "000002", "source": "def get_memory_bytes(self, args, kwargs) -> int:\n        x, target, dloss = args\n        M, N = x.shape\n        return (\n            2 * M * N * x.dtype.itemsize\n            + M * target.dtype.itemsize\n            + M * dloss.dtype.itemsize\n        )", "target": "def as_generator(*items):\n    return (v for v in items)", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "000003", "source": "def make_data(self, params):\n        (representation,) = params\n        if representation == \"dense\":\n            data = _synth_regression_dataset(n_samples=1000000, n_features=100)\n        else:\n            data = _synth_regression_sparse_dataset(\n                n_samples=10000, n_features=100000, density=0.01\n            )\n        return data", "target": "def make_data(self, params):\n        (representation,) = params\n        if representation == \"dense\":\n            data = _synth_regression_dataset(n_samples=100000, n_features=200)\n        else:\n            data = _synth_regression_sparse_dataset(\n                n_samples=100000, n_features=1000, density=0.01\n            )\n        return data", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "000004", "source": "def visit_Expr(self, node: ast.Expr) -> Any:\n        if (\n            isinstance(node.value, ast.Constant)\n            and isinstance(node.value.value, str)\n            and self.previous_node_type is ast.AnnAssign\n        ):\n            docstring = inspect.cleandoc(node.value.value)\n            if self.target:\n                self.attrs[self.target] = docstring\n            self.target = None", "target": "def test_total_time(shapes):\n    print(\"shape; torch mm; triton mm; inductor aten mm; inductor triton mm\")\n    for i in range(len(shapes)):\n        a_shape, b_shape = shapes[i]\n        print(a_shape, \"x\", b_shape, end=\"; \")\n        a = torch.randn(a_shape, device=\"cuda\", dtype=torch.float16)\n        b = torch.randn(b_shape, device=\"cuda\", dtype=a.dtype)\n        config.triton.mm = \"aten\"\n        inductor_aten_mm(a, b)\n        config.triton.mm = \"triton\"\n        inductor_triton_mm(a, b)\n        torch_ms = time_with_torch_timer(torch_mm, (a, b)).mean * 1000\n        triton_ms = time_with_torch_timer(triton_mm, (a, b)).mean * 1000\n        config.triton.mm = \"aten\"\n        ind_aten_ms = time_with_torch_timer(inductor_aten_mm, (a, b)).mean * 1000\n        config.triton.mm = \"triton\"\n        ind_triton_ms = time_with_torch_timer(inductor_triton_mm, (a, b)).mean * 1000\n        print(torch_ms, triton_ms, ind_aten_ms, ind_triton_ms, sep=\"; \")\n        torch._dynamo.reset()", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "000005", "source": "def run_once(model: Callable, inp: InputsType, task: str, v: VType, **kwargs) -> None:\n    func = get_task_func(task)\n    if v is not None:\n        func(model, inp, v=v, strict=True)\n    else:\n        func(model, inp, strict=True)", "target": "def test_wrong_return_type():\n    s = SchemaSerializer(\n        core_schema.any_schema(\n            serialization=core_schema.plain_serializer_function_ser_schema(\n                repr_function, info_arg=True, return_schema=core_schema.int_schema()\n            )\n        )\n    )\n    with pytest.warns(\n        UserWarning,\n        match=r\"Expected `int` - serialized value may not be as expected \\[input_value='123', input_type=str\\]\",\n    ):\n        assert s.to_python(123) == '123'\n    with pytest.warns(\n        UserWarning,\n        match=r\"Expected `int` - serialized value may not be as expected \\[input_value='123', input_type=str\\]\",\n    ):\n        assert s.to_python(123, mode='json') == '123'\n    with pytest.warns(\n        UserWarning,\n        match=r\"Expected `int` - serialized value may not be as expected \\[input_value='123', input_type=str\\]\",\n    ):\n        assert s.to_json(123) == b'\"123\"'", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "000006", "source": "def _dataclass_getstate(self: Any) -> list[Any]:\n                    return [getattr(self, f.name) for f in dataclasses.fields(self)]", "target": "def setup(rank, world_size):\n    os.environ[\"MASTER_ADDR\"] = os.getenv(\"MASTER_ADDR\", \"localhost\")\n    os.environ[\"MASTER_PORT\"] = os.getenv(\"MASTER_PORT\", \"12355\")\n    os.environ[\"RANK\"] = os.getenv(\"RANK\", \"0\")\n    os.environ[\"WORLD_SIZE\"] = os.getenv(\"WORLD_SIZE\", \"1\")\n    dist.init_process_group(\"nccl\")", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pytorch/pytorch", "data_split": "train", "example_id": "000007", "source": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "target": "def temp_environ(updates: dict[str, str]):\n    missing = object()\n    old: dict[str, str | object] = {k: os.environ.get(k, missing) for k in updates}\n    try:\n        os.environ.update(updates)\n        yield\n    finally:\n        for k, v in old.items():\n            if v is missing:\n                os.environ.pop(k, None)\n            else:\n                os.environ[k] = v", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|scikit-learn/scikit-learn", "data_split": "train", "example_id": "000008", "source": "def test_set_kwargs(kwargs: dict[str, Any], input_value, expected):\n    v = SchemaValidator(cs.set_schema(**kwargs))\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            r = v.validate_python(input_value)\n            print(f'unexpected result: {r!r}')\n    else:\n        assert v.validate_python(input_value) == expected", "target": "def load_data(dtype=np.float32, order=\"F\"):\n    print(\"Loading dataset...\")\n    data = fetch_openml(\"mnist_784\", as_frame=True)\n    X = check_array(data[\"data\"], dtype=dtype, order=order)\n    y = data[\"target\"]\n    X = X / 255\n    print(\"Creating train-test split...\")\n    n_train = 60000\n    X_train = X[:n_train]\n    y_train = y[:n_train]\n    X_test = X[n_train:]\n    y_test = y[n_train:]\n    return X_train, X_test, y_train, y_test", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000009", "source": "def foo(x: int, y: int) -> int:\n            return x + y", "target": "def foo(x: int, y: int) -> int:\n            return x + y", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|opencv/opencv", "data_split": "train", "example_id": "000010", "source": "def test_peopledetect(self):\n        hog = cv.HOGDescriptor()\n        hog.setSVMDetector( cv.HOGDescriptor_getDefaultPeopleDetector() )\n        dirPath = 'samples/data/'\n        samples = ['basketball1.png', 'basketball2.png']\n        testPeople = [\n        [[23, 76, 164, 477], [440, 22, 637, 478]],\n        [[23, 76, 164, 477], [440, 22, 637, 478]]\n        ]\n        eps = 0.5\n        for sample in samples:\n            img = self.get_sample(dirPath + sample, 0)\n            found, _w = hog.detectMultiScale(img, winStride=(8,8), padding=(32,32), scale=1.05)\n            found_filtered = []\n            for ri, r in enumerate(found):\n                for qi, q in enumerate(found):\n                    if ri != qi and inside(r, q):\n                        break\n                else:\n                    found_filtered.append(r)\n            matches = 0\n            for i in range(len(found_filtered)):\n                for j in range(len(testPeople)):\n                    found_rect = (found_filtered[i][0], found_filtered[i][1],\n                        found_filtered[i][0] + found_filtered[i][2],\n                        found_filtered[i][1] + found_filtered[i][3])\n                    if intersectionRate(found_rect, testPeople[j][0]) > eps or intersectionRate(found_rect, testPeople[j][1]) > eps:\n                        matches += 1\n            self.assertGreater(matches, 0)", "target": "def run(lhs, rhs, state):\n                    state.sum+= lhs + rhs\n                    return state.sum", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "000011", "source": "def get_commit(self, ref: str) -> GitCommit:\n        return parse_fuller_format(\n            self._run_git(\"show\", \"--format=fuller\", \"--date=unix\", \"--shortstat\", ref)\n        )", "target": "def test_list_allowed_inputs_python(testcase: ListInputTestCase):\n    v = SchemaValidator(core_schema.list_schema(core_schema.int_schema(), strict=testcase.strict))\n    if isinstance(testcase.output, Err):\n        with pytest.raises(ValidationError, match=re.escape(testcase.output.message)):\n            v.validate_python(testcase.input)\n    else:\n        output = v.validate_python(testcase.input)\n        assert output == testcase.output\n        assert output is not testcase.input", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|pydantic/pydantic", "data_split": "train", "example_id": "000012", "source": "def build_clfs(cd_iters, pg_iters, mu_iters):\n    clfs = [\n        (\"Coordinate Descent\", NMF, cd_iters, {\"solver\": \"cd\"}),\n        (\"Projected Gradient\", _PGNMF, pg_iters, {\"solver\": \"pg\"}),\n        (\"Multiplicative Update\", NMF, mu_iters, {\"solver\": \"mu\"}),\n    ]\n    return clfs", "target": "def schema_validator(self) -> SchemaValidator:\n        return SchemaValidator(\n            schema=core_schema.union_schema(\n                choices=[\n                    core_schema.model_schema(\n                        cls=self.ModelA,\n                        schema=core_schema.model_fields_schema(\n                            fields={\n                                'a': core_schema.model_field(schema=core_schema.int_schema()),\n                                'b': core_schema.model_field(schema=core_schema.str_schema()),\n                            }\n                        ),\n                    ),\n                    core_schema.model_schema(\n                        cls=self.ModelB,\n                        schema=core_schema.model_fields_schema(\n                            fields={\n                                'c': core_schema.model_field(schema=core_schema.int_schema()),\n                                'd': core_schema.model_field(schema=core_schema.str_schema()),\n                            }\n                        ),\n                    ),\n                ]\n            )\n        )", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000013", "source": "def liger(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        from liger_kernel.transformers.cross_entropy import LigerCrossEntropyLoss\n        x, target, dloss = args\n        cross_entropy = LigerCrossEntropyLoss(reduction=\"none\")\n        loss = cross_entropy(x, target)\n        return lambda: torch.autograd.grad(\n            loss, x, grad_outputs=dloss, retain_graph=True\n        )", "target": "def liger(self, args, kwargs=None) -> Any:\n        from liger_kernel.transformers.softmax import LigerSoftmax\n        assert kwargs is None\n        (x,) = args\n        softmax = LigerSoftmax().to(\"cuda\")\n        return lambda: softmax(x)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000014", "source": "def test_function_after():\n    s = SchemaSerializer(\n        core_schema.with_info_after_validator_function(lambda v, info: v + 1, core_schema.int_schema())\n    )\n    assert plain_repr(s) == 'SchemaSerializer(serializer=Int(IntSerializer),definitions=[])'", "target": "def test_function_after():\n    def f(input_value, _info):\n        return input_value + ' Changed'\n    v = SchemaValidator(core_schema.with_info_after_validator_function(f, core_schema.str_schema()))\n    assert v.validate_python('input value') == 'input value Changed'", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "000015", "source": "def general_plain_validator_function(*args, **kwargs):\n    warnings.warn(\n        '`general_plain_validator_function` is deprecated, use `with_info_plain_validator_function` instead.',\n        DeprecationWarning,\n    )\n    return with_info_plain_validator_function(*args, **kwargs)", "target": "def _trim_class_name_from_argument_types(\n    overloads: Iterable[FunctionNode.Overload],\n    class_name: str\n) -> None:\n    separator = f\"{class_name}_\"\n    for overload in overloads:\n        for arg in [arg for arg in overload.arguments\n                    if arg.type_node is not None]:\n            ast_node = cast(ASTNodeTypeNode, arg.type_node)\n            if class_name in ast_node.ctype_name:\n                fixed_name = ast_node._typename.split(separator)[-1]\n                ast_node._typename = fixed_name", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "000016", "source": "def test_tagged_union_with_aliases() -> None:\n    @dataclasses.dataclass\n    class ModelA:\n        field: int\n        tag: Literal['a'] = 'a'\n    @dataclasses.dataclass\n    class ModelB:\n        field: int\n        tag: Literal['b'] = 'b'\n    s = SchemaSerializer(\n        core_schema.tagged_union_schema(\n            choices={\n                'a': core_schema.dataclass_schema(\n                    ModelA,\n                    core_schema.dataclass_args_schema(\n                        'ModelA',\n                        [\n                            core_schema.dataclass_field(name='field', schema=core_schema.int_schema()),\n                            core_schema.dataclass_field(\n                                name='tag',\n                                schema=core_schema.literal_schema(['a']),\n                                validation_alias='TAG',\n                                serialization_alias='TAG',\n                            ),\n                        ],\n                    ),\n                    ['field', 'tag'],\n                ),\n                'b': core_schema.dataclass_schema(\n                    ModelB,\n                    core_schema.dataclass_args_schema(\n                        'ModelB',\n                        [\n                            core_schema.dataclass_field(name='field', schema=core_schema.int_schema()),\n                            core_schema.dataclass_field(\n                                name='tag',\n                                schema=core_schema.literal_schema(['b']),\n                                validation_alias='TAG',\n                                serialization_alias='TAG',\n                            ),\n                        ],\n                    ),\n                    ['field', 'tag'],\n                ),\n            },\n            discriminator=[['tag'], ['TAG']],\n        )\n    )\n    assert 'TaggedUnionSerializer' in repr(s)\n    model_a = ModelA(field=1)\n    model_b = ModelB(field=1)\n    assert s.to_python(model_a, by_alias=True) == {'field': 1, 'TAG': 'a'}\n    assert s.to_python(model_b, by_alias=True) == {'field': 1, 'TAG': 'b'}", "target": "def profile():\n    x = torch.randn(16)\n    y = torch.randn(16)\n    torch._dynamo.reset()\n    pr = cProfile.Profile()\n    pr.enable()\n    symbolic_convert_overhead_stress_test(x, y, 33000)\n    pr.disable()\n    ps = pstats.Stats(pr)\n    ps.dump_stats(\"dynamo_microbenchmarks.prof\")\n    print(\"snakeviz dynamo_microbenchmarks.prof\")", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000017", "source": "def benchmark(self):\n        for M, N in self.get_shapes():\n            print(f\"Tensor dimensions: [{M}, {N}]\")\n            torch_dtype = cutlass_torch.dtype(cutlass.BFloat16)\n            x = 0.1 * torch.randn(M, N, device=\"cuda\", dtype=torch_dtype)\n            self.benchmark_single_shape((x,), setting=f\"shape: [{M}, {N}]\")", "target": "def benchmark(self):\n        for M, N in self.get_shapes():\n            print(f\"Tensor dimensions: [{M}, {N}]\")\n            torch_dtype = cutlass_torch.dtype(cutlass.BFloat16)\n            x = torch.randn(M, N, device=\"cuda\", dtype=torch_dtype)\n            w = torch.randn(N, device=\"cuda\", dtype=torch.float32)\n            self.benchmark_single_shape((x, w), setting=f\"shape: [{M}, {N}]\")", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000018", "source": "def dec(f: FieldSerializer) -> _decorators.PydanticDescriptorProxy[Any]:\n        dec_info = _decorators.FieldSerializerDecoratorInfo(\n            fields=fields,\n            mode=mode,\n            return_type=return_type,\n            when_used=when_used,\n            check_fields=check_fields,\n        )\n        return _decorators.PydanticDescriptorProxy(f, dec_info)", "target": "def dec(f: ModelSerializer) -> _decorators.PydanticDescriptorProxy[Any]:\n        dec_info = _decorators.ModelSerializerDecoratorInfo(mode=mode, return_type=return_type, when_used=when_used)\n        return _decorators.PydanticDescriptorProxy(f, dec_info)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000019", "source": "def fn(x):\n        for _ in range(N):\n            x = x.sin()\n        return x", "target": "def fn(x):\n        for _ in range(N):\n            x = x.sin()\n        return x", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "000020", "source": "def replace_tag(filename):\n    with open(filename) as f:\n        lines = f.read().split(\"\\\\n\")\n    for i, line in enumerate(lines):\n        if not line.startswith(\"Tag: \"):\n            continue\n        lines[i] = line.replace(\"-linux_\", \"-manylinux2014_\")\n        print(f\"Updated tag from {line} to {lines[i]}\")\n    with open(filename, \"w\") as f:\n        f.write(\"\\\\n\".join(lines))", "target": "def test_decimal_kwargs(py_and_json: PyAndJson, kwargs: dict[str, Any], input_value, expected):\n    v = py_and_json({'type': 'decimal', **kwargs})\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            v.validate_test(input_value)\n    else:\n        output = v.validate_test(input_value)\n        assert output == expected\n        assert isinstance(output, Decimal)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000021", "source": "def get_memory_bytes(self, args, kwargs) -> int:\n        (x,) = args\n        M, N = x.shape\n        return 2 * M * N * x.dtype.itemsize", "target": "def get_memory_bytes(self, args, kwargs) -> int:\n        x, w = args\n        M, N = x.shape\n        return 2 * M * N * x.dtype.itemsize + N * w.dtype.itemsize", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000022", "source": "def test_simple(self):\n        finder = cv.ORB.create()\n        img1 = self.get_sample('stitching/a1.png')\n        img2 = self.get_sample('stitching/a2.png')\n        img_feat1 = cv.detail.computeImageFeatures2(finder, img1)\n        img_feat2 = cv.detail.computeImageFeatures2(finder, img2)\n        matcher = cv.detail.BestOf2NearestMatcher_create()\n        matches_info = matcher.apply(img_feat1, img_feat2)\n        self.assertIsNotNone(matches_info.matches)\n        self.assertIsNotNone(matches_info.inliers_mask)", "target": "def test_simple(self):\n        images = [\n            self.get_sample('stitching/a1.png'),\n            self.get_sample('stitching/a2.png'),\n            self.get_sample('stitching/a3.png')\n        ]\n        orb = cv.ORB_create()\n        features = [cv.detail.computeImageFeatures2(orb, img) for img in images]\n        matcher = cv.detail_BestOf2NearestRangeMatcher(range_width=1)\n        matches = matcher.apply2(features)\n        self.assertNotEqual(matches[1].confidence, 0)\n        self.assertEqual(matches[2].confidence, 0)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "000023", "source": "def wrap_serializer_function_ser_schema(\n    function: WrapSerializerFunction,\n    *,\n    is_field_serializer: bool | None = None,\n    info_arg: bool | None = None,\n    schema: CoreSchema | None = None,\n    return_schema: CoreSchema | None = None,\n    when_used: WhenUsed = 'always',\n) -> WrapSerializerFunctionSerSchema:\n    if when_used == 'always':\n        when_used = None\n    return _dict_not_none(\n        type='function-wrap',\n        function=function,\n        is_field_serializer=is_field_serializer,\n        info_arg=info_arg,\n        schema=schema,\n        return_schema=return_schema,\n        when_used=when_used,\n    )", "target": "def test_callable(self):\n        res = cv.getDefaultAlgorithmHint()\n        self.assertTrue(res is not None)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "000024", "source": "def upload_pytest_cache(\n    pr_identifier: PRIdentifier,\n    repo: GithubRepo,\n    job_identifier: str,\n    sha: str,\n    test_config: str,\n    shard: str,\n    cache_dir: Path,\n    temp_dir: Path,\n    bucket: str = BUCKET,\n) -> None:\n    if not isinstance(pr_identifier, PRIdentifier):\n        raise ValueError(\n            f\"pr_identifier must be of type PRIdentifier, not {type(pr_identifier)}\"\n        )\n    if not bucket:\n        bucket = BUCKET\n    obj_key_prefix = _get_s3_key_prefix(\n        pr_identifier, repo, job_identifier, sha, test_config, shard\n    )\n    zip_file_path = zip_folder(cache_dir, temp_dir / ZIP_UPLOAD / obj_key_prefix)\n    obj_key = f\"{obj_key_prefix}{os.path.splitext(zip_file_path)[1]}\"\n    upload_file_to_s3(zip_file_path, bucket, obj_key)", "target": "def is_pydantic_dataclass(class_: type[Any], /) -> TypeGuard[type[PydanticDataclass]]:\n    try:\n        return '__is_pydantic_dataclass__' in class_.__dict__ and dataclasses.is_dataclass(class_)\n    except AttributeError:\n        return False", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000025", "source": "def validate_b(cls, v: str, info: core_schema.ValidationInfo) -> str:\n            assert v == 'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return 'hello world!'", "target": "def validate_b(cls, v: bytes, info: core_schema.ValidationInfo) -> str:\n            assert v == b'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return 'hello world!'", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000026", "source": "def test_constrained_bytes(py_and_json: PyAndJson, opts: dict[str, Any], input, expected):\n    v = py_and_json({'type': 'bytes', **opts})\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            v.validate_test(input)\n        assert v.isinstance_test(input) is False\n    else:\n        assert v.validate_test(input) == expected\n        assert v.isinstance_test(input) is True", "target": "def test_validate_assignment(pydantic_version) -> None:\n    @dataclass\n    class Model:\n        x: list['Model']\n    schema = core_schema.definitions_schema(\n        core_schema.definition_reference_schema('model'),\n        [\n            core_schema.dataclass_schema(\n                Model,\n                core_schema.dataclass_args_schema(\n                    'Model',\n                    [\n                        core_schema.dataclass_field(\n                            name='x',\n                            schema=core_schema.list_schema(core_schema.definition_reference_schema('model')),\n                            kw_only=False,\n                        )\n                    ],\n                ),\n                ['x'],\n                ref='model',\n                config=core_schema.CoreConfig(revalidate_instances='always'),\n            )\n        ],\n    )\n    v = SchemaValidator(schema)\n    data = [Model(x=[Model(x=[])])]\n    instance = Model(x=[])\n    v.validate_assignment(instance, 'x', data)\n    assert instance.x == data\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_assignment(instance, 'x', [Model(x=[Model(x=[Model(x=[123])])])])\n    assert exc_info.value.errors() == [\n        {\n            'type': 'dataclass_type',\n            'loc': ('x', 0, 'x', 0, 'x', 0, 'x', 0),\n            'msg': 'Input should be a dictionary or an instance of Model',\n            'input': 123,\n            'ctx': {'class_name': 'Model'},\n            'url': f'https://errors.pydantic.dev/{pydantic_version}/v/dataclass_type',\n        }\n    ]", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000027", "source": "def f(value, serializer, _info):\n        return f'result={serializer(len(value))}'", "target": "def f(prefix, value, _info):\n        return f'{prefix}{value}'", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000028", "source": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "target": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000029", "source": "def eager(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, target = args\n        return lambda: F.cross_entropy(x, target, reduction=\"none\")", "target": "def eager(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, w, dy = args\n        y = self.rms_norm_ref(x, w)\n        return lambda: torch.autograd.grad(\n            y, [x, w], grad_outputs=dy, retain_graph=True\n        )", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000030", "source": "def test_custom_ser():\n    s = SchemaSerializer(\n        core_schema.definitions_schema(\n            core_schema.list_schema(core_schema.definition_reference_schema('foobar')),\n            [core_schema.int_schema(ref='foobar', serialization=core_schema.to_string_ser_schema(when_used='always'))],\n        )\n    )\n    assert s.to_python([1, 2, 3]) == ['1', '2', '3']", "target": "def test_custom_ser():\n    s = SchemaSerializer(\n        core_schema.definitions_schema(\n            core_schema.definition_reference_schema('Branch'),\n            [\n                core_schema.typed_dict_schema(\n                    {\n                        'name': core_schema.typed_dict_field(core_schema.str_schema()),\n                        'sub_branch': core_schema.typed_dict_field(\n                            core_schema.nullable_schema(\n                                core_schema.definition_reference_schema(\n                                    'Branch', serialization=core_schema.to_string_ser_schema(when_used='always')\n                                )\n                            )\n                        ),\n                    },\n                    ref='Branch',\n                )\n            ],\n        )\n    )\n    assert s.to_python({'name': 'root', 'sub_branch': {'name': 'branch', 'sub_branch': None}}) == {\n        'name': 'root',\n        'sub_branch': \"{'name': 'branch', 'sub_branch': None}\",\n    }", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000031", "source": "def dynamically_quantize_per_channel(x, quant_min, quant_max, target_dtype):\n    eps = torch.finfo(torch.float32).eps\n    min_val, max_val = torch.aminmax(x, dim=1)\n    min_val_neg = torch.min(min_val, torch.zeros_like(min_val))\n    max_val_pos = torch.max(max_val, torch.zeros_like(max_val))\n    device = min_val_neg.device\n    max_val_pos = torch.max(-min_val_neg, max_val_pos)\n    scales = max_val_pos / (float(quant_max - quant_min) / 2)\n    scales = torch.clamp(scales, min=eps).to(x.dtype)\n    zero_points = torch.zeros(min_val_neg.size(), dtype=torch.int64, device=device)\n    x_div = x / scales.unsqueeze(-1)\n    x_round = torch.round(x_div)\n    x_zp = x_round + zero_points.unsqueeze(-1)\n    quant = torch.clamp(x_zp, quant_min, quant_max).to(target_dtype)\n    return quant, scales, zero_points", "target": "def dynamically_quantize_per_channel(x, quant_min, quant_max, target_dtype):\n    eps = torch.finfo(torch.float32).eps\n    min_val, max_val = torch.aminmax(x, dim=1)\n    min_val_neg = torch.min(min_val, torch.zeros_like(min_val))\n    max_val_pos = torch.max(max_val, torch.zeros_like(max_val))\n    device = min_val_neg.device\n    max_val_pos = torch.max(-min_val_neg, max_val_pos)\n    scales = max_val_pos / (float(quant_max - quant_min) / 2)\n    scales = torch.clamp(scales, min=eps).to(x.dtype)\n    zero_points = torch.zeros(min_val_neg.size(), dtype=torch.int64, device=device)\n    x_div = x / scales.unsqueeze(-1)\n    x_round = torch.round(x_div)\n    x_zp = x_round + zero_points.unsqueeze(-1)\n    quant = torch.clamp(x_zp, quant_min, quant_max).to(target_dtype)\n    return quant, scales, zero_points", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000032", "source": "def full_typename(self) -> str:\n        return \"cv2.typing.\" + self.typename", "target": "def full_typename(self) -> str:\n        return \"cv2.typing.\" + self.typename", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000033", "source": "def preproc(ppp):\n                ppp.input().model().set_layout(Layout(\"NCHW\"))\n                ppp.input().tensor().set_element_type(Type.u8)                              \\\n                                    .set_spatial_static_shape(img1.shape[0], img2.shape[1]) \\\n                                    .set_layout(Layout(\"NHWC\"))\n                ppp.input().preprocess().resize(ResizeAlgorithm.RESIZE_LINEAR)", "target": "def preproc(ppp):\n                ppp.input().tensor().set_element_type(Type.u8)                            \\\n                                    .set_spatial_static_shape(img.shape[0], img.shape[1])\n                ppp.input().preprocess().resize(ResizeAlgorithm.RESIZE_LINEAR)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000034", "source": "def test_aliases_path_negative_json(py_and_json: PyAndJson, input_value, expected):\n    v = py_and_json(\n        {\n            'type': 'model-fields',\n            'fields': {'field_a': {'validation_alias': ['foo', -2], 'type': 'model-field', 'schema': {'type': 'int'}}},\n        }\n    )\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=expected.message):\n            val = v.validate_test(input_value)\n            print(f'UNEXPECTED OUTPUT: {val!r}')\n    else:\n        output = v.validate_test(input_value)\n        assert output == expected", "target": "def test_aliases_path_negative_json(py_and_json: PyAndJson, input_value, expected):\n    v = py_and_json(\n        {\n            'type': 'typed-dict',\n            'fields': {\n                'field_a': {'validation_alias': ['foo', -2], 'type': 'typed-dict-field', 'schema': {'type': 'int'}}\n            },\n        }\n    )\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=expected.message):\n            val = v.validate_test(input_value)\n            print(f'UNEXPECTED OUTPUT: {val!r}')\n    else:\n        output = v.validate_test(input_value)\n        assert output == expected", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000035", "source": "def test_alias_extra_by_name(py_and_json: PyAndJson):\n    v = py_and_json(\n        {\n            'type': 'model-fields',\n            'extra_behavior': 'allow',\n            'from_attributes': True,\n            'fields': {'field_a': {'validation_alias': 'FieldA', 'type': 'model-field', 'schema': {'type': 'int'}}},\n        },\n        config=CoreConfig(validate_by_name=True),\n    )\n    assert v.validate_test({'FieldA': 1}) == ({'field_a': 1}, {}, {'field_a'})\n    assert v.validate_test({'field_a': 1}) == ({'field_a': 1}, {}, {'field_a'})\n    assert v.validate_python(Cls(FieldA=1)) == ({'field_a': 1}, {}, {'field_a'})\n    assert v.validate_python(Cls(field_a=1)) == ({'field_a': 1}, {}, {'field_a'})", "target": "def test_alias_extra_by_name(py_and_json: PyAndJson):\n    v = py_and_json(\n        {\n            'type': 'typed-dict',\n            'extra_behavior': 'allow',\n            'fields': {\n                'field_a': {'validation_alias': 'FieldA', 'type': 'typed-dict-field', 'schema': {'type': 'int'}}\n            },\n            'config': {'validate_by_name': True},\n        },\n    )\n    assert v.validate_test({'FieldA': 1}) == {'field_a': 1}\n    assert v.validate_test({'field_a': 1}) == {'field_a': 1}", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000036", "source": "def validate_b(cls, v: bytes, info: core_schema.ValidationInfo) -> str:\n            assert v == b'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return 'hello world!'", "target": "def validate_b(cls, v: str, info: core_schema.ValidationInfo) -> str:\n            assert v == 'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return 'hello world!'", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|pydantic/pydantic", "data_split": "train", "example_id": "000037", "source": "def generate_perturbed_logarithm_dataset(size):\n    return np.random.randint(-50, 50, size=size) + 50.0 * np.log(1 + np.arange(size))", "target": "def test_repeat_after():\n    with pytest.raises(SchemaError, match='SchemaError: Duplicate ref: `foobar`'):\n        SchemaSerializer(\n            core_schema.definitions_schema(\n                core_schema.tuple_positional_schema(\n                    [\n                        core_schema.definitions_schema(\n                            core_schema.definition_reference_schema('foobar'), [core_schema.int_schema(ref='foobar')]\n                        ),\n                        core_schema.definition_reference_schema('foobar'),\n                    ]\n                ),\n                [core_schema.int_schema(ref='foobar')],\n            )\n        )", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "000038", "source": "def train(self, samples, responses):\n        _sample_n, var_n = samples.shape\n        new_samples = self.unroll_samples(samples)\n        new_responses = self.unroll_responses(responses)\n        var_types = np.array([cv.ml.VAR_NUMERICAL] * var_n + [cv.ml.VAR_CATEGORICAL, cv.ml.VAR_CATEGORICAL], np.uint8)\n        self.model.setWeakCount(15)\n        self.model.setMaxDepth(10)\n        self.model.train(cv.ml.TrainData_create(new_samples, cv.ml.ROW_SAMPLE, new_responses.astype(int), varType = var_types))", "target": "def test_dict_py():\n    v = SchemaValidator(\n        core_schema.dict_schema(keys_schema=core_schema.time_schema(), values_schema=core_schema.int_schema())\n    )\n    assert v.validate_python({time(12, 1, 1): 2, time(12, 1, 2): 4}) == {time(12, 1, 1): 2, time(12, 1, 2): 4}", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000039", "source": "def get_memory_bytes(self, args, kwargs) -> int:\n        x, target = args\n        M, N = x.shape\n        dtype = x.dtype\n        return (M * N + M + M) * dtype.itemsize", "target": "def get_memory_bytes(self, args, kwargs) -> int:\n        x, w, dy = args\n        M, N = x.shape\n        return (\n            2 * M * N * x.dtype.itemsize\n            + 2 * N * w.dtype.itemsize\n            + M * N * dy.dtype.itemsize\n        )", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000040", "source": "def test_forbid_extra():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={'field_a': core_schema.model_field(schema=core_schema.str_schema())}, extra_behavior='forbid'\n        )\n    )\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python({'field_a': 'abc', 'field_b': 1})\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'extra_forbidden', 'loc': ('field_b',), 'msg': 'Extra inputs are not permitted', 'input': 1}\n    ]", "target": "def test_forbid_extra(schema_extra_behavior: dict[str, Any], validate_fn_extra_kw: Union[ExtraBehavior, None]):\n    v = SchemaValidator(\n        core_schema.typed_dict_schema(\n            fields={'field_a': core_schema.typed_dict_field(schema=core_schema.str_schema())}, **schema_extra_behavior\n        )\n    )\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python({'field_a': 'abc', 'field_b': 1}, extra=validate_fn_extra_kw)\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'extra_forbidden', 'loc': ('field_b',), 'msg': 'Extra inputs are not permitted', 'input': 1}\n    ]", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "000041", "source": "def checkout(self, branch: str) -> None:\n        self._run_git(\"checkout\", branch)", "target": "def test_nullable():\n    v = SchemaValidator(core_schema.nullable_schema(schema=core_schema.int_schema()))\n    assert v.validate_python(None) is None\n    assert v.validate_python(1) == 1\n    assert v.validate_python('123') == 123\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python('hello')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': (),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'hello',\n        }\n    ]", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000042", "source": "def quack(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, target = args\n        from quack.cross_entropy import _cross_entropy\n        return lambda: _cross_entropy(x, target)", "target": "def quack(self, args, kwargs) -> Any:\n        from quack.layernorm import layernorm\n        x, w = args\n        return lambda: layernorm(x, w, eps=1e-6)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000043", "source": "def test_aware_specific():\n    v = SchemaValidator(core_schema.datetime_schema(tz_constraint=0))\n    value = datetime.now(tz=timezone.utc)\n    assert value is v.validate_python(value)\n    assert v.validate_python('2022-06-08T12:13:14Z') == datetime(2022, 6, 8, 12, 13, 14, tzinfo=timezone.utc)\n    value = datetime.now()\n    with pytest.raises(ValidationError, match='Input should have timezone info'):\n        v.validate_python(value)\n    value = datetime.now(tz=timezone(timedelta(hours=1)))\n    with pytest.raises(ValidationError, match='Timezone offset of 0 required, got 3600') as exc_info:\n        v.validate_python(value)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'timezone_offset',\n            'loc': (),\n            'msg': 'Timezone offset of 0 required, got 3600',\n            'input': value,\n            'ctx': {'tz_expected': 0, 'tz_actual': 3600},\n        }\n    ]\n    with pytest.raises(ValidationError, match='Timezone offset of 0 required, got 3600'):\n        v.validate_python('2022-06-08T12:13:14+01:00')", "target": "def test_aware_specific():\n    v = SchemaValidator(core_schema.time_schema(tz_constraint=0))\n    value = time(12, 13, 15, tzinfo=timezone.utc)\n    assert value is v.validate_python(value)\n    assert v.validate_python('12:13:14Z') == time(12, 13, 14, tzinfo=timezone.utc)\n    value = time(12, 13, 14)\n    with pytest.raises(ValidationError, match='Input should have timezone info'):\n        v.validate_python(value)\n    value = time(12, 13, 15, tzinfo=timezone(timedelta(hours=1)))\n    with pytest.raises(ValidationError, match='Timezone offset of 0 required, got 3600') as exc_info:\n        v.validate_python(value)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'timezone_offset',\n            'loc': (),\n            'msg': 'Timezone offset of 0 required, got 3600',\n            'input': value,\n            'ctx': {'tz_expected': 0, 'tz_actual': 3600},\n        }\n    ]\n    with pytest.raises(ValidationError, match='Timezone offset of 0 required, got 3600'):\n        v.validate_python('12:13:14+01:00')", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000044", "source": "def test_dataclass_post_init():\n    @dataclasses.dataclass\n    class Foo:\n        a: str\n        b: bool\n        def __post_init__(self):\n            self.a = self.a.upper()\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n            ],\n        ),\n        ['a', 'b'],\n        post_init=True,\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 'hello', 'b': True})\n    assert foo.a == 'HELLO'\n    assert foo.b is True", "target": "def test_cycle_change():\n    def fallback_func_change_id(obj):\n        return Foobar()\n    f = Foobar()\n    with pytest.raises(ValueError, match=r'Circular reference detected \\(depth exceeded\\)'):\n        to_jsonable_python(f, fallback=fallback_func_change_id)\n    with pytest.raises(ValueError, match=r'Circular reference detected \\(depth exceeded\\)'):\n        to_json(f, fallback=fallback_func_change_id)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000045", "source": "def test_aliases_path_negative(input_value, expected):\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={'field_a': core_schema.model_field(validation_alias=['foo', -2], schema=core_schema.int_schema())}\n        ),\n        config=CoreConfig(loc_by_alias=False),\n    )\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=expected.message):\n            val = v.validate_python(input_value)\n            print(f'UNEXPECTED OUTPUT: {val!r}')\n    else:\n        output = v.validate_python(input_value)\n        assert output == expected", "target": "def test_aliases_path_negative(input_value, expected):\n    v = SchemaValidator(\n        core_schema.typed_dict_schema(\n            fields={\n                'field_a': core_schema.typed_dict_field(validation_alias=['foo', -2], schema=core_schema.int_schema())\n            },\n            config=CoreConfig(loc_by_alias=False),\n        )\n    )\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=expected.message):\n            val = v.validate_python(input_value)\n            print(f'UNEXPECTED OUTPUT: {val!r}')\n    else:\n        output = v.validate_python(input_value)\n        assert output == expected", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000046", "source": "def compiled(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        (x,) = args\n        torch._dynamo.mark_dynamic(x, 0)\n        compiled_softmax = torch.compile(\n            lambda x: F.softmax(x, dim=-1), mode=self.compile_mode, fullgraph=True\n        )\n        return lambda: compiled_softmax(x)", "target": "def compiled(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, w, dy = args\n        compiled_layernorm = torch.compile(\n            self.layernorm_ref, mode=self.compile_mode, fullgraph=True\n        )\n        y = compiled_layernorm(x, w)\n        return lambda: torch.autograd.grad(\n            y, [x, w], grad_outputs=dy, retain_graph=True\n        )", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000047", "source": "def test_dict_py():\n    v = SchemaValidator(cs.dict_schema(keys_schema=cs.date_schema(), values_schema=cs.int_schema()))\n    assert v.validate_python({date(2000, 1, 1): 2, date(2000, 1, 2): 4}) == {date(2000, 1, 1): 2, date(2000, 1, 2): 4}", "target": "def test_dict_py():\n    v = SchemaValidator(cs.dict_schema(keys_schema=cs.datetime_schema(), values_schema=cs.int_schema()))\n    assert v.validate_python({datetime(2000, 1, 1): 2, datetime(2000, 1, 2): 4}) == {\n        datetime(2000, 1, 1): 2,\n        datetime(2000, 1, 2): 4,\n    }", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000048", "source": "def load_hook(self, state_dict, prefix, *args):\n        if prefix + \"wq.weight\" in state_dict:\n            wq = state_dict.pop(prefix + \"wq.weight\")\n            wk = state_dict.pop(prefix + \"wk.weight\")\n            wv = state_dict.pop(prefix + \"wv.weight\")\n            state_dict[prefix + \"wqkv.weight\"] = torch.cat([wq, wk, wv])", "target": "def load_hook(self, state_dict, prefix, *args):\n        if prefix + \"wq.weight\" in state_dict:\n            wq = state_dict.pop(prefix + \"wq.weight\")\n            wk = state_dict.pop(prefix + \"wk.weight\")\n            wv = state_dict.pop(prefix + \"wv.weight\")\n            state_dict[prefix + \"wqkv.weight\"] = torch.cat([wq, wk, wv])", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000049", "source": "def build(self):\n        self.cmake_path = 'cmake'\n        build_config = 'Release' if not self.config.build_debug else 'Debug'\n        cmd = [self.cmake_path, '-G', 'Visual Studio 16 2019', '-A', 'x64']\n        cmake_vars = dict(\n            CMAKE_BUILD_TYPE=build_config,\n            TREAT_WARNING_AS_ERROR='OFF',\n            ENABLE_SAMPLES='OFF',\n            ENABLE_TESTS='OFF',\n            BUILD_TESTS='OFF',\n            ENABLE_OPENCV='OFF',\n            ENABLE_GNA='OFF',\n            ENABLE_SPEECH_DEMO='OFF',\n            NGRAPH_DOC_BUILD_ENABLE='OFF',\n            NGRAPH_UNIT_TEST_ENABLE='OFF',\n            NGRAPH_UNIT_TEST_OPENVINO_ENABLE='OFF',\n            NGRAPH_TEST_UTIL_ENABLE='OFF',\n            NGRAPH_ONNX_IMPORT_ENABLE='OFF',\n            CMAKE_INSTALL_PREFIX=str(self.build_dir / 'install'),\n            OUTPUT_ROOT=str(self.build_dir),\n        )\n        self.build_config_file = str(self.cpath / 'build.config.py')\n        if os.path.exists(str(self.build_config_file)):\n            with open(self.build_config_file, 'r') as f:\n                cfg = f.read()\n            exec(compile(cfg, str(self.build_config_file), 'exec'))\n            log.info('DLDT processed build configuration script')\n        cmd += [ '-D%s=%s' % (k, v) for (k, v) in cmake_vars.items() if v is not None]\n        if self.config.cmake_option_dldt:\n            cmd += self.config.cmake_option_dldt\n        cmd.append(str(self.srcdir))\n        build_dir = self.build_dir\n        try:\n            execute(cmd, cwd=build_dir)\n            cmd = [self.cmake_path, '--build', '.', '--config', build_config,\n                    '--',\n                    '/v:n', '/consoleloggerparameters:NoSummary',\n            ]\n            execute(cmd, cwd=build_dir)\n            cmd = [self.cmake_path, '-DBUILD_TYPE=' + build_config, '-P', 'cmake_install.cmake']\n            execute(cmd, cwd=build_dir / 'ngraph')\n        except:\n            raise\n        log.info('DLDT build completed')", "target": "def build(self, builderDLDT):\n        self.cmake_path = 'cmake'\n        build_config = 'Release' if not self.config.build_debug else 'Debug'\n        cmd = [self.cmake_path, '-G', 'Visual Studio 16 2019', '-A', 'x64']\n        cmake_vars = dict(\n            CMAKE_BUILD_TYPE=build_config,\n            INSTALL_CREATE_DISTRIB='ON',\n            BUILD_opencv_world='OFF',\n            BUILD_TESTS='OFF',\n            BUILD_PERF_TESTS='OFF',\n            ENABLE_CXX11='ON',\n            WITH_INF_ENGINE='ON',\n            WITH_TBB='ON',\n            CPU_BASELINE='AVX2',\n            CMAKE_INSTALL_PREFIX=str(self.install_dir),\n            INSTALL_PDB='ON',\n            INSTALL_PDB_COMPONENT_EXCLUDE_FROM_ALL='OFF',\n            VIDEOIO_PLUGIN_LIST='all',\n            OPENCV_SKIP_CMAKE_ROOT_CONFIG='ON',\n            OPENCV_BIN_INSTALL_PATH='bin',\n            OPENCV_INCLUDE_INSTALL_PATH='include',\n            OPENCV_LIB_INSTALL_PATH='lib',\n            OPENCV_CONFIG_INSTALL_PATH='cmake',\n            OPENCV_3P_LIB_INSTALL_PATH='3rdparty',\n            OPENCV_SAMPLES_SRC_INSTALL_PATH='samples',\n            OPENCV_DOC_INSTALL_PATH='doc',\n            OPENCV_OTHER_INSTALL_PATH='etc',\n            OPENCV_LICENSES_INSTALL_PATH='etc/licenses',\n            OPENCV_INSTALL_DATA_DIR_RELATIVE='../../src/opencv',\n            BUILD_opencv_python2='OFF',\n            BUILD_opencv_python3='ON',\n            PYTHON3_LIMITED_API='ON',\n            OPENCV_PYTHON_INSTALL_PATH='python',\n        )\n        if self.config.dldt_release:\n            cmake_vars['INF_ENGINE_RELEASE'] = str(self.config.dldt_release)\n        InferenceEngine_DIR = str(builderDLDT.sysrootdir / 'deployment_tools' / 'inference_engine' / 'cmake')\n        assert os.path.exists(InferenceEngine_DIR), InferenceEngine_DIR\n        cmake_vars['InferenceEngine_DIR:PATH'] = InferenceEngine_DIR\n        ngraph_DIR = str(builderDLDT.sysrootdir / 'ngraph/cmake')\n        if not os.path.exists(ngraph_DIR):\n            ngraph_DIR = str(builderDLDT.sysrootdir / 'ngraph/deployment_tools/ngraph/cmake')\n        assert os.path.exists(ngraph_DIR), ngraph_DIR\n        cmake_vars['ngraph_DIR:PATH'] = ngraph_DIR\n        cmake_vars['TBB_DIR:PATH'] = str(builderDLDT.sysrootdir / 'tbb/cmake')\n        assert os.path.exists(cmake_vars['TBB_DIR:PATH']), cmake_vars['TBB_DIR:PATH']\n        if self.config.build_debug:\n            cmake_vars['CMAKE_BUILD_TYPE'] = 'Debug'\n            cmake_vars['BUILD_opencv_python3'] ='OFF'\n            cmake_vars['OPENCV_INSTALL_APPS_LIST'] = 'all'\n        if self.config.build_tests:\n            cmake_vars['BUILD_TESTS'] = 'ON'\n            cmake_vars['BUILD_PERF_TESTS'] = 'ON'\n            cmake_vars['BUILD_opencv_ts'] = 'ON'\n            cmake_vars['INSTALL_TESTS']='ON'\n        if self.config.build_tests_dnn:\n            cmake_vars['BUILD_TESTS'] = 'ON'\n            cmake_vars['BUILD_PERF_TESTS'] = 'ON'\n            cmake_vars['BUILD_opencv_ts'] = 'ON'\n            cmake_vars['OPENCV_BUILD_TEST_MODULES_LIST'] = 'dnn'\n            cmake_vars['OPENCV_BUILD_PERF_TEST_MODULES_LIST'] = 'dnn'\n            cmake_vars['INSTALL_TESTS']='ON'\n        cmd += [ \"-D%s=%s\" % (k, v) for (k, v) in cmake_vars.items() if v is not None]\n        if self.config.cmake_option:\n            cmd += self.config.cmake_option\n        cmd.append(str(self.src_dir))\n        log.info('Configuring OpenCV...')\n        execute(cmd, cwd=self.build_dir)\n        log.info('Building OpenCV...')\n        cmd = [self.cmake_path, '--build', '.', '--config', build_config, '--target', 'install',\n                '--', '/v:n', '/m:2', '/consoleloggerparameters:NoSummary'\n        ]\n        execute(cmd, cwd=self.build_dir)\n        log.info('OpenCV build/install completed')", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000050", "source": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "target": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "train", "example_id": "000051", "source": "def get_subparser(\n    parser: argparse.ArgumentParser, name: str\n) -> argparse.ArgumentParser:\n    subparsers_action = next(\n        a\n        for a in parser._subparsers._group_actions\n        if isinstance(a, argparse._SubParsersAction)\n    )\n    return subparsers_action.choices[name]", "target": "def with_legacy():\n    torch._C._jit_override_can_fuse_on_cpu(True)\n    torch._C._jit_override_can_fuse_on_gpu(True)\n    torch._C._jit_set_texpr_fuser_enabled(False)\n    torch._C._jit_set_nvfuser_enabled(False)\n    torch._C._jit_set_profiling_executor(False)\n    torch._C._jit_set_profiling_mode(False)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000052", "source": "def validate_b(\n            cls, v: bytes, nxt: core_schema.ValidatorFunctionWrapHandler, info: core_schema.ValidationInfo\n        ) -> bytes:\n            assert v == b'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return nxt(b'hello world!')", "target": "def validate_b(cls, v: str, info: core_schema.ValidationInfo) -> str:\n            assert v == 'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return 'hello world!'", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "train", "example_id": "000053", "source": "def set_soname(self, file_name: str, new_soname: str) -> None:\n        check_call(\n            [\"patchelf\", \"--page-size\", \"65536\", \"--set-soname\", new_soname, file_name]\n        )", "target": "def get_git_repo_dir() -> str:\n    from pathlib import Path\n    return os.getenv(\"GIT_REPO_DIR\", str(Path(__file__).resolve().parents[2]))", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000054", "source": "def area(self) -> int:\n            return self.width * self.height", "target": "def area(self) -> int:\n            return self.width * self.height", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000055", "source": "def liger(self, args, kwargs) -> Any:\n        from liger_kernel.transformers.rms_norm import LigerRMSNorm\n        x, w = args\n        M, N = x.shape\n        liger_rmsnorm = LigerRMSNorm(hidden_size=N, eps=1e-6).cuda()\n        liger_rmsnorm.weight.data.copy_(w)\n        return lambda: liger_rmsnorm(x)", "target": "def liger(self, args, kwargs) -> Any:\n        from liger_kernel.transformers.layer_norm import LigerLayerNorm\n        x, w = args\n        M, N = x.shape\n        liger_layernorm = LigerLayerNorm(hidden_size=N, eps=1e-6).cuda()\n        liger_layernorm.weight.data.copy_(w)\n        liger_layernorm.bias.data.copy_(\n            torch.zeros(N, device=\"cuda\", dtype=torch.float32)\n        )\n        return lambda: liger_layernorm(x)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000056", "source": "def area(self) -> int:\n            return self.width * self.height", "target": "def area(self) -> int:\n            return self.width * self.height", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000057", "source": "def test_dict_py():\n    v = SchemaValidator(\n        core_schema.dict_schema(keys_schema=core_schema.time_schema(), values_schema=core_schema.int_schema())\n    )\n    assert v.validate_python({time(12, 1, 1): 2, time(12, 1, 2): 4}) == {time(12, 1, 1): 2, time(12, 1, 2): 4}", "target": "def test_dict_py():\n    v = SchemaValidator(\n        core_schema.dict_schema(keys_schema=core_schema.timedelta_schema(), values_schema=core_schema.int_schema())\n    )\n    assert v.validate_python({timedelta(days=2, hours=1): 2, timedelta(days=2, hours=2): 4}) == {\n        timedelta(days=2, hours=1): 2,\n        timedelta(days=2, hours=2): 4,\n    }", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000058", "source": "def is_function_with_inner_schema(\n    schema: CoreSchemaOrField,\n) -> TypeGuard[FunctionSchemaWithInnerSchema]:\n    return schema['type'] in _FUNCTION_WITH_INNER_SCHEMA_TYPES", "target": "def _wrapper2(fields_tuple: RootValidatorFieldsTuple, _: core_schema.ValidationInfo) -> RootValidatorFieldsTuple:\n        if len(fields_tuple) == 2:\n            values, init_vars = fields_tuple\n            values = validator(values)\n            return values, init_vars\n        else:\n            model_dict, model_extra, fields_set = fields_tuple\n            if model_extra:\n                fields = set(model_dict.keys())\n                model_dict.update(model_extra)\n                model_dict_new = validator(model_dict)\n                for k in list(model_dict_new.keys()):\n                    if k not in fields:\n                        model_extra[k] = model_dict_new.pop(k)\n            else:\n                model_dict_new = validator(model_dict)\n            return model_dict_new, model_extra, fields_set", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "000059", "source": "def sequence_(cls, ctype_name: str, item: TypeNode,\n                  export_name: Optional[str] = None,\n                  doc: Optional[str] = None,\n                  required_modules: Tuple[str, ...] = ()):\n        return cls(ctype_name, SequenceTypeNode(ctype_name, item),\n                   export_name, doc, required_modules)", "target": "def test_validate_assignment():\n    def f(input_value):\n        input_value.more = 'foobar'\n        return input_value\n    class Model:\n        __slots__ = '__dict__', '__pydantic_fields_set__', '__pydantic_extra__', '__pydantic_private__'\n        field_a: str\n        def __init__(self):\n            self.__pydantic_extra__ = None\n    v = SchemaValidator(\n        core_schema.no_info_after_validator_function(\n            f,\n            core_schema.model_schema(\n                Model, core_schema.model_fields_schema({'field_a': core_schema.model_field(core_schema.str_schema())})\n            ),\n        )\n    )\n    m = v.validate_python({'field_a': 'test'})\n    assert isinstance(m, Model)\n    assert m.field_a == 'test'\n    assert m.__pydantic_fields_set__ == {'field_a'}\n    assert m.__dict__ == {'field_a': 'test', 'more': 'foobar'}\n    assert m.__pydantic_extra__ is None\n    m2 = Model()\n    m2.field_a = 'test'\n    assert v.validate_assignment(m2, 'field_a', b'abc') is m2\n    assert m2.__dict__ == {'field_a': 'abc', 'more': 'foobar'}\n    assert not hasattr(m2, '__pydantic_fields_set__')", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pytorch/pytorch", "data_split": "train", "example_id": "000060", "source": "def getCMakeArgs(self):\n        args = [\n            \"cmake\",\n            \"-GXcode\",\n            \"-DFRAMEWORK_DIR=%s\" % self.framework_dir,\n            \"-DFRAMEWORK_NAME=%s\" % self.framework_name,\n        ]\n        return args", "target": "def dynamic_rnn(\n        input: Tensor,\n        hidden: tuple[Tensor, Tensor],\n        w_ih: Tensor,\n        w_hh: Tensor,\n        b_ih: Tensor,\n        b_hh: Tensor,\n    ) -> tuple[Tensor, tuple[Tensor, Tensor]]:\n        hx, cx = hidden\n        outputs = []\n        inputs = input.unbind(0)\n        hy, cy = hx[0], cx[0]\n        for seq_idx in range(len(inputs)):\n            hy, cy = cell(inputs[seq_idx], (hy, cy), w_ih, w_hh, b_ih, b_hh)\n            outputs += [hy]\n        return torch.stack(outputs), (hy.unsqueeze(0), cy.unsqueeze(0))", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|pydantic/pydantic", "data_split": "train", "example_id": "000061", "source": "def generate_pathological_dataset(size):\n    return np.r_[\n        np.arange(size), np.arange(-(size - 1), size), np.arange(-(size - 1), 1)\n    ]", "target": "def test_strict():\n    v = SchemaValidator(\n        {\n            'type': 'typed-dict',\n            'fields': {\n                'field_a': {'type': 'typed-dict-field', 'schema': {'type': 'str'}},\n                'field_b': {'type': 'typed-dict-field', 'schema': {'type': 'int'}},\n            },\n            'config': CoreConfig(strict=True),\n        }\n    )\n    assert v.validate_python({'field_a': 'hello', 'field_b': 12}) == {'field_a': 'hello', 'field_b': 12}\n    with pytest.raises(ValidationError) as exc_info:\n        assert v.validate_python({'field_a': 123, 'field_b': '123'})\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'string_type', 'loc': ('field_a',), 'msg': 'Input should be a valid string', 'input': 123},\n        {'type': 'int_type', 'loc': ('field_b',), 'msg': 'Input should be a valid integer', 'input': '123'},\n    ]", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "000062", "source": "def load_data(dtype=np.float32, order=\"C\", random_state=13):\n    print(\"Loading dataset...\")\n    data = fetch_covtype(\n        download_if_missing=True, shuffle=True, random_state=random_state\n    )\n    X = check_array(data[\"data\"], dtype=dtype, order=order)\n    y = (data[\"target\"] != 1).astype(int)\n    print(\"Creating train-test split...\")\n    n_train = 522911\n    X_train = X[:n_train]\n    y_train = y[:n_train]\n    X_test = X[n_train:]\n    y_test = y[n_train:]\n    mean = X_train.mean(axis=0)\n    std = X_train.std(axis=0)\n    mean[10:] = 0.0\n    std[10:] = 1.0\n    X_train = (X_train - mean) / std\n    X_test = (X_test - mean) / std\n    return X_train, X_test, y_train, y_test", "target": "def load_data(dtype=np.float32, order=\"C\", shuffle=True, seed=0):\n    print(\"Loading dataset...\")\n    data = fetch_openml(\"mnist_784\", as_frame=True)\n    X = check_array(data[\"data\"], dtype=dtype, order=order)\n    y = data[\"target\"]\n    if shuffle:\n        X, y = _shuffle(X, y, random_state=seed)\n    X /= 255\n    return X, y", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pytorch/pytorch", "data_split": "train", "example_id": "000063", "source": "def test_stateful_throw_setup(self):\n            @cv.gapi.kernel(GStatefulCounter)\n            class GThrowStatefulCounterImpl:\n                @staticmethod\n                def setup(desc):\n                    raise Exception('Throw from setup method')\n                @staticmethod\n                def run(value, state):\n                    raise Exception('Unreachable')\n            g_in  = cv.GOpaque.Int()\n            g_out = GStatefulCounter.on(g_in)\n            comp  = cv.GComputation(cv.GIn(g_in), cv.GOut(g_out))\n            pkg   = cv.gapi.kernels(GThrowStatefulCounterImpl)\n            with self.assertRaises(Exception): comp.apply(cv.gin(42),\n                                                          args=cv.gapi.compile_args(pkg))", "target": "def parse_cmd_args() -> argparse.Namespace:\n    parser = argparse.ArgumentParser(description=\"Run a TorchCache benchmark.\")\n    parser.add_argument(\n        \"-m\",\n        \"--model\",\n        help=\"Name of the model to run\",\n    )\n    parser.add_argument(\n        \"--dynamic\",\n        action=\"store_true\",\n        help=\"Whether to run with dynamic enabled\",\n    )\n    parser.add_argument(\n        \"--benchmark\",\n        choices=(\"torchbench\", \"huggingface\"),\n        required=True,\n        help=\"Name of benchmark suite to run\",\n    )\n    parser.add_argument(\n        \"--mode\",\n        choices=(\"inference\", \"training\"),\n        default=\"training\",\n    )\n    parser.add_argument(\n        \"--device\",\n        default=\"cuda\",\n        choices=(\"cuda\", \"cpu\"),\n    )\n    parser.add_argument(\n        \"--output\",\n        required=True,\n        help=\"The output filename (json)\",\n    )\n    parser.add_argument(\n        \"--repeat\",\n        type=int,\n        default=1,\n        choices=range(1, 10),\n        help=\"Number of times to repeat the compilation (reduce noise)\",\n    )\n    args, _ = parser.parse_known_args()\n    return args", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|pydantic/pydantic", "data_split": "train", "example_id": "000064", "source": "def benchmark(estimator, data):\n    gc.collect()\n    print(\"Benching %s\" % estimator)\n    t0 = time()\n    estimator.fit(data)\n    training_time = time() - t0\n    data_t = estimator.transform(data)\n    data_r = estimator.inverse_transform(data_t)\n    reconstruction_error = np.mean(np.abs(data - data_r))\n    return {\"time\": training_time, \"error\": reconstruction_error}", "target": "def nullable_schema(\n    schema: CoreSchema,\n    *,\n    strict: bool | None = None,\n    ref: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> NullableSchema:\n    return _dict_not_none(\n        type='nullable', schema=schema, strict=strict, ref=ref, metadata=metadata, serialization=serialization\n    )", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "000065", "source": "def make_estimator(self, params):\n        (representation,) = params\n        max_iter = 60 if representation == \"dense\" else 300\n        estimator = SGDRegressor(max_iter=max_iter, tol=None, random_state=0)\n        return estimator", "target": "def make_estimator(self, params):\n        representation, precompute = params\n        estimator = ElasticNet(precompute=precompute, alpha=0.001, random_state=0)\n        return estimator", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|opencv/opencv", "data_split": "train", "example_id": "000066", "source": "def track(self, frame):\n        self.frame_points, frame_descrs = self.detect_features(frame)\n        if len(self.frame_points) < MIN_MATCH_COUNT:\n            return []\n        matches = self.matcher.knnMatch(frame_descrs, k = 2)\n        matches = [m[0] for m in matches if len(m) == 2 and m[0].distance < m[1].distance * 0.75]\n        if len(matches) < MIN_MATCH_COUNT:\n            return []\n        matches_by_id = [[] for _ in xrange(len(self.targets))]\n        for m in matches:\n            matches_by_id[m.imgIdx].append(m)\n        tracked = []\n        for imgIdx, matches in enumerate(matches_by_id):\n            if len(matches) < MIN_MATCH_COUNT:\n                continue\n            target = self.targets[imgIdx]\n            p0 = [target.keypoints[m.trainIdx].pt for m in matches]\n            p1 = [self.frame_points[m.queryIdx].pt for m in matches]\n            p0, p1 = np.float32((p0, p1))\n            H, status = cv.findHomography(p0, p1, cv.RANSAC, 3.0)\n            status = status.ravel() != 0\n            if status.sum() < MIN_MATCH_COUNT:\n                continue\n            p0, p1 = p0[status], p1[status]\n            x0, y0, x1, y1 = target.rect\n            quad = np.float32([[x0, y0], [x1, y0], [x1, y1], [x0, y1]])\n            quad = cv.perspectiveTransform(quad.reshape(1, -1, 2), H).reshape(-1, 2)\n            track = TrackedTarget(target=target, p0=p0, p1=p1, H=H, quad=quad)\n            tracked.append(track)\n        tracked.sort(key = lambda t: len(t.p0), reverse=True)\n        return tracked", "target": "def required_modules(self) -> Tuple[str, ...]:\n        return self._required_modules", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pytorch/pytorch", "data_split": "train", "example_id": "000067", "source": "def parse_arguments():\n    parser = argparse.ArgumentParser(\n        description='Copies generated typing stubs only when generation '\n        'succeeded. This is identified by presence of the `py.typed` file '\n        'inside typing stubs directory.'\n    )\n    parser.add_argument('--stubs_dir', type=str,\n                        help='Path to directory containing generated typing '\n                        'stubs file')\n    parser.add_argument('--output_dir', type=str,\n                        help='Path to output directory')\n    return parser.parse_args()", "target": "def test_cli_run_build_external(self, mock_init, mock_run):\n        from cli.run import main\n        test_args = [\"cli.run\", \"build\", \"external\", \"vllm\"]\n        with patch.object(sys, \"argv\", test_args):\n            try:\n                main()\n            except SystemExit:\n                pass\n        mock_init.assert_called_once()\n        mock_run.assert_called_once_with()", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "000068", "source": "def make_estimator(self, params):\n        representation, solver = params\n        estimator = Ridge(solver=solver, fit_intercept=False, random_state=0)\n        return estimator", "target": "def make_estimator(self, params):\n        estimator = LinearRegression()\n        return estimator", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000069", "source": "def type_format(self) -> str:\n        if TypeNode.compatible_to_runtime_usage:\n            return \"_typing.Optional[{}]\"\n        return \"{} | None\"", "target": "def type_format(self) -> str:\n        return \"_typing.Type[{}]\"", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "000070", "source": "def test_function_plain_field_serializer_with_computed_field():\n    @dataclasses.dataclass\n    class Model:\n        x: int\n        @property\n        def computed_field_x(self) -> int:\n            return self.x + 200\n        def ser_func(self, v: Any, info: core_schema.FieldSerializationInfo) -> str:\n            return info.field_name + '_' + str(v * 2)\n    field_str_with_field_serializer = core_schema.str_schema(\n        serialization=core_schema.plain_serializer_function_ser_schema(\n            Model.ser_func,\n            is_field_serializer=True,\n            info_arg=True,\n            return_schema=core_schema.any_schema(),\n        )\n    )\n    s = SchemaSerializer(\n        core_schema.model_schema(\n            Model,\n            core_schema.model_fields_schema(\n                {'x': core_schema.model_field(field_str_with_field_serializer)},\n                computed_fields=[\n                    core_schema.computed_field('computed_field_x', field_str_with_field_serializer),\n                ],\n            ),\n        )\n    )\n    assert json.loads(s.to_json(Model(x=1000))) == {'x': 'x_2000', 'computed_field_x': 'computed_field_x_2400'}\n    assert s.to_python(Model(x=2000)) == {'x': 'x_4000', 'computed_field_x': 'computed_field_x_4400'}", "target": "def inductor_mm(a, b):\n    return torch.mm(a, b)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|opencv/opencv", "data_split": "train", "example_id": "000071", "source": "def main():\n    parser = argparse.ArgumentParser(\n        \"Main script to compare results from the benchmarks\"\n    )\n    parser.add_argument(\n        \"--before\",\n        type=str,\n        default=\"before.txt\",\n        help=\"Text file containing the times to use as base\",\n    )\n    parser.add_argument(\n        \"--after\",\n        type=str,\n        default=\"after.txt\",\n        help=\"Text file containing the times to use as new version\",\n    )\n    parser.add_argument(\n        \"--output\", type=str, default=\"\", help=\"Text file where to write the output\"\n    )\n    args = parser.parse_args()\n    with open(args.before) as f:\n        content = f.read()\n    res_before = from_markdown_table(content)\n    with open(args.after) as f:\n        content = f.read()\n    res_after = from_markdown_table(content)\n    diff = defaultdict(defaultdict)\n    for model in res_before:\n        for task in res_before[model]:\n            mean_before, var_before = res_before[model][task]\n            if task not in res_after[model]:\n                diff[model][task] = (None, mean_before, var_before, None, None)\n            else:\n                mean_after, var_after = res_after[model][task]\n                diff[model][task] = (\n                    mean_before / mean_after,\n                    mean_before,\n                    var_before,\n                    mean_after,\n                    var_after,\n                )\n    for model in res_after:\n        for task in res_after[model]:\n            if task not in res_before[model]:\n                mean_after, var_after = res_after[model][task]\n                diff[model][task] = (None, None, None, mean_after, var_after)\n    header = (\n        \"model\",\n        \"task\",\n        \"speedup\",\n        \"mean (before)\",\n        \"var (before)\",\n        \"mean (after)\",\n        \"var (after)\",\n    )\n    out = to_markdown_table(diff, header=header)\n    print(out)\n    if args.output:\n        with open(args.output, \"w\") as f:\n            f.write(out)", "target": "def setInitialRect(self, rect):\n        self.initialRect = rect", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000072", "source": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "target": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "000073", "source": "def main() -> None:\n    args = parse_args()\n    print(f\"Exporting labels for {args.org}/{args.repo}\")\n    labels_file_name = \"pytorch_labels.json\"\n    obj = boto3.resource(\"s3\").Object(\"ossci-metrics\", labels_file_name)\n    obj.put(Body=json.dumps(gh_get_labels(args.org, args.repo)).encode())", "target": "def field_before_validator_function(function: WithInfoValidatorFunction, field_name: str, schema: CoreSchema, **kwargs):\n    warnings.warn(\n        '`field_before_validator_function` is deprecated, use `with_info_before_validator_function` instead.',\n        DeprecationWarning,\n    )\n    return with_info_before_validator_function(function, schema, field_name=field_name, **kwargs)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|opencv/opencv", "data_split": "train", "example_id": "000074", "source": "def resolve_type_nodes(self, root: ASTNode) -> None:\n        errors = []\n        for child in itertools.chain(self.properties,\n                                     self.functions.values(),\n                                     self.classes.values()):\n            try:\n                try:\n                    child.resolve_type_nodes(self)\n                except TypeResolutionError:\n                    child.resolve_type_nodes(root)\n            except TypeResolutionError as e:\n                errors.append(str(e))\n        if len(errors) > 0:\n            raise TypeResolutionError(\n                'Failed to resolve \"{}\" class against \"{}\". Errors: {}'.format(\n                    self.full_export_name, root.full_export_name, errors\n                )\n            )", "target": "def get_ocv_arithm_op_table(apply_saturation=False):\n    def saturate(func):\n        def wrapped_func(x, y):\n            dst_dtype = x.dtype\n            if apply_saturation:\n                if np.issubdtype(x.dtype, np.integer):\n                    x = x.astype(np.int64)\n            if not isinstance(y, (float, int)):\n                if len(y) > x.shape[-1]:\n                    y = y[:x.shape[-1]]\n                else:\n                    y = rpad(y, x.shape[-1], pad_value=0)\n            dst = func(x, y)\n            if apply_saturation:\n                min_val, max_val = get_limits(dst_dtype)\n                dst = np.clip(dst, min_val, max_val)\n            return dst.astype(dst_dtype)\n        return wrapped_func\n    @saturate\n    def subtract(x, y):\n        return x - y\n    @saturate\n    def add(x, y):\n        return x + y\n    @saturate\n    def divide(x, y):\n        if not isinstance(y, (int, float)):\n            dst_dtype = np.result_type(x, y)\n            y = np.array(y).astype(dst_dtype)\n            _, max_value = get_limits(dst_dtype)\n            y[y == 0] = max_value\n        dst = 1.0 * x / y\n        if np.issubdtype(x.dtype, np.integer):\n            dst = np.rint(dst)\n        return dst\n    @saturate\n    def multiply(x, y):\n        return x * y\n    @saturate\n    def absdiff(x, y):\n        res = np.abs(x - y)\n        return res\n    return {\n        cv.subtract: subtract,\n        cv.add: add,\n        cv.multiply: multiply,\n        cv.divide: divide,\n        cv.absdiff: absdiff\n    }", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000075", "source": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "target": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000076", "source": "def get_memory_bytes(self, args, kwargs) -> int:\n        x, w, dy = args\n        M, N = x.shape\n        return 3 * M * N * x.dtype.itemsize + 2 * N * w.dtype.itemsize", "target": "def get_memory_bytes(self, args, kwargs) -> int:\n        x, w, dy = args\n        M, N = x.shape\n        return (\n            2 * M * N * x.dtype.itemsize\n            + 2 * N * w.dtype.itemsize\n            + M * N * dy.dtype.itemsize\n        )", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "000077", "source": "def str_lower(self: _Pipeline[_InT, str]) -> _Pipeline[_InT, str]:\n        return self.transform(str.lower)", "target": "def hessian_fwdrev(model, inp, v=None, strict=None):\n        argnums = tuple(range(len(inp)))\n        return ft.jacfwd(ft.jacrev(model, argnums=argnums), argnums=argnums)(*inp)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000078", "source": "def predict(self, samples):\n        _retval, results, _neigh_resp, _dists = self.model.findNearest(samples, self.k)\n        return results.ravel()", "target": "def predict(self, samples):\n        _retval, results, _neigh_resp, _dists = self.model.findNearest(samples, k = 10)\n        return results.ravel()", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000079", "source": "def required_definition_imports(self) -> Generator[str, None, None]:\n        yield from ()", "target": "def required_definition_imports(self) -> Generator[str, None, None]:\n        return self.value.required_usage_imports", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000080", "source": "def test_ignore_extra():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'field_a': core_schema.model_field(schema=core_schema.str_schema()),\n                'field_b': core_schema.model_field(schema=core_schema.int_schema()),\n            }\n        )\n    )\n    assert v.validate_python({'field_a': b'123', 'field_b': 1, 'field_c': 123}) == (\n        {'field_a': '123', 'field_b': 1},\n        None,\n        {'field_b', 'field_a'},\n    )", "target": "def test_ignore_extra():\n    v = SchemaValidator(\n        core_schema.typed_dict_schema(\n            fields={\n                'field_a': core_schema.typed_dict_field(schema=core_schema.str_schema()),\n                'field_b': core_schema.typed_dict_field(schema=core_schema.int_schema()),\n            }\n        )\n    )\n    assert v.validate_python({'field_a': b'123', 'field_b': 1, 'field_c': 123}) == {'field_a': '123', 'field_b': 1}", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000081", "source": "def test_neg_7200():\n    v = SchemaValidator(core_schema.datetime_schema(tz_constraint=-7200))\n    value = datetime.now(tz=timezone(timedelta(hours=-2)))\n    assert value is v.validate_python(value)\n    value = datetime.now()\n    with pytest.raises(ValidationError, match='Input should have timezone info'):\n        v.validate_python(value)\n    value = datetime.now(tz=timezone.utc)\n    with pytest.raises(ValidationError, match='Timezone offset of -7200 required, got 0'):\n        v.validate_python(value)\n    with pytest.raises(ValidationError, match='Timezone offset of -7200 required, got 0'):\n        v.validate_python('2022-06-08T12:13:14Z')", "target": "def test_neg_7200():\n    v = SchemaValidator(core_schema.time_schema(tz_constraint=-7200))\n    value = time(12, 13, 15, tzinfo=timezone(timedelta(hours=-2)))\n    assert value is v.validate_python(value)\n    value = time(12, 13, 14)\n    with pytest.raises(ValidationError, match='Input should have timezone info'):\n        v.validate_python(value)\n    value = time(12, 13, 15, tzinfo=timezone.utc)\n    with pytest.raises(ValidationError, match='Timezone offset of -7200 required, got 0'):\n        v.validate_python(value)\n    with pytest.raises(ValidationError, match='Timezone offset of -7200 required, got 0'):\n        v.validate_python('12:13:14Z')", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "000082", "source": "def make_data(self, params):\n        representation, n_jobs = params\n        if representation == \"sparse\":\n            data = _20newsgroups_highdim_dataset()\n        else:\n            data = _20newsgroups_lowdim_dataset()\n        return data", "target": "def make_data(self, params):\n        data = _synth_classification_dataset(\n            n_samples=10000, n_features=100, n_classes=5\n        )\n        return data", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "train", "example_id": "000083", "source": "def output_csv(output_file, headers, row):\n    if os.path.exists(output_file):\n        with open(output_file) as fd:\n            lines = list(csv.reader(fd)) or [[]]\n            if headers and len(headers) > len(lines[0]):\n                lines[0] = headers\n            else:\n                headers = lines[0]\n    else:\n        lines = [headers]\n    if output_file != DEFAULT_OUTPUT_FILE:\n        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n    lines.append([(f\"{x:.6f}\" if isinstance(x, float) else x) for x in row])\n    with open(output_file, \"w\") as fd:\n        writer = csv.writer(fd, lineterminator=\"\\n\")\n        for line in lines:\n            writer.writerow(list(line) + [\"0\"] * (len(headers) - len(line)))", "target": "def peek(self) -> Optional[str]:\n        if self._idx + 1 >= len(self._val):\n            return None\n        return self._val[self._idx + 1]", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000084", "source": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "target": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000085", "source": "def test_positional_or_keyword(py_and_json: PyAndJson, input_value, expected):\n    v = py_and_json(\n        {\n            'type': 'arguments',\n            'arguments_schema': [\n                {'name': 'a', 'mode': 'positional_or_keyword', 'schema': {'type': 'int'}},\n                {'name': 'b', 'schema': {'type': 'str'}},\n                {'name': 'c', 'mode': 'keyword_only', 'schema': {'type': 'bool'}},\n            ],\n        }\n    )\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)) as exc_info:\n            v.validate_test(input_value)\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        assert v.validate_test(input_value) == expected", "target": "def test_datetime_future(py_and_json: PyAndJson, input_value, expected):\n    v = py_and_json(core_schema.datetime_schema(now_utc_offset=0, now_op='future'))\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            v.validate_test(input_value)\n    else:\n        output = v.validate_test(input_value)\n        assert output == expected", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000086", "source": "def test_repeated_ref():\n    with pytest.raises(SchemaError, match='SchemaError: Duplicate ref: `foobar`'):\n        SchemaSerializer(\n            core_schema.tuple_positional_schema(\n                [\n                    core_schema.definitions_schema(\n                        core_schema.definition_reference_schema('foobar'), [core_schema.int_schema(ref='foobar')]\n                    ),\n                    core_schema.definitions_schema(\n                        core_schema.definition_reference_schema('foobar'), [core_schema.int_schema(ref='foobar')]\n                    ),\n                ]\n            )\n        )", "target": "def test_repeated_ref():\n    with pytest.raises(SchemaError, match='SchemaError: Duplicate ref: `foobar`'):\n        SchemaValidator(\n            schema=core_schema.tuple_positional_schema(\n                [\n                    core_schema.definitions_schema(\n                        core_schema.definition_reference_schema('foobar'), [core_schema.int_schema(ref='foobar')]\n                    ),\n                    core_schema.definitions_schema(\n                        core_schema.definition_reference_schema('foobar'), [core_schema.int_schema(ref='foobar')]\n                    ),\n                ]\n            )\n        )", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "train", "example_id": "000087", "source": "def update(self, op_code, cur, max=None, message=\"\"):\n        msg = self._cur_line or message\n        if max and cur:\n            percent = int(cur / max * 100)\n            if percent != self._last_percent and percent % self._interval == 0:\n                self._last_percent = percent\n                logger.info(\"Progress: %d%% - %s\", percent, msg)\n        elif msg:\n            logger.info(msg)", "target": "def forward(self, x, y):\n        return (x + y,)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000088", "source": "def _prepare_once(self) -> None:\n        self.mesh = torch.distributed.device_mesh.init_device_mesh(\n            \"cuda\", (self.world_size,), mesh_dim_names=(\"dp\",)\n        )\n        self.a = DTensor.from_local(\n            torch.ones(10, 10, device=self.device()), self.mesh, [Replicate()]\n        )\n        self.b = DTensor.from_local(\n            torch.ones(10, 10, device=self.device()), self.mesh, [Replicate()]\n        )", "target": "def _prepare_once(self) -> None:\n        self.a = torch.ones(10, 10, device=self.device())\n        self.b = torch.torch.ones(10, 10, device=self.device())", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "000089", "source": "def create_model():\n    model = resnet18()\n    return model", "target": "def _get_value(\n    cls: type[BaseModel],\n    v: Any,\n    to_dict: bool,\n    by_alias: bool,\n    include: AbstractSetIntStr | MappingIntStrAny | None,\n    exclude: AbstractSetIntStr | MappingIntStrAny | None,\n    exclude_unset: bool,\n    exclude_defaults: bool,\n    exclude_none: bool,\n) -> Any:\n    from .. import BaseModel\n    if isinstance(v, BaseModel):\n        if to_dict:\n            return v.model_dump(\n                by_alias=by_alias,\n                exclude_unset=exclude_unset,\n                exclude_defaults=exclude_defaults,\n                include=include,\n                exclude=exclude,\n                exclude_none=exclude_none,\n            )\n        else:\n            return v.copy(include=include, exclude=exclude)\n    value_exclude = _utils.ValueItems(v, exclude) if exclude else None\n    value_include = _utils.ValueItems(v, include) if include else None\n    if isinstance(v, dict):\n        return {\n            k_: _get_value(\n                cls,\n                v_,\n                to_dict=to_dict,\n                by_alias=by_alias,\n                exclude_unset=exclude_unset,\n                exclude_defaults=exclude_defaults,\n                include=value_include and value_include.for_element(k_),\n                exclude=value_exclude and value_exclude.for_element(k_),\n                exclude_none=exclude_none,\n            )\n            for k_, v_ in v.items()\n            if (not value_exclude or not value_exclude.is_excluded(k_))\n            and (not value_include or value_include.is_included(k_))\n        }\n    elif _utils.sequence_like(v):\n        seq_args = (\n            _get_value(\n                cls,\n                v_,\n                to_dict=to_dict,\n                by_alias=by_alias,\n                exclude_unset=exclude_unset,\n                exclude_defaults=exclude_defaults,\n                include=value_include and value_include.for_element(i),\n                exclude=value_exclude and value_exclude.for_element(i),\n                exclude_none=exclude_none,\n            )\n            for i, v_ in enumerate(v)\n            if (not value_exclude or not value_exclude.is_excluded(i))\n            and (not value_include or value_include.is_included(i))\n        )\n        return v.__class__(*seq_args) if _typing_extra.is_namedtuple(v.__class__) else v.__class__(seq_args)\n    elif isinstance(v, Enum) and getattr(cls.model_config, 'use_enum_values', False):\n        return v.value\n    else:\n        return v", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000090", "source": "def get_shapes(self) -> tuple[tuple[int, ...], ...]:\n        return (\n            (32768, 256),\n            (32768, 512),\n            (32768, 1024),\n            (32768, 2048),\n            (32768, 4096),\n            (32768, 8192),\n            (32768, 16384),\n            (32768, 32768),\n            (32768, 65536),\n            (16384, 131072),\n            (8192, 262144),\n        )", "target": "def get_shapes(self) -> tuple[tuple[int, ...], ...]:\n        return (\n            (32768, 256),\n            (32768, 512),\n            (32768, 1024),\n            (32768, 2048),\n            (32768, 4096),\n            (32768, 8192),\n            (32768, 16384),\n            (32768, 32768),\n            (32768, 65536),\n        ) + extra_shapes_for_norm", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000091", "source": "def test_frozen_field():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'name': core_schema.model_field(schema=core_schema.str_schema()),\n                'age': core_schema.model_field(schema=core_schema.int_schema()),\n                'is_developer': core_schema.model_field(\n                    schema=core_schema.with_default_schema(schema=core_schema.bool_schema(), default=True), frozen=True\n                ),\n            }\n        )\n    )\n    r1, model_extra, fields_set = v.validate_python({'name': 'Samuel', 'age': '36'})\n    assert r1 == {'name': 'Samuel', 'age': 36, 'is_developer': True}\n    assert model_extra is None\n    assert fields_set == {'name', 'age'}\n    v.validate_assignment(r1, 'age', '35')\n    assert r1 == {'name': 'Samuel', 'age': 35, 'is_developer': True}\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_assignment(r1, 'is_developer', False)\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'frozen_field', 'loc': ('is_developer',), 'msg': 'Field is frozen', 'input': False}\n    ]", "target": "def predicate_func(v: Any) -> Any:\n            if not func(v):\n                raise PydanticCustomError(\n                    'predicate_failed',\n                    f'Predicate {predicate_name}failed',\n                )\n            return v", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000092", "source": "def full_typename(self) -> str:\n        return self.typename", "target": "def full_typename(self) -> str:\n        return \"cv2.typing.\" + self.typename", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "000093", "source": "def test_custom_invalid_tz():\n    class CustomTz(tzinfo):\n        def tzname(self, _dt):\n            return 'CustomTZ'\n    schema = SchemaValidator(cs.datetime_schema(gt=datetime(2022, 1, 1, 15, 0, 0)))\n    dt = datetime(2022, 1, 1, 16, 0, 0, tzinfo=CustomTz())\n    with pytest.raises(ValidationError) as excinfo:\n        schema.validate_python(dt)\n    if platform.python_implementation() in ('PyPy', 'GraalVM'):\n        error_message = 'NotImplementedError: tzinfo subclass must override utcoffset()'\n    else:\n        error_message = 'NotImplementedError: a tzinfo subclass must implement utcoffset()'\n    assert excinfo.value.errors(include_url=False) == [\n        {\n            'type': 'datetime_object_invalid',\n            'loc': (),\n            'msg': f'Invalid datetime object, got {error_message}',\n            'input': dt,\n            'ctx': {'error': error_message},\n        }\n    ]", "target": "def add_function(self, name: str, arguments: Sequence[FunctionNode.Arg] = (),\n                     return_type: Optional[FunctionNode.RetType] = None) -> FunctionNode:\n        return self._add_child(FunctionNode, name, arguments=arguments,\n                               return_type=return_type)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|scikit-learn/scikit-learn", "data_split": "train", "example_id": "000094", "source": "def predict(self, samples):\n        new_samples = self.unroll_samples(samples)\n        _ret, resp = self.model.predict(new_samples)\n        return resp.ravel().reshape(-1, self.class_n).argmax(1)", "target": "def plot_power_iter_vs_s(power_iter, s, title):\n    plt.figure()\n    for l in sorted(s.keys()):\n        plt.plot(power_iter, s[l], label=l, marker=\"o\")\n    plt.legend(loc=\"lower right\", prop={\"size\": 10})\n    plt.suptitle(title)\n    plt.ylabel(\"norm discrepancy\")\n    plt.xlabel(\"n_iter\")", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "train", "example_id": "000095", "source": "def scaled_compute_loss(self, pred):\n        return reduce_to_scalar_loss(pred) / 1000.0", "target": "def forward(self, x, lengths):\n        lengths = lengths.cpu().int()\n        output_lengths = self.get_seq_lens(lengths)\n        x, _ = self.conv(x, output_lengths)\n        sizes = x.size()\n        x = x.view(\n            sizes[0], sizes[1] * sizes[2], sizes[3]\n        )\n        x = x.transpose(1, 2).transpose(0, 1).contiguous()\n        for rnn in self.rnns:\n            x = rnn(x, output_lengths)\n        if not self.bidirectional:\n            x = self.lookahead(x)\n        x = self.fc(x)\n        x = x.transpose(0, 1)\n        x = self.inference_softmax(x)\n        return x, output_lengths", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000096", "source": "def f(value, serializer, _info):\n        return f'result={serializer(len(value))} repr={serializer!r}'", "target": "def f(value, serializer):\n        if value == 42:\n            return 42\n        return f'result={serializer(value)}'", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000097", "source": "def test_format_when_used_json():\n    s = SchemaSerializer(core_schema.any_schema(serialization=core_schema.format_ser_schema('0.1f', when_used='json')))\n    assert s.to_python(42.12345) == 42.12345\n    assert s.to_python(None) is None\n    assert s.to_python(42.12345, mode='json') == '42.1'\n    assert s.to_json(42.12345) == b'\"42.1\"'\n    with pytest.raises(PydanticSerializationError, match=r'Error calling `format\\(value, \\'0.1f\\'\\)`: TypeError:'):\n        s.to_json(None)", "target": "def test_ser_json_int_subclass_value_larger_than_i64():\n    class IntSubclass(int):\n        pass\n    schema = core_schema.model_schema(\n        MyModel,\n        core_schema.model_fields_schema(\n            dict(\n                stuff=core_schema.model_field(\n                    core_schema.dict_schema(\n                        keys_schema=core_schema.str_schema(),\n                        values_schema=core_schema.any_schema(),\n                    )\n                )\n            )\n        ),\n    )\n    s = SchemaSerializer(schema)\n    assert (\n        s.to_json(\n            MyModel(stuff={'value': IntSubclass(9_223_372_036_854_775_809)}),\n        )\n        == b'{\"stuff\":{\"value\":9223372036854775809}}'\n    )\n    assert str(\n        s.to_python(\n            MyModel(stuff={'value': IntSubclass(9_223_372_036_854_775_809)}),\n            mode='json',\n        )\n    ) == str({'stuff': {'value': 9223372036854775809}})", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|opencv/opencv", "data_split": "train", "example_id": "000098", "source": "def decode_n_tokens(\n    model: torch.nn.Module,\n    cur_token: torch.Tensor,\n    input_pos: torch.Tensor,\n    num_new_tokens: int,\n    **sampling_kwargs,\n):\n    new_tokens, new_probs = [], []\n    for i in range(num_new_tokens):\n        with torch.nn.attention.sdpa_kernel(\n            torch.nn.attention.SDPBackend.MATH\n        ):\n            next_token, next_prob = decode_one_token(\n                model, cur_token, input_pos, **sampling_kwargs\n            )\n            input_pos += 1\n            new_tokens.append(next_token.clone())\n            new_probs.append(next_prob.clone())\n            cur_token = next_token.view(1, -1)\n    return new_tokens, new_probs", "target": "def guess(s, delims):\n        for delim in delims:\n            tmp = s.partition(delim)\n            if len(tmp[1]) != 0:\n                return tmp[0]\n        return None", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000099", "source": "def f(value, handler, _info):\n        return handler(value)", "target": "def test_aliases_path_negative(input_value, expected):\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={'field_a': core_schema.model_field(validation_alias=['foo', -2], schema=core_schema.int_schema())}\n        ),\n        config=CoreConfig(loc_by_alias=False),\n    )\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=expected.message):\n            val = v.validate_python(input_value)\n            print(f'UNEXPECTED OUTPUT: {val!r}')\n    else:\n        output = v.validate_python(input_value)\n        assert output == expected", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|scikit-learn/scikit-learn", "data_split": "train", "example_id": "000100", "source": "def f(input_value):\n        return input_value + ' Changed'", "target": "def fit(self, X, y=None):\n        self.clusterer_ = clone(self.clusterer)\n        self.classifier_ = clone(self.classifier)\n        y = self.clusterer_.fit_predict(X)\n        self.classifier_.fit(X, y)\n        return self", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000101", "source": "def test_dict():\n    v = SchemaValidator(\n        core_schema.dict_schema(keys_schema=core_schema.int_schema(), values_schema=core_schema.int_schema())\n    )\n    assert v.validate_json('{\"1\": 2, \"3\": 4}') == {1: 2, 3: 4}\n    assert json.loads('{\"1\": 1, \"1\": 2}') == {'1': 2}\n    assert v.validate_json('{\"1\": 1, \"1\": 2}') == {1: 2}", "target": "def test_dict(py_and_json: PyAndJson):\n    v = py_and_json({'type': 'dict', 'keys_schema': {'type': 'date'}, 'values_schema': {'type': 'int'}})\n    assert v.validate_test({'2000-01-01': 2, '2000-01-02': 4}) == {date(2000, 1, 1): 2, date(2000, 1, 2): 4}", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "000102", "source": "def is_numeric(dtype):\n    return np.issubdtype(dtype, np.integer) or np.issubdtype(dtype, np.floating)", "target": "def test_json_key_fallback():\n    x = {FoobarHash(): 1}\n    assert to_jsonable_python(x, serialize_unknown=True) == {'Foobar.__str__': 1}\n    assert to_jsonable_python(x, fallback=fallback_func) == {'fallback:FoobarHash': 1}\n    assert to_json(x, serialize_unknown=True) == b'{\"Foobar.__str__\":1}'\n    assert to_json(x, fallback=fallback_func) == b'{\"fallback:FoobarHash\":1}'", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "000103", "source": "def main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--actual\", type=str, required=True)\n    parser.add_argument(\"--expected\", type=str, required=True)\n    args = parser.parse_args()\n    actual = pd.read_csv(args.actual)\n    expected = pd.read_csv(args.expected)\n    failed, msg = check_accuracy(actual, expected, args.expected)\n    if failed:\n        print(msg)\n        sys.exit(1)", "target": "def test_alias():\n    s = SchemaSerializer(\n        core_schema.model_schema(\n            BasicModel,\n            core_schema.model_fields_schema(\n                {\n                    'cat': core_schema.model_field(core_schema.int_schema(), serialization_alias='Meow'),\n                    'dog': core_schema.model_field(core_schema.int_schema(), serialization_alias='Woof'),\n                    'bird': core_schema.model_field(core_schema.int_schema()),\n                }\n            ),\n        )\n    )\n    value = BasicModel(cat=0, dog=1, bird=2)\n    assert s.to_python(value, by_alias=True) == IsStrictDict(Meow=0, Woof=1, bird=2)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000104", "source": "def test_from_attributes(input_value, expected, from_attributes_mode):\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'a': core_schema.model_field(schema=core_schema.int_schema()),\n                'b': core_schema.model_field(schema=core_schema.int_schema()),\n                'c': core_schema.model_field(schema=core_schema.str_schema()),\n            },\n            from_attributes=from_attributes_mode == 'schema',\n        )\n    )\n    kwargs = {}\n    if from_attributes_mode == 'validation':\n        kwargs['from_attributes'] = True\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            val = v.validate_python(input_value, **kwargs)\n            print(f'UNEXPECTED OUTPUT: {val!r}')\n    else:\n        output = v.validate_python(input_value, **kwargs)\n        assert output == expected", "target": "def test_from_attributes():\n    v = SchemaValidator(\n        core_schema.tagged_union_schema(\n            discriminator='foobar',\n            choices={\n                'apple': core_schema.model_fields_schema(\n                    fields={\n                        'a': core_schema.model_field(schema=core_schema.str_schema()),\n                        'b': core_schema.model_field(schema=core_schema.int_schema()),\n                    }\n                ),\n                'banana': core_schema.model_fields_schema(\n                    fields={\n                        'c': core_schema.model_field(schema=core_schema.str_schema()),\n                        'd': core_schema.model_field(schema=core_schema.int_schema()),\n                    }\n                ),\n            },\n        ),\n        config=CoreConfig(from_attributes=True),\n    )\n    assert v.validate_python({'foobar': 'apple', 'a': 'apple', 'b': '13'}) == (\n        {'a': 'apple', 'b': 13},\n        None,\n        {'a', 'b'},\n    )\n    assert v.validate_python(Cls(foobar='apple', a='apple', b='13')) == ({'a': 'apple', 'b': 13}, None, {'a', 'b'})\n    assert v.validate_python({'foobar': 'banana', 'c': 'banana', 'd': '31'}) == (\n        {'c': 'banana', 'd': 31},\n        None,\n        {'c', 'd'},\n    )\n    assert v.validate_python(Cls(foobar='banana', c='banana', d='31')) == ({'c': 'banana', 'd': 31}, None, {'c', 'd'})", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000105", "source": "def forward(self, x):\n        x = self.relu_a(x)\n        x = x + self.sub_mods(x)\n        return x + self.relu_b(x) + self.a", "target": "def forward(self, input):\n        mu, sigma = self.compute_layernorm_stats(input)\n        return (input - mu) / sigma * self.weight + self.bias", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000106", "source": "def train(self, samples, responses):\n        _sample_n, var_n = samples.shape\n        new_samples = self.unroll_samples(samples)\n        new_responses = self.unroll_responses(responses)\n        var_types = np.array([cv.ml.VAR_NUMERICAL] * var_n + [cv.ml.VAR_CATEGORICAL, cv.ml.VAR_CATEGORICAL], np.uint8)\n        self.model.setWeakCount(15)\n        self.model.setMaxDepth(10)\n        self.model.train(cv.ml.TrainData_create(new_samples, cv.ml.ROW_SAMPLE, new_responses.astype(int), varType = var_types))", "target": "def train(self, samples, responses):\n        self.model.setType(cv.ml.SVM_C_SVC)\n        self.model.setC(1)\n        self.model.setKernel(cv.ml.SVM_RBF)\n        self.model.setGamma(.1)\n        self.model.train(samples, cv.ml.ROW_SAMPLE, responses.astype(int))", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000107", "source": "def dynamic_rnn(\n        sequences: list[Tensor],\n        hiddens: tuple[Tensor, Tensor],\n        w_ih: Tensor,\n        w_hh: Tensor,\n        b_ih: Tensor,\n        b_hh: Tensor,\n    ) -> tuple[list[Tensor], tuple[list[Tensor], list[Tensor]]]:\n        hx, cx = hiddens\n        hxs = hx.unbind(1)\n        cxs = cx.unbind(1)\n        outputs = []\n        hx_outs = []\n        cx_outs = []\n        for batch in range(len(sequences)):\n            output = []\n            hy, cy = hxs[batch], cxs[batch]\n            inputs = sequences[batch].unbind(0)\n            for seq_idx in range(len(inputs)):\n                hy, cy = cell(\n                    inputs[seq_idx].unsqueeze(0), (hy, cy), w_ih, w_hh, b_ih, b_hh\n                )\n                output += [hy]\n            outputs += [torch.stack(output)]\n            hx_outs += [hy.unsqueeze(0)]\n            cx_outs += [cy.unsqueeze(0)]\n        return outputs, (hx_outs, cx_outs)", "target": "def dynamic_rnn(\n        input: Tensor, hidden: tuple[Tensor, Tensor], params: list[Tensor]\n    ) -> tuple[Tensor, tuple[Tensor, Tensor]]:\n        params_stride = 4\n        hx, cx = hidden\n        hy, cy = hidden\n        inputs, outputs = input.unbind(0), []\n        for layer in range(hx.size(0)):\n            hy = hx[layer]\n            cy = cx[layer]\n            base_idx = layer * params_stride\n            w_ih = params[base_idx]\n            w_hh = params[base_idx + 1]\n            b_ih = params[base_idx + 2]\n            b_hh = params[base_idx + 3]\n            for seq_idx in range(len(inputs)):\n                hy, cy = cell(inputs[seq_idx], (hy, cy), w_ih, w_hh, b_ih, b_hh)\n                outputs += [hy]\n            inputs, outputs = outputs, []\n        return torch.stack(inputs), (hy.unsqueeze(0), cy.unsqueeze(0))", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "000108", "source": "def make_data(self, params):\n        representation, n_jobs = params\n        if representation == \"sparse\":\n            data = _20newsgroups_highdim_dataset()\n        else:\n            data = _20newsgroups_lowdim_dataset()\n        return data", "target": "def make_data(self, params):\n        (representation,) = params\n        if representation == \"sparse\":\n            data = _20newsgroups_highdim_dataset()\n        else:\n            data = _20newsgroups_lowdim_dataset()\n        return data", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000109", "source": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "target": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "000110", "source": "def find_multiple(n: int, k: int) -> int:\n    if n % k == 0:\n        return n\n    return n + k - (n % k)", "target": "def validate_as(self, tp: type[_NewOutT], *, strict: bool = ...) -> _Pipeline[_InT, _NewOutT]: ...", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pytorch/pytorch", "data_split": "train", "example_id": "000111", "source": "def scan_namespace_functions(ns, ns_name, files_dict):\n    functions = ns.findall(\"./member[@kind='function']\")\n    for f in functions:\n        f_name = f.find(\"./name\").text\n        name = ns_name + '::' + f_name\n        file = f.find(\"./anchorfile\").text\n        anchor = f.find(\"./anchor\").text\n        add_to_file(files_dict, file, Symbol(anchor, \"fn\", name))", "target": "def replace_tag(filename) -> None:\n    with open(filename) as f:\n        lines = f.readlines()\n    for i, line in enumerate(lines):\n        if line.startswith(\"Tag:\"):\n            lines[i] = line.replace(\"-linux_\", \"-manylinux_2_28_\")\n            print(f\"Updated tag from {line} to {lines[i]}\")\n            break\n    with open(filename, \"w\") as f:\n        f.writelines(lines)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000112", "source": "def test_smart_union_json_string_types(schema: core_schema.CoreSchema, input_value: str, expected_value: Any):\n    validator = SchemaValidator(core_schema.union_schema([schema, core_schema.str_schema()]))\n    assert validator.validate_json(f'\"{input_value}\"') == expected_value\n    assert validator.validate_python(input_value) == input_value", "target": "def test_cache_strings():\n    v = SchemaValidator(cs.str_schema())\n    assert 'cache_strings=True' in plain_repr(v)\n    v = SchemaValidator(cs.str_schema(), config=CoreConfig(cache_strings=True))\n    assert 'cache_strings=True' in plain_repr(v)\n    v = SchemaValidator(cs.str_schema(), config=CoreConfig(cache_strings=False))\n    assert 'cache_strings=False' in plain_repr(v)\n    v = SchemaValidator(cs.str_schema(), config=CoreConfig(cache_strings='keys'))\n    assert \"cache_strings='keys'\" in plain_repr(v)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pytorch/pytorch", "data_split": "train", "example_id": "000113", "source": "def bool_(cls, ctype_name: Optional[str] = None,\n              required_modules: Tuple[str, ...] = ()):\n        if ctype_name is None:\n            ctype_name = \"bool\"\n        return PrimitiveTypeNode(ctype_name, typename=\"bool\", required_modules=required_modules)", "target": "def test_get_env_set_returns_value(self):\n        with patch.dict(os.environ, {\"FOO\": \"bar\"}, clear=True):\n            self.assertEqual(m.get_env(\"FOO\", \"default\"), \"bar\")", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|pydantic/pydantic", "data_split": "train", "example_id": "000114", "source": "def load_data(dtype=np.float32, order=\"C\", random_state=13):\n    print(\"Loading dataset...\")\n    data = fetch_covtype(\n        download_if_missing=True, shuffle=True, random_state=random_state\n    )\n    X = check_array(data[\"data\"], dtype=dtype, order=order)\n    y = (data[\"target\"] != 1).astype(int)\n    print(\"Creating train-test split...\")\n    n_train = 522911\n    X_train = X[:n_train]\n    y_train = y[:n_train]\n    X_test = X[n_train:]\n    y_test = y[n_train:]\n    mean = X_train.mean(axis=0)\n    std = X_train.std(axis=0)\n    mean[10:] = 0.0\n    std[10:] = 1.0\n    X_train = (X_train - mean) / std\n    X_test = (X_test - mean) / std\n    return X_train, X_test, y_train, y_test", "target": "def test_int_discriminator_function(py_and_json: PyAndJson, input_value, expected):\n    def discriminator_function(obj):\n        if isinstance(obj, str):\n            return 'a'\n        elif isinstance(obj, int):\n            return 1\n        elif obj is None:\n            return None\n        else:\n            return 'other'\n    v = py_and_json(\n        {\n            'type': 'tagged-union',\n            'discriminator': discriminator_function,\n            'choices': {'a': {'type': 'str'}, 1: {'type': 'int'}},\n        }\n    )\n    assert 'discriminator: Function' in repr(v.validator)\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=expected.message) as exc_info:\n            v.validate_python(input_value)\n        assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        assert v.validate_test(input_value) == expected", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000115", "source": "def dataclass(\n        _cls: type[_T],\n        *,\n        init: Literal[False] = False,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool | None = None,\n        config: ConfigDict | type[object] | None = None,\n        validate_on_init: bool | None = None,\n        kw_only: bool = ...,\n        slots: bool = ...,\n    ) -> type[PydanticDataclass]: ...", "target": "def dataclass(\n        *,\n        init: Literal[False] = False,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool | None = None,\n        config: ConfigDict | type[object] | None = None,\n        validate_on_init: bool | None = None,\n    ) -> Callable[[type[_T]], type[PydanticDataclass]]:\n        ...", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000116", "source": "def area(self) -> int:\n            return self.width * self.height", "target": "def area(self) -> int:\n            raise ValueError('xxx')", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "000117", "source": "def make_estimator(self, params):\n        representation, solver = params\n        estimator = Ridge(solver=solver, fit_intercept=False, random_state=0)\n        return estimator", "target": "def make_estimator(self, params):\n        (representation,) = params\n        max_iter = 60 if representation == \"dense\" else 300\n        estimator = SGDRegressor(max_iter=max_iter, tol=None, random_state=0)\n        return estimator", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000118", "source": "def test_dict_py():\n    v = SchemaValidator(cs.dict_schema(keys_schema=cs.datetime_schema(), values_schema=cs.int_schema()))\n    assert v.validate_python({datetime(2000, 1, 1): 2, datetime(2000, 1, 2): 4}) == {\n        datetime(2000, 1, 1): 2,\n        datetime(2000, 1, 2): 4,\n    }", "target": "def test_dict_py():\n    v = SchemaValidator(\n        core_schema.dict_schema(keys_schema=core_schema.time_schema(), values_schema=core_schema.int_schema())\n    )\n    assert v.validate_python({time(12, 1, 1): 2, time(12, 1, 2): 4}) == {time(12, 1, 1): 2, time(12, 1, 2): 4}", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000119", "source": "def required_usage_imports(self) -> Generator[str, None, None]:\n        yield \"import numpy\"\n        yield \"import typing as _typing\"", "target": "def required_usage_imports(self) -> Generator[str, None, None]:\n        yield \"import os\"", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|scikit-learn/scikit-learn", "data_split": "train", "example_id": "000120", "source": "def test_core_json(self, core_validator: SchemaValidator, core_serializer: SchemaSerializer, benchmark):\n        m = core_validator.validate_python(self.data)\n        assert json.loads(core_serializer.to_json(m)) == self.data\n        benchmark(core_serializer.to_json, m)", "target": "def make_data(self, params):\n        (representation,) = params\n        if representation == \"sparse\":\n            data = _20newsgroups_highdim_dataset()\n        else:\n            data = _20newsgroups_lowdim_dataset()\n        return data", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "000121", "source": "def test_kwargs_typed_dict(py_and_json: PyAndJson, input_value, expected):\n    v = py_and_json(\n        {\n            'type': 'arguments',\n            'arguments_schema': [],\n            'var_kwargs_mode': 'unpacked-typed-dict',\n            'var_kwargs_schema': {\n                'type': 'typed-dict',\n                'fields': {\n                    'x': {\n                        'type': 'typed-dict-field',\n                        'schema': {'type': 'int', 'strict': True},\n                        'required': True,\n                    },\n                    'y': {\n                        'type': 'typed-dict-field',\n                        'schema': {'type': 'str'},\n                        'required': False,\n                        'validation_alias': 'z',\n                    },\n                },\n                'config': {'extra_fields_behavior': 'forbid'},\n            },\n        }\n    )\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            v.validate_test(input_value)\n    else:\n        assert v.validate_test(input_value) == expected", "target": "def normalize_ctype_name(typename: str) -> str:\n    for prefix_to_remove in (\"cv\", \"std\"):\n        if typename.startswith(prefix_to_remove):\n            typename = typename[len(prefix_to_remove):]\n    typename = typename.replace(\"::\", \"_\").lstrip(\"_\")\n    if typename.endswith('&'):\n        typename = typename[:-1]\n    typename = typename.strip()\n    if typename == 'void*':\n        return typename\n    if is_pointer_type(typename):\n        for suffix in (\"*\", \"_Ptr\", \"Ptr\"):\n            if typename.endswith(suffix):\n                return typename[:-len(suffix)]\n        if _is_template_instantiation(typename):\n            return normalize_ctype_name(\n                get_template_instantiation_type(typename)\n            )\n        return typename.split(\"_\", maxsplit=1)[-1]\n    if typename.startswith(\"GArray_\") or typename.startswith(\"GArray<\"):\n        return \"GArrayT\"\n    if typename.startswith(\"GOpaque_\") or typename.startswith(\"GOpaque<\"):\n        return \"GOpaqueT\"\n    if typename == \"GStreamerPipeline\" or typename.startswith(\"GStreamerSource\"):\n        return \"gst_\" + typename\n    return typename", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000122", "source": "def area(self) -> int:\n            return self.width * self.height", "target": "def area(self) -> None:\n            self.side = 0.0", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "000123", "source": "def secs_to_us(time_s):\n    return time_s * 1e6", "target": "def test_function_types():\n    @validate()\n    def foobar(a: int, b: int, *, c: int):\n        return a, b, c\n    assert foobar(1, 2, c='3') == (1, 2, 3)\n    assert foobar(a=1, b='2', c=3) == (1, 2, 3)\n    with pytest.raises(ValidationError, match='Unexpected positional argument'):\n        foobar(1, 2, 3)\n    with pytest.raises(ValidationError) as exc_info:\n        foobar(1, 'b')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': (1,),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'b',\n        },\n        {\n            'type': 'missing_keyword_only_argument',\n            'loc': ('c',),\n            'msg': 'Missing required keyword only argument',\n            'input': ArgsKwargs((1, 'b')),\n        },\n    ]\n    with pytest.raises(ValidationError) as exc_info:\n        foobar(1, 'b', c='c')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': (1,),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'b',\n        },\n        {\n            'type': 'int_parsing',\n            'loc': ('c',),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'c',\n        },\n    ]", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000124", "source": "def test_slots_mixed():\n    @dataclasses.dataclass(slots=True)\n    class Model:\n        x: int\n        y: dataclasses.InitVar[str]\n        z: ClassVar[str] = 'z-classvar'\n    @dataclasses.dataclass\n    class SubModel(Model):\n        x2: int\n        y2: dataclasses.InitVar[str]\n        z2: ClassVar[str] = 'z2-classvar'\n    schema = core_schema.dataclass_schema(\n        SubModel,\n        core_schema.dataclass_args_schema(\n            'SubModel',\n            [\n                core_schema.dataclass_field(name='x', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(name='y', init_only=True, schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='x2', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(name='y2', init_only=True, schema=core_schema.str_schema()),\n            ],\n        ),\n        ['x'],\n        slots=True,\n    )\n    v = SchemaValidator(schema)\n    dc = v.validate_python({'x': 1, 'y': 'a', 'x2': 2, 'y2': 'b'})\n    assert dc.x == 1\n    assert dc.x2 == 2\n    assert dataclasses.asdict(dc) == {'x': 1, 'x2': 2}", "target": "def test_extra_ignore(py_and_json: PyAndJson, input_value) -> None:\n    v = py_and_json(\n        cs.arguments_v3_schema(\n            [\n                cs.arguments_v3_parameter(name='a', schema=cs.int_schema(), mode='keyword_only'),\n                cs.arguments_v3_parameter(name='b', schema=cs.int_schema(), alias='c', mode='keyword_only'),\n            ],\n            extra_behavior='ignore',\n        ),\n    )\n    assert v.validate_test(input_value) == ((), {'a': 1, 'b': 3})", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000125", "source": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "target": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000126", "source": "def f(a, b):\n            z = torch.mm(a, b)\n            for i in range(200):\n                z = torch.mm(z, b)\n            return z", "target": "def f(a, b):\n            xs = b.tolist()\n            for x in xs:\n                torch._check(x >= 0)\n                torch._check(x <= self.N)\n            return a.split(xs)", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000127", "source": "def required_definition_imports(self) -> Generator[str, None, None]:\n        for item in self:\n            yield from item.required_definition_imports", "target": "def required_definition_imports(self) -> Generator[str, None, None]:\n        yield \"import typing as _typing\"\n        yield from super().required_definition_imports", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000128", "source": "def resolve_type_nodes(self, root: ASTNode):\n        def has_unresolved_type_node(item) -> bool:\n            return item.type_node is not None and not item.type_node.is_resolved\n        errors = []\n        for overload in self.overloads:\n            for arg in filter(has_unresolved_type_node, overload.arguments):\n                try:\n                    arg.type_node.resolve(root)\n                except TypeResolutionError as e:\n                    errors.append(\n                        'Failed to resolve \"{}\" argument: {}'.format(arg.name, e)\n                    )\n            if overload.return_type is not None and \\\n                    has_unresolved_type_node(overload.return_type):\n                try:\n                    overload.return_type.type_node.resolve(root)\n                except TypeResolutionError as e:\n                    errors.append('Failed to resolve return type: {}'.format(e))\n        if len(errors) > 0:\n            raise TypeResolutionError(\n                'Failed to resolve \"{}\" function against \"{}\". Errors: {}'.format(\n                    self.full_export_name, root.full_export_name,\n                    \", \".join(\"[{}]: {}\".format(i, e) for i, e in enumerate(errors))\n                )\n            )", "target": "def resolve_type_nodes(self, root: Optional[ASTNode] = None) -> None:\n        errors = []\n        for child in itertools.chain(self.functions.values(),\n                                     self.classes.values(),\n                                     self.namespaces.values()):\n            try:\n                try:\n                    child.resolve_type_nodes(self)\n                except TypeResolutionError:\n                    if root is not None:\n                        child.resolve_type_nodes(root)\n                    else:\n                        raise\n            except TypeResolutionError as e:\n                errors.append(str(e))\n        if len(errors) > 0:\n            raise TypeResolutionError(\n                'Failed to resolve \"{}\" namespace against \"{}\". '\n                'Errors: {}'.format(\n                    self.full_export_name,\n                    root if root is None else root.full_export_name,\n                    errors\n                )\n            )", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000129", "source": "def test_function_plain_field_serializer_to_json_no_info():\n    class Model(TypedDict):\n        x: int\n    def ser_x(data: Model, v: Any) -> str:\n        assert data['x'] == 1_000\n        return f'{v:_}'\n    s = SchemaSerializer(\n        core_schema.typed_dict_schema(\n            {\n                'x': core_schema.typed_dict_field(\n                    core_schema.int_schema(\n                        serialization=core_schema.plain_serializer_function_ser_schema(ser_x, is_field_serializer=True)\n                    )\n                )\n            }\n        )\n    )\n    assert json.loads(s.to_json(Model(x=1000))) == {'x': '1_000'}", "target": "def test_function_before_error():\n    def my_function(input_value, info):\n        return input_value + 'x'\n    v = SchemaValidator(\n        {\n            'type': 'function-before',\n            'function': {'type': 'with-info', 'function': my_function},\n            'schema': cs.str_schema(max_length=5),\n        }\n    )\n    assert v.validate_python('1234') == '1234x'\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python('12345')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'string_too_long',\n            'loc': (),\n            'msg': 'String should have at most 5 characters',\n            'input': '12345x',\n            'ctx': {'max_length': 5},\n        }\n    ]\n    assert repr(exc_info.value).startswith('1 validation error for function-before[my_function(), constrained-str]\\n')", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "train", "example_id": "000130", "source": "def run_experiment(\n    x: GPTModelConfig,\n    num_samples: int = 5,\n    max_new_tokens: int = 200,\n    top_k: int = 200,\n    temperature: float = 0.8,\n    device: str = \"cuda\",\n) -> None:\n    print(f\"Loading model {x.name}\")\n    t0 = time.time()\n    model = _load_model(x, device=device)\n    device_sync(device=device)\n    print(f\"Time to load model: {time.time() - t0:.02f} seconds\")\n    prompt = torch.tensor(\n        [1, 15043, 29892, 590, 1024, 338], device=device, dtype=torch.int32\n    )\n    prompt_length = prompt.size(0)\n    torch.manual_seed(1234)\n    model_size = _get_model_size(model)\n    aggregate_metrics = {\"tokens_per_sec\": [], \"memory_bandwidth\": []}\n    start = -1\n    compilation_time = None\n    if x.mode == \"autoquant\":\n        print(\"Using autoquant\")\n        model = torchao.autoquant(model, manual=True, error_on_unseen=False)\n        generate(model, prompt, max_new_tokens, temperature=temperature, top_k=top_k)\n        model.finalize_autoquant()\n    if x.mode == \"autoquant_v2\":\n        print(\"Using autoquant_v2\")\n        from torchao.prototype.quantization.autoquant_v2 import autoquant_v2\n        p = prompt.view(1, -1)\n        T = prompt.size(0)\n        T_new = T + max_new_tokens\n        max_seq_length = min(T_new, model.config.block_size)\n        input_pos = torch.arange(0, T, device=device)\n        example_input = (p, input_pos)\n        with torch.device(device):\n            model.setup_caches(max_batch_size=1, max_seq_length=max_seq_length)\n        model = autoquant_v2(\n            model,\n            manual=True,\n            error_on_unseen=False,\n            example_input=example_input,\n            batch_size=x.batch_size,\n        )\n        torch.compiler.cudagraph_mark_step_begin()\n        generate(model, prompt, max_new_tokens, temperature=temperature, top_k=top_k)\n        model.finalize_autoquant()\n    global decode_one_token, prefill, compiled\n    if not compiled:\n        compiled = True\n        decode_one_token = torch.compile(\n            decode_one_token, mode=\"reduce-overhead\", fullgraph=True\n        )\n        prefill = torch.compile(prefill, fullgraph=True)\n    for i in range(start, num_samples):\n        device_sync(device=device)\n        torch.compiler.cudagraph_mark_step_begin()\n        t0 = time.perf_counter()\n        y = generate(\n            model, prompt, max_new_tokens, temperature=temperature, top_k=top_k\n        )\n        if i == -1:\n            compilation_time = time.perf_counter() - t0\n            print(f\"Compilation time: {compilation_time:.2f} seconds\")\n            continue\n        device_sync(device=device)\n        t = time.perf_counter() - t0\n        tokens_generated = y.size(0) - prompt_length\n        tokens_sec = tokens_generated / t\n        aggregate_metrics[\"tokens_per_sec\"].append(tokens_sec)\n        aggregate_metrics[\"memory_bandwidth\"].append(model_size * tokens_sec / 1e9)\n    token_per_sec = torch.mean(torch.tensor(aggregate_metrics[\"tokens_per_sec\"])).item()\n    memory_bandwidth = torch.mean(\n        torch.tensor(aggregate_metrics[\"memory_bandwidth\"])\n    ).item()\n    print(f\"Average tokens/sec: {token_per_sec:.2f} tokens/sec\")\n    print(f\"Average bandwidth achieved: {memory_bandwidth:.02f} GB/s\")\n    print(f\"Memory used: {torch.cuda.max_memory_reserved() / 1e9:.02f} GB\")\n    return token_per_sec, memory_bandwidth, compilation_time", "target": "def test_changes_and_restores(self):\n        start = Path.cwd()\n        with tempfile.TemporaryDirectory() as td:\n            target = Path(td) / \"wd\"\n            target.mkdir()\n            with working_directory(str(target)):\n                self.assertEqual(Path.cwd().resolve(), target.resolve())\n        self.assertEqual(Path.cwd(), start)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000131", "source": "def test_complete_core_serializer_to_json(benchmark):\n    core_schema = schema()\n    v = SchemaValidator(core_schema)\n    model = v.validate_python(input_data_lax())\n    serializer = SchemaSerializer(core_schema)\n    benchmark(serializer.to_json, model)", "target": "def test_timedelta_key():\n    v = SchemaSerializer(core_schema.dict_schema(core_schema.timedelta_schema(), core_schema.int_schema()))\n    assert v.to_python({timedelta(days=2, hours=3, minutes=4): 1}) == {timedelta(days=2, hours=3, minutes=4): 1}\n    assert v.to_python({timedelta(days=2, hours=3, minutes=4): 1}, mode='json') == {'P2DT3H4M': 1}\n    assert v.to_json({timedelta(days=2, hours=3, minutes=4): 1}) == b'{\"P2DT3H4M\":1}'", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000132", "source": "def model_serializer(f: _ModelPlainSerializerT, /) -> _ModelPlainSerializerT: ...", "target": "def test_alias_allow_pop(py_and_json: PyAndJson):\n    v = py_and_json(\n        {\n            'type': 'typed-dict',\n            'fields': {\n                'field_a': {'validation_alias': 'FieldA', 'type': 'typed-dict-field', 'schema': {'type': 'int'}}\n            },\n            'config': {'validate_by_name': True, 'validate_by_alias': True},\n        },\n    )\n    assert v.validate_test({'FieldA': '123'}) == {'field_a': 123}\n    assert v.validate_test({'field_a': '123'}) == {'field_a': 123}\n    assert v.validate_test({'FieldA': '1', 'field_a': '2'}) == {'field_a': 1}\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'foobar': '123'})", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000133", "source": "def skip_accuracy_check_as_eager_non_deterministic(self):\n        if self.args.accuracy and self.args.training:\n            return SKIP_ACCURACY_CHECK_AS_EAGER_NON_DETERMINISTIC_MODELS\n        return set()", "target": "def skip_accuracy_check_as_eager_non_deterministic(self):\n        if self.args.accuracy and self.args.training:\n            return self._accuracy[\"skip\"][\"eager_not_deterministic\"]\n        return set()", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "000134", "source": "def _prepare_once(self):\n        class M(torch.nn.Module):\n            def forward(self, x):\n                total = sum(t.item() for t in x)\n                return total // 2\n        self.m = M()\n        self.input = [torch.tensor(i + 2) for i in range(self.N)]", "target": "def test_timedelta_kwargs(kwargs: dict[str, Any], input_value, expected):\n    v = SchemaValidator(core_schema.timedelta_schema(**kwargs))\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            v.validate_python(input_value)\n    else:\n        output = v.validate_python(input_value)\n        assert output == expected", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|opencv/opencv", "data_split": "train", "example_id": "000135", "source": "def layernorm_ref(self, x: torch.Tensor, w: torch.Tensor, eps: float = 1e-6):\n        x_f32 = x.float()\n        return F.layer_norm(x_f32, w.shape, w, None, eps).to(x.dtype)", "target": "def check_dir(d, create=False, clean=False):\n    d = os.path.abspath(d)\n    log.info(\"Check dir %s (create: %s, clean: %s)\", d, create, clean)\n    if os.path.exists(d):\n        if not os.path.isdir(d):\n            raise Fail(\"Not a directory: %s\" % d)\n        if clean:\n            for x in glob.glob(os.path.join(d, \"*\")):\n                rm_one(x)\n    else:\n        if create:\n            os.makedirs(d)\n    return d", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000136", "source": "def get_memory_bytes(self, args, kwargs) -> int:\n        x, target, dloss = args\n        M, N = x.shape\n        return (\n            2 * M * N * x.dtype.itemsize\n            + M * target.dtype.itemsize\n            + M * dloss.dtype.itemsize\n        )", "target": "def get_memory_bytes(self, args, kwargs) -> int:\n        x, w = args\n        M, N = x.shape\n        return 2 * M * N * x.dtype.itemsize + N * w.dtype.itemsize", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|opencv/opencv", "data_split": "train", "example_id": "000137", "source": "def make_estimator(self, params):\n        (n_jobs,) = params\n        clf = RandomForestClassifier(random_state=0)\n        if Benchmark.data_size == \"large\":\n            n_estimators_list = [10, 25, 50, 100, 500]\n            max_depth_list = [5, 10, None]\n            max_features_list = [0.1, 0.4, 0.8, 1.0]\n        else:\n            n_estimators_list = [10, 25, 50]\n            max_depth_list = [5, 10]\n            max_features_list = [0.1, 0.4, 0.8]\n        param_grid = {\n            \"n_estimators\": n_estimators_list,\n            \"max_depth\": max_depth_list,\n            \"max_features\": max_features_list,\n        }\n        estimator = GridSearchCV(clf, param_grid, n_jobs=n_jobs, cv=4)\n        return estimator", "target": "def _create_marker_bits(markerSize_bits, byteList):\n        marker = np.zeros((markerSize_bits+2, markerSize_bits+2))\n        bits = marker[1:markerSize_bits+1, 1:markerSize_bits+1]\n        for i in range(markerSize_bits):\n            for j in range(markerSize_bits):\n                bits[i][j] = int(byteList[i*markerSize_bits+j])\n        return marker", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "000138", "source": "def test_model_custom_init_nested():\n    calls = []\n    class ModelInner:\n        __slots__ = '__dict__', '__pydantic_fields_set__', '__pydantic_extra__', '__pydantic_private__'\n        a: int\n        b: int\n        def __init__(self, **data):\n            calls.append(f'inner: {data!r}')\n            self.__pydantic_validator__.validate_python(data, self_instance=self)\n    inner_schema = core_schema.model_schema(\n        ModelInner,\n        core_schema.model_fields_schema(\n            {\n                'a': core_schema.model_field(core_schema.with_default_schema(core_schema.int_schema(), default=1)),\n                'b': core_schema.model_field(core_schema.int_schema()),\n            }\n        ),\n        custom_init=True,\n    )\n    ModelInner.__pydantic_validator__ = SchemaValidator(inner_schema)\n    class ModelOuter:\n        __slots__ = '__dict__', '__pydantic_fields_set__'\n        a: int\n        b: ModelInner\n        def __init__(self, **data):\n            calls.append(f'outer: {data!r}')\n            self.__pydantic_validator__.validate_python(data, self_instance=self)\n    ModelOuter.__pydantic_validator__ = SchemaValidator(\n        core_schema.model_schema(\n            ModelOuter,\n            core_schema.model_fields_schema(\n                {\n                    'a': core_schema.model_field(core_schema.with_default_schema(core_schema.int_schema(), default=1)),\n                    'b': core_schema.model_field(inner_schema),\n                }\n            ),\n            custom_init=True,\n        )\n    )\n    m = ModelOuter(a=2, b={'b': 3})\n    assert m.__pydantic_fields_set__ == {'a', 'b'}\n    assert m.a == 2\n    assert isinstance(m.b, ModelInner)\n    assert m.b.a == 1\n    assert m.b.b == 3\n    assert calls == [\"outer: {'a': 2, 'b': {'b': 3}}\", \"inner: {'b': 3}\"]", "target": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000139", "source": "def test_union_float(py_and_json: PyAndJson):\n    v = py_and_json(\n        {'type': 'union', 'choices': [{'type': 'float', 'strict': True}, {'type': 'float', 'multiple_of': 7}]}\n    )\n    assert v.validate_test('14') == 14\n    assert v.validate_test(5) == 5\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_test('5')\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'float_type', 'loc': ('float',), 'msg': 'Input should be a valid number', 'input': '5'},\n        {\n            'type': 'multiple_of',\n            'loc': ('constrained-float',),\n            'msg': 'Input should be a multiple of 7',\n            'input': '5',\n            'ctx': {'multiple_of': 7.0},\n        },\n    ]", "target": "def test_timedelta_json(input_value, expected):\n    v = SchemaValidator(core_schema.timedelta_schema())\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            v.validate_json(input_value)\n    else:\n        output = v.validate_json(input_value)\n        assert output == expected", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "000140", "source": "def with_nnc():\n    torch._C._jit_override_can_fuse_on_cpu(True)\n    torch._C._jit_override_can_fuse_on_gpu(True)\n    torch._C._jit_set_texpr_fuser_enabled(True)\n    torch._C._jit_set_nvfuser_enabled(False)\n    torch._C._jit_set_profiling_executor(True)\n    torch._C._jit_set_profiling_mode(True)", "target": "def test_extra_config_nested_model():\n    class OuterModel:\n        pass\n    class InnerModel:\n        pass\n    schema = core_schema.model_schema(\n        OuterModel,\n        core_schema.model_fields_schema(\n            {\n                'sub_model': core_schema.model_field(\n                    core_schema.model_schema(\n                        InnerModel,\n                        core_schema.model_fields_schema({'int': core_schema.model_field(core_schema.int_schema())}),\n                        config=core_schema.CoreConfig(extra_fields_behavior='allow'),\n                    )\n                )\n            }\n        ),\n        config={},\n    )\n    s = SchemaSerializer(schema)\n    s_repr = plain_repr(s)\n    assert 'has_extra:true,root_model:false,name:\"InnerModel\"' in s_repr\n    assert 'has_extra:false,root_model:false,name:\"OuterModel\"' in s_repr", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000141", "source": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "target": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "train", "example_id": "000142", "source": "def check_accuracy(actual_csv, expected_csv, expected_filename):\n    failed = []\n    improved = []\n    if \"rocm\" in expected_filename:\n        flaky_models.update(\n            {\n                \"Background_Matting\",\n                \"alexnet\",\n                \"demucs\",\n                \"densenet121\",\n                \"detectron2_fcos_r_50_fpn\",\n                \"doctr_det_predictor\",\n                \"doctr_reco_predictor\",\n                \"dpn107\",\n                \"fbnetv3_b\",\n                \"levit_128\",\n                \"llava\",\n                \"microbench_unbacked_tolist_sum\",\n                \"mnasnet1_0\",\n                \"mobilenet_v2\",\n                \"pytorch_CycleGAN_and_pix2pix\",\n                \"pytorch_stargan\",\n                \"repvgg_a2\",\n                \"resnet152\",\n                \"resnet18\",\n                \"resnet50\",\n                \"resnext50_32x4d\",\n                \"sam\",\n                \"sam_fast\",\n                \"shufflenet_v2_x1_0\",\n                \"squeezenet1_1\",\n                \"stable_diffusion_text_encoder\",\n                \"stable_diffusion_unet\",\n                \"swsl_resnext101_32x16d\",\n                \"torchrec_dlrm\",\n                \"vgg16\",\n                \"BERT_pytorch\",\n                \"coat_lite_mini\",\n                \"mobilenet_v3_large\",\n                \"vision_maskrcnn\",\n                \"meta-llama/Llama-3.2-1B\",\n                \"google/gemma-2-2b\",\n                \"google/gemma-3-4b-it\",\n                \"openai/whisper-tiny\",\n                \"Qwen/Qwen3-0.6B\",\n                \"mistralai/Mistral-7B-Instruct-v0.3\",\n                \"openai/gpt-oss-20b\",\n            }\n        )\n    for model in actual_csv[\"name\"]:\n        accuracy = get_field(actual_csv, model, \"accuracy\")\n        expected_accuracy = get_field(expected_csv, model, \"accuracy\")\n        if accuracy == expected_accuracy:\n            status = \"PASS\" if expected_accuracy == \"pass\" else \"XFAIL\"\n            print(f\"{model:34}  {status}\")\n            continue\n        elif model in flaky_models:\n            if accuracy == \"pass\":\n                status = \"PASS_BUT_FLAKY:\"\n            else:\n                status = \"FAIL_BUT_FLAKY:\"\n        elif accuracy != \"pass\":\n            status = \"FAIL:\"\n            failed.append(model)\n        else:\n            status = \"IMPROVED:\"\n            improved.append(model)\n        print(\n            f\"{model:34}  {status:9} accuracy={accuracy}, expected={expected_accuracy}\"\n        )\n    msg = \"\"\n    if failed or improved:\n        if failed:\n            msg += textwrap.dedent(\n                f\n            )\n        if improved:\n            msg += textwrap.dedent(\n                f\n            )\n        sha = os.getenv(\"SHA1\", \"{your CI commit sha}\")\n        msg += textwrap.dedent(\n            f\n        )\n    return failed or improved, msg", "target": "def name(self):\n        prefix = f\"{self.category()}_{self.mode()}\"\n        if self._subclass:\n            prefix += \"_subclass\"\n        else:\n            prefix += \"_nosubclass\"\n        if self.device() == \"cpu\":\n            prefix += \"_cpu\"\n        return prefix", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000143", "source": "def double(value, info):\n        nonlocal f_info\n        f_info = vars(info)\n        return value * 2", "target": "def double(value, _):\n        return value * 2", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pytorch/pytorch", "data_split": "train", "example_id": "000144", "source": "def _configure_acc_eval(self, log_path):\n        self._accuracy_evaluator = ClsAccEvaluation(\n            log_path,\n            self.test_config.img_cls_file,\n            self.test_config.batch_size\n        )", "target": "def get_git_remote_name() -> str:\n    return os.getenv(\"GIT_REMOTE_NAME\", \"origin\")", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "000145", "source": "def test_slots_dataclass_subclass(revalidate_instances, input_value, expected):\n    schema = core_schema.dataclass_schema(\n        FooDataclassSlots,\n        core_schema.dataclass_args_schema(\n            'FooDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n            ],\n            extra_behavior='forbid',\n        ),\n        ['a', 'b'],\n        revalidate_instances=revalidate_instances,\n        slots=True,\n    )\n    v = SchemaValidator(schema)\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=expected.message) as exc_info:\n            print(v.validate_python(input_value))\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        dc = v.validate_python(input_value)\n        assert dataclasses.is_dataclass(dc)\n        assert dataclasses.asdict(dc) == expected", "target": "def add_target(self, image, rect, data=None):\n        x0, y0, x1, y1 = rect\n        raw_points, raw_descrs = self.detect_features(image)\n        points, descs = [], []\n        for kp, desc in zip(raw_points, raw_descrs):\n            x, y = kp.pt\n            if x0 <= x <= x1 and y0 <= y <= y1:\n                points.append(kp)\n                descs.append(desc)\n        descs = np.uint8(descs)\n        self.matcher.add([descs])\n        target = PlanarTarget(image = image, rect=rect, keypoints = points, descrs=descs, data=data)\n        self.targets.append(target)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000146", "source": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "target": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "train", "example_id": "000147", "source": "def test_local_image_exists_uses_lazy_singleton(self):\n        with mock.patch(\n            \"cli.lib.common.docker_helper.docker.from_env\"\n        ) as mock_from_env:\n            mock_docker_client = MagicMock()\n            mock_from_env.return_value = mock_docker_client\n            c1 = _get_client()\n            self.assertIs(c1, mock_docker_client)\n            mock_from_env.assert_called_once()\n            c2 = _get_client()\n            self.assertIs(c2, mock_docker_client)\n            mock_from_env.assert_called_once()", "target": "def test_force_create_dir_clears_existing(self):\n        d = self.tmp_path / \"fresh\"\n        (d / \"inner\").mkdir(parents=True)\n        (d / \"inner\" / \"f.txt\").write_text(\"x\")\n        force_create_dir(d)\n        self.assertTrue(d.exists())\n        self.assertEqual(list(d.iterdir()), [])", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "000148", "source": "def get_ref(s: core_schema.CoreSchema) -> None | str:\n    return s.get('ref', None)", "target": "def test_parse_to_rect_not_convertible(self):\n        for not_convertible in (np.empty(shape=(4, 1)), (), [], np.array([]), (12, ),\n                                [3, 4, 5, 10, 123], {1: 2, 3:4, 5:10, 6:30},\n                                '1234', np.array([1, 2, 3, 4], dtype=np.float32),\n                                np.array([[1, 2], [3, 4], [5, 6], [6, 8]]), (1, 2, 5, 1.5)):\n            with self.assertRaises((TypeError), msg=get_no_exception_msg(not_convertible)):\n                _ = cv.utils.dumpRect(not_convertible)", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|pytorch/pytorch", "data_split": "train", "example_id": "000149", "source": "def predict(est, data_test, target_test):\n    if args.no_predict:\n        return\n    tic = time()\n    predicted_test = est.predict(data_test)\n    predicted_proba_test = est.predict_proba(data_test)\n    toc = time()\n    roc_auc = roc_auc_score(target_test, predicted_proba_test[:, 1])\n    acc = accuracy_score(target_test, predicted_test)\n    print(f\"predicted in {toc - tic:.3f}s, ROC AUC: {roc_auc:.4f}, ACC: {acc:.4f}\")", "target": "def eqStr(self, input: str) -> str:\n        return input", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pytorch/pytorch", "data_split": "train", "example_id": "000150", "source": "def _run(self):\n        if not os.path.isdir(self.build_dir):\n            os.makedirs(self.build_dir)\n        self.runTest()", "target": "def remove_dir(path: Union[str, Path, None]) -> None:\n    if not path:\n        return\n    path_obj = get_path(path)\n    if path_obj.exists():\n        shutil.rmtree(path_obj)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000151", "source": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "target": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|opencv/opencv", "data_split": "train", "example_id": "000152", "source": "def make_paragraph_for_estimator_type(estimator_type):\n        intro = nodes.list_item()\n        intro += nodes.strong(text=\"Estimators that allow NaN values for type \")\n        intro += nodes.literal(text=f\"{estimator_type}\")\n        intro += nodes.strong(text=\":\\n\")\n        exists = False\n        lst = nodes.bullet_list()\n        for name, est_class in all_estimators(type_filter=estimator_type):\n            with suppress(SkipTest):\n                est = next(_construct_instances(est_class))\n                if est.__sklearn_tags__().input_tags.allow_nan:\n                    module_name = \".\".join(est_class.__module__.split(\".\")[:2])\n                    class_title = f\"{est_class.__name__}\"\n                    class_url = f\"./generated/{module_name}.{class_title}.html\"\n                    item = nodes.list_item()\n                    para = nodes.paragraph()\n                    para += nodes.reference(\n                        class_title, text=class_title, internal=False, refuri=class_url\n                    )\n                    exists = True\n                    item += para\n                    lst += item\n        intro += lst\n        return [intro] if exists else None", "target": "def wrapCommand(self, module, cmd, env):\n        if self.options.valgrind:\n            res = ['valgrind']\n            supp = self.options.valgrind_supp or []\n            for f in supp:\n                if os.path.isfile(f):\n                    res.append(\"--suppressions=%s\" % f)\n                else:\n                    print(\"WARNING: Valgrind suppression file is missing, SKIP: %s\" % f)\n            res.extend(self.options.valgrind_opt)\n            has_gtest_filter = next((True for x in cmd if x.startswith('--gtest_filter=')), False)\n            return res + cmd + ([longTestFilter(LONG_TESTS_DEBUG_VALGRIND, module)] if not has_gtest_filter else [])\n        elif self.options.qemu:\n            import shlex\n            res = shlex.split(self.options.qemu)\n            for (name, value) in [entry for entry in os.environ.items() if entry[0].startswith('OPENCV') and not entry[0] in env]:\n                res += ['-E', '\"{}={}\"'.format(name, value)]\n            for (name, value) in env.items():\n                res += ['-E', '\"{}={}\"'.format(name, value)]\n            return res + ['--'] + cmd\n        return cmd", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "000153", "source": "def execute(cmd, cwd = None, output = None):\n    if not output:\n        print(\"Executing: %s in %s\" % (cmd, cwd), file=sys.stderr)\n        print('Executing: ' + ' '.join(cmd))\n        retcode = check_call(cmd, cwd = cwd)\n        if retcode != 0:\n            raise Exception(\"Child returned:\", retcode)\n    else:\n        with open(output, \"a\") as f:\n            f.flush()\n            p = Popen(cmd, cwd = cwd, stdout = f)\n            os.waitpid(p.pid, 0)", "target": "def test_function_validator_wrapping_args_schema_wrap() -> None:\n    calls: list[Any] = []\n    def func(*args: Any) -> Any:\n        assert len(args) == 2\n        input, handler = args\n        output = handler(input)\n        calls.append((input, output))\n        return output\n    @dataclasses.dataclass\n    class Model:\n        number: int = 1\n    cs = core_schema.dataclass_schema(\n        Model,\n        core_schema.no_info_wrap_validator_function(\n            func,\n            core_schema.dataclass_args_schema(\n                'Model', [core_schema.dataclass_field('number', core_schema.int_schema())]\n            ),\n        ),\n        ['number'],\n    )\n    v = SchemaValidator(cs)\n    instance: Model = v.validate_python({'number': 1})\n    assert instance.number == 1\n    assert calls == [({'number': 1}, ({'number': 1}, None))]\n    v.validate_assignment(instance, 'number', 2)\n    assert instance.number == 2\n    assert calls == [({'number': 1}, ({'number': 1}, None)), ({'number': 2}, ({'number': 2}, None))]", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000154", "source": "def ser_x(self, v: Any, _) -> str:\n            assert self.x == 1_000\n            return f'{v:_}'", "target": "def ser_x(data: Model, v: Any, info: core_schema.FieldSerializationInfo) -> str:\n        assert data['x'] == 1_000\n        return f'{v:_}-{info.field_name}'", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "000155", "source": "def traverse_definition_ref(def_ref_schema: DefinitionReferenceSchema, ctx: GatherContext) -> None:\n    schema_ref = def_ref_schema['schema_ref']\n    if schema_ref not in ctx.collected_references:\n        definition = ctx.definitions.get(schema_ref)\n        if definition is None:\n            raise MissingDefinitionError(schema_ref)\n        ctx.collected_references[schema_ref] = def_ref_schema\n        traverse_schema(definition, ctx)\n        if 'serialization' in def_ref_schema:\n            traverse_schema(def_ref_schema['serialization'], ctx)\n        traverse_metadata(def_ref_schema, ctx)\n    else:\n        ctx.collected_references[schema_ref] = None", "target": "def test_ExistedFile(self):\n        res = cv.samples.findFile('HappyFish.jpg', False)\n        self.assertNotEqual(res, '')", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000156", "source": "def liger(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        from liger_kernel.transformers.cross_entropy import LigerCrossEntropyLoss\n        x, target, dloss = args\n        cross_entropy = LigerCrossEntropyLoss(reduction=\"none\")\n        loss = cross_entropy(x, target)\n        return lambda: torch.autograd.grad(\n            loss, x, grad_outputs=dloss, retain_graph=True\n        )", "target": "def liger(self, args, kwargs=None) -> Any:\n        from liger_kernel.transformers.softmax import LigerSoftmax\n        assert kwargs is None\n        x, dy = args\n        softmax = LigerSoftmax().to(\"cuda\")\n        y = softmax(x)\n        return lambda: torch.autograd.grad(y, x, grad_outputs=dy, retain_graph=True)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000157", "source": "def my_function(a, b, c):\n        return a + b + c", "target": "def my_function(a, b, c):\n        return a + b + c", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "000158", "source": "def train(self, samples, responses):\n        _sample_n, var_n = samples.shape\n        new_responses = self.unroll_responses(responses).reshape(-1, self.class_n)\n        layer_sizes = np.int32([var_n, 100, 100, self.class_n])\n        self.model.setLayerSizes(layer_sizes)\n        self.model.setTrainMethod(cv.ml.ANN_MLP_BACKPROP)\n        self.model.setBackpropMomentumScale(0)\n        self.model.setBackpropWeightScale(0.001)\n        self.model.setTermCriteria((cv.TERM_CRITERIA_COUNT, 20, 0.01))\n        self.model.setActivationFunction(cv.ml.ANN_MLP_SIGMOID_SYM, 2, 1)\n        self.model.train(samples, cv.ml.ROW_SAMPLE, np.float32(new_responses))", "target": "def test_core_model_json_extra(benchmark, basic_model_serializer_extra):\n    m = BasicModel(a=1, b=2, c=3, d=4, e=5, f=6, g=7, h=8, __pydantic_extra__={'i': 9})\n    assert basic_model_serializer_extra.to_json(m) == b'{\"a\":1,\"b\":2,\"c\":3,\"d\":4,\"e\":5,\"f\":6,\"g\":7,\"h\":8,\"i\":9}'\n    benchmark(basic_model_serializer_extra.to_json, m)", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "000159", "source": "def make_estimator(self, params):\n        (method,) = params\n        estimator = TSNE(random_state=0, method=method)\n        return estimator", "target": "def make_estimator(self, params):\n        representation, precompute = params\n        estimator = ElasticNet(precompute=precompute, alpha=0.001, random_state=0)\n        return estimator", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "000160", "source": "def intersectionRate(s1, s2):\n    x1, y1, x2, y2 = s1\n    s1 = np.array([[x1, y1], [x2,y1], [x2, y2], [x1, y2]])\n    area, _intersection = cv.intersectConvexConvex(s1, np.array(s2))\n    return 2 * area / (cv.contourArea(s1) + cv.contourArea(np.array(s2)))", "target": "def multiple_of(self: _Pipeline[_InT, _NewOutMod], multiple_of: _NewOutMod) -> _Pipeline[_InT, _NewOutMod]: ...", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000161", "source": "def append_func(input_value, info):\n        return f'{input_value} Changed'", "target": "def test_property_setter():\n    class Square:\n        side: float\n        def __init__(self, **kwargs):\n            self.__dict__ = kwargs\n        @property\n        def area(self) -> float:\n            return self.side**2\n        @area.setter\n        def area(self, area: float) -> None:\n            self.side = area**0.5\n        @area.deleter\n        def area(self) -> None:\n            self.side = 0.0\n        @cached_property\n        def random_n(self) -> int:\n            return randint(0, 1_000)\n    s = SchemaSerializer(\n        core_schema.model_schema(\n            Square,\n            core_schema.model_fields_schema(\n                {'side': core_schema.model_field(core_schema.float_schema())},\n                computed_fields=[\n                    core_schema.computed_field('area', core_schema.float_schema()),\n                    core_schema.computed_field('random_n', core_schema.int_schema(), alias='The random number'),\n                ],\n            ),\n        )\n    )\n    sq = Square(side=10.0)\n    the_random_n = sq.random_n\n    assert s.to_python(sq, by_alias=True) == {'side': 10.0, 'area': 100.0, 'The random number': the_random_n}\n    assert s.to_json(sq, by_alias=True) == b'{\"side\":10.0,\"area\":100.0,\"The random number\":%d}' % the_random_n\n    sq.area = 49.0\n    assert s.to_python(sq, by_alias=False) == {'side': 7, 'area': 49, 'random_n': the_random_n}\n    assert s.to_json(sq, by_alias=False) == b'{\"side\":7.0,\"area\":49.0,\"random_n\":%d}' % the_random_n\n    del sq.area\n    assert s.to_python(sq, by_alias=False) == {'side': 0, 'area': 0, 'random_n': the_random_n}\n    assert s.to_python(sq, exclude={'random_n'}) == {'side': 0, 'area': 0}", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000162", "source": "def get_memory_bytes(self, args, kwargs) -> int:\n        x, target = args\n        M, N = x.shape\n        dtype = x.dtype\n        return (M * N + M + M) * dtype.itemsize", "target": "def get_memory_bytes(self, args, kwargs) -> int:\n        x, w, dy = args\n        M, N = x.shape\n        return 3 * M * N * x.dtype.itemsize + 2 * N * w.dtype.itemsize", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "000163", "source": "def test_constraints_schema_validation() -> None:\n    val = SchemaValidator(cs.date_schema(gt='2020-01-01'))\n    with pytest.raises(ValidationError):\n        val.validate_python('2019-01-01')", "target": "def preprocess(self, img):\n        return img - self.mean_blob", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|opencv/opencv", "data_split": "train", "example_id": "000164", "source": "def add_relu_softmax(x, a):\n        return (torch.softmax(torch.relu(x + a), -1),)", "target": "def classes(self) -> Dict[str, \"ClassNode\"]:\n        return self._children[ASTNodeType.Class]", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000165", "source": "def check_metadata(metadata: dict[str, Any], allowed: Iterable[str], source_type: Any) -> None:\n    unknown = metadata.keys() - set(allowed)\n    if unknown:\n        raise TypeError(\n            f'The following constraints cannot be applied to {source_type!r}: {\", \".join([f\"{k!r}\" for k in unknown])}'\n        )", "target": "def wrap_function(input_value, validator, info):\n            try:\n                return validator(input_value)\n            except ValidationError:\n                if isinstance(input_value, list):\n                    return str(len(input_value))\n                else:\n                    return repr(input_value)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000166", "source": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "target": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000167", "source": "def is_field_frozen(stmt: AssignmentStmt) -> bool:\n        expr = stmt.rvalue\n        if isinstance(expr, TempNode):\n            return False\n        if not (\n            isinstance(expr, CallExpr) and isinstance(expr.callee, RefExpr) and expr.callee.fullname == FIELD_FULLNAME\n        ):\n            return False\n        for i, arg_name in enumerate(expr.arg_names):\n            if arg_name == 'frozen':\n                arg = expr.args[i]\n                return isinstance(arg, NameExpr) and arg.fullname == 'builtins.True'\n        return False", "target": "def f(value, serializer, _info):\n        return f'result={serializer(len(value))}'", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000168", "source": "def test_allow_inf_nan_true_json() -> None:\n    v = SchemaValidator(cs.int_schema(), config=cs.CoreConfig(allow_inf_nan=True))\n    assert v.validate_json('123') == 123\n    with pytest.raises(ValidationError, match=r'Input should be a finite number \\[type=finite_number'):\n        v.validate_json('NaN')\n    with pytest.raises(ValidationError, match=r'Input should be a finite number \\[type=finite_number'):\n        v.validate_json('Infinity')\n    with pytest.raises(ValidationError, match=r'Input should be a finite number \\[type=finite_number'):\n        v.validate_json('-Infinity')", "target": "def test_validate_max_digits_and_decimal_places_edge_case() -> None:\n    v = SchemaValidator(cs.decimal_schema(max_digits=34, decimal_places=18))\n    assert v.validate_python(Decimal('9999999999999999.999999999999999999')) == Decimal(\n        '9999999999999999.999999999999999999'\n    )", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "000169", "source": "def test_schema_typing_error() -> None:\n    _: CoreSchema = {'type': 'wrong'}", "target": "def main():\n    result_path = sys.argv[1]\n    Benchmark().enable_compile_time_instruction_count().collect_all().append_results(\n        result_path\n    )", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000170", "source": "def with_info_after_validator_function(\n    function: WithInfoValidatorFunction,\n    schema: CoreSchema,\n    *,\n    field_name: str | None = None,\n    ref: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> AfterValidatorFunctionSchema:\n    if field_name is not None:\n        warnings.warn(\n            'The `field_name` argument on `with_info_after_validator_function` is deprecated, it will be passed to the function through `ValidationState` instead.',\n            DeprecationWarning,\n            stacklevel=2,\n        )\n    return _dict_not_none(\n        type='function-after',\n        function=_dict_not_none(type='with-info', function=function, field_name=field_name),\n        schema=schema,\n        ref=ref,\n        metadata=metadata,\n        serialization=serialization,\n    )", "target": "def model_construct(cls, root: RootModelRootType, _fields_set: set[str] | None = None) -> Self:\n        return super().model_construct(root=root, _fields_set=_fields_set)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000171", "source": "def volume(self) -> None:\n            return None", "target": "def volume(self) -> None:\n            return None", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000172", "source": "def get_shapes(self) -> tuple[tuple[int, ...], ...]:\n        return (\n            (32768, 256),\n            (32768, 512),\n            (32768, 1024),\n            (32768, 2048),\n            (32768, 4096),\n            (32768, 8192),\n            (32768, 16384),\n        ) + extra_shapes_for_norm", "target": "def get_shapes(self) -> tuple[tuple[int, ...], ...]:\n        return (\n            (32768, 256),\n            (32768, 512),\n            (32768, 1024),\n            (32768, 2048),\n            (32768, 4096),\n            (32768, 8192),\n            (32768, 16384),\n            (32768, 32768),\n            (32768, 65536),\n        ) + extra_shapes_for_norm", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000173", "source": "def value_type(self) -> str:\n        return self._value_type", "target": "def value_type(self) -> TypeNode:\n        return self.items[1]", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000174", "source": "def test_function_plain_field_serializer_to_json():\n    class Model(RootModel):\n        def ser_root(self, v: Any, _) -> str:\n            assert self.root == 1_000\n            return f'{v:_}'\n    s = SchemaSerializer(\n        core_schema.model_schema(\n            Model,\n            core_schema.int_schema(\n                serialization=core_schema.plain_serializer_function_ser_schema(\n                    Model.ser_root, is_field_serializer=True, info_arg=True\n                )\n            ),\n            root_model=True,\n        )\n    )\n    assert json.loads(s.to_json(Model(1000))) == '1_000'", "target": "def test_function_plain_field_serializer_to_json():\n    @dataclasses.dataclass\n    class Model:\n        x: int\n        def ser_x(self, v: Any, _) -> str:\n            assert self.x == 1_000\n            return f'{v:_}'\n    s = SchemaSerializer(\n        core_schema.model_schema(\n            Model,\n            core_schema.model_fields_schema(\n                {\n                    'x': core_schema.model_field(\n                        core_schema.int_schema(\n                            serialization=core_schema.plain_serializer_function_ser_schema(\n                                Model.ser_x, is_field_serializer=True, info_arg=True\n                            )\n                        )\n                    )\n                }\n            ),\n        )\n    )\n    assert json.loads(s.to_json(Model(x=1000))) == {'x': '1_000'}", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000175", "source": "def forward(self, inputs):\n        output = self.pool(F.relu(self.conv(inputs)))\n        output = output.view(1)\n        return output", "target": "def forward(self, x):\n        for i, l in enumerate(self.linears):\n            x = self.linears[i // 2](x) + l(x)\n        return x", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000176", "source": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "target": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000177", "source": "def _work(self):\n        with (\n            fresh_cache(),\n        ):\n            opt_m = torch.compile(backend=self.backend(), dynamic=self.is_dynamic())(\n                self.m.cuda() if self._is_gpu else self.m\n            )\n            opt_m(self.input)", "target": "def _work(self) -> None:\n        @torch.compile(\n            backend=\"inductor\",\n            fullgraph=True,\n            dynamic=self._dynamic,\n        )\n        def f(a, b):\n            z = torch.mm(a, b)\n            for i in range(200):\n                z = torch.mm(z, b)\n            return z\n        with fresh_cache(), torch._inductor.config.patch(max_autotune=True):\n            f(self.a, self.b)", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000178", "source": "def relative_typename(self, full_node_name: str) -> str:\n        return self.type_node.relative_typename(full_node_name)", "target": "def relative_typename(self, module: str) -> str:\n        return '_typing.Callable[[{}], {}]'.format(\n            ', '.join(arg.relative_typename(module) for arg in self.arg_types),\n            self.ret_type.relative_typename(module)\n        )", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000179", "source": "def test_only_allow_alias(py_and_json) -> None:\n    schema = core_schema.dataclass_schema(\n        BasicDataclass,\n        core_schema.dataclass_args_schema(\n            'BasicDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), validation_alias='FieldA'),\n            ],\n        ),\n        ['a'],\n        config=core_schema.CoreConfig(validate_by_name=False, validate_by_alias=True),\n    )\n    v = py_and_json(schema)\n    assert v.validate_test({'FieldA': 'hello'}) == BasicDataclass(a='hello')\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'a': 'hello'})", "target": "def test_only_allow_alias(py_and_json) -> None:\n    v = py_and_json(\n        {\n            'type': 'typed-dict',\n            'fields': {\n                'field_a': {'validation_alias': 'FieldA', 'type': 'typed-dict-field', 'schema': {'type': 'int'}}\n            },\n            'config': {'validate_by_name': False, 'validate_by_alias': True},\n        }\n    )\n    assert v.validate_test({'FieldA': '123'}) == {'field_a': 123}\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'field_a': '123'})", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000180", "source": "def ser_x(data: Model, v: Any, serializer: core_schema.SerializerFunctionWrapHandler, _) -> str:\n        x = serializer(v)\n        assert data['x'] == 1_000\n        return f'{x:_}'", "target": "def ser_x(data: Model, v: Any) -> str:\n        assert data['x'] == 1_000\n        return f'{v:_}'", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "000181", "source": "def str_schema(\n    *,\n    pattern: str | Pattern[str] | None = None,\n    max_length: int | None = None,\n    min_length: int | None = None,\n    strip_whitespace: bool | None = None,\n    to_lower: bool | None = None,\n    to_upper: bool | None = None,\n    regex_engine: Literal['rust-regex', 'python-re'] | None = None,\n    strict: bool | None = None,\n    coerce_numbers_to_str: bool | None = None,\n    ref: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> StringSchema:\n    return _dict_not_none(\n        type='str',\n        pattern=pattern,\n        max_length=max_length,\n        min_length=min_length,\n        strip_whitespace=strip_whitespace,\n        to_lower=to_lower,\n        to_upper=to_upper,\n        regex_engine=regex_engine,\n        strict=strict,\n        coerce_numbers_to_str=coerce_numbers_to_str,\n        ref=ref,\n        metadata=metadata,\n        serialization=serialization,\n    )", "target": "def node_type(self) -> ASTNodeType:\n        return ASTNodeType.Function", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000182", "source": "def area(self) -> bytes:\n            a = self.width * self.height\n            return b'%d' % a", "target": "def area(self) -> int:\n            return self.width * self.height", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000183", "source": "def test_union_float_simple(py_and_json: PyAndJson):\n    v = py_and_json({'type': 'union', 'choices': [{'type': 'float'}, {'type': 'list'}]})\n    assert v.validate_test('5') == 5\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_test('xxx')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'float_parsing',\n            'loc': ('float',),\n            'msg': 'Input should be a valid number, unable to parse string as a number',\n            'input': 'xxx',\n        },\n        {\n            'type': 'list_type',\n            'loc': ('list[any]',),\n            'msg': IsStr(regex='Input should be a valid (list|array)'),\n            'input': 'xxx',\n        },\n    ]", "target": "def test_alias(py_and_json: PyAndJson):\n    v = py_and_json(\n        {\n            'type': 'model-fields',\n            'fields': {'field_a': {'validation_alias': 'FieldA', 'type': 'model-field', 'schema': {'type': 'int'}}},\n        }\n    )\n    assert v.validate_test({'FieldA': '123'}) == ({'field_a': 123}, None, {'field_a'})\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'foobar': '123'})\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'field_a': '123'})", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000184", "source": "def field_name(self) -> str | None:\n        return self._generate_schema.field_name_stack.get()", "target": "def field_name(self) -> str | None:\n        raise NotImplementedError", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000185", "source": "def test_wrap(py_and_json: PyAndJson):\n    def f(input_value, validator, info):\n        return validator(input_value) + f'| context: {info.context}'\n    v = py_and_json(core_schema.with_info_wrap_validator_function(f, core_schema.str_schema()))\n    assert v.validate_test('foobar') == 'foobar| context: None'\n    assert v.validate_test('foobar', None, {1: 10}) == 'foobar| context: {1: 10}'\n    assert v.validate_test('foobar', None, 'frogspawn') == 'foobar| context: frogspawn'", "target": "def is_excluded(self, item: Any) -> bool:\n        return self.is_true(self._items.get(item))", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000186", "source": "def test_allow_inf_nan_true_json() -> None:\n    v = SchemaValidator(core_schema.float_schema())\n    assert v.validate_json('123') == 123\n    assert v.validate_json('NaN') == IsFloatNan()\n    assert v.validate_json('Infinity') == float('inf')\n    assert v.validate_json('-Infinity') == float('-inf')\n    assert v.validate_json('\"NaN\"') == IsFloatNan()\n    assert v.validate_json('\"Infinity\"') == float('inf')\n    assert v.validate_json('\"-Infinity\"') == float('-inf')", "target": "def test_allow_inf_nan_true_json() -> None:\n    v = SchemaValidator(cs.int_schema(), config=cs.CoreConfig(allow_inf_nan=True))\n    assert v.validate_json('123') == 123\n    with pytest.raises(ValidationError, match=r'Input should be a finite number \\[type=finite_number'):\n        v.validate_json('NaN')\n    with pytest.raises(ValidationError, match=r'Input should be a finite number \\[type=finite_number'):\n        v.validate_json('Infinity')\n    with pytest.raises(ValidationError, match=r'Input should be a finite number \\[type=finite_number'):\n        v.validate_json('-Infinity')", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000187", "source": "def b(self) -> str:\n            return 'b'", "target": "def b(self):\n            return [1, 2, b'3']", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000188", "source": "def validate_b(cls, v: bytes, info: core_schema.ValidationInfo) -> str:\n            assert v == b'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return 'hello world!'", "target": "def validate_b(cls, v: bytes, info: core_schema.ValidationInfo) -> bytes:\n            assert v == b'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return b'hello world!'", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000189", "source": "def liger(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        from liger_kernel.transformers.cross_entropy import LigerCrossEntropyLoss\n        x, target = args\n        cross_entropy = LigerCrossEntropyLoss(reduction=\"none\")\n        return lambda: cross_entropy(x, target)", "target": "def liger(self, args, kwargs=None) -> Any:\n        from liger_kernel.transformers.softmax import LigerSoftmax\n        assert kwargs is None\n        (x,) = args\n        softmax = LigerSoftmax().to(\"cuda\")\n        return lambda: softmax(x)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000190", "source": "def attempt_rebuild_fn(attr_fn: Callable[[type[BaseModel]], T]) -> Callable[[], T | None]:\n        def handler() -> T | None:\n            if cls.model_rebuild(raise_errors=False, _parent_namespace_depth=5) is not False:\n                return attr_fn(cls)\n            return None\n        return handler", "target": "def attempt_rebuild_fn(attr_fn: Callable[[type[PydanticDataclass]], T]) -> Callable[[], T | None]:\n        def handler() -> T | None:\n            if rebuild_dataclass(cls, raise_errors=False, _parent_namespace_depth=5) is not False:\n                return attr_fn(cls)\n            return None\n        return handler", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000191", "source": "def children_types(self) -> Tuple[ASTNodeType, ...]:\n        return ()", "target": "def children_types(self) -> Tuple[ASTNodeType, ...]:\n        return ()", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000192", "source": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "target": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000193", "source": "def full_typename(self) -> str:\n        return \"cv2.typing.\" + self.typename", "target": "def full_typename(self) -> str:\n        return '_typing.Callable[[{}], {}]'.format(\n            ', '.join(arg.full_typename for arg in self.arg_types),\n            self.ret_type.full_typename\n        )", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000194", "source": "def f(input_value: Any, *args: Any) -> Any:\n        if mode == 'wrap':\n            handler, _ = args\n            calls.append({'value': input_value})\n            return handler(input_value)\n        else:\n            calls.append({'value': input_value})\n            return input_value", "target": "def validate(config=None):\n    def decorator(function):\n        parameters = signature(function).parameters\n        type_hints = get_type_hints(function)\n        mode_lookup = {\n            Parameter.POSITIONAL_ONLY: 'positional_only',\n            Parameter.POSITIONAL_OR_KEYWORD: 'positional_or_keyword',\n            Parameter.KEYWORD_ONLY: 'keyword_only',\n        }\n        arguments_schema = []\n        schema = {'type': 'arguments', 'arguments_schema': arguments_schema}\n        for i, (name, p) in enumerate(parameters.items()):\n            if p.annotation is p.empty:\n                annotation = Any\n            else:\n                annotation = type_hints[name]\n            assert annotation in (bool, int, float, str, Any), f'schema for {annotation} not implemented'\n            if annotation in (bool, int, float, str):\n                arg_schema = {'type': annotation.__name__}\n            else:\n                assert annotation is Any\n                arg_schema = {'type': 'any'}\n            if p.kind in mode_lookup:\n                if p.default is not p.empty:\n                    arg_schema = {'type': 'default', 'schema': arg_schema, 'default': p.default}\n                s = {'name': name, 'mode': mode_lookup[p.kind], 'schema': arg_schema}\n                arguments_schema.append(s)\n            elif p.kind == Parameter.VAR_POSITIONAL:\n                schema['var_args_schema'] = arg_schema\n            else:\n                assert p.kind == Parameter.VAR_KEYWORD, p.kind\n                schema['var_kwargs_schema'] = arg_schema\n        validator = SchemaValidator(schema, config=config)\n        @wraps(function)\n        def wrapper(*args, **kwargs):\n            validated_args, validated_kwargs = validator.validate_python(ArgsKwargs(args, kwargs))\n            return function(*validated_args, **validated_kwargs)\n        return wrapper\n    return decorator", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|scikit-learn/scikit-learn", "data_split": "train", "example_id": "000195", "source": "def getOSIdentifier(self):\n        return \"Android\" + self.run([\"shell\", \"getprop ro.build.version.release\"], silent=True).strip()", "target": "def make_estimator(self, params):\n        representation, precompute = params\n        estimator = ElasticNet(precompute=precompute, alpha=0.001, random_state=0)\n        return estimator", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "000196", "source": "def test_copy_dir_into_existing_dir_overwrite_true_merges(self):\n        src = self.tmp_path / \"srcdir\"\n        dst = self.tmp_path / \"destdir\"\n        (src / \"x\").mkdir(parents=True)\n        (src / \"x\" / \"new.txt\").write_text(\"new\")\n        dst.mkdir()\n        (dst / \"existing.txt\").write_text(\"old\")\n        copy(src, dst)\n        self.assertEqual((dst / \"existing.txt\").read_text(), \"old\")\n        self.assertEqual((dst / \"x\" / \"new.txt\").read_text(), \"new\")", "target": "def test_set_no_validators_python(input_value, expected):\n    v = SchemaValidator(cs.set_schema())\n    assert v.validate_python(input_value) == expected", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "000197", "source": "def forward(self, x):\n        x = x.transpose(0, 1).transpose(1, 2)\n        x = F.pad(x, pad=self.pad, value=0)\n        x = self.conv(x)\n        x = x.transpose(1, 2).transpose(0, 1).contiguous()\n        return x", "target": "def test_core_dict(self, core_validator: SchemaValidator, core_serializer: SchemaSerializer, benchmark):\n        m = core_validator.validate_python(self.data)\n        assert core_serializer.to_python(m) == self.data\n        benchmark(core_serializer.to_python, m)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000198", "source": "def add_function(self, name: str, arguments: Sequence[FunctionNode.Arg] = (),\n                     return_type: Optional[FunctionNode.RetType] = None) -> FunctionNode:\n        return self._add_child(FunctionNode, name, arguments=arguments,\n                               return_type=return_type)", "target": "def add_function(self, name: str, arguments: Sequence[FunctionNode.Arg] = (),\n                     return_type: Optional[FunctionNode.RetType] = None,\n                     is_static: bool = False) -> FunctionNode:\n        arguments = list(arguments)\n        if return_type is not None:\n            is_classmethod = return_type.typename == self.name\n        else:\n            is_classmethod = False\n        if not is_static:\n            arguments.insert(0, FunctionNode.Arg(\"self\"))\n        elif is_classmethod:\n            is_static = False\n            arguments.insert(0, FunctionNode.Arg(\"cls\"))\n        return self._add_child(FunctionNode, name, arguments=arguments,\n                               return_type=return_type, is_static=is_static,\n                               is_classmethod=is_classmethod)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000199", "source": "def test_from_attributes_extra_forbid() -> None:\n    class Source:\n        a = 1\n        b = 2\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={'a': core_schema.model_field(schema=core_schema.int_schema())},\n            from_attributes=True,\n            extra_behavior='forbid',\n        )\n    )\n    assert v.validate_python(Source()) == ({'a': 1}, None, {'a'})", "target": "def test_frozenset_no_validators_both(py_and_json: PyAndJson, input_value, expected):\n    v = py_and_json({'type': 'frozenset'})\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            v.validate_test(input_value)\n    else:\n        output = v.validate_test(input_value)\n        assert output == expected\n        assert isinstance(output, frozenset)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000200", "source": "def test_alias_allow_pop(py_and_json: PyAndJson):\n    schema = core_schema.dataclass_schema(\n        BasicDataclass,\n        core_schema.dataclass_args_schema(\n            'BasicDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), validation_alias='FieldA'),\n            ],\n        ),\n        ['a'],\n        config=core_schema.CoreConfig(validate_by_name=True, validate_by_alias=True),\n    )\n    v = py_and_json(schema)\n    assert v.validate_test({'FieldA': 'hello'}) == BasicDataclass(a='hello')\n    assert v.validate_test({'a': 'hello'}) == BasicDataclass(a='hello')\n    assert v.validate_test(\n        {\n            'FieldA': 'hello',\n            'a': 'world',\n        }\n    ) == BasicDataclass(a='hello')\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'foobar': 'hello'})", "target": "def test_alias_allow_pop(py_and_json: PyAndJson):\n    v = py_and_json(\n        {\n            'type': 'typed-dict',\n            'fields': {\n                'field_a': {'validation_alias': 'FieldA', 'type': 'typed-dict-field', 'schema': {'type': 'int'}}\n            },\n            'config': {'validate_by_name': True, 'validate_by_alias': True},\n        },\n    )\n    assert v.validate_test({'FieldA': '123'}) == {'field_a': 123}\n    assert v.validate_test({'field_a': '123'}) == {'field_a': 123}\n    assert v.validate_test({'FieldA': '1', 'field_a': '2'}) == {'field_a': 1}\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'foobar': '123'})", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000201", "source": "def test_invalid_type():\n    with pytest.raises(SchemaError, match=\"TypeError: 'Foo' object cannot be cast as 'type\"):\n        SchemaValidator(core_schema.is_subclass_schema(Foo()))", "target": "def test_positional_only(py_and_json: PyAndJson, input_value) -> None:\n    v = py_and_json(\n        cs.arguments_v3_schema(\n            [\n                cs.arguments_v3_parameter(name='a', schema=cs.int_schema(), mode='positional_only'),\n                cs.arguments_v3_parameter(\n                    name='b', schema=cs.with_default_schema(cs.bool_schema(), default=True), mode='positional_only'\n                ),\n            ]\n        )\n    )\n    assert v.validate_test(input_value) == ((1, True), {})", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000202", "source": "def test_generator_error():\n    def gen(error: bool):\n        yield 1\n        yield 2\n        if error:\n            raise RuntimeError('my error')\n        yield 3\n    v = SchemaValidator(cs.set_schema(items_schema=cs.int_schema()))\n    r = v.validate_python(gen(False))\n    assert r == {1, 2, 3}\n    assert isinstance(r, set)\n    msg = r'Error iterating over object, error: RuntimeError: my error \\[type=iteration_error,'\n    with pytest.raises(ValidationError, match=msg):\n        v.validate_python(gen(True))", "target": "def test_generator_error():\n    def gen(error: bool):\n        yield 1\n        yield 2\n        if error:\n            raise RuntimeError('error')\n        yield 3\n    v = SchemaValidator(core_schema.tuple_schema(items_schema=[core_schema.int_schema()], variadic_item_index=0))\n    assert v.validate_python(gen(False)) == (1, 2, 3)\n    msg = r'Error iterating over object, error: RuntimeError: error \\[type=iteration_error,'\n    with pytest.raises(ValidationError, match=msg):\n        v.validate_python(gen(True))", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "000203", "source": "def test_include():\n    v = SchemaSerializer(\n        core_schema.generator_schema(\n            core_schema.any_schema(), serialization=core_schema.filter_seq_schema(include={1, 3, 5})\n        )\n    )\n    assert v.to_python(gen_ok(0, 1, 2, 3), mode='json') == [1, 3]\n    assert list(v.to_python(gen_ok(0, 1, 2, 3))) == [1, 3]\n    assert v.to_python(gen_ok('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'), mode='json') == ['b', 'd', 'f']\n    assert v.to_python(gen_ok('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'), mode='json') == ['b', 'd', 'f']\n    assert v.to_json(gen_ok('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h')) == b'[\"b\",\"d\",\"f\"]'\n    assert v.to_python(gen_ok('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'), include={6}, mode='json') == ['b', 'd', 'f', 'g']\n    assert list(v.to_python(gen_ok('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'), include={6})) == ['b', 'd', 'f', 'g']\n    assert v.to_json(gen_ok('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'), include={6}) == b'[\"b\",\"d\",\"f\",\"g\"]'\n    assert v.to_python(gen_ok('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'), include={6: None}, mode='json') == [\n        'b',\n        'd',\n        'f',\n        'g',\n    ]\n    with pytest.raises(ValueError, match='Negative indices cannot be used to exclude items on unsized iterables'):\n        v.to_python(gen_ok('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'), include={-1: None, -2: None}, mode='json')\n    v.to_python(gen_ok('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'), include={'__all__': None}, mode='json')", "target": "def sub(a, b):\n    return 3 * a - b", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000204", "source": "def name(self):\n        prefix = f\"{self.category()}_{self.backend()}\"\n        if self.is_dynamic():\n            prefix += \"_dynamic\"\n        if self.device() == \"cuda\":\n            prefix += \"_gpu\"\n        return prefix", "target": "def name(self) -> str:\n        prefix = f\"{self.category()}\"\n        return prefix", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000205", "source": "def typename(self) -> str:\n            return self.type_node.full_typename", "target": "def typename(self) -> str:\n        if self._export_name is not None:\n            return self._export_name\n        return self.ctype_name", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000206", "source": "def typename(self) -> str:\n        return \"\"", "target": "def typename(self) -> str:\n        return \"None\"", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000207", "source": "def test_simple(self):\n        img1 = self.get_sample('stitching/a1.png')\n        img2 = self.get_sample('stitching/a2.png')\n        stitcher = cv.Stitcher.create(cv.Stitcher_PANORAMA)\n        (_result, pano) = stitcher.stitch((img1, img2))\n        self.assertAlmostEqual(pano.shape[0], 685, delta=100, msg=\"rows: %r\" % list(pano.shape))\n        self.assertAlmostEqual(pano.shape[1], 1025, delta=100, msg=\"cols: %r\" % list(pano.shape))", "target": "def test_simple(self):\n        img1 = self.get_sample('stitching/a1.png')\n        img2 = self.get_sample('stitching/a2.png')\n        stitcher = cv.Stitcher.create(cv.Stitcher_PANORAMA)\n        stitcher.estimateTransform((img1, img2))\n        result, _ = stitcher.composePanorama()\n        assert result == 0", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000208", "source": "def required_usage_imports(self) -> Generator[str, None, None]:\n        yield from ()", "target": "def required_usage_imports(self) -> Generator[str, None, None]:\n        yield \"import numpy\"\n        yield \"import typing as _typing\"", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000209", "source": "def constrain(self: _Pipeline[_InT, _OutT], constraint: _Eq) -> _Pipeline[_InT, _OutT]: ...", "target": "def constrain(self: _Pipeline[_InT, _OutT], constraint: _NotEq) -> _Pipeline[_InT, _OutT]: ...", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "000210", "source": "def pretty_print_core_schema(\n    val: Any,\n    *,\n    console: Console | None = None,\n    max_depth: int | None = None,\n    strip_metadata: bool = True,\n) -> None:\n    from rich.pretty import pprint\n    from pydantic import BaseModel, TypeAdapter\n    from pydantic.dataclasses import is_pydantic_dataclass\n    if (inspect.isclass(val) and issubclass(val, BaseModel)) or is_pydantic_dataclass(val):\n        val = val.__pydantic_core_schema__\n    if isinstance(val, TypeAdapter):\n        val = val.core_schema\n    cleaned_schema = _clean_schema_for_pretty_print(val, strip_metadata=strip_metadata)\n    pprint(cleaned_schema, console=console, max_depth=max_depth)", "target": "def test_houghcircles_alt(self):\n        fn = \"samples/data/board.jpg\"\n        src = self.get_sample(fn, 1)\n        img = cv.cvtColor(src, cv.COLOR_BGR2GRAY)\n        img = cv.medianBlur(img, 5)\n        circles = cv.HoughCircles(img, cv.HOUGH_GRADIENT_ALT, 1, 10, np.array([]), 300, 0.9, 1, 30)\n        self.assertEqual(circles.shape, (1, 18, 3))\n        circles = circles[0]\n        testCircles = [[38, 181, 17.6],\n        [99.7, 166, 13.12],\n        [142.7, 160, 13.52],\n        [223.6, 110, 8.62],\n        [79.1, 206.7, 8.62],\n        [47.5, 351.6, 11.64],\n        [189.5, 354.4, 11.64],\n        [189.8, 298.9, 10.64],\n        [189.5, 252.4, 14.62],\n        [252.5, 393.4, 15.62],\n        [602.9, 467.5, 11.42],\n        [222, 210.4, 9.12],\n        [263.1, 216.7, 9.12],\n        [359.8, 222.6, 9.12],\n        [518.9, 120.9, 9.12],\n        [413.8, 113.4, 9.12],\n        [489, 127.2, 9.12],\n        [448.4, 121.3, 9.12],\n        [384.6, 128.9, 8.62]]\n        matches_counter = 0\n        for i in range(len(testCircles)):\n            for j in range(len(circles)):\n                tstCircle = circleApproximation(testCircles[i])\n                circle = circleApproximation(circles[j])\n                if convContoursIntersectiponRate(tstCircle, circle) > 0.6:\n                    matches_counter += 1\n        self.assertGreater(float(matches_counter) / len(testCircles), .5)\n        self.assertLess(float(len(circles) - matches_counter) / len(circles), .75)\n        circles_acc = cv.HoughCirclesWithAccumulator(\n            image=img,\n            method=cv.HOUGH_GRADIENT_ALT,\n            dp=1,\n            minDist=10,\n            circles=np.array([]),\n            param1=300,\n            param2=0.9,\n            minRadius=13,\n            maxRadius=15)\n        self.assertEqual(circles_acc.shape, (1, 3, 4))\n        self.assertEqual(circles_acc[0, 0, 3], 62.)\n        self.assertEqual(circles_acc[0, 1, 3], 59.)\n        self.assertEqual(circles_acc[0, 2, 3], 47.)", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "000211", "source": "def fit(self, X, y):\n        super().fit(X, y)\n        df = self.decision_function(X)\n        self.df_min_ = df.min()\n        self.df_max_ = df.max()", "target": "def fit(self, X, y):\n        super().fit(X, y)\n        df = self.decision_function(X)\n        self.df_min_ = df.min()\n        self.df_max_ = df.max()", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "000212", "source": "def _wrap_validator(cls, v, validator, info):\n                return validator(v)", "target": "def skip_accuracy_check_as_eager_non_deterministic(self):\n        if self.args.accuracy and self.args.training:\n            return self._accuracy[\"skip\"][\"eager_not_deterministic\"]\n        return set()", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000213", "source": "def huge_graph():\n    def fn(x):\n        for _ in range(N):\n            x = x.sin()\n        return x\n    return torch.fx.symbolic_trace(fn)", "target": "def huge_graph():\n    def fn(x):\n        for _ in range(N):\n            x = x.sin()\n        return x\n    return torch.fx.symbolic_trace(fn)", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "000214", "source": "def required_usage_imports(self) -> Generator[str, None, None]:\n        yield \"import cv2.typing\"", "target": "def test_dataclass():\n    @dataclasses.dataclass\n    class MyDataClass:\n        field_a: int\n        field_b: date\n    v = SchemaValidator(\n        core_schema.dataclass_schema(\n            MyDataClass,\n            core_schema.dataclass_args_schema(\n                'MyDataClass',\n                [\n                    core_schema.dataclass_field('field_a', core_schema.int_schema()),\n                    core_schema.dataclass_field('field_b', core_schema.date_schema()),\n                ],\n            ),\n            ['field_a', 'field_b'],\n        )\n    )\n    m2 = v.validate_strings({'field_a': '1', 'field_b': '2017-01-01'})\n    assert m2.__dict__ == {'field_a': 1, 'field_b': date(2017, 1, 1)}\n    m2 = v.validate_strings({'field_a': '1', 'field_b': '2017-01-01'}, strict=True)\n    assert m2.__dict__ == {'field_a': 1, 'field_b': date(2017, 1, 1)}", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000215", "source": "def resolve(self, root: ASTNode):\n        try:\n            self.value.resolve(root)\n        except TypeResolutionError as e:\n            raise TypeResolutionError(\n                'Failed to resolve alias \"{}\" exposed as \"{}\"'.format(\n                    self.ctype_name, self.typename\n                )\n            ) from e", "target": "def resolve(self, root: ASTNode):\n        try:\n            self.positive_branch_type.resolve(root)\n            self.negative_branch_type.resolve(root)\n        except TypeResolutionError as e:\n            raise TypeResolutionError(\n                'Failed to resolve alias \"{}\" exposed as \"{}\"'.format(\n                    self.ctype_name, self.typename\n                )\n            ) from e", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000216", "source": "def area(self) -> int:\n            return self.width * self.height", "target": "def area(self) -> float:\n            return self.side**2", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000217", "source": "def test_only_validate_by_name(py_and_json) -> None:\n    v = py_and_json(\n        {\n            'type': 'model-fields',\n            'fields': {'field_a': {'validation_alias': 'FieldA', 'type': 'model-field', 'schema': {'type': 'int'}}},\n        },\n        config=CoreConfig(validate_by_name=True, validate_by_alias=False),\n    )\n    assert v.validate_test({'field_a': '123'}) == ({'field_a': 123}, None, {'field_a'})\n    with pytest.raises(ValidationError, match=r'field_a\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'FieldA': '123'})", "target": "def test_only_validate_by_name(py_and_json) -> None:\n    v = py_and_json(\n        {\n            'type': 'typed-dict',\n            'fields': {\n                'field_a': {'validation_alias': 'FieldA', 'type': 'typed-dict-field', 'schema': {'type': 'int'}}\n            },\n            'config': {'validate_by_name': True, 'validate_by_alias': False},\n        }\n    )\n    assert v.validate_test({'field_a': '123'}) == {'field_a': 123}\n    with pytest.raises(ValidationError, match=r'field_a\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'FieldA': '123'})", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "000218", "source": "def make_scorers(self):\n        self.train_scorer = lambda _, __: self.estimator.kl_divergence_\n        self.test_scorer = lambda _, __: self.estimator.kl_divergence_", "target": "def make_scorers(self):\n        self.train_scorer = lambda _, __: neg_mean_inertia(\n            self.X, self.estimator.predict(self.X), self.estimator.cluster_centers_\n        )\n        self.test_scorer = lambda _, __: neg_mean_inertia(\n            self.X_val,\n            self.estimator.predict(self.X_val),\n            self.estimator.cluster_centers_,\n        )", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000219", "source": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "target": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "000220", "source": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "target": "def test_with_default():\n    v = SchemaValidator(\n        core_schema.typed_dict_schema(\n            fields={\n                'field_a': core_schema.typed_dict_field(schema=core_schema.str_schema()),\n                'field_b': core_schema.typed_dict_field(\n                    schema=core_schema.with_default_schema(schema=core_schema.int_schema(), default=666)\n                ),\n            }\n        )\n    )\n    assert v.validate_python({'field_a': b'abc'}) == {'field_a': 'abc', 'field_b': 666}\n    assert v.validate_python({'field_a': b'abc', 'field_b': 1}) == {'field_a': 'abc', 'field_b': 1}", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000221", "source": "def wrap_function(value, handler, _info):\n        nonlocal calls\n        calls += 1\n        return handler(value)", "target": "def test_negative_json(input_value, expected):\n    v = SchemaValidator(cs.int_schema(lt=0))\n    json_input = json.dumps(input_value)\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            v.validate_json(json_input)\n    else:\n        output = v.validate_json(json_input)\n        assert output == expected\n        assert isinstance(output, int)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "000222", "source": "def get_list_of_3rdparty_libs(sdk_dir, abis):\n    libs = []\n    for abi in abis:\n        files = os.listdir(path.join(sdk_dir, \"sdk/native/3rdparty/libs/\" + abi))\n        cur_libs = [f[3:-2] for f in files if f[:3] == \"lib\" and f[-2:] == \".a\"]\n        for lib in cur_libs:\n            if lib not in libs:\n                libs.append(lib)\n    return libs", "target": "def test_aliases_path_negative_json(py_and_json: PyAndJson, input_value, expected):\n    v = py_and_json(\n        {\n            'type': 'typed-dict',\n            'fields': {\n                'field_a': {'validation_alias': ['foo', -2], 'type': 'typed-dict-field', 'schema': {'type': 'int'}}\n            },\n        }\n    )\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=expected.message):\n            val = v.validate_test(input_value)\n            print(f'UNEXPECTED OUTPUT: {val!r}')\n    else:\n        output = v.validate_test(input_value)\n        assert output == expected", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pytorch/pytorch", "data_split": "train", "example_id": "000223", "source": "def test_createMILTracker(self):\n        t = cv.TrackerMIL.create()\n        self.assertTrue(t is not None)", "target": "def bench(name, fn):\n    torch._dynamo.reset()\n    inps = [[torch.randn(i) for _ in range(100)] for i in range(10, 101, 10)]\n    def run_fn():\n        for inp in inps:\n            fn(*inp)\n    start = time.perf_counter()\n    for _ in range(3):\n        run_fn()\n    end = time.perf_counter()\n    results = timeit.repeat(lambda: run_fn(), number=1000, repeat=10)\n    print(f\"{name} {np.median(results) * 1000:.1f}us (warmup={end - start:.1f}s)\")", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|scikit-learn/scikit-learn", "data_split": "train", "example_id": "000224", "source": "def pretty_print_core_schema(\n    val: Any,\n    *,\n    console: Console | None = None,\n    max_depth: int | None = None,\n    strip_metadata: bool = True,\n) -> None:\n    from rich.pretty import pprint\n    from pydantic import BaseModel, TypeAdapter\n    from pydantic.dataclasses import is_pydantic_dataclass\n    if (inspect.isclass(val) and issubclass(val, BaseModel)) or is_pydantic_dataclass(val):\n        val = val.__pydantic_core_schema__\n    if isinstance(val, TypeAdapter):\n        val = val.core_schema\n    cleaned_schema = _clean_schema_for_pretty_print(val, strip_metadata=strip_metadata)\n    pprint(cleaned_schema, console=console, max_depth=max_depth)", "target": "def setup_grid_search():\n    try:\n        import pandas\n    except ImportError:\n        raise SkipTest(\"Skipping grid_search.rst, pandas not installed\")", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "000225", "source": "def ref_(cls, ctype_name: str, alias_ctype_name: str,\n             alias_export_name: Optional[str] = None,\n             export_name: Optional[str] = None,\n             doc: Optional[str] = None,\n             required_modules: Tuple[str, ...] = ()):\n        return cls(ctype_name,\n                   AliasRefTypeNode(alias_ctype_name, alias_export_name),\n                   export_name, doc, required_modules)", "target": "def test_filter():\n    v = SchemaSerializer(\n        core_schema.list_schema(\n            core_schema.any_schema(), serialization=core_schema.filter_seq_schema(include={1, 3, 5}, exclude={5, 6})\n        )\n    )\n    assert v.to_python([0, 1, 2, 3, 4, 5, 6, 7]) == [1, 3]", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pytorch/pytorch", "data_split": "train", "example_id": "000226", "source": "def run(img0, img1):\n                    return img0 + img1", "target": "def lstm_cell(\n    input: Tensor,\n    hidden: tuple[Tensor, Tensor],\n    w_ih: Tensor,\n    w_hh: Tensor,\n    b_ih: Tensor,\n    b_hh: Tensor,\n) -> tuple[Tensor, Tensor]:\n    hx, cx = hidden\n    gates = torch.mm(input, w_ih.t()) + torch.mm(hx, w_hh.t()) + b_ih + b_hh\n    ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)\n    ingate = torch.sigmoid(ingate)\n    forgetgate = torch.sigmoid(forgetgate)\n    cellgate = torch.tanh(cellgate)\n    outgate = torch.sigmoid(outgate)\n    cy = (forgetgate * cx) + (ingate * cellgate)\n    hy = outgate * torch.tanh(cy)\n    return hy, cy", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "000227", "source": "def make_data(self, params):\n        representation, solver = params\n        if representation == \"dense\":\n            data = _synth_regression_dataset(n_samples=500000, n_features=100)\n        else:\n            data = _synth_regression_sparse_dataset(\n                n_samples=100000, n_features=10000, density=0.005\n            )\n        return data", "target": "def make_data(self, params):\n        data = _synth_classification_dataset(\n            n_samples=10000, n_features=100, n_classes=5\n        )\n        return data", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|opencv/opencv", "data_split": "train", "example_id": "000228", "source": "def test_is_str_path_exist(self):\n        p = self.tmp_path / \"x.txt\"\n        p.write_text(\"1\")\n        self.assertTrue(is_path_exist(str(p)))\n        self.assertTrue(is_path_exist(p))\n        self.assertFalse(is_path_exist(str(self.tmp_path / \"missing\")))\n        self.assertFalse(is_path_exist(self.tmp_path / \"missing\"))\n        self.assertFalse(is_path_exist(\"\"))", "target": "def relative_typename(self, module: str) -> str:\n        return '_typing.Callable[[{}], {}]'.format(\n            ', '.join(arg.relative_typename(module) for arg in self.arg_types),\n            self.ret_type.relative_typename(module)\n        )", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000229", "source": "def import_cached_field_info() -> type['FieldInfo']:\n    from pydantic.fields import FieldInfo\n    return FieldInfo", "target": "def test_json_trailing_strings():\n    v = SchemaValidator(core_schema.list_schema(core_schema.json_schema()))\n    assert v.validate_python(['{\"a\": 1}', '{\"b\": \"x'], allow_partial=True) == snapshot([{'a': 1}, {}])\n    assert v.validate_python(['{\"a\": 1}', '{\"b\": \"x'], allow_partial='trailing-strings') == snapshot(\n        [{'a': 1}, {'b': 'x'}]\n    )\n    assert v.validate_json('[\"{\\\\\"a\\\\\": 1}\", \"{\\\\\"b\\\\\": 2}\"]') == snapshot([{'a': 1}, {'b': 2}])\n    assert v.validate_json('[\"{\\\\\"a\\\\\": 1}\", \"{\\\\\"b\\\\\": 2, \\\\\"c\\\\\": \\\\\"x', allow_partial=True) == snapshot([{'a': 1}])\n    assert v.validate_json(\n        '[\"{\\\\\"a\\\\\": 1}\", \"{\\\\\"b\\\\\": 2, \\\\\"c\\\\\": \\\\\"x', allow_partial='trailing-strings'\n    ) == snapshot([{'a': 1}, {'b': 2, 'c': 'x'}])", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pytorch/pytorch", "data_split": "train", "example_id": "000230", "source": "def test_norm_for_two_arrays(self):\n        np.random.seed(456)\n        for norm_type, norm in norm_type_under_test.items():\n            element_types = get_element_types(norm_type)\n            for shape, element_type in product(shapes, element_types):\n                first = generate_vector(shape, element_type)\n                second = generate_vector(shape, element_type)\n                expected = norm(first, second)\n                actual = cv.norm(first, second, norm_type)\n                self.assertAlmostEqual(\n                    expected, actual, places=2,\n                    msg='Arrays {0} {1} of type {2} and norm {3}'.format(\n                        first, second, element_type.__name__,\n                        norm_name[norm_type]\n                    )\n                )", "target": "def are_ghstack_branches_in_sync(\n    repo: GitRepo, head_ref: str, base_ref: Optional[str] = None\n) -> bool:\n    orig_ref = re.sub(r\"/head$\", \"/orig\", head_ref)\n    if base_ref is None:\n        base_ref = re.sub(r\"/head$\", \"/base\", head_ref)\n    orig_diff_sha = _shasum(repo.diff(f\"{repo.remote}/{orig_ref}\"))\n    head_diff_sha = _shasum(\n        repo.diff(\n            base_ref if is_commit_hash(base_ref) else f\"{repo.remote}/{base_ref}\",\n            f\"{repo.remote}/{head_ref}\",\n        )\n    )\n    return orig_diff_sha == head_diff_sha", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000231", "source": "def test_is_instance():\n    v = SchemaValidator(cs.is_instance_schema(cls=Foo))\n    foo = Foo()\n    assert v.validate_python(foo) == foo\n    assert v.isinstance_python(foo) is True\n    bar = Bar()\n    assert v.validate_python(bar) == bar\n    s = Spam()\n    assert v.isinstance_python(s) is False\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python(s)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'is_instance_of',\n            'loc': (),\n            'msg': 'Input should be an instance of Foo',\n            'input': s,\n            'ctx': {'class': 'Foo'},\n        }\n    ]\n    with pytest.raises(ValidationError, match='type=is_instance_of'):\n        v.validate_python(Foo)", "target": "def value(self) -> Any:\n            return self.get_value()", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "000232", "source": "def test_json_error():\n    v = SchemaValidator(\n        core_schema.typed_dict_schema(\n            fields={\n                'field_a': core_schema.typed_dict_field(\n                    schema=core_schema.list_schema(items_schema=core_schema.int_schema())\n                )\n            }\n        )\n    )\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_json('{\"field_a\": [123, \"wrong\"]}')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('field_a', 1),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'wrong',\n        }\n    ]", "target": "def main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--op\", default=\"add_op\", dest=\"op\", type=str)\n    parser.add_argument(\n        \"--use-throughput-benchmark\",\n        \"--use_throughput_benchmark\",\n        default=False,\n        dest=\"use_throughput_benchmark\",\n        action=\"store_true\",\n    )\n    parser.add_argument(\"--debug\", default=False, dest=\"debug\", action=\"store_true\")\n    parser.add_argument(\"--save\", default=False, dest=\"save\", action=\"store_true\")\n    parser.add_argument(\n        \"--eager-mode\",\n        \"--eager_mode\",\n        default=False,\n        dest=\"eager_mode\",\n        action=\"store_true\",\n    )\n    parser.add_argument(\n        \"--num-warmup-iters\", \"--num_warmup_iters\", type=int, default=100\n    )\n    parser.add_argument(\"--num-iters\", \"--num_iters\", type=int, default=1000)\n    args = parser.parse_args()\n    if args.op not in SUPPORTED_OPS:\n        print(f\"Op {args.op} is not supported: Supported ops are:{SUPPORTED_OPS}\")\n        return\n    num_warmup_iters = args.num_warmup_iters\n    num_iters = args.num_iters\n    config = BenchmarkConfig(num_warmup_iters, num_iters)\n    graph_mode = True\n    if args.eager_mode:\n        graph_mode = False\n    result = {}\n    if args.op == \"add_op\":\n        num_params = 2\n        module_config = ModuleConfig(add_tensors_loop, None, num_params, graph_mode)\n        benchmark_simple_fn(args, config, module_config, SimpleAddModule, result)\n    print_results(result)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000233", "source": "def test_union_list_bool_int():\n    v = SchemaValidator(\n        core_schema.union_schema(\n            choices=[\n                core_schema.list_schema(items_schema=core_schema.bool_schema()),\n                core_schema.list_schema(items_schema=core_schema.int_schema()),\n            ]\n        )\n    )\n    assert v.validate_python(['true', True, 'no']) == [True, True, False]\n    assert v.validate_python([5, 6, '789']) == [5, 6, 789]\n    assert v.validate_python(['1', '0']) == [1, 0]\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python([3, 'true'])\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'bool_parsing',\n            'loc': ('list[bool]', 0),\n            'msg': 'Input should be a valid boolean, unable to interpret input',\n            'input': 3,\n        },\n        {\n            'type': 'int_parsing',\n            'loc': ('list[int]', 1),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'true',\n        },\n    ]", "target": "def test_not_schema_definition_error():\n    schema = {\n        'type': 'typed-dict',\n        'fields': {\n            f'f_{i}': {'type': 'typed-dict-field', 'schema': {'type': 'nullable', 'schema': {'type': 'int'}}}\n            for i in range(101)\n        },\n    }\n    v = SchemaValidator(schema)\n    assert repr(v).count('TypedDictField') == 101", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000234", "source": "def find_multiple(n: int, k: int) -> int:\n    if n % k == 0:\n        return n\n    return n + k - (n % k)", "target": "def find_multiple(n: int, k: int) -> int:\n    if n % k == 0:\n        return n\n    return n + k - (n % k)", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000235", "source": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "target": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000236", "source": "def test_tz_cmp() -> None:\n    v = SchemaValidator(core_schema.datetime_schema())\n    validated1 = v.validate_python('2022-06-08T12:13:14-12:15')\n    validated2 = v.validate_python('2022-06-08T12:13:14-12:14')\n    assert validated1 > validated2\n    assert validated2 < validated1", "target": "def test_from_attributes_path_error():\n    class PropertyError:\n        @property\n        def foo(self):\n            raise RuntimeError('intentional error')\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'my_field': core_schema.model_field(\n                    validation_alias=[['foo', 'bar', 'bat'], ['foo', 3], ['spam']], schema=core_schema.int_schema()\n                )\n            },\n            from_attributes=True,\n        )\n    )\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python(PropertyError())\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'get_attribute_error',\n            'loc': ('my_field',),\n            'msg': 'Error extracting attribute: RuntimeError: intentional error',\n            'input': HasRepr(IsStr(regex='.+PropertyError object at.+')),\n            'ctx': {'error': 'RuntimeError: intentional error'},\n        }\n    ]", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000237", "source": "def test_bad_default_factory(default_factory, error_message):\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'x': core_schema.model_field(\n                    schema=core_schema.with_default_schema(\n                        schema=core_schema.str_schema(), default_factory=default_factory\n                    )\n                )\n            }\n        )\n    )\n    with pytest.raises(TypeError, match=re.escape(error_message)):\n        v.validate_python({})", "target": "def test_bad_default_factory(default_factory, error_message):\n    v = SchemaValidator(\n        core_schema.typed_dict_schema(\n            fields={\n                'x': core_schema.typed_dict_field(\n                    schema=core_schema.with_default_schema(\n                        schema=core_schema.str_schema(), default_factory=default_factory\n                    )\n                )\n            }\n        )\n    )\n    with pytest.raises(TypeError, match=re.escape(error_message)):\n        v.validate_python({})", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "000238", "source": "def _pydantic_model_metaclass_marker_callback(self, ctx: ClassDefContext) -> None:\n        if self.plugin_config.debug_dataclass_transform:\n            return\n        info_metaclass = ctx.cls.info.declared_metaclass\n        assert info_metaclass, \"callback not passed from 'get_metaclass_hook'\"\n        if getattr(info_metaclass.type, 'dataclass_transform_spec', None):\n            info_metaclass.type.dataclass_transform_spec = None", "target": "def create_function_node(root: NamespaceNode, func_info) -> FunctionNode:\n    func_symbol_name = SymbolName(\n        func_info.namespace.split(\".\") if len(func_info.namespace) else (),\n        func_info.classname.split(\".\") if len(func_info.classname) else (),\n        func_info.name\n    )\n    return create_function_node_in_scope(find_scope(root, func_symbol_name),\n                                         func_info)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000239", "source": "def create_schema_validator(\n    schema: CoreSchema,\n    schema_type: Any,\n    schema_type_module: str,\n    schema_type_name: str,\n    schema_kind: SchemaKind,\n    config: CoreConfig | None = None,\n    plugin_settings: dict[str, Any] | None = None,\n) -> SchemaValidator | PluggableSchemaValidator:\n    from . import SchemaTypePath\n    from ._loader import get_plugins\n    plugins = get_plugins()\n    if plugins:\n        return PluggableSchemaValidator(\n            schema,\n            schema_type,\n            SchemaTypePath(schema_type_module, schema_type_name),\n            schema_kind,\n            config,\n            plugins,\n            plugin_settings or {},\n        )\n    else:\n        return SchemaValidator(schema, config)", "target": "def test_union_container_strictness():\n    v = SchemaValidator(\n        core_schema.definitions_schema(\n            core_schema.typed_dict_schema(\n                {\n                    'b': core_schema.typed_dict_field(\n                        core_schema.union_schema(\n                            [core_schema.definition_reference_schema('int-type'), core_schema.str_schema()]\n                        )\n                    ),\n                    'a': core_schema.typed_dict_field(core_schema.definition_reference_schema('int-type')),\n                }\n            ),\n            [core_schema.int_schema(ref='int-type')],\n        )\n    )\n    assert v.validate_python({'a': 1, 'b': '2'}) == {'a': 1, 'b': '2'}\n    assert v.validate_python({'a': 1, 'b': 2}) == {'a': 1, 'b': 2}\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python({'a': 1, 'b': []})\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'int_type', 'loc': ('b', 'int'), 'msg': 'Input should be a valid integer', 'input': []},\n        {'type': 'string_type', 'loc': ('b', 'str'), 'msg': 'Input should be a valid string', 'input': []},\n    ]", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "000240", "source": "def make_data(self, params):\n        representation, n_jobs = params\n        if representation == \"sparse\":\n            data = _20newsgroups_highdim_dataset()\n        else:\n            data = _20newsgroups_lowdim_dataset()\n        return data", "target": "def make_data(self, params):\n        algorithm, dimension, n_jobs = params\n        if Benchmark.data_size == \"large\":\n            n_components = 40 if dimension == \"low\" else 200\n        else:\n            n_components = 10 if dimension == \"low\" else 50\n        data = _20newsgroups_lowdim_dataset(n_components=n_components)\n        return data", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000241", "source": "def execute(cmd, cwd = None):\n    print(\"Executing: %s in %s\" % (cmd, cwd), file=sys.stderr)\n    print('Executing: ' + ' '.join(cmd))\n    retcode = check_call(cmd, cwd = cwd)\n    if retcode != 0:\n        raise Exception(\"Child returned:\", retcode)", "target": "def execute(cmd, cwd = None, output = None):\n    if not output:\n        print(\"Executing: %s in %s\" % (cmd, cwd), file=sys.stderr)\n        print('Executing: ' + ' '.join(cmd))\n        retcode = check_call(cmd, cwd = cwd)\n        if retcode != 0:\n            raise Exception(\"Child returned:\", retcode)\n    else:\n        with open(output, \"a\") as f:\n            f.flush()\n            p = Popen(cmd, cwd = cwd, stdout = f)\n            os.waitpid(p.pid, 0)", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000242", "source": "def relative_typename(self, root: str) -> Optional[str]:\n            if self.type_node is not None:\n                return self.type_node.relative_typename(root)\n            return None", "target": "def relative_typename(self, root: str) -> Optional[str]:\n            return self.type_node.relative_typename(root)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000243", "source": "def test_validate_max_digits_and_decimal_places() -> None:\n    v = SchemaValidator(cs.decimal_schema(max_digits=5, decimal_places=2))\n    assert v.validate_json('1.23') == Decimal('1.23')\n    assert v.validate_json('123.45') == Decimal('123.45')\n    assert v.validate_json('-123.45') == Decimal('-123.45')\n    with pytest.raises(ValidationError):\n        v.validate_json('1234.56')\n    with pytest.raises(ValidationError):\n        v.validate_json('123.456')\n    with pytest.raises(ValidationError):\n        v.validate_json('123456')\n    with pytest.raises(ValidationError):\n        v.validate_json('abc')", "target": "def test_constraints_schema_validation_error(constraint: str) -> None:\n    with pytest.raises(SchemaError, match=f\"'{constraint}' must be coercible to a timedelta instance\"):\n        SchemaValidator(core_schema.timedelta_schema(**{constraint: 'bad_value'}))", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000244", "source": "def types_separator(self) -> str:\n        return \"\"", "target": "def types_separator(self) -> str:\n        return \", \"", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|opencv/opencv", "data_split": "train", "example_id": "000245", "source": "def bench(factory, X, Y, X_test, Y_test, ref_coef):\n    gc.collect()\n    tstart = time()\n    clf = factory(alpha=alpha).fit(X, Y)\n    delta = time() - tstart\n    print(\"duration: %0.3fs\" % delta)\n    print(\"rmse: %f\" % rmse(Y_test, clf.predict(X_test)))\n    print(\"mean coef abs diff: %f\" % abs(ref_coef - clf.coef_.ravel()).mean())\n    return delta", "target": "def get_output(self, input_blob):\n        tensor = torch.FloatTensor(input_blob)\n        out = self.net.forward(tensor).numpy()\n        return out", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000246", "source": "def test_fromutc(self):\n        for tz in [self.EST, self.ACDT]:\n            utctime = self.DT.replace(tzinfo=tz)\n            local = tz.fromutc(utctime)\n            self.assertEqual(local - utctime, tz.utcoffset(local))\n            self.assertEqual(local, self.DT.replace(tzinfo=timezone.utc))", "target": "def serialize_deque(value, serializer, info: core_schema.SerializationInfo):\n        items = []\n        for index, item in enumerate(value):\n            try:\n                v = serializer(item, index)\n            except PydanticOmit:\n                pass\n            else:\n                items.append(v)\n        if info.mode_is_json():\n            return items\n        else:\n            return deque(items)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|opencv/opencv", "data_split": "train", "example_id": "000247", "source": "def run(in_ys, in_ps, in_rs):\n        return [np.array([ys[0], ps[0], rs[0]]).T for ys, ps, rs in zip(in_ys, in_ps, in_rs)]", "target": "def test_raiseGeneralException(self):\n        with self.assertRaises((cv.error,),\n                            msg='C++ exception is not propagated to Python in the right way') as cm:\n            cv.utils.testRaiseGeneralException()\n        self.assertEqual(str(cm.exception), 'exception text')", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000248", "source": "def area(self) -> bytes:\n            a = self.width * self.height\n            return b'%d' % a", "target": "def area(self, area: float) -> None:\n            self.side = area**0.5", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000249", "source": "def compiled(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, w = args\n        torch._dynamo.mark_dynamic(x, 0)\n        compiled_rms_norm = torch.compile(\n            self.rms_norm_ref, mode=self.compile_mode, fullgraph=True\n        )\n        return lambda: compiled_rms_norm(x, w)", "target": "def compiled(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, w, dy = args\n        compiled_layernorm = torch.compile(\n            self.layernorm_ref, mode=self.compile_mode, fullgraph=True\n        )\n        y = compiled_layernorm(x, w)\n        return lambda: torch.autograd.grad(\n            y, [x, w], grad_outputs=dy, retain_graph=True\n        )", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|scikit-learn/scikit-learn", "data_split": "train", "example_id": "000250", "source": "def test_any_decimal_key():\n    v = SchemaSerializer(core_schema.dict_schema())\n    input_value = {Decimal('123.456'): 1}\n    assert v.to_python(input_value, mode='json') == {'123.456': 1}\n    assert v.to_json(input_value) == b'{\"123.456\":1}'", "target": "def rmse(a, b):\n    return np.sqrt(np.mean((a - b) ** 2))", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000251", "source": "def test_function_no_info():\n    def f(input_value):\n        return input_value + ' Changed'\n    v = SchemaValidator(core_schema.no_info_after_validator_function(f, core_schema.str_schema()))\n    assert v.validate_python('input value') == 'input value Changed'", "target": "def test_tz_comparison():\n    tz = zoneinfo.ZoneInfo('Europe/London')\n    uk_3pm = datetime(2022, 1, 1, 15, 0, 0, tzinfo=tz)\n    v = SchemaValidator(cs.datetime_schema(le=uk_3pm)).validate_python('2022-01-01T16:00:00+01:00')\n    assert v == datetime(2022, 1, 1, 16, 0, 0, tzinfo=timezone(timedelta(hours=1)))\n    v = SchemaValidator(cs.datetime_schema(ge=uk_3pm)).validate_python('2022-01-01T16:00:00+01:00')\n    assert v == datetime(2022, 1, 1, 16, 0, 0, tzinfo=timezone(timedelta(hours=1)))\n    with pytest.raises(ValidationError, match=r'Input should be greater than 2022-01-01T15:00:00Z \\[type=greater_than'):\n        SchemaValidator(cs.datetime_schema(gt=uk_3pm)).validate_python('2022-01-01T16:00:00+01:00')", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "000252", "source": "def is_subclass_schema(\n    cls: type[Any],\n    *,\n    cls_repr: str | None = None,\n    ref: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> IsInstanceSchema:\n    return _dict_not_none(\n        type='is-subclass', cls=cls, cls_repr=cls_repr, ref=ref, metadata=metadata, serialization=serialization\n    )", "target": "def test_copy_file_to_file(self):\n        src = self.tmp_path / \"src.txt\"\n        dst = self.tmp_path / \"out\" / \"dst.txt\"\n        src.write_text(\"hello\")\n        copy(src, dst)\n        self.assertEqual(dst.read_text(), \"hello\")", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "000253", "source": "def filter_dict_schema(*, include: IncExDict | None = None, exclude: IncExDict | None = None) -> IncExDictSerSchema:\n    return _dict_not_none(type='include-exclude-dict', include=include, exclude=exclude)", "target": "def required_usage_imports(self) -> Generator[str, None, None]:\n        if TypeNode.compatible_to_runtime_usage:\n            yield \"import typing as _typing\"\n        yield from super().required_usage_imports", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pytorch/pytorch", "data_split": "train", "example_id": "000254", "source": "def intersectionRate(s1, s2):\n    area, _intersection = cv.intersectConvexConvex(np.array(s1), np.array(s2))\n    return 2 * area / (cv.contourArea(np.array(s1)) + cv.contourArea(np.array(s2)))", "target": "def hardswish_int(a):\n    return a * (a + 3).clamp(0, 6) / 6", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000255", "source": "def ser_x(self, v: Any, _) -> str:\n            assert self.x == 1_000\n            return f'{v:_}'", "target": "def ser_x(data: Model, v: Any, serializer: core_schema.SerializerFunctionWrapHandler, _) -> str:\n        x = serializer(v)\n        assert data['x'] == 1_000\n        return f'{x:_}'", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000256", "source": "def validate_b(\n            cls, v: bytes, nxt: core_schema.ValidatorFunctionWrapHandler, info: core_schema.ValidationInfo\n        ) -> str:\n            assert v == b'hello'\n            v = nxt(v)\n            assert v == 'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return 'hello world!'", "target": "def validate_b(cls, v: bytes, info: core_schema.ValidationInfo) -> bytes:\n            assert v == b'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return b'hello world!'", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pytorch/pytorch", "data_split": "train", "example_id": "000257", "source": "def test_texture_flow(self):\n        img = self.get_sample('samples/data/chessboard.png')\n        gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n        h, w = img.shape[:2]\n        eigen = cv.cornerEigenValsAndVecs(gray, 5, 3)\n        eigen = eigen.reshape(h, w, 3, 2)\n        flow = eigen[:,:,2]\n        d = 300\n        eps = d / 30\n        points =  np.dstack( np.mgrid[d/2:w:d, d/2:h:d] ).reshape(-1, 2)\n        textureVectors = []\n        for x, y in np.int32(points):\n            textureVectors.append(np.int32(flow[y, x]*d))\n        for i in range(len(textureVectors)):\n            self.assertTrue(cv.norm(textureVectors[i], cv.NORM_L2) < eps\n            or abs(cv.norm(textureVectors[i], cv.NORM_L2) - d) < eps)", "target": "def eqStr(self, input: str) -> str:\n        return input", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "000258", "source": "def test_bytes_invalid_cpython():\n    s = SchemaSerializer(core_schema.bytes_schema())\n    with pytest.raises(UnicodeDecodeError, match=\"'utf-8' codec can't decode byte 0x81 in position 0: invalid utf-8\"):\n        s.to_python(b'\\x81', mode='json')", "target": "def parseLogFile(filename):\n    log = parse(filename)\n    properties = {\n        attr_name[3:]: attr_value\n        for (attr_name, attr_value) in log.documentElement.attributes.items()\n        if attr_name.startswith('cv_')\n    }\n    tests = list(map(TestInfo, log.getElementsByTagName(\"testcase\")))\n    return TestRunInfo(properties, tests)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pytorch/pytorch", "data_split": "train", "example_id": "000259", "source": "def test_custom_op_boundingRect(self):\n            points = [(0,0), (0,1), (1,0), (1,1)]\n            expected = cv.boundingRect(np.array(points))\n            g_pts = cv.GArray.Point()\n            g_br  = GBoundingRect.on(g_pts)\n            comp  = cv.GComputation(cv.GIn(g_pts), cv.GOut(g_br))\n            pkg = cv.gapi.kernels(GBoundingRectImpl)\n            actual = comp.apply(cv.gin(points), args=cv.gapi.compile_args(pkg))\n            self.assertEqual(0.0, cv.norm(expected, actual, cv.NORM_INF))", "target": "def read_probot_config() -> dict[str, Any]:\n    with (GITHUB_DIR / \"pytorch-probot.yml\").open(\"r\") as f:\n        return cast(dict[str, Any], yaml.safe_load(f))", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000260", "source": "def resolve_type_nodes(self, root: ASTNode) -> None:\n        try:\n            self.type_node.resolve(root)\n        except TypeResolutionError as e:\n            raise TypeResolutionError(\n                'Failed to resolve \"{}\" property'.format(self.name)\n            ) from e", "target": "def resolve_type_nodes(self, root: ASTNode) -> None:\n        errors = []\n        for child in itertools.chain(self.properties,\n                                     self.functions.values(),\n                                     self.classes.values()):\n            try:\n                try:\n                    child.resolve_type_nodes(self)\n                except TypeResolutionError:\n                    child.resolve_type_nodes(root)\n            except TypeResolutionError as e:\n                errors.append(str(e))\n        if len(errors) > 0:\n            raise TypeResolutionError(\n                'Failed to resolve \"{}\" class against \"{}\". Errors: {}'.format(\n                    self.full_export_name, root.full_export_name, errors\n                )\n            )", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000261", "source": "def forward(self, x):\n        for i, l in enumerate(self.linears):\n            x = self.linears[i // 2](x) + l(x)\n        return x", "target": "def forward(self, input):\n        mu, sigma = self.compute_layernorm_stats(input)\n        return (input - mu) / sigma * self.weight + self.bias", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000262", "source": "def get_memory_bytes(self, args, kwargs) -> int:\n        x, w = args\n        M, N = x.shape\n        return 2 * M * N * x.dtype.itemsize + N * w.dtype.itemsize", "target": "def get_memory_bytes(self, args, kwargs) -> int:\n        x, w, dy = args\n        M, N = x.shape\n        return (\n            2 * M * N * x.dtype.itemsize\n            + 2 * N * w.dtype.itemsize\n            + M * N * dy.dtype.itemsize\n        )", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "000263", "source": "def torchao_optimize_ctx(quantization: str):\n    from torchao.quantization.quant_api import (\n        autoquant,\n        int4_weight_only,\n        int8_dynamic_activation_int8_weight,\n        int8_weight_only,\n        quantize_,\n    )\n    from torchao.utils import unwrap_tensor_subclass\n    def inner(model_iter_fn: Callable):\n        def _torchao_apply(module: torch.nn.Module, example_inputs: Any):\n            if getattr(module, \"_quantized\", None) is None:\n                if quantization == \"int8dynamic\":\n                    quantize_(\n                        module,\n                        int8_dynamic_activation_int8_weight(),\n                        set_inductor_config=False,\n                    )\n                elif quantization == \"int8weightonly\":\n                    quantize_(module, int8_weight_only(), set_inductor_config=False)\n                elif quantization == \"int4weightonly\":\n                    quantize_(module, int4_weight_only(), set_inductor_config=False)\n                if quantization == \"autoquant\":\n                    autoquant(module, error_on_unseen=False, set_inductor_config=False)\n                    if isinstance(example_inputs, dict):\n                        module(**example_inputs)\n                    else:\n                        module(*example_inputs)\n                    from torchao.quantization.autoquant import AUTOQUANT_CACHE\n                    if len(AUTOQUANT_CACHE) == 0:\n                        raise Exception(\n                            \"NotAutoquantizable\"\n                            f\"Found no autoquantizable layers in model {type(module)}, stopping autoquantized run\"\n                        )\n                else:\n                    unwrap_tensor_subclass(module)\n                setattr(module, \"_quantized\", True)\n            model_iter_fn(module, example_inputs)\n        return _torchao_apply\n    return inner", "target": "def test_smart_union_does_nested_dataclass_field_counting() -> None:\n    @dataclass\n    class SubModelA:\n        x: int = 1\n    @dataclass\n    class SubModelB:\n        y: int = 2\n    @dataclass\n    class ModelA:\n        sub: SubModelA\n    @dataclass\n    class ModelB:\n        sub: SubModelB\n    dc_a_schema = core_schema.dataclass_schema(\n        ModelA,\n        core_schema.dataclass_args_schema(\n            'ModelA',\n            [\n                core_schema.dataclass_field(\n                    'sub',\n                    core_schema.with_default_schema(\n                        core_schema.dataclass_schema(\n                            SubModelA,\n                            core_schema.dataclass_args_schema(\n                                'SubModelA',\n                                [\n                                    core_schema.dataclass_field(\n                                        'x', core_schema.with_default_schema(core_schema.int_schema(), default=1)\n                                    )\n                                ],\n                            ),\n                            ['x'],\n                        ),\n                        default=SubModelA(),\n                    ),\n                )\n            ],\n        ),\n        ['sub'],\n    )\n    dc_b_schema = core_schema.dataclass_schema(\n        ModelB,\n        core_schema.dataclass_args_schema(\n            'ModelB',\n            [\n                core_schema.dataclass_field(\n                    'sub',\n                    core_schema.with_default_schema(\n                        core_schema.dataclass_schema(\n                            SubModelB,\n                            core_schema.dataclass_args_schema(\n                                'SubModelB',\n                                [\n                                    core_schema.dataclass_field(\n                                        'y', core_schema.with_default_schema(core_schema.int_schema(), default=2)\n                                    )\n                                ],\n                            ),\n                            ['y'],\n                        ),\n                        default=SubModelB(),\n                    ),\n                )\n            ],\n        ),\n        ['sub'],\n    )\n    for choices in permute_choices([dc_a_schema, dc_b_schema]):\n        validator = SchemaValidator(core_schema.union_schema(choices=choices))\n        assert isinstance(validator.validate_python({'sub': {'x': 1}}), ModelA)\n        assert isinstance(validator.validate_python({'sub': {'y': 3}}), ModelB)\n        assert isinstance(validator.validate_python({'sub': {}}), choices[0]['cls'])", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "000264", "source": "def make_data(self, params):\n        (representation,) = params\n        if representation == \"dense\":\n            data = _synth_regression_dataset(n_samples=100000, n_features=200)\n        else:\n            data = _synth_regression_sparse_dataset(\n                n_samples=100000, n_features=1000, density=0.01\n            )\n        return data", "target": "def make_data(self, params):\n        algorithm, dimension, n_jobs = params\n        if Benchmark.data_size == \"large\":\n            n_components = 40 if dimension == \"low\" else 200\n        else:\n            n_components = 10 if dimension == \"low\" else 50\n        data = _20newsgroups_lowdim_dataset(n_components=n_components)\n        return data", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000265", "source": "def eager(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        (x,) = args\n        return lambda: F.softmax(x, dim=-1)", "target": "def eager(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, dy = args\n        y = F.softmax(x, dim=-1)\n        return lambda: torch.autograd.grad(y, x, grad_outputs=dy, retain_graph=True)", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000266", "source": "def predict(self, samples):\n        _retval, results, _neigh_resp, _dists = self.model.findNearest(samples, k = 10)\n        return results.ravel()", "target": "def predict(self, samples):\n        _ret, resp = self.model.predict(samples)\n        return resp.argmax(-1)", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "000267", "source": "def is_pointer_type(typename: str) -> bool:\n    return typename.endswith(\"Ptr\") or typename.endswith(\"*\") \\\n        or typename.startswith(\"Ptr\")", "target": "def test_simple_serializers_fallback(schema_type):\n    s = SchemaSerializer({'type': schema_type})\n    with pytest.warns(\n        UserWarning,\n        match=rf'Expected `{schema_type}` - serialized value may not be as expected \\[input_value=\\[1, 2, 3\\], input_type=list\\]',\n    ):\n        assert s.to_python([1, 2, 3]) == [1, 2, 3]\n    with pytest.warns(\n        UserWarning,\n        match=rf\"Expected `{schema_type}` - serialized value may not be as expected \\[input_value=\\[1, 2, b'bytes'\\], input_type=list\\]\",\n    ):\n        assert s.to_python([1, 2, b'bytes'], mode='json') == [1, 2, 'bytes']\n    with pytest.warns(\n        UserWarning,\n        match=rf'Expected `{schema_type}` - serialized value may not be as expected \\[input_value=\\[1, 2, 3\\], input_type=list\\]',\n    ):\n        assert s.to_json([1, 2, 3]) == b'[1,2,3]'", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|scikit-learn/scikit-learn", "data_split": "train", "example_id": "000268", "source": "def test_tagged_union_with_aliases() -> None:\n    @dataclasses.dataclass\n    class ModelA:\n        field: int\n        tag: Literal['a'] = 'a'\n    @dataclasses.dataclass\n    class ModelB:\n        field: int\n        tag: Literal['b'] = 'b'\n    s = SchemaSerializer(\n        core_schema.tagged_union_schema(\n            choices={\n                'a': core_schema.dataclass_schema(\n                    ModelA,\n                    core_schema.dataclass_args_schema(\n                        'ModelA',\n                        [\n                            core_schema.dataclass_field(name='field', schema=core_schema.int_schema()),\n                            core_schema.dataclass_field(\n                                name='tag',\n                                schema=core_schema.literal_schema(['a']),\n                                validation_alias='TAG',\n                                serialization_alias='TAG',\n                            ),\n                        ],\n                    ),\n                    ['field', 'tag'],\n                ),\n                'b': core_schema.dataclass_schema(\n                    ModelB,\n                    core_schema.dataclass_args_schema(\n                        'ModelB',\n                        [\n                            core_schema.dataclass_field(name='field', schema=core_schema.int_schema()),\n                            core_schema.dataclass_field(\n                                name='tag',\n                                schema=core_schema.literal_schema(['b']),\n                                validation_alias='TAG',\n                                serialization_alias='TAG',\n                            ),\n                        ],\n                    ),\n                    ['field', 'tag'],\n                ),\n            },\n            discriminator=[['tag'], ['TAG']],\n        )\n    )\n    assert 'TaggedUnionSerializer' in repr(s)\n    model_a = ModelA(field=1)\n    model_b = ModelB(field=1)\n    assert s.to_python(model_a, by_alias=True) == {'field': 1, 'TAG': 'a'}\n    assert s.to_python(model_b, by_alias=True) == {'field': 1, 'TAG': 'b'}", "target": "def heldout_score(clf, X_test, y_test):\n    score = np.zeros((n_estimators,), dtype=np.float64)\n    for i, y_proba in enumerate(clf.staged_predict_proba(X_test)):\n        score[i] = 2 * log_loss(y_test, y_proba[:, 1])\n    return score", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "000269", "source": "def weight_path(model_path):\n    assert model_path.endswith('.xml'), \"Wrong topology path was provided\"\n    return model_path[:-3] + 'bin'", "target": "def f(\n        input: Any, serialize: core_schema.SerializerFunctionWrapHandler, info: core_schema.SerializationInfo, /\n    ) -> str:\n        return f'{serialize} {info}'", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "000270", "source": "def pretty_print_core_schema(\n    val: Any,\n    *,\n    console: Console | None = None,\n    max_depth: int | None = None,\n    strip_metadata: bool = True,\n) -> None:\n    from rich.pretty import pprint\n    from pydantic import BaseModel, TypeAdapter\n    from pydantic.dataclasses import is_pydantic_dataclass\n    if (inspect.isclass(val) and issubclass(val, BaseModel)) or is_pydantic_dataclass(val):\n        val = val.__pydantic_core_schema__\n    if isinstance(val, TypeAdapter):\n        val = val.core_schema\n    cleaned_schema = _clean_schema_for_pretty_print(val, strip_metadata=strip_metadata)\n    pprint(cleaned_schema, console=console, max_depth=max_depth)", "target": "def get_normal_nd_mat():\n        shape = (2, 2, 1, 2)\n        cn = 4\n        image = np.zeros(shape + (cn,), np.float64)\n        image[:] = (0.888, 0.111, 0.666, 0.444)\n        return image", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|scikit-learn/scikit-learn", "data_split": "train", "example_id": "000271", "source": "def eqFloat(self, input: float) -> float:\n        return input", "target": "def clean():\n    util.run([sys.executable, \"-m\", \"pip\", \"uninstall\", \"scikit-learn\", \"-y\"])\n    default_meson_build_dir = (\n        f\"build/cp{sys.version_info.major}{sys.version_info.minor}\"\n    )\n    click.secho(\n        f\"removing default Meson build dir: {default_meson_build_dir}\",\n        bold=True,\n        fg=\"bright_blue\",\n    )\n    shutil.rmtree(default_meson_build_dir, ignore_errors=True)", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "000272", "source": "def fit(self, X, y=None, **params):\n        self.fit_transform(X, **params)\n        return self", "target": "def fit(self, X, y=None):\n        self.clusterer_ = clone(self.clusterer)\n        self.classifier_ = clone(self.classifier)\n        y = self.clusterer_.fit_predict(X)\n        self.classifier_.fit(X, y)\n        return self", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000273", "source": "def invalid_schema(ref: str | None = None, metadata: dict[str, Any] | None = None) -> InvalidSchema:\n    return _dict_not_none(type='invalid', ref=ref, metadata=metadata)", "target": "def core_config(self, title: str | None) -> core_schema.CoreConfig:\n        config = self.config_dict\n        if config.get('schema_generator') is not None:\n            warnings.warn(\n                'The `schema_generator` setting has been deprecated since v2.10. This setting no longer has any effect.',\n                PydanticDeprecatedSince210,\n                stacklevel=2,\n            )\n        if (populate_by_name := config.get('populate_by_name')) is not None:\n            if config.get('validate_by_name') is None:\n                config['validate_by_alias'] = True\n                config['validate_by_name'] = populate_by_name\n        if config.get('validate_by_alias') is False and config.get('validate_by_name') is None:\n            config['validate_by_name'] = True\n        if (not config.get('validate_by_alias', True)) and (not config.get('validate_by_name', False)):\n            raise PydanticUserError(\n                'At least one of `validate_by_alias` or `validate_by_name` must be set to True.',\n                code='validate-by-alias-and-name-false',\n            )\n        return core_schema.CoreConfig(\n            **{\n                k: v\n                for k, v in (\n                    ('title', config.get('title') or title or None),\n                    ('extra_fields_behavior', config.get('extra')),\n                    ('allow_inf_nan', config.get('allow_inf_nan')),\n                    ('str_strip_whitespace', config.get('str_strip_whitespace')),\n                    ('str_to_lower', config.get('str_to_lower')),\n                    ('str_to_upper', config.get('str_to_upper')),\n                    ('strict', config.get('strict')),\n                    ('ser_json_timedelta', config.get('ser_json_timedelta')),\n                    ('ser_json_temporal', config.get('ser_json_temporal')),\n                    ('val_temporal_unit', config.get('val_temporal_unit')),\n                    ('ser_json_bytes', config.get('ser_json_bytes')),\n                    ('val_json_bytes', config.get('val_json_bytes')),\n                    ('ser_json_inf_nan', config.get('ser_json_inf_nan')),\n                    ('from_attributes', config.get('from_attributes')),\n                    ('loc_by_alias', config.get('loc_by_alias')),\n                    ('revalidate_instances', config.get('revalidate_instances')),\n                    ('validate_default', config.get('validate_default')),\n                    ('str_max_length', config.get('str_max_length')),\n                    ('str_min_length', config.get('str_min_length')),\n                    ('hide_input_in_errors', config.get('hide_input_in_errors')),\n                    ('coerce_numbers_to_str', config.get('coerce_numbers_to_str')),\n                    ('regex_engine', config.get('regex_engine')),\n                    ('validation_error_cause', config.get('validation_error_cause')),\n                    ('cache_strings', config.get('cache_strings')),\n                    ('validate_by_alias', config.get('validate_by_alias')),\n                    ('validate_by_name', config.get('validate_by_name')),\n                    ('serialize_by_alias', config.get('serialize_by_alias')),\n                    ('url_preserve_empty_path', config.get('url_preserve_empty_path')),\n                )\n                if v is not None\n            }\n        )", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000274", "source": "def setup_caches(self, max_batch_size, max_seq_length):\n        if (\n            self.max_seq_length >= max_seq_length\n            and self.max_batch_size >= max_batch_size\n        ):\n            return\n        head_dim = self.config.dim // self.config.n_head\n        max_seq_length = find_multiple(max_seq_length, 8)\n        self.max_seq_length = max_seq_length\n        self.max_batch_size = max_batch_size\n        for b in self.layers:\n            b.attention.kv_cache = KVCache(\n                max_batch_size, max_seq_length, self.config.n_local_heads, head_dim\n            )\n        self.freqs_cis = precompute_freqs_cis(\n            self.config.block_size,\n            self.config.dim // self.config.n_head,\n            self.config.rope_base,\n        )\n        self.causal_mask = torch.tril(\n            torch.ones(self.max_seq_length, self.max_seq_length, dtype=torch.bool)\n        )", "target": "def setup_caches(self, max_batch_size, max_seq_length):\n        if (\n            self.max_seq_length >= max_seq_length\n            and self.max_batch_size >= max_batch_size\n        ):\n            return\n        head_dim = self.config.dim // self.config.n_head\n        max_seq_length = find_multiple(max_seq_length, 8)\n        self.max_seq_length = max_seq_length\n        self.max_batch_size = max_batch_size\n        for b in self.layers:\n            b.attention.kv_cache = KVCache(\n                max_batch_size, max_seq_length, self.config.n_local_heads, head_dim\n            )\n        self.freqs_cis = precompute_freqs_cis(\n            self.config.block_size,\n            self.config.dim // self.config.n_head,\n            self.config.rope_base,\n        )\n        self.causal_mask = torch.tril(\n            torch.ones(self.max_seq_length, self.max_seq_length, dtype=torch.bool)\n        )", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000275", "source": "def time_schema(\n    *,\n    strict: bool | None = None,\n    le: time | None = None,\n    ge: time | None = None,\n    lt: time | None = None,\n    gt: time | None = None,\n    tz_constraint: Literal['aware', 'naive'] | int | None = None,\n    microseconds_precision: Literal['truncate', 'error'] = 'truncate',\n    ref: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> TimeSchema:\n    return _dict_not_none(\n        type='time',\n        strict=strict,\n        le=le,\n        ge=ge,\n        lt=lt,\n        gt=gt,\n        tz_constraint=tz_constraint,\n        microseconds_precision=microseconds_precision,\n        ref=ref,\n        metadata=metadata,\n        serialization=serialization,\n    )", "target": "def test_tuple_fix_extra(input_value, expected):\n    v = SchemaValidator(\n        core_schema.tuple_schema(\n            items_schema=[core_schema.int_schema(), core_schema.str_schema(), core_schema.str_schema()],\n            variadic_item_index=2,\n        )\n    )\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)) as exc_info:\n            v.validate_python(input_value)\n        assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        assert v.validate_python(input_value) == expected", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "000276", "source": "def test_model_root():\n    s = SchemaSerializer(core_schema.model_schema(RootModel, core_schema.int_schema(), root_model=True))\n    print(plain_repr(s))\n    assert 'has_extra:false' in plain_repr(s)\n    assert s.to_python(RootModel(1)) == 1\n    assert s.to_python(RootSubModel(1)) == 1\n    j = s.to_json(RootModel(1))\n    if on_pypy:\n        assert json.loads(j) == 1\n    else:\n        assert j == b'1'\n    assert json.loads(s.to_json(RootSubModel(1))) == 1", "target": "def main():\n    g = huge_graph()\n    def fn():\n        for n in g.graph.nodes:\n            pass\n    t = min(timeit.repeat(fn, number=K, repeat=3))\n    print(f\"iterating over {N * K} FX nodes took {t:.1f}s ({N * K / t:.0f} nodes/s)\")", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pytorch/pytorch", "data_split": "train", "example_id": "000277", "source": "def multiply(x, y):\n        return x * y", "target": "def generate_dataclass_help(cls) -> str:\n    if not is_dataclass(cls):\n        raise TypeError(f\"{cls} is not a dataclass\")\n    def get_value(f):\n        if f.default is not MISSING:\n            return f.default\n        if f.default_factory is not MISSING:\n            try:\n                return f.default_factory()\n            except Exception as e:\n                return f\"<error: {e}>\"\n        return \"<required>\"\n    lines = [f\"{f.name:<22} = {repr(get_value(f))}\" for f in fields(cls)]\n    return indent(\"\\n\".join(lines), \"    \")", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pytorch/pytorch", "data_split": "train", "example_id": "000278", "source": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "target": "def forward(self, x):\n        r\n        x = x + self.pe[: x.size(0), :]\n        return self.dropout(x)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|opencv/opencv", "data_split": "train", "example_id": "000279", "source": "def test_write_read_dictionary(self):\n        try:\n            aruco_dict = cv.aruco.getPredefinedDictionary(cv.aruco.DICT_5X5_50)\n            markers_gold = aruco_dict.bytesList\n            fd, filename = tempfile.mkstemp(prefix=\"opencv_python_aruco_dict_\", suffix=\".yml\")\n            os.close(fd)\n            fs_write = cv.FileStorage(filename, cv.FileStorage_WRITE)\n            aruco_dict.writeDictionary(fs_write)\n            fs_write.release()\n            aruco_dict = cv.aruco.getPredefinedDictionary(cv.aruco.DICT_6X6_250)\n            fs_read = cv.FileStorage(filename, cv.FileStorage_READ)\n            aruco_dict.readDictionary(fs_read.root())\n            fs_read.release()\n            self.assertEqual(aruco_dict.markerSize, 5)\n            self.assertEqual(aruco_dict.maxCorrectionBits, 3)\n            np.testing.assert_array_equal(aruco_dict.bytesList, markers_gold)\n        finally:\n            if os.path.exists(filename):\n                os.remove(filename)", "target": "def test_charuco_match_image_points(self):\n        aruco_dict = cv.aruco.getPredefinedDictionary(cv.aruco.DICT_4X4_50)\n        board_size = (3, 4)\n        board = cv.aruco.CharucoBoard(board_size, 5.0, 1.0, aruco_dict)\n        chessboard_corners = np.array(board.getChessboardCorners())[:, :2]\n        chessboard_ids = board.getIds()\n        obj_points, img_points = board.matchImagePoints(chessboard_corners, chessboard_ids)\n        self.assertEqual(chessboard_corners.shape[0], obj_points.shape[0])\n        self.assertEqual(img_points.shape[0], obj_points.shape[0])\n        self.assertEqual(2, img_points.shape[2])\n        np.testing.assert_array_equal(chessboard_corners, obj_points[:, :, :2].reshape(-1, 2))", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "000280", "source": "def make_data(self, params):\n        (method,) = params\n        n_samples = 500 if method == \"exact\" else None\n        return _digits_dataset(n_samples=n_samples)", "target": "def make_data(self, params):\n        (representation,) = params\n        if representation == \"dense\":\n            data = _synth_regression_dataset(n_samples=100000, n_features=200)\n        else:\n            data = _synth_regression_sparse_dataset(\n                n_samples=100000, n_features=1000, density=0.01\n            )\n        return data", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "000281", "source": "def test_strict_union_flag() -> None:\n    v = SchemaValidator(core_schema.union_schema(choices=[core_schema.bool_schema(), core_schema.int_schema()]))\n    assert v.validate_python(1, strict=True) == 1\n    assert v.validate_python(123, strict=True) == 123\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python('123', strict=True)\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'bool_type', 'loc': ('bool',), 'msg': 'Input should be a valid boolean', 'input': '123'},\n        {'type': 'int_type', 'loc': ('int',), 'msg': 'Input should be a valid integer', 'input': '123'},\n    ]", "target": "def test_texture_flow(self):\n        img = self.get_sample('samples/data/chessboard.png')\n        gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n        h, w = img.shape[:2]\n        eigen = cv.cornerEigenValsAndVecs(gray, 5, 3)\n        eigen = eigen.reshape(h, w, 3, 2)\n        flow = eigen[:,:,2]\n        d = 300\n        eps = d / 30\n        points =  np.dstack( np.mgrid[d/2:w:d, d/2:h:d] ).reshape(-1, 2)\n        textureVectors = []\n        for x, y in np.int32(points):\n            textureVectors.append(np.int32(flow[y, x]*d))\n        for i in range(len(textureVectors)):\n            self.assertTrue(cv.norm(textureVectors[i], cv.NORM_L2) < eps\n            or abs(cv.norm(textureVectors[i], cv.NORM_L2) - d) < eps)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "000282", "source": "def _dedent_source_lines(source: list[str]) -> str:\n    dedent_source = textwrap.dedent(''.join(source))\n    if dedent_source.startswith((' ', '\\t')):\n        dedent_source = f'def dedent_workaround():\\n{dedent_source}'\n    return dedent_source", "target": "def bench(name, fn, requires_grad):\n    torch._dynamo.reset()\n    x = torch.randn(1, requires_grad=requires_grad)\n    start = time.perf_counter()\n    for _ in range(3):\n        fn(x)\n    end = time.perf_counter()\n    results = timeit.repeat(lambda: fn(x), number=1000, repeat=1000)\n    print(f\"{name} {np.median(results) * 1000:.1f}us (warmup={end - start:.1f}s)\")", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000283", "source": "def test_time_strict(input_value, expected):\n    v = SchemaValidator(core_schema.time_schema(strict=True))\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            v.validate_python(input_value)\n    else:\n        output = v.validate_python(input_value)\n        assert output == expected", "target": "def check_tz_aware(v: object) -> bool:\n                    assert isinstance(v, datetime.datetime)\n                    return v.tzinfo is not None", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "000284", "source": "def compare_pypi_to_torch_versions(\n    package: str, pypi_version: str, torch_version: str\n) -> None:\n    if pypi_version is None:\n        raise RuntimeError(f\"Can't find {package} in PyPI for Torch: {torch_version}\")\n    if pypi_version.startswith(torch_version):\n        print(f\"Found matching {package}. Torch: {torch_version} PyPI {pypi_version}\")\n    else:\n        raise RuntimeError(\n            f\"Wrong {package} version. Torch: {torch_version} PyPI: {pypi_version}\"\n        )", "target": "def test_custom_serializer():\n    s = SchemaSerializer(core_schema.any_schema(serialization=core_schema.simple_ser_schema('json')))\n    assert s.to_python({1: 2}) == {1: 2}\n    assert s.to_python({1: 2}, mode='json') == {'1': 2}\n    assert s.to_python({1: 2}, mode='json', round_trip=True) == '{\"1\":2}'\n    assert s.to_json({1: 2}) == b'{\"1\":2}'\n    assert s.to_json({1: 2}, round_trip=True) == b'\"{\\\\\"1\\\\\":2}\"'", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000285", "source": "def test_deep():\n    v = SchemaSerializer(\n        core_schema.typed_dict_schema(\n            {\n                'a': core_schema.typed_dict_field(core_schema.int_schema()),\n                'b': core_schema.typed_dict_field(\n                    core_schema.definitions_schema(\n                        core_schema.typed_dict_schema(\n                            {\n                                'c': core_schema.typed_dict_field(core_schema.int_schema()),\n                                'd': core_schema.typed_dict_field(core_schema.definition_reference_schema('foobar')),\n                            }\n                        ),\n                        [\n                            core_schema.int_schema(\n                                ref='foobar', serialization=core_schema.to_string_ser_schema(when_used='always')\n                            )\n                        ],\n                    )\n                ),\n            }\n        )\n    )\n    assert v.to_python({'a': 1, 'b': {'c': 2, 'd': 3}}) == {'a': 1, 'b': {'c': 2, 'd': '3'}}", "target": "def test_deep():\n    v = SchemaValidator(\n        core_schema.typed_dict_schema(\n            {\n                'a': core_schema.typed_dict_field(core_schema.int_schema()),\n                'b': core_schema.typed_dict_field(\n                    core_schema.definitions_schema(\n                        core_schema.typed_dict_schema(\n                            {\n                                'c': core_schema.typed_dict_field(core_schema.int_schema()),\n                                'd': core_schema.typed_dict_field(core_schema.definition_reference_schema('foobar')),\n                            }\n                        ),\n                        [core_schema.str_schema(ref='foobar')],\n                    )\n                ),\n            }\n        )\n    )\n    assert v.validate_python({'a': 1, 'b': {'c': 2, 'd': b'dd'}}) == {'a': 1, 'b': {'c': 2, 'd': 'dd'}}", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000286", "source": "def test_function_wrap():\n    def f(input_value, validator, info):\n        return validator(input_value=input_value) + ' Changed'\n    v = SchemaValidator(core_schema.with_info_wrap_validator_function(f, core_schema.str_schema()))\n    assert v.validate_python('input value') == 'input value Changed'", "target": "def test_alias_build_error(alias_schema, error):\n    with pytest.raises(SchemaError, match=error):\n        SchemaValidator(\n            schema={\n                'type': 'typed-dict',\n                'fields': {'field_a': {'type': 'typed-dict-field', 'schema': {'type': 'int'}, **alias_schema}},\n            }\n        )", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000287", "source": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "target": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000288", "source": "def get_memory_bytes(self, args, kwargs) -> int:\n        x, target = args\n        M, N = x.shape\n        dtype = x.dtype\n        return (M * N + M + M) * dtype.itemsize", "target": "def get_memory_bytes(self, args, kwargs) -> int:\n        x, target, dloss = args\n        M, N = x.shape\n        return (\n            2 * M * N * x.dtype.itemsize\n            + M * target.dtype.itemsize\n            + M * dloss.dtype.itemsize\n        )", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000289", "source": "def validate_b(cls, v: bytes, info: core_schema.ValidationInfo) -> str:\n            assert v == b'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return 'hello world!'", "target": "def validate_b(\n            cls, v: bytes, nxt: core_schema.ValidatorFunctionWrapHandler, info: core_schema.ValidationInfo\n        ) -> str:\n            assert v == b'hello'\n            v = nxt(v)\n            assert v == 'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return 'hello world!'", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000290", "source": "def get_shapes(self) -> tuple[tuple[int, ...], ...]:\n        return (\n            (32768, 256),\n            (32768, 512),\n            (32768, 1024),\n            (32768, 2048),\n            (32768, 4096),\n            (32768, 8192),\n            (32768, 16384),\n            (32768, 32768),\n            (32768, 65536),\n            (16384, 131072),\n            (8192, 262144),\n        )", "target": "def get_shapes(self) -> tuple[tuple[int, ...], ...]:\n        return (\n            (32768, 256),\n            (32768, 512),\n            (32768, 1024),\n            (32768, 2048),\n            (32768, 4096),\n            (32768, 8192),\n            (32768, 16384),\n            (32768, 32768),\n            (32768, 65536),\n        ) + extra_shapes_for_norm", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000291", "source": "def required_usage_imports(self) -> Generator[str, None, None]:\n        yield \"import typing as _typing\"", "target": "def required_usage_imports(self) -> Generator[str, None, None]:\n        yield \"import cv2.typing\"", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000292", "source": "def test_only_validate_by_name(py_and_json) -> None:\n    v = py_and_json(\n        cs.arguments_v3_schema(\n            [\n                cs.arguments_v3_parameter(\n                    name='a', schema=cs.str_schema(), alias='FieldA', mode='positional_or_keyword'\n                ),\n            ],\n            validate_by_name=True,\n            validate_by_alias=False,\n        )\n    )\n    assert v.validate_test(ArgsKwargs((), {'a': 'hello'})) == ((), {'a': 'hello'})\n    assert v.validate_test({'a': 'hello'}) == (('hello',), {})\n    with pytest.raises(ValidationError, match=r'a\\n +Missing required argument \\[type=missing_argument,'):\n        assert v.validate_test(ArgsKwargs((), {'FieldA': 'hello'}))\n    with pytest.raises(ValidationError, match=r'a\\n +Missing required argument \\[type=missing_argument,'):\n        assert v.validate_test({'FieldA': 'hello'})", "target": "def test_only_validate_by_name(py_and_json) -> None:\n    v = py_and_json(\n        {\n            'type': 'model-fields',\n            'fields': {'field_a': {'validation_alias': 'FieldA', 'type': 'model-field', 'schema': {'type': 'int'}}},\n        },\n        config=CoreConfig(validate_by_name=True, validate_by_alias=False),\n    )\n    assert v.validate_test({'field_a': '123'}) == ({'field_a': 123}, None, {'field_a'})\n    with pytest.raises(ValidationError, match=r'field_a\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'FieldA': '123'})", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000293", "source": "def constants(self) -> Dict[str, ConstantNode]:\n        return self._children[ASTNodeType.Constant]", "target": "def constants(self) -> Dict[str, ConstantNode]:\n        return self._children[ASTNodeType.Constant]", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|opencv/opencv", "data_split": "train", "example_id": "000294", "source": "def svd_timing(\n    X, n_comps, n_iter, n_oversamples, power_iteration_normalizer=\"auto\", method=None\n):\n    print(\"... running SVD ...\")\n    if method != \"fbpca\":\n        gc.collect()\n        t0 = time()\n        U, mu, V = randomized_svd(\n            X,\n            n_comps,\n            n_oversamples=n_oversamples,\n            n_iter=n_iter,\n            power_iteration_normalizer=power_iteration_normalizer,\n            random_state=random_state,\n            transpose=False,\n        )\n        call_time = time() - t0\n    else:\n        gc.collect()\n        t0 = time()\n        U, mu, V = fbpca.pca(\n            X, n_comps, raw=True, n_iter=n_iter, l=n_oversamples + n_comps\n        )\n        call_time = time() - t0\n    return U, mu, V, call_time", "target": "def type_format(self) -> str:\n        if TypeNode.compatible_to_runtime_usage:\n            return \"_typing.Tuple[{}]\"\n        return \"tuple[{}]\"", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000295", "source": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "target": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|pytorch/pytorch", "data_split": "train", "example_id": "000296", "source": "def generate_api_toc_html(kind=\"html\"):\n        soup = context[\"pst_generate_toc_html\"](kind=\"soup\")\n        try:\n            soup.ul.unwrap()\n            soup.li.unwrap()\n            soup.a.decompose()\n            lis = soup.ul.select(\"li.toc-h2\")\n            main_li = lis[0]\n            meth_list = main_li.ul\n            if meth_list is not None:\n                meth_list[\"class\"].append(\"visible\")\n                for meth in meth_list.find_all(\"li\", {\"class\": \"toc-h3\"}):\n                    target = meth.a.code.span\n                    target.string = target.string.split(\".\", 1)[1]\n            return str(soup) if kind == \"html\" else soup\n        except Exception as e:\n            logger.warning(\n                f\"Failed to generate API pagetoc for {pagename}: {e}; falling back\"\n            )\n            return context[\"pst_generate_toc_html\"](kind=kind)", "target": "def secs_to_ms(time_s):\n    return time_s * 1e3", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000297", "source": "def required_modules(self) -> Tuple[str, ...]:\n        return (*self.positive_branch_type.required_modules,\n                *self.negative_branch_type.required_modules)", "target": "def required_modules(self) -> Tuple[str, ...]:\n        return (*chain.from_iterable(item.required_modules for item in self.items),\n                *self._required_modules)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "000298", "source": "def eager(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, target = args\n        return lambda: F.cross_entropy(x, target, reduction=\"none\")", "target": "def apply_known_metadata(annotation: Any, schema: CoreSchema) -> CoreSchema | None:\n    import annotated_types as at\n    from ._validators import NUMERIC_VALIDATOR_LOOKUP, forbid_inf_nan_check\n    schema = schema.copy()\n    schema_update, other_metadata = collect_known_metadata([annotation])\n    schema_type = schema['type']\n    chain_schema_constraints: set[str] = {\n        'pattern',\n        'strip_whitespace',\n        'to_lower',\n        'to_upper',\n        'coerce_numbers_to_str',\n    }\n    chain_schema_steps: list[CoreSchema] = []\n    for constraint, value in schema_update.items():\n        if constraint not in CONSTRAINTS_TO_ALLOWED_SCHEMAS:\n            raise ValueError(f'Unknown constraint {constraint}')\n        allowed_schemas = CONSTRAINTS_TO_ALLOWED_SCHEMAS[constraint]\n        if schema_type in {'function-before', 'function-wrap', 'function-after'} and constraint == 'strict':\n            schema['schema'] = apply_known_metadata(annotation, schema['schema'])\n            return schema\n        if schema_type in allowed_schemas:\n            if constraint == 'union_mode' and schema_type == 'union':\n                schema['mode'] = value\n            else:\n                schema[constraint] = value\n            continue\n        if constraint in chain_schema_constraints:\n            def _apply_constraint_with_incompatibility_info(\n                value: Any, handler: cs.ValidatorFunctionWrapHandler\n            ) -> Any:\n                try:\n                    x = handler(value)\n                except ValidationError as ve:\n                    if 'type' in ve.errors()[0]['type']:\n                        raise TypeError(\n                            f\"Unable to apply constraint '{constraint}' to supplied value {value} for schema of type '{schema_type}'\"\n                        )\n                    raise ve\n                return x\n            chain_schema_steps.append(\n                cs.no_info_wrap_validator_function(\n                    _apply_constraint_with_incompatibility_info, cs.str_schema(**{constraint: value})\n                )\n            )\n        elif constraint in NUMERIC_VALIDATOR_LOOKUP:\n            if constraint in LENGTH_CONSTRAINTS:\n                inner_schema = schema\n                while inner_schema['type'] in {'function-before', 'function-wrap', 'function-after'}:\n                    inner_schema = inner_schema['schema']\n                inner_schema_type = inner_schema['type']\n                if inner_schema_type == 'list' or (\n                    inner_schema_type == 'json-or-python' and inner_schema['json_schema']['type'] == 'list'\n                ):\n                    js_constraint_key = 'minItems' if constraint == 'min_length' else 'maxItems'\n                else:\n                    js_constraint_key = 'minLength' if constraint == 'min_length' else 'maxLength'\n            else:\n                js_constraint_key = constraint\n            schema = cs.no_info_after_validator_function(\n                partial(NUMERIC_VALIDATOR_LOOKUP[constraint], **{constraint: value}), schema\n            )\n            metadata = schema.get('metadata', {})\n            if (existing_json_schema_updates := metadata.get('pydantic_js_updates')) is not None:\n                metadata['pydantic_js_updates'] = {\n                    **existing_json_schema_updates,\n                    **{js_constraint_key: as_jsonable_value(value)},\n                }\n            else:\n                metadata['pydantic_js_updates'] = {js_constraint_key: as_jsonable_value(value)}\n            schema['metadata'] = metadata\n        elif constraint == 'allow_inf_nan' and value is False:\n            schema = cs.no_info_after_validator_function(\n                forbid_inf_nan_check,\n                schema,\n            )\n        else:\n            raise RuntimeError(f\"Unable to apply constraint '{constraint}' to schema of type '{schema_type}'\")\n    for annotation in other_metadata:\n        if (annotation_type := type(annotation)) in (at_to_constraint_map := _get_at_to_constraint_map()):\n            constraint = at_to_constraint_map[annotation_type]\n            validator = NUMERIC_VALIDATOR_LOOKUP.get(constraint)\n            if validator is None:\n                raise ValueError(f'Unknown constraint {constraint}')\n            schema = cs.no_info_after_validator_function(\n                partial(validator, {constraint: getattr(annotation, constraint)}), schema\n            )\n            continue\n        elif isinstance(annotation, (at.Predicate, at.Not)):\n            predicate_name = f'{annotation.func.__qualname__!r} ' if hasattr(annotation.func, '__qualname__') else ''\n            if isinstance(annotation, at.Predicate):\n                def val_func(v: Any) -> Any:\n                    predicate_satisfied = annotation.func(v)\n                    if not predicate_satisfied:\n                        raise PydanticCustomError(\n                            'predicate_failed',\n                            f'Predicate {predicate_name}failed',\n                        )\n                    return v\n            else:\n                def val_func(v: Any) -> Any:\n                    predicate_satisfied = annotation.func(v)\n                    if predicate_satisfied:\n                        raise PydanticCustomError(\n                            'not_operation_failed',\n                            f'Not of {predicate_name}failed',\n                        )\n                    return v\n            schema = cs.no_info_after_validator_function(val_func, schema)\n        else:\n            return None\n    if chain_schema_steps:\n        chain_schema_steps = [schema] + chain_schema_steps\n        return cs.chain_schema(chain_schema_steps)\n    return schema", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000299", "source": "def name(self):\n        if self.use_loop:\n            return f\"{self.category()}_loop\"\n        return self.category()", "target": "def name(self) -> str:\n        return \"aten\"", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "000300", "source": "def model_construct(cls, root: RootModelRootType, _fields_set: set[str] | None = None) -> Self:\n        return super().model_construct(root=root, _fields_set=_fields_set)", "target": "def test_copy_dir_into_existing_dir_overwrite_true_merges(self):\n        src = self.tmp_path / \"srcdir\"\n        dst = self.tmp_path / \"destdir\"\n        (src / \"x\").mkdir(parents=True)\n        (src / \"x\" / \"new.txt\").write_text(\"new\")\n        dst.mkdir()\n        (dst / \"existing.txt\").write_text(\"old\")\n        copy(src, dst)\n        self.assertEqual((dst / \"existing.txt\").read_text(), \"old\")\n        self.assertEqual((dst / \"x\" / \"new.txt\").read_text(), \"new\")", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "000301", "source": "def replace_needed(self, file_name: str, soname: str, new_soname: str) -> None:\n        check_call(\n            [\n                \"patchelf\",\n                \"--page-size\",\n                \"65536\",\n                \"--replace-needed\",\n                soname,\n                new_soname,\n                file_name,\n            ]\n        )", "target": "def test_tuple_fail_fast(fail_fast, expected):\n    s = core_schema.tuple_schema(\n        [\n            core_schema.str_schema(),\n            core_schema.int_schema(),\n            core_schema.float_schema(),\n        ],\n        variadic_item_index=None,\n        fail_fast=fail_fast,\n    )\n    v = SchemaValidator(s)\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python(['str', 'not-num', 'again'])\n    assert exc_info.value.errors(include_url=False) == expected", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000302", "source": "def constrain(self: _Pipeline[_InT, _OutT], constraint: annotated_types.Predicate) -> _Pipeline[_InT, _OutT]: ...", "target": "def constrain(\n        self: _Pipeline[_InT, _NewOutInterval], constraint: annotated_types.Interval\n    ) -> _Pipeline[_InT, _NewOutInterval]: ...", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000303", "source": "def quack(self, args, kwargs=None) -> Any:\n        from quack.softmax import softmax\n        assert kwargs is None\n        (x,) = args\n        return lambda: softmax(x)", "target": "def quack(self, args, kwargs=None) -> Any:\n        from quack.softmax import softmax\n        assert kwargs is None\n        x, dy = args\n        y = softmax(x)\n        return lambda: torch.autograd.grad(y, x, grad_outputs=dy, retain_graph=True)", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000304", "source": "def train(self, samples, responses):\n        self.model.train(samples, cv.ml.ROW_SAMPLE, responses)", "target": "def train(self, samples, responses):\n        self.model.setMaxDepth(20)\n        self.model.train(samples, cv.ml.ROW_SAMPLE, responses.astype(int))", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "000305", "source": "def forward(self, idx: Tensor, input_pos: Optional[Tensor] = None) -> Tensor:\n        assert self.freqs_cis is not None, \"Caches must be initialized first\"\n        mask = self.causal_mask[None, None, input_pos]\n        freqs_cis = self.freqs_cis[input_pos]\n        x = self.tok_embeddings(idx)\n        for i, layer in enumerate(self.layers):\n            x = layer(x, input_pos, freqs_cis, mask)\n        x = self.norm(x)\n        logits = self.output(x)\n        return logits", "target": "def test_schema_validator_wrong() -> None:\n    try:\n        SchemaValidator({'type': 'bad'})\n    except SchemaError:\n        pass\n    else:\n        raise AssertionError('SchemaValidator did not raise SchemaError')", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000306", "source": "def validate_b(cls, v: bytes, info: core_schema.ValidationInfo) -> bytes:\n            assert v == b'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return b'hello world!'", "target": "def validate_b(\n            cls, v: bytes, nxt: core_schema.ValidatorFunctionWrapHandler, info: core_schema.ValidationInfo\n        ) -> bytes:\n            assert v == b'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return nxt(b'hello world!')", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|scikit-learn/scikit-learn", "data_split": "train", "example_id": "000307", "source": "def expand_grouped_metadata(annotations: Iterable[Any]) -> Iterable[Any]:\n    import annotated_types as at\n    FieldInfo = import_cached_field_info()\n    for annotation in annotations:\n        if isinstance(annotation, at.GroupedMetadata):\n            yield from annotation\n        elif isinstance(annotation, FieldInfo):\n            yield from annotation.metadata\n            annotation = copy(annotation)\n            annotation.metadata = []\n            yield annotation\n        else:\n            yield annotation", "target": "def variable_batch_size_comparison(data):\n    batch_sizes = [\n        i.astype(int) for i in np.linspace(data.shape[0] // 10, data.shape[0], num=10)\n    ]\n    for n_components in [\n        i.astype(int) for i in np.linspace(data.shape[1] // 10, data.shape[1], num=4)\n    ]:\n        all_times = defaultdict(list)\n        all_errors = defaultdict(list)\n        pca = PCA(n_components=n_components)\n        rpca = PCA(\n            n_components=n_components, svd_solver=\"randomized\", random_state=1999\n        )\n        results_dict = {\n            k: benchmark(est, data) for k, est in [(\"pca\", pca), (\"rpca\", rpca)]\n        }\n        all_times[\"pca\"].extend([results_dict[\"pca\"][\"time\"]] * len(batch_sizes))\n        all_errors[\"pca\"].extend([results_dict[\"pca\"][\"error\"]] * len(batch_sizes))\n        all_times[\"rpca\"].extend([results_dict[\"rpca\"][\"time\"]] * len(batch_sizes))\n        all_errors[\"rpca\"].extend([results_dict[\"rpca\"][\"error\"]] * len(batch_sizes))\n        for batch_size in batch_sizes:\n            ipca = IncrementalPCA(n_components=n_components, batch_size=batch_size)\n            results_dict = {k: benchmark(est, data) for k, est in [(\"ipca\", ipca)]}\n            all_times[\"ipca\"].append(results_dict[\"ipca\"][\"time\"])\n            all_errors[\"ipca\"].append(results_dict[\"ipca\"][\"error\"])\n        plot_batch_times(all_times, n_components, batch_sizes, data)\n        plot_batch_errors(all_errors, n_components, batch_sizes, data)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "000308", "source": "def test_validation_error_multiple(pydantic_version):\n    class MyModel:\n        __slots__ = '__dict__', '__pydantic_fields_set__', '__pydantic_extra__', '__pydantic_private__'\n        field_a: str\n        field_b: int\n    v = SchemaValidator(\n        core_schema.model_schema(\n            cls=MyModel,\n            schema=core_schema.model_fields_schema(\n                fields={\n                    'x': core_schema.model_field(schema=core_schema.float_schema()),\n                    'y': core_schema.model_field(schema=core_schema.int_schema()),\n                }\n            ),\n        )\n    )\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python({'x': 'x' * 60, 'y': 'y'})\n    assert exc_info.value.title == 'MyModel'\n    assert exc_info.value.error_count() == 2\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'float_parsing',\n            'loc': ('x',),\n            'msg': 'Input should be a valid number, unable to parse string as a number',\n            'input': 'x' * 60,\n        },\n        {\n            'type': 'int_parsing',\n            'loc': ('y',),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'y',\n        },\n    ]\n    include_urls = os.environ.get('PYDANTIC_ERRORS_INCLUDE_URL', '1') != 'false'\n    assert repr(exc_info.value) == (\n        '2 validation errors for MyModel\\n'\n        'x\\n'\n        '  Input should be a valid number, unable to parse string as a number '\n        \"[type=float_parsing, input_value='xxxxxxxxxxxxxxxxxxxxxxxx...xxxxxxxxxxxxxxxxxxxxxxx', input_type=str]\\n\"\n        + (\n            f'    For further information visit https://errors.pydantic.dev/{pydantic_version}/v/float_parsing\\n'\n            if include_urls\n            else ''\n        )\n        + 'y\\n'\n        '  Input should be a valid integer, unable to parse string as an integer '\n        \"[type=int_parsing, input_value='y', input_type=str]\"\n        + (\n            f'\\n    For further information visit https://errors.pydantic.dev/{pydantic_version}/v/int_parsing'\n            if include_urls\n            else ''\n        )\n    )", "target": "def clear(self):\n        self.targets = []\n        self.matcher.clear()", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000309", "source": "def test_smart_union_model_field():\n    class ModelA:\n        x: int\n    class ModelB:\n        x: str\n    schema = core_schema.union_schema(\n        [\n            core_schema.model_schema(\n                ModelA, core_schema.model_fields_schema({'x': core_schema.model_field(core_schema.int_schema())})\n            ),\n            core_schema.model_schema(\n                ModelB, core_schema.model_fields_schema({'x': core_schema.model_field(core_schema.str_schema())})\n            ),\n        ]\n    )\n    validator = SchemaValidator(schema)\n    result = validator.validate_python({'x': 1})\n    assert isinstance(result, ModelA)\n    assert result.x == 1\n    result = validator.validate_python({'x': '1'})\n    assert isinstance(result, ModelB)\n    assert result.x == '1'", "target": "def func(*args: Any) -> Any:\n        calls.append(args)\n        return args[0]", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "000310", "source": "def _set_unique_choice_for_values(self, choice: core_schema.CoreSchema, values: Sequence[str | int]) -> None:\n        for discriminator_value in values:\n            if discriminator_value in self._tagged_union_choices:\n                existing_choice = self._tagged_union_choices[discriminator_value]\n                if existing_choice != choice:\n                    raise TypeError(\n                        f'Value {discriminator_value!r} for discriminator '\n                        f'{self.discriminator!r} mapped to multiple choices'\n                    )\n            else:\n                self._tagged_union_choices[discriminator_value] = choice", "target": "def foo(x: int, y: int) -> int:\n            return x + y", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|scikit-learn/scikit-learn", "data_split": "train", "example_id": "000311", "source": "def test_mser(self):\n        img = self.get_sample('cv/mser/puzzle.png', 0)\n        smallImg = [\n         [255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255],\n         [255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255],\n         [255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255],\n         [255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255],\n         [255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255],\n         [255, 255, 255, 255, 255,   0,   0,   0,   0, 255, 255, 255, 255, 255, 255, 255, 255, 255,   0,   0,   0,   0, 255, 255, 255, 255],\n         [255, 255, 255, 255, 255,   0,   0,   0,   0,   0, 255, 255, 255, 255, 255, 255, 255, 255,   0,   0,   0,   0, 255, 255, 255, 255],\n         [255, 255, 255, 255, 255,   0,   0,   0,   0,   0, 255, 255, 255, 255, 255, 255, 255, 255,   0,   0,   0,   0, 255, 255, 255, 255],\n         [255, 255, 255, 255, 255,   0,   0,   0,   0, 255, 255, 255, 255, 255, 255, 255, 255, 255,   0,   0,   0,   0, 255, 255, 255, 255],\n         [255, 255, 255, 255, 255, 255,   0,   0, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,   0,   0, 255, 255, 255, 255, 255],\n         [255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255],\n         [255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255],\n         [255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255],\n         [255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255]\n        ]\n        thresharr = [ 0, 70, 120, 180, 255 ]\n        kDelta = 5\n        mserExtractor = cv.MSER_create()\n        mserExtractor.setDelta(kDelta)\n        np.random.seed(10)\n        for _i in range(100):\n            use_big_image = int(np.random.rand(1,1)*7) != 0\n            invert = int(np.random.rand(1,1)*2) != 0\n            binarize = int(np.random.rand(1,1)*5) != 0 if use_big_image else False\n            blur = int(np.random.rand(1,1)*2) != 0\n            thresh = thresharr[int(np.random.rand(1,1)*5)]\n            src0 = img if use_big_image else np.array(smallImg).astype('uint8')\n            src = src0.copy()\n            kMinArea = 256 if use_big_image else 10\n            kMaxArea = int(src.shape[0]*src.shape[1]/4)\n            mserExtractor.setMinArea(kMinArea)\n            mserExtractor.setMaxArea(kMaxArea)\n            if invert:\n                cv.bitwise_not(src, src)\n            if binarize:\n                _, src = cv.threshold(src, thresh, 255, cv.THRESH_BINARY)\n            if blur:\n                src = cv.GaussianBlur(src, (5, 5), 1.5, 1.5)\n            minRegs = 7 if use_big_image else 2\n            maxRegs = 1000 if use_big_image else 20\n            if binarize and (thresh == 0 or thresh == 255):\n                minRegs = maxRegs = 0\n            msers, boxes = mserExtractor.detectRegions(src)\n            nmsers = len(msers)\n            self.assertEqual(nmsers, len(boxes))\n            self.assertLessEqual(minRegs, nmsers)\n            self.assertGreaterEqual(maxRegs, nmsers)", "target": "def tsne_fit_transform(model, data):\n    transformed = model.fit_transform(data)\n    return transformed, model.n_iter_", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000312", "source": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "target": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "train", "example_id": "000313", "source": "def common_args(parser: argparse.ArgumentParser) -> None:\n    parser.add_argument(\n        \"--shard-id\",\n        type=int,\n        default=1,\n        help=\"a shard id to run, e.g. '0,1,2,3'\",\n    )\n    parser.add_argument(\n        \"--num-shards\",\n        type=int,\n        default=1,\n        help=\"a number of shards to run, e.g. '4'\",\n    )\n    group = parser.add_mutually_exclusive_group(required=True)\n    group.add_argument(\n        \"-tp\",\n        \"--test-plan\",\n        type=str,\n        help=\"a pre-defined test plan to run, e.g. 'basic_correctness_test'\",\n    )", "target": "def main(args):\n    actual = pd.read_csv(args.actual)\n    expected = pd.read_csv(args.expected)\n    failed = []\n    for name in actual[\"name\"]:\n        actual_memory_compression = float(\n            actual.loc[actual[\"name\"] == name][\"compression_ratio\"]\n        )\n        try:\n            expected_memory_compression = float(\n                expected.loc[expected[\"name\"] == name][\"compression_ratio\"]\n            )\n        except TypeError:\n            print(f\"{name:34} is missing from {args.expected}\")\n            continue\n        if actual_memory_compression >= expected_memory_compression * 0.95:\n            status = \"PASS\"\n        else:\n            status = \"FAIL\"\n            failed.append(name)\n        print(\n            f\n        )\n    if failed:\n        print(\n            textwrap.dedent(\n                f\n            )\n        )\n        sys.exit(1)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|scikit-learn/scikit-learn", "data_split": "train", "example_id": "000314", "source": "def test_alias_build_error(alias_schema, error):\n    with pytest.raises(SchemaError, match=error):\n        SchemaValidator(\n            schema={\n                'type': 'typed-dict',\n                'fields': {'field_a': {'type': 'typed-dict-field', 'schema': {'type': 'int'}, **alias_schema}},\n            }\n        )", "target": "def plot_species_distribution(\n    species=(\"bradypus_variegatus_0\", \"microryzomys_minutus_0\"),\n):\n    if len(species) > 2:\n        print(\n            \"Note: when more than two species are provided,\"\n            \" only the first two will be used\"\n        )\n    t0 = time()\n    data = fetch_species_distributions()\n    xgrid, ygrid = construct_grids(data)\n    X, Y = np.meshgrid(xgrid, ygrid[::-1])\n    BV_bunch = create_species_bunch(\n        species[0], data.train, data.test, data.coverages, xgrid, ygrid\n    )\n    MM_bunch = create_species_bunch(\n        species[1], data.train, data.test, data.coverages, xgrid, ygrid\n    )\n    np.random.seed(13)\n    background_points = np.c_[\n        np.random.randint(low=0, high=data.Ny, size=10000),\n        np.random.randint(low=0, high=data.Nx, size=10000),\n    ].T\n    land_reference = data.coverages[6]\n    for i, species in enumerate([BV_bunch, MM_bunch]):\n        print(\"_\" * 80)\n        print(\"Modeling distribution of species '%s'\" % species.name)\n        mean = species.cov_train.mean(axis=0)\n        std = species.cov_train.std(axis=0)\n        train_cover_std = (species.cov_train - mean) / std\n        print(\" - fit OneClassSVM ... \", end=\"\")\n        clf = svm.OneClassSVM(nu=0.1, kernel=\"rbf\", gamma=0.5)\n        clf.fit(train_cover_std)\n        print(\"done.\")\n        plt.subplot(1, 2, i + 1)\n        if basemap:\n            print(\" - plot coastlines using basemap\")\n            m = Basemap(\n                projection=\"cyl\",\n                llcrnrlat=Y.min(),\n                urcrnrlat=Y.max(),\n                llcrnrlon=X.min(),\n                urcrnrlon=X.max(),\n                resolution=\"c\",\n            )\n            m.drawcoastlines()\n            m.drawcountries()\n        else:\n            print(\" - plot coastlines from coverage\")\n            plt.contour(\n                X, Y, land_reference, levels=[-9998], colors=\"k\", linestyles=\"solid\"\n            )\n            plt.xticks([])\n            plt.yticks([])\n        print(\" - predict species distribution\")\n        Z = np.ones((data.Ny, data.Nx), dtype=np.float64)\n        idx = (land_reference > -9999).nonzero()\n        coverages_land = data.coverages[:, idx[0], idx[1]].T\n        pred = clf.decision_function((coverages_land - mean) / std)\n        Z *= pred.min()\n        Z[idx[0], idx[1]] = pred\n        levels = np.linspace(Z.min(), Z.max(), 25)\n        Z[land_reference == -9999] = -9999\n        plt.contourf(X, Y, Z, levels=levels, cmap=plt.cm.Reds)\n        plt.colorbar(format=\"%.2f\")\n        plt.scatter(\n            species.pts_train[\"dd long\"],\n            species.pts_train[\"dd lat\"],\n            s=2**2,\n            c=\"black\",\n            marker=\"^\",\n            label=\"train\",\n        )\n        plt.scatter(\n            species.pts_test[\"dd long\"],\n            species.pts_test[\"dd lat\"],\n            s=2**2,\n            c=\"black\",\n            marker=\"x\",\n            label=\"test\",\n        )\n        plt.legend()\n        plt.title(species.name)\n        plt.axis(\"equal\")\n        pred_background = Z[background_points[0], background_points[1]]\n        pred_test = clf.decision_function((species.cov_test - mean) / std)\n        scores = np.r_[pred_test, pred_background]\n        y = np.r_[np.ones(pred_test.shape), np.zeros(pred_background.shape)]\n        fpr, tpr, thresholds = metrics.roc_curve(y, scores)\n        roc_auc = metrics.auc(fpr, tpr)\n        plt.text(-35, -70, \"AUC: %.3f\" % roc_auc, ha=\"right\")\n        print(\"\\n Area under the ROC curve : %f\" % roc_auc)\n    print(\"\\ntime elapsed: %.2fs\" % (time() - t0))", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|opencv/opencv", "data_split": "train", "example_id": "000315", "source": "def ensure_dir_exists(path: Union[str, Path]) -> Path:\n    path_obj = get_path(path)\n    path_obj.mkdir(parents=True, exist_ok=True)\n    return path_obj", "target": "def test_simple(self):\n        img1 = self.get_sample('stitching/a1.png')\n        img2 = self.get_sample('stitching/a2.png')\n        stitcher = cv.Stitcher.create(cv.Stitcher_PANORAMA)\n        (_result, pano) = stitcher.stitch((img1, img2))\n        self.assertAlmostEqual(pano.shape[0], 685, delta=100, msg=\"rows: %r\" % list(pano.shape))\n        self.assertAlmostEqual(pano.shape[1], 1025, delta=100, msg=\"cols: %r\" % list(pano.shape))", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "000316", "source": "def test_set_as_dict_keys(py_and_json: PyAndJson):\n    v = py_and_json({'type': 'dict', 'keys_schema': {'type': 'set'}, 'values_schema': {'type': 'int'}})\n    with pytest.raises(ValidationError, match=re.escape(\"[type=set_type, input_value='foo', input_type=str]\")):\n        v.validate_test({'foo': 'bar'})", "target": "def unzip_columns(mat):\n        assert isinstance(mat, list)\n        assert isinstance(mat[0], list)\n        layers = len(mat)\n        columns = len(mat[0])\n        return [[mat[layer][col] for layer in range(layers)] for col in range(columns)]", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000317", "source": "def area(self) -> int:\n            return self.width * self.height", "target": "def area(self) -> int:\n            raise ValueError('xxx')", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000318", "source": "def _apply_field_title_generator_to_field_info(\n    title_generator: Callable[[str, FieldInfo], str],\n    field_name: str,\n    field_info: FieldInfo,\n):\n    if field_info.title is None:\n        title = title_generator(field_name, field_info)\n        if not isinstance(title, str):\n            raise TypeError(f'field_title_generator {title_generator} must return str, not {title.__class__}')\n        field_info.title = title", "target": "def test_repeated_ref():\n    with pytest.raises(SchemaError, match='SchemaError: Duplicate ref: `foobar`'):\n        SchemaSerializer(\n            core_schema.tuple_positional_schema(\n                [\n                    core_schema.definitions_schema(\n                        core_schema.definition_reference_schema('foobar'), [core_schema.int_schema(ref='foobar')]\n                    ),\n                    core_schema.definitions_schema(\n                        core_schema.definition_reference_schema('foobar'), [core_schema.int_schema(ref='foobar')]\n                    ),\n                ]\n            )\n        )", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "000319", "source": "def getCMakeArgs(self, arch, target):\n        args = Builder.getCMakeArgs(self, arch, target)\n        args = args + [\n            '-DVISIONOS_ARCH=%s' % arch\n        ]\n        return args", "target": "def test_model_exclude_unset_true(benchmark, fs_model_serializer):\n    m = FieldsSetModel(a=1, b=2, c=3, d=4, e=5, f=6, g=7, h=8, __pydantic_fields_set__={'a', 'b', 'c', 'd', 'e', 'f'})\n    assert fs_model_serializer.to_python(m) == {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8}\n    assert fs_model_serializer.to_python(m, exclude_unset=True) == {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6}\n    @benchmark\n    def r():\n        fs_model_serializer.to_python(m, exclude_unset=True)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|opencv/opencv", "data_split": "train", "example_id": "000320", "source": "def liger(self, args, kwargs=None) -> Any:\n        from liger_kernel.transformers.rms_norm import LigerRMSNorm\n        x, w, dy = args\n        M, N = x.shape\n        liger_rmsnorm = LigerRMSNorm(\n            hidden_size=N, eps=1e-6, casting_mode=\"gemma\"\n        ).cuda()\n        liger_rmsnorm.weight.data.copy_(w)\n        y = liger_rmsnorm(x)\n        return lambda: torch.autograd.grad(\n            y, [x, liger_rmsnorm.weight], grad_outputs=dy, retain_graph=True\n        )", "target": "def read_classes(img_classes_file):\n        result = {}\n        with open(img_classes_file) as file:\n            for l in file.readlines():\n                result[l.split()[0]] = int(l.split()[1])\n        return result", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000321", "source": "def test_filter():\n    s = SchemaSerializer(\n        core_schema.dict_schema(\n            serialization=core_schema.filter_dict_schema(include={'1', '3', '5'}, exclude={'5', '6'})\n        )\n    )\n    assert s.to_python({'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7}) == {'1': 1, '3': 3}", "target": "def test_filter():\n    v = SchemaSerializer(\n        core_schema.list_schema(\n            core_schema.any_schema(), serialization=core_schema.filter_seq_schema(include={1, 3, 5}, exclude={5, 6})\n        )\n    )\n    assert v.to_python([0, 1, 2, 3, 4, 5, 6, 7]) == [1, 3]", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000322", "source": "def forward(self, x):\n        return torch.mm(x, self.weight)", "target": "def forward(self, x):\n        for i, l in enumerate(self.linears):\n            x = self.linears[i // 2](x) + l(x)\n        return x", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "train", "example_id": "000323", "source": "def main():\n    result_path = sys.argv[1]\n    all = [\n        Benchmark(),\n    ]\n    for benchmark in all:\n        benchmark.enable_compile_time_instruction_count().collect_all().append_results(\n            result_path\n        )", "target": "def log_to_file(self, output_filename, *, skip_non_compute_operators=True):\n        sorted_operators = sorted(self.func_db.keys())\n        with open(output_filename, \"w\") as f:\n            for operator in sorted_operators:\n                if skip_non_compute_operators and non_compute_operator(eval(operator)):\n                    continue\n                f.write(f\"Operator: {operator}\\n\")\n                operator_inputs = self.func_db[operator]\n                for inps, count in operator_inputs.items():\n                    f.write(f\"cnt: {count}, \")\n                    for dtype_abbr in dtype_abbrs.values():\n                        inps = inps.replace(\"'\" + dtype_abbr + \"'\", dtype_abbr)\n                    f.write(inps)\n                    f.write(\"\\n\")", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "000324", "source": "def f(value, serializer, _info):\n        return f'result={serializer(len(value))} repr={serializer!r}'", "target": "def description(self):\n        return \"information at https://github.com/pytorch/pytorch/issues/134133\"", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "train", "example_id": "000325", "source": "def calculate_table_data(results: list[ExperimentResults]) -> dict:\n    table_data = defaultdict(list)\n    aten_perf: Optional[float] = None\n    for experiment_result in results:\n        for key, value in experiment_result.asdict().items():\n            assert key in UNITS, f\"Unknown key {key}\"\n            table_data[key + UNITS[key]].append(value)\n        if experiment_result.name == \"aten\":\n            aten_perf = experiment_result.forward_time\n            table_data[PERF_OVER_ATEN_STR].append(\"NA\")\n        elif aten_perf is not None:\n            perf_over_aten = (\n                (experiment_result.forward_time - aten_perf) / aten_perf * 100\n            )\n            table_data[PERF_OVER_ATEN_STR].append(perf_over_aten)\n        else:\n            table_data[PERF_OVER_ATEN_STR].append(\"NA\")\n    return table_data", "target": "def main(args):\n    options = parser.parse_args(args)\n    build = confu.Build.from_options(options)\n    build.export_cpath(\"include\", [\"clog.h\"])\n    with build.options(source_dir=\"src\", extra_include_dirs=\"src\"):\n        build.static_library(\"clog\", build.cc(\"clog.c\"))\n    with build.options(\n        source_dir=\"test\",\n        deps={(build, build.deps.googletest): all, \"log\": build.target.is_android},\n    ):\n        build.unittest(\"clog-test\", build.cxx(\"clog.cc\"))\n    return build", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000326", "source": "def f(a, b):\n            result = a.clone()\n            for i in range(1000):\n                if i % 3 == 0:\n                    result = result + b\n                elif i % 3 == 1:\n                    result = result + 8 * b\n                else:\n                    result = result.sin()\n            return result", "target": "def f(x):\n            tmps = [x + i for i in range(16)]\n            tmps = [x + tmp for tmp in tmps]\n            for i in range(len(tmps) - 4):\n                tmps[i] = tmps[i].sin().mul(tmps[i])\n                tmps[i + 1] -= tmps[i]\n                tmps[i + 2] -= tmps[i]\n                tmps[i + 3] -= tmps[i]\n            return sum(tmps)", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000327", "source": "def required_usage_imports(self) -> Generator[str, None, None]:\n        yield \"import typing as _typing\"", "target": "def required_usage_imports(self) -> Generator[str, None, None]:\n        yield \"import os\"", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "000328", "source": "def test_script_stacked_lstm_dropout(\n    seq_len, batch, input_size, hidden_size, num_layers\n):\n    inp = torch.randn(seq_len, batch, input_size)\n    states = [\n        LSTMState(torch.randn(batch, hidden_size), torch.randn(batch, hidden_size))\n        for _ in range(num_layers)\n    ]\n    rnn = script_lstm(input_size, hidden_size, num_layers, dropout=True)\n    out, out_state = rnn(inp, states)", "target": "def pattern_either_validator(input_value: Any, /) -> re.Pattern[Any]:\n    if isinstance(input_value, re.Pattern):\n        return input_value\n    elif isinstance(input_value, (str, bytes)):\n        return compile_pattern(input_value)\n    else:\n        raise PydanticCustomError('pattern_type', 'Input should be a valid pattern')", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000329", "source": "def test_only_validate_by_name(py_and_json) -> None:\n    schema = core_schema.arguments_schema(\n        [\n            core_schema.arguments_parameter(name='a', schema=core_schema.str_schema(), alias='FieldA'),\n        ],\n        validate_by_name=True,\n        validate_by_alias=False,\n    )\n    v = py_and_json(schema)\n    assert v.validate_test(ArgsKwargs((), {'a': 'hello'})) == ((), {'a': 'hello'})\n    with pytest.raises(ValidationError, match=r'a\\n +Missing required argument \\[type=missing_argument,'):\n        assert v.validate_test(ArgsKwargs((), {'FieldA': 'hello'}))", "target": "def test_only_validate_by_name(py_and_json) -> None:\n    v = py_and_json(\n        {\n            'type': 'model-fields',\n            'fields': {'field_a': {'validation_alias': 'FieldA', 'type': 'model-field', 'schema': {'type': 'int'}}},\n        },\n        config=CoreConfig(validate_by_name=True, validate_by_alias=False),\n    )\n    assert v.validate_test({'field_a': '123'}) == ({'field_a': 123}, None, {'field_a'})\n    with pytest.raises(ValidationError, match=r'field_a\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'FieldA': '123'})", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000330", "source": "def test_allow_inf_nan_false_json() -> None:\n    v = SchemaValidator(core_schema.float_schema(), config=core_schema.CoreConfig(allow_inf_nan=False))\n    assert v.validate_json('123') == 123\n    with pytest.raises(ValidationError) as exc_info1:\n        v.validate_json('NaN')\n    assert exc_info1.value.errors(include_url=False) == [\n        {'type': 'finite_number', 'loc': (), 'msg': 'Input should be a finite number', 'input': IsFloatNan()}\n    ]\n    with pytest.raises(ValidationError) as exc_info2:\n        v.validate_json('Infinity')\n    assert exc_info2.value.errors(include_url=False) == [\n        {'type': 'finite_number', 'loc': (), 'msg': 'Input should be a finite number', 'input': float('inf')}\n    ]\n    with pytest.raises(ValidationError) as exc_info3:\n        v.validate_json('-Infinity')\n    assert exc_info3.value.errors(include_url=False) == [\n        {'type': 'finite_number', 'loc': (), 'msg': 'Input should be a finite number', 'input': float('-inf')}\n    ]", "target": "def test_allow_inf_nan_false_json() -> None:\n    v = SchemaValidator(cs.int_schema(), config=cs.CoreConfig(allow_inf_nan=False))\n    assert v.validate_json('123') == 123\n    with pytest.raises(ValidationError, match=r'Input should be a finite number \\[type=finite_number'):\n        v.validate_json('NaN')\n    with pytest.raises(ValidationError, match=r'Input should be a finite number \\[type=finite_number'):\n        v.validate_json('Infinity')\n    with pytest.raises(ValidationError, match=r'Input should be a finite number \\[type=finite_number'):\n        v.validate_json('-Infinity')", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "000331", "source": "def test_json_direct_list_str(benchmark):\n    serializer = SchemaSerializer({'type': 'list', 'items_schema': {'type': 'str'}})\n    assert serializer.to_json(list(map(str, range(5)))) == b'[\"0\",\"1\",\"2\",\"3\",\"4\"]'\n    items = list(map(str, range(1000)))\n    benchmark(serializer.to_json, items)", "target": "def compiled(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        (x,) = args\n        torch._dynamo.mark_dynamic(x, 0)\n        compiled_softmax = torch.compile(\n            lambda x: F.softmax(x, dim=-1), mode=self.compile_mode, fullgraph=True\n        )\n        return lambda: compiled_softmax(x)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "000332", "source": "def quicklint(ctx, apply_patches, **kwargs):\n    ctx.invoke(lazy_setup_lint)\n    cmd = LINTRUNNER_BASE_CMD\n    if apply_patches:\n        cmd += [\"--apply-patches\"]\n    spin.util.run(cmd)", "target": "def test_serialize_with_custom_type_and_subclasses():\n    class CustomType:\n        value: ParentModel\n    CustomType.__pydantic_core_schema__ = core_schema.model_schema(\n        CustomType,\n        core_schema.model_fields_schema(\n            {\n                'value': core_schema.model_field(ParentModel.__pydantic_core_schema__),\n            }\n        ),\n    )\n    CustomType.__pydantic_validator__ = SchemaValidator(CustomType.__pydantic_core_schema__)\n    CustomType.__pydantic_serializer__ = SchemaSerializer(CustomType.__pydantic_core_schema__)\n    value = CustomType.__pydantic_validator__.validate_python({'value': {'x': 1}})\n    value.value = ChildModel.__pydantic_validator__.validate_python({'x': 1, 'y': 'hopefully not a secret'})\n    assert CustomType.__pydantic_serializer__.to_python(value, serialize_as_any=False) == {\n        'value': {'x': 1},\n    }\n    assert CustomType.__pydantic_serializer__.to_python(value, serialize_as_any=True) == {\n        'value': {'x': 1, 'y': 'hopefully not a secret'}\n    }\n    assert CustomType.__pydantic_serializer__.to_json(value, serialize_as_any=False) == b'{\"value\":{\"x\":1}}'\n    assert (\n        CustomType.__pydantic_serializer__.to_json(value, serialize_as_any=True)\n        == b'{\"value\":{\"x\":1,\"y\":\"hopefully not a secret\"}}'\n    )", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "000333", "source": "def lstm_creator(script=True, **kwargs):\n    input, hidden, params, _ = lstm_inputs(return_module=False, **kwargs)\n    inputs = [input, hidden] + params[0]\n    return ModelDef(\n        inputs=inputs,\n        params=flatten_list(params),\n        forward=lstm_factory(lstm_cell, script),\n        backward_setup=lstm_backward_setup,\n        backward=simple_backward,\n    )", "target": "def test_bad_repr():\n    b = BadRepr()\n    error_msg = '^Unable to serialize unknown type: <unprintable BedReprMeta object>$'\n    with pytest.raises(PydanticSerializationError, match=error_msg):\n        to_jsonable_python(b)\n    assert to_jsonable_python(b, serialize_unknown=True) == '<Unserializable BadRepr object>'\n    with pytest.raises(PydanticSerializationError, match=error_msg):\n        to_json(b)\n    assert to_json(b, serialize_unknown=True) == b'\"<Unserializable BadRepr object>\"'", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|opencv/opencv", "data_split": "train", "example_id": "000334", "source": "def fmt_bin(base, prec, model):\n    return \"%s/%s/%s/%s.xml\" % (base, model, prec, model)", "target": "def test_InputArrayOfArrays(self):\n        res1 = cv.utils.dumpInputArrayOfArrays(None)\n        self.assertEqual(res1, \"InputArrayOfArrays: empty()=true kind=0x00050000 flags=0x01050000 total(-1)=0 dims(-1)=1 size(-1)=0x0\")\n        res2_1 = cv.utils.dumpInputArrayOfArrays((1, 2))\n        self.assertEqual(res2_1, \"InputArrayOfArrays: empty()=false kind=0x00050000 flags=0x01050000 total(-1)=2 dims(-1)=1 size(-1)=2x1 type(0)=CV_64FC1 dims(0)=2 size(0)=1x4\")\n        res2_2 = cv.utils.dumpInputArrayOfArrays([1.5])\n        self.assertEqual(res2_2, \"InputArrayOfArrays: empty()=false kind=0x00050000 flags=0x01050000 total(-1)=1 dims(-1)=1 size(-1)=1x1 type(0)=CV_64FC1 dims(0)=2 size(0)=1x4\")\n        a = np.array([[1, 2], [3, 4], [5, 6]])\n        b = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n        res3 = cv.utils.dumpInputArrayOfArrays([a, b])\n        self.assertEqual(res3, \"InputArrayOfArrays: empty()=false kind=0x00050000 flags=0x01050000 total(-1)=2 dims(-1)=1 size(-1)=2x1 type(0)=CV_32SC1 dims(0)=2 size(0)=2x3\")\n        c = np.array([[[1, 2], [3, 4], [5, 6]]], dtype='f')\n        res4 = cv.utils.dumpInputArrayOfArrays([c, a, b])\n        self.assertEqual(res4, \"InputArrayOfArrays: empty()=false kind=0x00050000 flags=0x01050000 total(-1)=3 dims(-1)=1 size(-1)=3x1 type(0)=CV_32FC2 dims(0)=2 size(0)=3x1\")\n        a = np.zeros((2,3,4), dtype='f')\n        res5 = cv.utils.dumpInputArrayOfArrays([a, b])\n        self.assertEqual(res5, \"InputArrayOfArrays: empty()=false kind=0x00050000 flags=0x01050000 total(-1)=2 dims(-1)=1 size(-1)=2x1 type(0)=CV_32FC4 dims(0)=2 size(0)=3x2\")", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000335", "source": "def area(self) -> bytes:\n            a = self.width * self.height\n            return b'%d' % a", "target": "def area(self) -> None:\n            self.side = 0.0", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000336", "source": "def f(input_value, validator, info):\n        return validator(input_value) + f'| context: {info.context}'", "target": "def test_unsorted_definitions_schema() -> None:\n    s = core_schema.definitions_schema(\n        core_schema.definition_reference_schema('td'),\n        [\n            core_schema.typed_dict_schema(\n                {'x': core_schema.typed_dict_field(core_schema.definition_reference_schema('int'))}, ref='td'\n            ),\n            core_schema.int_schema(ref='int'),\n        ],\n    )\n    v = SchemaValidator(s)\n    assert v.validate_python({'x': 123}) == {'x': 123}\n    with pytest.raises(ValidationError):\n        v.validate_python({'x': 'abc'})", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "000337", "source": "def make_estimator(self, params):\n        representation, precompute = params\n        estimator = Lasso(precompute=precompute, alpha=0.001, random_state=0)\n        return estimator", "target": "def make_estimator(self, params):\n        representation, n_jobs = params\n        n_estimators = 500 if Benchmark.data_size == \"large\" else 100\n        estimator = RandomForestClassifier(\n            n_estimators=n_estimators,\n            min_samples_split=10,\n            max_features=\"log2\",\n            n_jobs=n_jobs,\n            random_state=0,\n        )\n        return estimator", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000338", "source": "def test_function_change_id(strict: bool):\n    def f(input_value, info):\n        _, count = input_value.split('-')\n        return f'f-{int(count) + 1}'\n    v = SchemaValidator(\n        core_schema.definitions_schema(\n            core_schema.definition_reference_schema('root-schema'),\n            [\n                core_schema.union_schema(\n                    [\n                        core_schema.with_info_before_validator_function(\n                            f, core_schema.definition_reference_schema('root-schema')\n                        )\n                    ],\n                    auto_collapse=False,\n                    ref='root-schema',\n                )\n            ],\n        ),\n        config=CoreConfig(strict=strict),\n    )\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python('start-0')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'recursion_loop',\n            'loc': IsTuple(length=(1, 255)),\n            'msg': 'Recursion error - cyclic reference detected',\n            'input': IsStr(regex=r'f-\\d+'),\n        }\n    ]", "target": "def wrap_function(input_value, validator, info):\n            try:\n                return validator(input_value)\n            except ValidationError:\n                if isinstance(input_value, list):\n                    return str(len(input_value))\n                else:\n                    return repr(input_value)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "000339", "source": "def should_init_forbid_extra(self, fields: list[PydanticModelField], config: ModelConfigData) -> bool:\n        if not (config.validate_by_name or config.populate_by_name):\n            if self.is_dynamic_alias_present(fields, bool(config.has_alias_generator)):\n                return False\n        if config.forbid_extra:\n            return True\n        return self.plugin_config.init_forbid_extra", "target": "def set_output(name: str, val: str) -> None:\n    print(f\"Setting output {name}={val}\")\n    if os.getenv(\"GITHUB_OUTPUT\"):\n        with open(str(os.getenv(\"GITHUB_OUTPUT\")), \"a\") as env:\n            print(f\"{name}={val}\", file=env)\n    else:\n        print(f\"::set-output name={name}::{val}\")", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000340", "source": "def ser_x(self, v: Any, serializer: core_schema.SerializerFunctionWrapHandler, _) -> str:\n            assert self.x == 1_000\n            x = serializer(v)\n            return f'{x:_}'", "target": "def ser_x(data: Model, v: Any) -> str:\n        assert data['x'] == 1_000\n        return f'{v:_}'", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "000341", "source": "def test_from_attributes_override_false():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={'a': core_schema.model_field(schema=core_schema.int_schema())}, from_attributes=True\n        )\n    )\n    with pytest.raises(ValidationError, match='Input should be a valid dictionary'):\n        v.validate_python(Cls(a=1), from_attributes=False)\n    assert v.validate_python(Cls(a=1)) == ({'a': 1}, None, {'a'})\n    assert v.isinstance_python(Cls(a=1)) is True\n    assert v.isinstance_python(Cls(a=1), from_attributes=False) is False", "target": "def _find_argument_index(arguments: Sequence[FunctionNode.Arg],\n                         name: str) -> Optional[int]:\n    for i, arg in enumerate(arguments):\n        if arg.name == name:\n            return i\n    return None", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "000342", "source": "def test_simple_serializers(schema_type, value, expected_python, expected_json, custom_type_schema):\n    if custom_type_schema is None:\n        schema = {'type': schema_type}\n    else:\n        schema = {'type': custom_type_schema}\n    s = SchemaSerializer(schema)\n    v = s.to_python(value)\n    assert v == expected_python\n    assert type(v) == type(expected_python)\n    assert s.to_json(value) == expected_json\n    v_json = s.to_python(value, mode='json')\n    v_json_expected = json.loads(expected_json)\n    assert v_json == v_json_expected\n    assert type(v_json) == type(v_json_expected)", "target": "def detect(img, cascade):\n    rects = cascade.detectMultiScale(img, scaleFactor=1.275, minNeighbors=4, minSize=(30, 30),\n                                     flags=cv.CASCADE_SCALE_IMAGE)\n    if len(rects) == 0:\n        return []\n    rects[:,2:] += rects[:,:2]\n    return rects", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000343", "source": "def get_output(self, input_blob):\n        if self.need_reshape:\n            self.net.blobs[self.in_blob_name].reshape(*input_blob.shape)\n        return self.net.forward_all(**{self.in_blob_name: input_blob})[self.out_blob_name]", "target": "def get_output(self, input_blob):\n        self.net.setInput(input_blob, self.in_blob_name)\n        return self.net.forward(self.out_blob_name)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "000344", "source": "def f(input_value: Any, *args: Any) -> Any:\n        if mode == 'wrap':\n            handler, _ = args\n            calls.append({'value': input_value})\n            return handler(input_value)\n        else:\n            calls.append({'value': input_value})\n            return input_value", "target": "def forward(self, x, output_lengths):\n        if self.batch_norm is not None:\n            x = self.batch_norm(x)\n        x = nn.utils.rnn.pack_padded_sequence(x, output_lengths, enforce_sorted=False)\n        x, h = self.rnn(x)\n        x, _ = nn.utils.rnn.pad_packed_sequence(x)\n        if self.bidirectional:\n            x = (\n                x.view(x.size(0), x.size(1), 2, -1)\n                .sum(2)\n                .view(x.size(0), x.size(1), -1)\n            )\n        return x", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|pytorch/pytorch", "data_split": "train", "example_id": "000345", "source": "def clone_with_theta(self, theta):\n        cloned = clone(self)\n        cloned.theta = theta\n        return cloned", "target": "def gh_fetch_json_dict(\n    url: str,\n    params: Optional[dict[str, Any]] = None,\n    data: Optional[dict[str, Any]] = None,\n) -> dict[str, Any]:\n    return cast(dict[str, Any], _gh_fetch_json_any(url, params, data))", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000346", "source": "def f(value, serializer):\n        if value == 42:\n            return 42\n        return f'result={serializer(value)}'", "target": "def f(value, handler, _info):\n        return handler(value)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000347", "source": "def test_tuple_json(py_and_json: PyAndJson, variadic_item_index, items, input_value, expected):\n    v = py_and_json(core_schema.tuple_schema(items_schema=items, variadic_item_index=variadic_item_index))\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            v.validate_test(input_value)\n    else:\n        assert v.validate_test(input_value) == expected", "target": "def test_generator_too_long():\n    v = SchemaValidator(cs.generator_schema(items_schema=cs.int_schema(), max_length=2))\n    validating_iterator = v.validate_python(gen())\n    assert next(validating_iterator) == 1\n    assert next(validating_iterator) == 2\n    with pytest.raises(ValidationError) as exc_info:\n        next(validating_iterator)\n    errors = exc_info.value.errors(include_url=False)\n    assert errors == [\n        {\n            'type': 'too_long',\n            'loc': (),\n            'input': HasRepr(IsStr(regex='<generator object gen at .+>')),\n            'msg': 'Generator should have at most 2 items after validation, not more',\n            'ctx': {'field_type': 'Generator', 'max_length': 2, 'actual_length': None},\n        }\n    ]", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000348", "source": "def typename(self) -> str:\n        return self.type_node.full_typename", "target": "def typename(self) -> str:\n        return \"_typing.Any\"", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000349", "source": "def test_simple(self):\n        img1 = self.get_sample('stitching/a1.png')\n        img2 = self.get_sample('stitching/a2.png')\n        stitcher = cv.Stitcher.create(cv.Stitcher_PANORAMA)\n        (_result, pano) = stitcher.stitch((img1, img2))\n        self.assertAlmostEqual(pano.shape[0], 685, delta=100, msg=\"rows: %r\" % list(pano.shape))\n        self.assertAlmostEqual(pano.shape[1], 1025, delta=100, msg=\"cols: %r\" % list(pano.shape))", "target": "def test_simple(self):\n        finder = cv.ORB.create()\n        img1 = self.get_sample('stitching/a1.png')\n        img2 = self.get_sample('stitching/a2.png')\n        img_feat1 = cv.detail.computeImageFeatures2(finder, img1)\n        img_feat2 = cv.detail.computeImageFeatures2(finder, img2)\n        matcher = cv.detail.BestOf2NearestMatcher_create()\n        matches_info = matcher.apply(img_feat1, img_feat2)\n        self.assertIsNotNone(matches_info.matches)\n        self.assertIsNotNone(matches_info.inliers_mask)", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000350", "source": "def outMeta(arr_desc0, arr_desc1, arr_desc2):\n        return cv.empty_array_desc()", "target": "def outMeta(desc1, desc2, depth):\n            return desc1", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000351", "source": "def test_only_allow_alias(py_and_json) -> None:\n    v = py_and_json(\n        cs.arguments_v3_schema(\n            [\n                cs.arguments_v3_parameter(\n                    name='a', schema=cs.str_schema(), alias='FieldA', mode='positional_or_keyword'\n                ),\n            ],\n            validate_by_name=False,\n            validate_by_alias=True,\n        )\n    )\n    assert v.validate_test(ArgsKwargs((), {'FieldA': 'hello'})) == ((), {'a': 'hello'})\n    assert v.validate_test({'FieldA': 'hello'}) == (('hello',), {})\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Missing required argument \\[type=missing_argument,'):\n        assert v.validate_test(ArgsKwargs((), {'a': 'hello'}))\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Missing required argument \\[type=missing_argument,'):\n        assert v.validate_test({'a': 'hello'})", "target": "def validate_before(data) -> dict:\n        data['x'] = data['x'] + ' modified'\n        return data", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000352", "source": "def ser_root(self, v: Any, _) -> str:\n            assert self.root == 1_000\n            return f'{v:_}'", "target": "def ser_root(self, v: Any, _) -> str:\n            assert self.root == 1_000\n            return f'{v:_}'", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000353", "source": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "target": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000354", "source": "def input_data_wrong():\n    return {\n        'field_str': ['fo'],\n        'field_str_con': 'f',\n        'field_int': 1.5,\n        'field_int_con': 11,\n        'field_float': False,\n        'field_float_con': 10.1,\n        'field_decimal': 'wrong',\n        'field_bool': 4,\n        'field_bytes': 42,\n        'field_bytes_con': b'foo',\n        'field_date': 'wrong',\n        'field_date_con': '2000-01-01',\n        'field_time': 'boom',\n        'field_time_con': '23:00:00',\n        'field_datetime': b'smash',\n        'field_datetime_con': '1900-01-01T00:00:00',\n        'field_uuid': '12345678-1234-5678-1234-567812345678',\n        'field_list_any': {1: 2, 3: 4},\n        'field_list_str': [(i,) for i in range(100)],\n        'field_list_str_con': ['a', 'b'],\n        'field_set_any': {'a': b'b', True: 1.0, None: 5},\n        'field_set_int': {f'x{i}' for i in range(100)},\n        'field_set_int_con': {i for i in range(40)},\n        'field_frozenset_any': 'wrong',\n        'field_frozenset_bytes': frozenset([i for i in range(100)]),\n        'field_frozenset_bytes_con': frozenset({b'a', b'b'}),\n        'field_tuple_var_len_any': b'wrong',\n        'field_tuple_var_len_float': tuple(f'x{i}' for i in range(100)),\n        'field_tuple_var_len_float_con': (1.0, 2.0),\n        'field_tuple_fix_len': ('a', 1, 1.0, True, 'more'),\n        'field_dict_any': {'a', 'b', 1, True, 1.0, 2.0},\n        'field_dict_str_float': {(i,): f'x{i}' for i in range(100)},\n        'field_literal_1_int': 2,\n        'field_literal_1_str': 'bat',\n        'field_literal_mult_int': 42,\n        'field_literal_mult_str': 'wrong',\n        'field_literal_assorted': 'wrong',\n        'field_list_nullable_int': [f'x{i}' for i in range(100)],\n        'field_union': {'field_str': ('foo',), 'field_int': 'x', 'field_float': b'y'},\n        'field_functions_model': {'field_before': 1, 'field_after': 1, 'field_wrap': 1, 'field_plain': 1},\n        'field_recursive': {'name': 'foo', 'sub_branch': {'name': 'bar', 'sub_branch': {}}},\n    }", "target": "def test_non_finite_float_values(strict, input_value, allow_inf_nan, expected):\n    v = SchemaValidator(cs.float_schema(allow_inf_nan=allow_inf_nan, strict=strict))\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            v.validate_python(input_value)\n    else:\n        assert v.validate_python(input_value) == expected", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "000355", "source": "def all_identical(left: Iterable[Any], right: Iterable[Any]) -> bool:\n    for left_item, right_item in zip_longest(left, right, fillvalue=_SENTINEL):\n        if left_item is not right_item:\n            return False\n    return True", "target": "def get_wheels(\n    output_dir: Path,\n    max_depth: Optional[int] = None,\n) -> list[str]:\n    root = Path(output_dir)\n    if not root.exists():\n        return []\n    items = []\n    for dirpath, _, filenames in os.walk(root):\n        depth = Path(dirpath).relative_to(root).parts\n        if max_depth is not None and len(depth) > max_depth:\n            continue\n        for fname in sorted(filenames):\n            if fname.endswith(\".whl\"):\n                pkg = fname.split(\"-\")[0]\n                relpath = str((Path(dirpath) / fname).relative_to(root))\n                items.append({\"pkg\": pkg, \"relpath\": relpath})\n    return items", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000356", "source": "def test_tuple_fix_len_errors(input_value, items, index):\n    v = SchemaValidator(core_schema.tuple_schema(items_schema=items))\n    with pytest.raises(ValidationError) as exc_info:\n        assert v.validate_python(input_value)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': (index,),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'wrong',\n        }\n    ]", "target": "def test_function_wrap_repr():\n    def f(input_value, validator, info):\n        assert repr(validator) == str(validator)\n        return plain_repr(validator)\n    v = SchemaValidator(core_schema.with_info_wrap_validator_function(f, core_schema.str_schema()))\n    assert (\n        v.validate_python('input value')\n        == 'ValidatorCallable(Str(StrValidator{strict:false,coerce_numbers_to_str:false}))'\n    )", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|pydantic/pydantic", "data_split": "train", "example_id": "000357", "source": "def track_train_score(self, *args):\n        if hasattr(self.estimator, \"predict\"):\n            y_pred = self.estimator.predict(self.X)\n        else:\n            y_pred = None\n        return float(self.train_scorer(self.y, y_pred))", "target": "def test_format_fallback():\n    s = SchemaSerializer(core_schema.any_schema(serialization=core_schema.format_ser_schema('^5s')))\n    assert s.to_python('abc') == 'abc'\n    assert s.to_python('abc', mode='json') == ' abc '\n    assert s.to_json('abc') == b'\" abc \"'\n    assert s.to_python(None) is None\n    assert s.to_python(None, mode='json') is None\n    assert s.to_json(None) == b'null'", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000358", "source": "def validate_b(cls, v: bytes, info: core_schema.ValidationInfo) -> str:\n            assert v == b'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return 'hello world!'", "target": "def validate_b(cls, v: bytes, info: core_schema.ValidationInfo) -> bytes:\n            assert v == b'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return b'hello world!'", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "000359", "source": "def compute_bench(samples_range, features_range):\n    it = 0\n    results = defaultdict(lambda: [])\n    chunk = 100\n    max_it = len(samples_range) * len(features_range)\n    for n_samples in samples_range:\n        for n_features in features_range:\n            it += 1\n            print(\"==============================\")\n            print(\"Iteration %03d of %03d\" % (it, max_it))\n            print(\"==============================\")\n            print()\n            data = nr.randint(-50, 51, (n_samples, n_features))\n            print(\"K-Means\")\n            tstart = time()\n            kmeans = KMeans(init=\"k-means++\", n_clusters=10).fit(data)\n            delta = time() - tstart\n            print(\"Speed: %0.3fs\" % delta)\n            print(\"Inertia: %0.5f\" % kmeans.inertia_)\n            print()\n            results[\"kmeans_speed\"].append(delta)\n            results[\"kmeans_quality\"].append(kmeans.inertia_)\n            print(\"Fast K-Means\")\n            mbkmeans = MiniBatchKMeans(\n                init=\"k-means++\", n_clusters=10, batch_size=chunk\n            )\n            tstart = time()\n            mbkmeans.fit(data)\n            delta = time() - tstart\n            print(\"Speed: %0.3fs\" % delta)\n            print(\"Inertia: %f\" % mbkmeans.inertia_)\n            print()\n            print()\n            results[\"MiniBatchKMeans Speed\"].append(delta)\n            results[\"MiniBatchKMeans Quality\"].append(mbkmeans.inertia_)\n    return results", "target": "def compute_bench(samples_range, features_range):\n    it = 0\n    results = dict()\n    lars = np.empty((len(features_range), len(samples_range)))\n    lars_gram = lars.copy()\n    omp = lars.copy()\n    omp_gram = lars.copy()\n    max_it = len(samples_range) * len(features_range)\n    for i_s, n_samples in enumerate(samples_range):\n        for i_f, n_features in enumerate(features_range):\n            it += 1\n            n_informative = n_features // 10\n            print(\"====================\")\n            print(\"Iteration %03d of %03d\" % (it, max_it))\n            print(\"====================\")\n            dataset_kwargs = {\n                \"n_samples\": 1,\n                \"n_components\": n_features,\n                \"n_features\": n_samples,\n                \"n_nonzero_coefs\": n_informative,\n                \"random_state\": 0,\n            }\n            print(\"n_samples: %d\" % n_samples)\n            print(\"n_features: %d\" % n_features)\n            y, X, _ = make_sparse_coded_signal(**dataset_kwargs)\n            X = np.asfortranarray(X.T)\n            gc.collect()\n            print(\"benchmarking lars_path (with Gram):\", end=\"\")\n            sys.stdout.flush()\n            tstart = time()\n            G = np.dot(X.T, X)\n            Xy = np.dot(X.T, y)\n            lars_path_gram(Xy=Xy, Gram=G, n_samples=y.size, max_iter=n_informative)\n            delta = time() - tstart\n            print(\"%0.3fs\" % delta)\n            lars_gram[i_f, i_s] = delta\n            gc.collect()\n            print(\"benchmarking lars_path (without Gram):\", end=\"\")\n            sys.stdout.flush()\n            tstart = time()\n            lars_path(X, y, Gram=None, max_iter=n_informative)\n            delta = time() - tstart\n            print(\"%0.3fs\" % delta)\n            lars[i_f, i_s] = delta\n            gc.collect()\n            print(\"benchmarking orthogonal_mp (with Gram):\", end=\"\")\n            sys.stdout.flush()\n            tstart = time()\n            orthogonal_mp(X, y, precompute=True, n_nonzero_coefs=n_informative)\n            delta = time() - tstart\n            print(\"%0.3fs\" % delta)\n            omp_gram[i_f, i_s] = delta\n            gc.collect()\n            print(\"benchmarking orthogonal_mp (without Gram):\", end=\"\")\n            sys.stdout.flush()\n            tstart = time()\n            orthogonal_mp(X, y, precompute=False, n_nonzero_coefs=n_informative)\n            delta = time() - tstart\n            print(\"%0.3fs\" % delta)\n            omp[i_f, i_s] = delta\n    results[\"time(LARS) / time(OMP)\\n (w/ Gram)\"] = lars_gram / omp_gram\n    results[\"time(LARS) / time(OMP)\\n (w/o Gram)\"] = lars / omp\n    return results", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000360", "source": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "target": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000361", "source": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "target": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "000362", "source": "def with_info_plain_validator_function(\n    function: WithInfoValidatorFunction,\n    *,\n    field_name: str | None = None,\n    ref: str | None = None,\n    json_schema_input_schema: CoreSchema | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> PlainValidatorFunctionSchema:\n    if field_name is not None:\n        warnings.warn(\n            'The `field_name` argument on `with_info_plain_validator_function` is deprecated, it will be passed to the function through `ValidationState` instead.',\n            DeprecationWarning,\n            stacklevel=2,\n        )\n    return _dict_not_none(\n        type='function-plain',\n        function=_dict_not_none(type='with-info', function=function, field_name=field_name),\n        ref=ref,\n        json_schema_input_schema=json_schema_input_schema,\n        metadata=metadata,\n        serialization=serialization,\n    )", "target": "def test_multiple_vars_and_missing_cleanup(self):\n        v1, v2 = \"TEST_ENV_V1\", \"TEST_ENV_V2\"\n        os.environ.pop(v1, None)\n        os.environ[v2] = \"keep\"\n        with temp_environ({v1: \"a\", v2: \"b\"}):\n            self.assertEqual(os.environ[v1], \"a\")\n            self.assertEqual(os.environ[v2], \"b\")\n        self.assertNotIn(v1, os.environ)\n        self.assertEqual(os.environ[v2], \"keep\")", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000363", "source": "def test_alias_extra_by_name(py_and_json: PyAndJson):\n    v = py_and_json(\n        {\n            'type': 'typed-dict',\n            'extra_behavior': 'allow',\n            'fields': {\n                'field_a': {'validation_alias': 'FieldA', 'type': 'typed-dict-field', 'schema': {'type': 'int'}}\n            },\n            'config': {'validate_by_name': True},\n        },\n    )\n    assert v.validate_test({'FieldA': 1}) == {'field_a': 1}\n    assert v.validate_test({'field_a': 1}) == {'field_a': 1}", "target": "def test_function_positional_only(import_execute):\n    m = import_execute(\n    )\n    foobar = m.create_function(validate)\n    assert foobar('1', 2, 3) == (1, 2, 3)\n    assert foobar('1', 2, c=3) == (1, 2, 3)\n    with pytest.raises(ValidationError) as exc_info:\n        foobar('1', b=2, c=3)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'missing_positional_only_argument',\n            'loc': (1,),\n            'msg': 'Missing required positional only argument',\n            'input': ArgsKwargs(('1',), {'b': 2, 'c': 3}),\n        },\n        {'type': 'unexpected_keyword_argument', 'loc': ('b',), 'msg': 'Unexpected keyword argument', 'input': 2},\n    ]\n    foobar = m.create_function(validate, config={'title': 'func', 'extra_fields_behavior': 'allow'})\n    assert foobar('1', '2', c=3, d=4) == (1, 2, 3)\n    foobar = m.create_function(validate, config={'title': 'func', 'extra_fields_behavior': 'ignore'})\n    assert foobar('1', '2', c=3, d=4) == (1, 2, 3)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000364", "source": "def test_validate_assignment():\n    def f(input_value):\n        input_value.more = 'foobar'\n        return input_value\n    class Model:\n        __slots__ = '__dict__', '__pydantic_fields_set__', '__pydantic_extra__', '__pydantic_private__'\n        field_a: str\n        def __init__(self):\n            self.__pydantic_extra__ = None\n    v = SchemaValidator(\n        core_schema.no_info_after_validator_function(\n            f,\n            core_schema.model_schema(\n                Model, core_schema.model_fields_schema({'field_a': core_schema.model_field(core_schema.str_schema())})\n            ),\n        )\n    )\n    m = v.validate_python({'field_a': 'test'})\n    assert isinstance(m, Model)\n    assert m.field_a == 'test'\n    assert m.__pydantic_fields_set__ == {'field_a'}\n    assert m.__dict__ == {'field_a': 'test', 'more': 'foobar'}\n    assert m.__pydantic_extra__ is None\n    m2 = Model()\n    m2.field_a = 'test'\n    assert v.validate_assignment(m2, 'field_a', b'abc') is m2\n    assert m2.__dict__ == {'field_a': 'abc', 'more': 'foobar'}\n    assert not hasattr(m2, '__pydantic_fields_set__')", "target": "def test_validate_assignment():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(fields={'field_a': core_schema.model_field(schema=core_schema.str_schema())})\n    )\n    assert v.validate_python({'field_a': 'test'}) == ({'field_a': 'test'}, None, {'field_a'})\n    data = {'field_a': 'test'}\n    assert v.validate_assignment(data, 'field_a', b'abc') == ({'field_a': 'abc'}, None, {'field_a'})\n    assert data == {'field_a': 'abc'}", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000365", "source": "def get_output(self, input_blob):\n        return super(DnnTfInceptionModel, self).get_output(input_blob)[..., 1:1001]", "target": "def get_output(self, input_blob):\n        self.net.setInput(input_blob, self.in_blob_name)\n        return self.net.forward(self.out_blob_name)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000366", "source": "def field_name(self) -> str | None:\n        ...", "target": "def field_name(self) -> str | None:\n        raise NotImplementedError", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000367", "source": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "target": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "train", "example_id": "000368", "source": "def clone_vllm(dst: str = \"vllm\"):\n    _, commit = clone_external_repo(\n        target=\"vllm\",\n        repo=\"https://github.com/vllm-project/vllm.git\",\n        dst=dst,\n        update_submodules=True,\n    )\n    return commit", "target": "def test_env_path_returns_path_when_present(self):\n        tmp = Path(\"./b\").resolve()\n        with patch.dict(os.environ, {\"P\": str(tmp)}, clear=True):\n            p = m.env_path(\"P\", None, resolve=True)\n            self.assertEqual(p, tmp)", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|pytorch/pytorch", "data_split": "train", "example_id": "000369", "source": "def bench_sample(sampling, n_population, n_samples):\n    gc.collect()\n    t_start = datetime.now()\n    sampling(n_population, n_samples)\n    delta = datetime.now() - t_start\n    time = compute_time(t_start, delta)\n    return time", "target": "def benchmark(self):\n        for M, N in self.get_shapes():\n            print(f\"Tensor dimensions: [{M}, {N}]\")\n            torch_dtype = cutlass_torch.dtype(cutlass.BFloat16)\n            x = 0.1 * torch.randn(\n                M, N, device=\"cuda\", dtype=torch_dtype, requires_grad=True\n            )\n            target = torch.randint(0, N, (M,), device=\"cuda\", dtype=torch.int64)\n            dloss = torch.randn(M, device=\"cuda\", dtype=torch.float32)\n            self.benchmark_single_shape(\n                (x, target, dloss), setting=f\"shape: [{M}, {N}]\"\n            )", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|pydantic/pydantic", "data_split": "train", "example_id": "000370", "source": "def can_document_member(cls, member, membername, isattr, parent):\n        return True", "target": "def no_info_after_validator_function(\n    function: NoInfoValidatorFunction,\n    schema: CoreSchema,\n    *,\n    ref: str | None = None,\n    json_schema_input_schema: CoreSchema | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> AfterValidatorFunctionSchema:\n    return _dict_not_none(\n        type='function-after',\n        function={'type': 'no-info', 'function': function},\n        schema=schema,\n        ref=ref,\n        json_schema_input_schema=json_schema_input_schema,\n        metadata=metadata,\n        serialization=serialization,\n    )", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000371", "source": "def collect_config(self) -> ModelConfigData:\n        cls = self._cls\n        config = ModelConfigData()\n        has_config_kwargs = False\n        has_config_from_namespace = False\n        for name, expr in cls.keywords.items():\n            config_data = self.get_config_update(name, expr)\n            if config_data:\n                has_config_kwargs = True\n                config.update(config_data)\n        stmt: Statement | None = None\n        for stmt in cls.defs.body:\n            if not isinstance(stmt, (AssignmentStmt, ClassDef)):\n                continue\n            if isinstance(stmt, AssignmentStmt):\n                lhs = stmt.lvalues[0]\n                if not isinstance(lhs, NameExpr) or lhs.name != 'model_config':\n                    continue\n                if isinstance(stmt.rvalue, CallExpr):\n                    for arg_name, arg in zip(stmt.rvalue.arg_names, stmt.rvalue.args):\n                        if arg_name is None:\n                            continue\n                        config.update(self.get_config_update(arg_name, arg, lax_extra=True))\n                elif isinstance(stmt.rvalue, DictExpr):\n                    for key_expr, value_expr in stmt.rvalue.items:\n                        if not isinstance(key_expr, StrExpr):\n                            continue\n                        config.update(self.get_config_update(key_expr.value, value_expr))\n            elif isinstance(stmt, ClassDef):\n                if stmt.name != 'Config':\n                    continue\n                for substmt in stmt.defs.body:\n                    if not isinstance(substmt, AssignmentStmt):\n                        continue\n                    lhs = substmt.lvalues[0]\n                    if not isinstance(lhs, NameExpr):\n                        continue\n                    config.update(self.get_config_update(lhs.name, substmt.rvalue))\n            if has_config_kwargs:\n                self._api.fail(\n                    'Specifying config in two places is ambiguous, use either Config attribute or class kwargs',\n                    cls,\n                )\n                break\n            has_config_from_namespace = True\n        if has_config_kwargs or has_config_from_namespace:\n            if (\n                stmt\n                and config.has_alias_generator\n                and not (config.validate_by_name or config.populate_by_name)\n                and self.plugin_config.warn_required_dynamic_aliases\n            ):\n                error_required_dynamic_aliases(self._api, stmt)\n        for info in cls.info.mro[1:]:\n            if METADATA_KEY not in info.metadata:\n                continue\n            self._api.add_plugin_dependency(make_wildcard_trigger(info.fullname))\n            for name, value in info.metadata[METADATA_KEY]['config'].items():\n                config.setdefault(name, value)\n        return config", "target": "def test_frozenset_from_dict_items(input_value, items_schema, expected):\n    v = SchemaValidator(core_schema.tuple_schema(items_schema=[items_schema], variadic_item_index=0))\n    output = v.validate_python(input_value)\n    assert isinstance(output, tuple)\n    assert output == expected", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000372", "source": "def types_separator(self) -> str:\n        return \", \"", "target": "def types_separator(self) -> str:\n        return \", \"", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000373", "source": "def test_dict():\n    v = SchemaValidator(core_schema.dict_schema(core_schema.int_schema(), core_schema.date_schema()))\n    assert v.validate_strings({'1': '2017-01-01', '2': '2017-01-02'}) == {1: date(2017, 1, 1), 2: date(2017, 1, 2)}\n    assert v.validate_strings({'1': '2017-01-01', '2': '2017-01-02'}, strict=True) == {\n        1: date(2017, 1, 1),\n        2: date(2017, 1, 2),\n    }", "target": "def test_dict(py_and_json: PyAndJson):\n    v = py_and_json({'type': 'dict', 'keys_schema': {'type': 'datetime'}, 'values_schema': {'type': 'int'}})\n    assert v.validate_test({'2000-01-01T00:00': 2, '2000-01-02T00:00': 4}) == {\n        datetime(2000, 1, 1): 2,\n        datetime(2000, 1, 2): 4,\n    }", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000374", "source": "def add_constant(self, name: str, value: str) -> ConstantNode:\n        return self._add_child(ConstantNode, name, value=value)", "target": "def add_constant(self, name: str, value: str) -> ConstantNode:\n        return self._add_child(ConstantNode, name, value=value)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "000375", "source": "def eager(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, dy = args\n        y = F.softmax(x, dim=-1)\n        return lambda: torch.autograd.grad(y, x, grad_outputs=dy, retain_graph=True)", "target": "def root_validator(\n    *__args,\n    pre: bool = False,\n    skip_on_failure: bool = False,\n    allow_reuse: bool = False,\n) -> Any:\n    warn(\n        'Pydantic V1 style `@root_validator` validators are deprecated.'\n        ' You should migrate to Pydantic V2 style `@model_validator` validators,'\n        ' see the migration guide for more details',\n        DeprecationWarning,\n        stacklevel=2,\n    )\n    if __args:\n        return root_validator()(*__args)\n    if allow_reuse is True:\n        warn(_ALLOW_REUSE_WARNING_MESSAGE, DeprecationWarning, stacklevel=2)\n    mode: Literal['before', 'after'] = 'before' if pre is True else 'after'\n    if pre is False and skip_on_failure is not True:\n        raise PydanticUserError(\n            'If you use `@root_validator` with pre=False (the default) you MUST specify `skip_on_failure=True`.'\n            ' Note that `@root_validator` is deprecated and should be replaced with `@model_validator`.',\n            code='root-validator-pre-skip',\n        )\n    wrap = partial(_decorators_v1.make_v1_generic_root_validator, pre=pre)\n    def dec(f: Callable[..., Any] | classmethod[Any, Any, Any] | staticmethod[Any, Any]) -> Any:\n        if _decorators.is_instance_method_from_sig(f):\n            raise TypeError('`@root_validator` cannot be applied to instance methods')\n        res = _decorators.ensure_classmethod_based_on_signature(f)\n        dec_info = _decorators.RootValidatorDecoratorInfo(mode=mode)\n        return _decorators.PydanticDescriptorProxy(res, dec_info, shim=wrap)\n    return dec", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "000376", "source": "def fit(est, data_train, target_train, libname, **fit_params):\n    print(f\"Fitting a {libname} model...\")\n    tic = time()\n    est.fit(data_train, target_train, **fit_params)\n    toc = time()\n    print(f\"fitted in {toc - tic:.3f}s\")", "target": "def fit(est, data_train, target_train, libname, **fit_params):\n    print(f\"Fitting a {libname} model...\")\n    tic = time()\n    est.fit(data_train, target_train, **fit_params)\n    toc = time()\n    print(f\"fitted in {toc - tic:.3f}s\")", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|opencv/opencv", "data_split": "train", "example_id": "000377", "source": "def test_runner_constructed_with_ns_and_run_called(self):\n        specs: dict[str, TargetSpec] = {\n            \"foo\": {\"runner\": FooRunner, \"add_arguments\": add_foo_args},\n        }\n        parser = build_parser(specs)\n        with (\n            patch.object(FooRunner, \"__init__\", return_value=None) as mock_init,\n            patch.object(FooRunner, \"run\", return_value=None) as mock_run,\n        ):\n            ns = parser.parse_args([\"foo\", \"--x\", \"3\", \"--verbose\"])\n            ns.func(ns)\n            self.assertEqual(mock_init.call_count, 1)\n            (called_ns,), _ = mock_init.call_args\n            self.assertIsInstance(called_ns, argparse.Namespace)\n            mock_run.assert_called_once_with()", "target": "def test_cuda_release(self):\n        npMat = (np.random.random((128, 128, 3)) * 255).astype(np.uint8)\n        cuMat = cv.cuda_GpuMat()\n        cuMat.upload(npMat)\n        cuMat.release()\n        self.assertTrue(cuMat.cudaPtr() == 0)\n        self.assertTrue(cuMat.step == 0)\n        self.assertTrue(cuMat.size() == (0, 0))", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "000378", "source": "def test_cuda_denoising(self):\n        self.assertEqual(True, hasattr(cv.cuda, 'fastNlMeansDenoising'))\n        self.assertEqual(True, hasattr(cv.cuda, 'fastNlMeansDenoisingColored'))\n        self.assertEqual(True, hasattr(cv.cuda, 'nonLocalMeans'))", "target": "def test_extra_forbid(\n    py_and_json: PyAndJson,\n    schema_extra_behavior: dict[str, Any],\n    validate_fn_extra_kw: Union[cs.ExtraBehavior, None],\n    input_value,\n    err_type,\n) -> None:\n    v = py_and_json(\n        cs.arguments_v3_schema(\n            [\n                cs.arguments_v3_parameter(name='a', schema=cs.int_schema()),\n                cs.arguments_v3_parameter(name='b', schema=cs.int_schema(), alias='c'),\n            ],\n            extra_behavior=schema_extra_behavior,\n        ),\n    )\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_test(input_value, extra=validate_fn_extra_kw)\n    error = exc_info.value.errors()[0]\n    assert error['type'] == err_type", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000379", "source": "def get_shapes(self) -> tuple[tuple[int, ...], ...]:\n        return (\n            (32768, 256),\n            (32768, 512),\n            (32768, 1024),\n            (32768, 2048),\n            (32768, 4096),\n            (32768, 8192),\n            (32768, 16384),\n            (32768, 32768),\n            (32768, 65536),\n            (16384, 131072),\n            (8192, 262144),\n        )", "target": "def get_shapes(self) -> tuple[tuple[int, ...], ...]:\n        return (\n            (32768, 256),\n            (32768, 512),\n            (32768, 1024),\n            (32768, 2048),\n            (32768, 4096),\n            (32768, 8192),\n            (32768, 16384),\n            (32768, 32768),\n            (32768, 65536),\n        ) + extra_shapes_for_norm", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "000380", "source": "def make_estimator(self, params):\n        representation, solver = params\n        estimator = Ridge(solver=solver, fit_intercept=False, random_state=0)\n        return estimator", "target": "def make_estimator(self, params):\n        representation, precompute = params\n        estimator = Lasso(precompute=precompute, alpha=0.001, random_state=0)\n        return estimator", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000381", "source": "def replace_linear_weight_only_int8_per_channel(module):\n    for name, child in module.named_children():\n        if isinstance(child, nn.Linear) and name != \"gate\":\n            setattr(\n                module,\n                name,\n                WeightOnlyInt8Linear(\n                    child.in_features, child.out_features, target_dtype=torch.int8\n                ),\n            )\n        elif isinstance(child, ConditionalFeedForward):\n            num_experts, intermediate_size, dim = child.w1.shape\n            setattr(\n                module,\n                name,\n                ConditionalFeedForwardInt8(\n                    num_experts, intermediate_size, dim, target_dtype=torch.int8\n                ),\n            )\n        else:\n            replace_linear_weight_only_int8_per_channel(child)", "target": "def replace_linear_weight_only_int8_per_channel(module):\n    for name, child in module.named_children():\n        if isinstance(child, nn.Linear):\n            setattr(\n                module,\n                name,\n                WeightOnlyInt8Linear(child.in_features, child.out_features),\n            )\n        else:\n            replace_linear_weight_only_int8_per_channel(child)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000382", "source": "def dataclass(\n        *,\n        init: Literal[False] = False,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool = False,\n        config: ConfigDict | type[object] | None = None,\n        validate_on_init: bool | None = None,\n        kw_only: bool = ...,\n        slots: bool = ...,\n    ) -> Callable[[type[_T]], type[PydanticDataclass]]:\n        ...", "target": "def dataclass(\n        _cls: type[_T],\n        *,\n        init: Literal[False] = False,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool | None = None,\n        config: ConfigDict | type[object] | None = None,\n        validate_on_init: bool | None = None,\n        kw_only: bool = ...,\n        slots: bool = ...,\n    ) -> type[PydanticDataclass]: ...", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000383", "source": "def area(self) -> int:\n            return self.width * self.height", "target": "def area(self) -> int:\n            return self.width * self.height", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "000384", "source": "def test_validation_error_include_context():\n    v = SchemaValidator(core_schema.list_schema(max_length=2))\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python([1, 2, 3])\n    assert exc_info.value.title == 'list[any]'\n    assert exc_info.value.error_count() == 1\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'too_long',\n            'loc': (),\n            'msg': 'List should have at most 2 items after validation, not 3',\n            'input': [1, 2, 3],\n            'ctx': {'field_type': 'List', 'max_length': 2, 'actual_length': 3},\n        }\n    ]\n    assert exc_info.value.errors(include_url=False, include_context=False) == [\n        {\n            'type': 'too_long',\n            'loc': (),\n            'msg': 'List should have at most 2 items after validation, not 3',\n            'input': [1, 2, 3],\n        }\n    ]", "target": "def resolve(self, root: ASTNode) -> None:\n        errors = []\n        for item in filter(lambda item: not item.is_resolved, self):\n            try:\n                item.resolve(root)\n            except TypeResolutionError as e:\n                errors.append(str(e))\n        if len(errors) > 0:\n            raise TypeResolutionError(\n                'Failed to resolve one of \"{}\" items. Errors: {}'.format(\n                    self.full_typename, errors\n                )\n            )", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000385", "source": "def test_filter(benchmark):\n    v = SchemaSerializer(core_schema.list_schema(core_schema.any_schema()))\n    assert v.to_python(['a', 'b', 'c', 'd', 'e'], include={-1, -2}) == ['d', 'e']\n    @benchmark\n    def t():\n        v.to_python(['a', 'b', 'c', 'd', 'e'], include={-1, -2})", "target": "def test_filter():\n    s = SchemaSerializer(\n        core_schema.dict_schema(\n            serialization=core_schema.filter_dict_schema(include={'1', '3', '5'}, exclude={'5', '6'})\n        )\n    )\n    assert s.to_python({'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7}) == {'1': 1, '3': 3}", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000386", "source": "def required_usage_imports(self) -> Generator[str, None, None]:\n        yield \"import numpy\"\n        yield \"import typing as _typing\"", "target": "def required_usage_imports(self) -> Generator[str, None, None]:\n        for item in self:\n            yield from item.required_usage_imports", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000387", "source": "def constrain(\n        self: _Pipeline[_InT, _NewOutInterval], constraint: annotated_types.Interval\n    ) -> _Pipeline[_InT, _NewOutInterval]: ...", "target": "def constrain(self: _Pipeline[_InT, _OutT], constraint: _Eq) -> _Pipeline[_InT, _OutT]: ...", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000388", "source": "def get_memory_bytes(self, args, kwargs) -> int:\n        x, target = args\n        M, N = x.shape\n        dtype = x.dtype\n        return (M * N + M + M) * dtype.itemsize", "target": "def get_memory_bytes(self, args, kwargs) -> int:\n        x, w = args\n        M, N = x.shape\n        return 2 * M * N * x.dtype.itemsize + N * w.dtype.itemsize", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000389", "source": "def test_positional_only_error_required(py_and_json: PyAndJson) -> None:\n    v = py_and_json(\n        cs.arguments_v3_schema(\n            [\n                cs.arguments_v3_parameter(name='a', schema=cs.int_schema(), mode='positional_only'),\n            ]\n        )\n    )\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_test(ArgsKwargs((), {}))\n    error = exc_info.value.errors()[0]\n    assert error['type'] == 'missing_positional_only_argument'\n    assert error['loc'] == (0,)\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_test({})\n    error = exc_info.value.errors()[0]\n    assert error['type'] == 'missing_positional_only_argument'\n    assert error['loc'] == ('a',)", "target": "def test_positional_only_error_required(py_and_json: PyAndJson, input_value) -> None:\n    v = py_and_json(\n        cs.arguments_v3_schema(\n            [\n                cs.arguments_v3_parameter(name='a', schema=cs.int_schema(), mode='positional_or_keyword'),\n            ]\n        )\n    )\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_test(input_value)\n    error = exc_info.value.errors()[0]\n    assert error['type'] == 'missing_argument'\n    assert error['loc'] == ('a',)", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000390", "source": "def train(self, samples, responses):\n        self.model.train(samples, cv.ml.ROW_SAMPLE, responses)", "target": "def train(self, samples, responses):\n        self.model.setMaxDepth(20)\n        self.model.train(samples, cv.ml.ROW_SAMPLE, responses.astype(int))", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "train", "example_id": "000391", "source": "def test_changes_and_restores(self):\n        start = Path.cwd()\n        with tempfile.TemporaryDirectory() as td:\n            target = Path(td) / \"wd\"\n            target.mkdir()\n            with working_directory(str(target)):\n                self.assertEqual(Path.cwd().resolve(), target.resolve())\n        self.assertEqual(Path.cwd(), start)", "target": "def hessian_revrev(model, inp, v=None, strict=None):\n        argnums = tuple(range(len(inp)))\n        return ft.jacrev(ft.jacrev(model, argnums=argnums), argnums=argnums)(*inp)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|opencv/opencv", "data_split": "train", "example_id": "000392", "source": "def check_csv(filename):\n    df = pd.read_csv(filename)\n    failed = []\n    for _, row in df.iterrows():\n        model_name = row[\"name\"]\n        status = row[\"accuracy\"]\n        if \"pass\" not in status:\n            failed.append(model_name)\n        print(f\"{model_name:34} {status}\")\n    if failed:\n        print(\n            textwrap.dedent(\n                f\n            )\n        )\n        sys.exit(1)", "target": "def test_properties_with_reserved_keywords_names_are_transformed(self):\n        obj = cv.utils.ClassWithKeywordProperties(except_arg=23)\n        self.assertTrue(hasattr(obj, \"lambda_\"),\n                        msg=\"Class doesn't have RW property with converted name\")\n        try:\n            obj.lambda_ = 32\n        except Exception as e:\n            self.fail(\"Failed to set value to RW property. Error: {}\".format(e))\n        self.assertTrue(hasattr(obj, \"except_\"),\n                        msg=\"Class doesn't have readonly property with converted name\")\n        self.assertEqual(obj.except_, 23,\n                         msg=\"Can't access readonly property value\")\n        with self.assertRaises(AttributeError):\n            obj.except_ = 32", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "000393", "source": "def compute_bench(alpha, n_samples, n_features, precompute):\n    lasso_results = []\n    lars_lasso_results = []\n    it = 0\n    for ns in n_samples:\n        for nf in n_features:\n            it += 1\n            print(\"==================\")\n            print(\"Iteration %s of %s\" % (it, max(len(n_samples), len(n_features))))\n            print(\"==================\")\n            n_informative = nf // 10\n            X, Y, coef_ = make_regression(\n                n_samples=ns,\n                n_features=nf,\n                n_informative=n_informative,\n                noise=0.1,\n                coef=True,\n            )\n            X /= np.sqrt(np.sum(X**2, axis=0))\n            gc.collect()\n            print(\"- benchmarking Lasso\")\n            clf = Lasso(alpha=alpha, fit_intercept=False, precompute=precompute)\n            tstart = time()\n            clf.fit(X, Y)\n            lasso_results.append(time() - tstart)\n            gc.collect()\n            print(\"- benchmarking LassoLars\")\n            clf = LassoLars(alpha=alpha, fit_intercept=False, precompute=precompute)\n            tstart = time()\n            clf.fit(X, Y)\n            lars_lasso_results.append(time() - tstart)\n    return lasso_results, lars_lasso_results", "target": "def compute_bench(samples_range, features_range, n_iter=3, rank=50):\n    it = 0\n    results = defaultdict(lambda: [])\n    max_it = len(samples_range) * len(features_range)\n    for n_samples in samples_range:\n        for n_features in features_range:\n            it += 1\n            print(\"====================\")\n            print(\"Iteration %03d of %03d\" % (it, max_it))\n            print(\"====================\")\n            X = make_low_rank_matrix(\n                n_samples, n_features, effective_rank=rank, tail_strength=0.2\n            )\n            gc.collect()\n            print(\"benchmarking scipy svd: \")\n            tstart = time()\n            svd(X, full_matrices=False)\n            results[\"scipy svd\"].append(time() - tstart)\n            gc.collect()\n            print(\"benchmarking scikit-learn randomized_svd: n_iter=0\")\n            tstart = time()\n            randomized_svd(X, rank, n_iter=0)\n            results[\"scikit-learn randomized_svd (n_iter=0)\"].append(time() - tstart)\n            gc.collect()\n            print(\"benchmarking scikit-learn randomized_svd: n_iter=%d \" % n_iter)\n            tstart = time()\n            randomized_svd(X, rank, n_iter=n_iter)\n            results[\"scikit-learn randomized_svd (n_iter=%d)\" % n_iter].append(\n                time() - tstart\n            )\n    return results", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000394", "source": "def get_shapes(self) -> tuple[tuple[int, ...], ...]:\n        return (\n            (32768, 256),\n            (32768, 512),\n            (32768, 1024),\n            (32768, 2048),\n            (32768, 4096),\n            (32768, 8192),\n            (32768, 16384),\n            (32768, 32768),\n            (32768, 65536),\n            (16384, 131072),\n            (8192, 262144),\n        )", "target": "def get_shapes(self) -> tuple[tuple[int, ...], ...]:\n        return (\n            (32768, 256),\n            (32768, 512),\n            (32768, 1024),\n            (32768, 2048),\n            (32768, 4096),\n            (32768, 8192),\n            (32768, 16384),\n            (32768, 32768),\n            (32768, 65536),\n        ) + extra_shapes_for_norm", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "000395", "source": "def test_env_bool_uses_default_when_unset(self):\n        with patch.dict(os.environ, {}, clear=True):\n            self.assertTrue(m.env_bool(\"FLAG\", default=True))\n            self.assertFalse(m.env_bool(\"FLAG\", default=False))", "target": "def gen(error: bool):\n        yield 1\n        yield 2\n        if error:\n            raise RuntimeError('error')\n        yield 3", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000396", "source": "def test_json_bytes_base64_invalid():\n    v = SchemaValidator(core_schema.bytes_schema(), config=CoreConfig(val_json_bytes='base64'))\n    wrong_input = 'wrong!'\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_json(json.dumps(wrong_input))\n    assert exc_info.value.errors(include_url=False, include_context=False) == [\n        {\n            'type': 'bytes_invalid_encoding',\n            'loc': (),\n            'msg': f'Data should be valid base64: Invalid symbol {ord(\"!\")}, offset {len(wrong_input) - 1}.',\n            'input': wrong_input,\n        }\n    ]", "target": "def generator_schema(\n    items_schema: CoreSchema | None = None,\n    *,\n    min_length: int | None = None,\n    max_length: int | None = None,\n    ref: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: IncExSeqOrElseSerSchema | None = None,\n) -> GeneratorSchema:\n    return _dict_not_none(\n        type='generator',\n        items_schema=items_schema,\n        min_length=min_length,\n        max_length=max_length,\n        ref=ref,\n        metadata=metadata,\n        serialization=serialization,\n    )", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000397", "source": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "target": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000398", "source": "def required_usage_imports(self) -> Generator[str, None, None]:\n        yield \"import cv2.typing\"", "target": "def required_usage_imports(self) -> Generator[str, None, None]:\n        if TypeNode.compatible_to_runtime_usage:\n            yield \"import typing as _typing\"\n        yield from super().required_usage_imports", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000399", "source": "def compiled(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, target, dloss = args\n        compiled_cross_entropy = torch.compile(\n            lambda x, target: F.cross_entropy(x, target, reduction=\"none\"),\n            mode=self.compile_mode,\n            fullgraph=True,\n        )\n        loss = compiled_cross_entropy(x, target)\n        return lambda: torch.autograd.grad(\n            loss, x, grad_outputs=dloss, retain_graph=True\n        )", "target": "def compiled(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, dy = args\n        compiled_softmax = torch.compile(\n            lambda x: F.softmax(x, dim=-1), mode=self.compile_mode, fullgraph=True\n        )\n        y = compiled_softmax(x)\n        return lambda: torch.autograd.grad(y, x, grad_outputs=dy, retain_graph=True)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "000400", "source": "def test_dict_key(py_and_json: PyAndJson):\n    v = py_and_json({'type': 'dict', 'keys_schema': {'type': 'timedelta'}, 'values_schema': {'type': 'int'}})\n    assert v.validate_test({'P2DT1H': 2, 'P2DT2H': 4}) == {timedelta(days=2, hours=1): 2, timedelta(days=2, hours=2): 4}\n    with pytest.raises(ValidationError, match=re.escape('[type=time_delta_parsing')):\n        v.validate_test({'errordata': 2})", "target": "def test_houghcircles(self):\n        fn = \"samples/data/board.jpg\"\n        src = self.get_sample(fn, 1)\n        img = cv.cvtColor(src, cv.COLOR_BGR2GRAY)\n        img = cv.medianBlur(img, 5)\n        circles = cv.HoughCircles(img, cv.HOUGH_GRADIENT, 1, 10, np.array([]), 100, 30, 1, 30)[0]\n        testCircles = [[38, 181, 17.6],\n        [99.7, 166, 13.12],\n        [142.7, 160, 13.52],\n        [223.6, 110, 8.62],\n        [79.1, 206.7, 8.62],\n        [47.5, 351.6, 11.64],\n        [189.5, 354.4, 11.64],\n        [189.8, 298.9, 10.64],\n        [189.5, 252.4, 14.62],\n        [252.5, 393.4, 15.62],\n        [602.9, 467.5, 11.42],\n        [222, 210.4, 9.12],\n        [263.1, 216.7, 9.12],\n        [359.8, 222.6, 9.12],\n        [518.9, 120.9, 9.12],\n        [413.8, 113.4, 9.12],\n        [489, 127.2, 9.12],\n        [448.4, 121.3, 9.12],\n        [384.6, 128.9, 8.62]]\n        matches_counter = 0\n        for i in range(len(testCircles)):\n            for j in range(len(circles)):\n                tstCircle = circleApproximation(testCircles[i])\n                circle = circleApproximation(circles[j])\n                if convContoursIntersectiponRate(tstCircle, circle) > 0.6:\n                    matches_counter += 1\n        self.assertGreater(float(matches_counter) / len(testCircles), .5)\n        self.assertLess(float(len(circles) - matches_counter) / len(circles), .75)\n        circles_acc = cv.HoughCirclesWithAccumulator(\n            image=img,\n            method=cv.HOUGH_GRADIENT,\n            dp=1,\n            minDist=10,\n            circles=np.array([]),\n            param1=150,\n            param2=45,\n            minRadius=1,\n            maxRadius=30)\n        self.assertEqual(circles_acc.shape, (1, 2, 4))\n        self.assertEqual(circles_acc[0, 0, 3], 66.)\n        self.assertEqual(circles_acc[0, 1, 3], 62.)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000401", "source": "def name(self):\n        prefix = f\"{self.category()}_{self.mode()}\"\n        if self._subclass:\n            prefix += \"_subclass\"\n        else:\n            prefix += \"_nosubclass\"\n        if self.device() == \"cpu\":\n            prefix += \"_cpu\"\n        return prefix", "target": "def name(self):\n        prefix = f\"{self.category()}_{self._name}_{self.backend()}\"\n        if self.is_dynamic():\n            prefix += \"_dynamic\"\n        if self._is_gpu:\n            prefix += \"_gpu\"\n        if self._force_shape_pad:\n            prefix += \"_force_shape_pad\"\n        return prefix", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000402", "source": "def wrap_function(input_value, validator, info):\n            try:\n                return validator(input_value)\n            except ValidationError:\n                if isinstance(input_value, list):\n                    return str(len(input_value))\n                else:\n                    return repr(input_value)", "target": "def wrap_function(input_value, validator, info):\n            try:\n                return validator(input_value)\n            except ValidationError:\n                if isinstance(input_value, list):\n                    return str(len(input_value))\n                else:\n                    return repr(input_value)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000403", "source": "def get_shapes(self) -> tuple[tuple[int, ...], ...]:\n        return (\n            (32768, 256),\n            (32768, 512),\n            (32768, 1024),\n            (32768, 2048),\n            (32768, 4096),\n            (32768, 8192),\n            (32768, 16384),\n            (32768, 32768),\n            (32768, 65536),\n            (16384, 131072),\n            (8192, 262144),\n        )", "target": "def get_shapes(self) -> tuple[tuple[int, ...], ...]:\n        return (\n            (32768, 256),\n            (32768, 512),\n            (32768, 1024),\n            (32768, 2048),\n            (32768, 4096),\n            (32768, 8192),\n            (32768, 16384),\n        ) + extra_shapes_for_norm", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "000404", "source": "def test_get_cache_dir(self):\n        path = cv.utils.fs.getCacheDirectoryForDownloads()\n        self.assertTrue(os.path.exists(path))\n        self.assertTrue(os.path.isdir(path))", "target": "def test_extra_behavior_forbid(\n    config: Union[core_schema.CoreConfig, None],\n    schema_extra_behavior_kw: dict[str, Any],\n    validate_fn_extra_kw: Union[ExtraBehavior, None],\n):\n    @dataclasses.dataclass\n    class MyModel:\n        f: str\n    v = SchemaValidator(\n        core_schema.dataclass_schema(\n            MyModel,\n            core_schema.dataclass_args_schema(\n                'MyModel', [core_schema.dataclass_field('f', core_schema.str_schema())], **schema_extra_behavior_kw\n            ),\n            ['f'],\n        ),\n        config=config,\n    )\n    m: MyModel = v.validate_python({'f': 'x'}, extra=validate_fn_extra_kw)\n    assert m.f == 'x'\n    v.validate_assignment(m, 'f', 'y', extra=validate_fn_extra_kw)\n    assert m.f == 'y'\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_assignment(m, 'not_f', 'xyz', extra=validate_fn_extra_kw)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'no_such_attribute',\n            'loc': ('not_f',),\n            'msg': \"Object has no attribute 'not_f'\",\n            'input': 'xyz',\n            'ctx': {'attribute': 'not_f'},\n        }\n    ]\n    assert not hasattr(m, 'not_f')", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000405", "source": "def area(self) -> int:\n            return self.width * self.height", "target": "def area(self) -> float:\n            return self.side**2", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000406", "source": "def required_modules(self) -> Tuple[str, ...]:\n        return self._required_modules", "target": "def required_modules(self) -> Tuple[str, ...]:\n        return (*chain.from_iterable(item.required_modules for item in self.items),\n                *self._required_modules)", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "000407", "source": "def make_data(self, params):\n        data = _synth_classification_dataset(n_samples=10000, n_features=100)\n        return data", "target": "def make_data(self, params):\n        (representation,) = params\n        if representation == \"sparse\":\n            data = _20newsgroups_highdim_dataset()\n        else:\n            data = _20newsgroups_lowdim_dataset()\n        return data", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000408", "source": "def test_constraints_schema_validation_error(constraint: str) -> None:\n    with pytest.raises(SchemaError, match=f\"'{constraint}' must be coercible to a date instance\"):\n        SchemaValidator(cs.date_schema(**{constraint: 'bad_value'}))", "target": "def test_constraints_schema_validation_error(constraint: str) -> None:\n    with pytest.raises(SchemaError, match=f\"'{constraint}' must be coercible to a time instance\"):\n        SchemaValidator(core_schema.time_schema(**{constraint: 'bad_value'}))", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000409", "source": "def train(self, samples, responses):\n        self.model.train(samples, cv.ml.ROW_SAMPLE, responses)", "target": "def train(self, samples, responses):\n        self.model.train(samples, cv.ml.ROW_SAMPLE, responses)", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "000410", "source": "def functions(self) -> Dict[str, FunctionNode]:\n        return self._children[ASTNodeType.Function]", "target": "def f(input_value, _info):\n        return input_value + ' Changed'", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000411", "source": "def constrain(self: _Pipeline[_InT, _OutT], constraint: _NotIn) -> _Pipeline[_InT, _OutT]: ...", "target": "def test_model_field_wrap_validator() -> None:\n    class Model:\n        x: str\n    def f(input_value: Any, val: core_schema.ValidatorFunctionWrapHandler, info: core_schema.ValidationInfo) -> Any:\n        assert info.field_name == 'x'\n        assert info.data == {}\n        assert isinstance(input_value, bytes)\n        return f'input: {val(input_value)}'\n    v = SchemaValidator(\n        core_schema.model_schema(\n            Model,\n            core_schema.model_fields_schema(\n                {\n                    'x': core_schema.model_field(\n                        core_schema.with_info_wrap_validator_function(f, core_schema.str_schema())\n                    )\n                }\n            ),\n        )\n    )\n    assert v.validate_python({'x': b'foo'}).x == 'input: foo'", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000412", "source": "def constrain(self: _Pipeline[_InT, _NewOutLe], constraint: annotated_types.Le) -> _Pipeline[_InT, _NewOutLe]: ...", "target": "def constrain(self: _Pipeline[_InT, _OutT], constraint: _NotEq) -> _Pipeline[_InT, _OutT]: ...", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "000413", "source": "def test_python_986(self):\n        cntls = []\n        img = np.zeros((100,100,3), dtype=np.uint8)\n        color = (0,0,0)\n        cnts = np.array(cntls, dtype=np.int32).reshape((1, -1, 2))\n        try:\n            cv.fillPoly(img, cnts, color)\n            assert False\n        except:\n            assert True", "target": "def decimal_encoder(dec_value: Decimal) -> Union[int, float]:\n    exponent = dec_value.as_tuple().exponent\n    if isinstance(exponent, int) and exponent >= 0:\n        return int(dec_value)\n    else:\n        return float(dec_value)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pytorch/pytorch", "data_split": "train", "example_id": "000414", "source": "def get_output(self, input_blob):\n        assert len(input_blob.shape) == 4\n        batch_tf = input_blob.transpose(0, 2, 3, 1)\n        out = self.sess.run(self.output,\n                       {self.in_blob_name+':0': batch_tf})\n        out = out[..., 1:1001]\n        return out", "target": "def forward(\n        self, input: Tensor, state: tuple[Tensor, Tensor]\n    ) -> tuple[Tensor, tuple[Tensor, Tensor]]:\n        hx, cx = state\n        igates = self.layernorm_i(torch.mm(input, self.weight_ih.t()))\n        hgates = self.layernorm_h(torch.mm(hx, self.weight_hh.t()))\n        gates = igates + hgates\n        ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)\n        ingate = torch.sigmoid(ingate)\n        forgetgate = torch.sigmoid(forgetgate)\n        cellgate = torch.tanh(cellgate)\n        outgate = torch.sigmoid(outgate)\n        cy = self.layernorm_c((forgetgate * cx) + (ingate * cellgate))\n        hy = outgate * torch.tanh(cy)\n        return hy, (hy, cy)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|scikit-learn/scikit-learn", "data_split": "train", "example_id": "000415", "source": "def setUp(self):\n        cv.setRNGSeed(10)\n        self.image_cache = {}", "target": "def skip(self, params):\n        representation, precompute = params\n        if representation == \"sparse\" and precompute is False:\n            return True\n        return False", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000416", "source": "def relative_typename(self, module: str) -> str:\n        return self.full_typename", "target": "def relative_typename(self, module: str) -> str:\n        return '_typing.Callable[[{}], {}]'.format(\n            ', '.join(arg.relative_typename(module) for arg in self.arg_types),\n            self.ret_type.relative_typename(module)\n        )", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000417", "source": "def getCMakeArgs(self):\n        args = [\n            \"cmake\",\n            \"-GXcode\",\n            \"-DFRAMEWORK_DIR=%s\" % self.framework_dir,\n            \"-DFRAMEWORK_NAME=%s\" % self.framework_name,\n        ]\n        return args", "target": "def getCMakeArgs(self):\n        args = TestRunner.getCMakeArgs(self)\n        args = args + [\n            \"-DIOS_ARCH=%s\" % self.arch,\n            \"-DIPHONEOS_DEPLOYMENT_TARGET=%s\" % os.environ['IPHONEOS_DEPLOYMENT_TARGET'],\n        ]\n        return args", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|scikit-learn/scikit-learn", "data_split": "train", "example_id": "000418", "source": "def set_model_mocks(cls: type[BaseModel], undefined_name: str = 'all referenced types') -> None:\n    undefined_type_error_message = (\n        f'`{cls.__name__}` is not fully defined; you should define {undefined_name},'\n        f' then call `{cls.__name__}.model_rebuild()`.'\n    )\n    def attempt_rebuild_fn(attr_fn: Callable[[type[BaseModel]], T]) -> Callable[[], T | None]:\n        def handler() -> T | None:\n            if cls.model_rebuild(raise_errors=False, _parent_namespace_depth=5) is not False:\n                return attr_fn(cls)\n            return None\n        return handler\n    cls.__pydantic_core_schema__ = MockCoreSchema(\n        undefined_type_error_message,\n        code='class-not-fully-defined',\n        attempt_rebuild=attempt_rebuild_fn(lambda c: c.__pydantic_core_schema__),\n    )\n    cls.__pydantic_validator__ = MockValSer(\n        undefined_type_error_message,\n        code='class-not-fully-defined',\n        val_or_ser='validator',\n        attempt_rebuild=attempt_rebuild_fn(lambda c: c.__pydantic_validator__),\n    )\n    cls.__pydantic_serializer__ = MockValSer(\n        undefined_type_error_message,\n        code='class-not-fully-defined',\n        val_or_ser='serializer',\n        attempt_rebuild=attempt_rebuild_fn(lambda c: c.__pydantic_serializer__),\n    )", "target": "def get_profile(login):\n    print(\"get profile for %s\" % (login,))\n    try:\n        profile = get(\"https://api.github.com/users/%s\" % login).json()\n    except requests.exceptions.HTTPError:\n        return dict(name=login, avatar_url=LOGO_URL, html_url=\"\")\n    if profile[\"name\"] is None:\n        profile[\"name\"] = profile[\"login\"]\n    missing_names = {\n        \"bthirion\": \"Bertrand Thirion\",\n        \"dubourg\": \"Vincent Dubourg\",\n        \"Duchesnay\": \"Edouard Duchesnay\",\n        \"Lars\": \"Lars Buitinck\",\n        \"MechCoder\": \"Manoj Kumar\",\n    }\n    if profile[\"name\"] in missing_names:\n        profile[\"name\"] = missing_names[profile[\"name\"]]\n    return profile", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "000419", "source": "def mode(self) -> Literal['python', 'json'] | str:\n        ...", "target": "def test_parse_to_int64_convertible(self):\n        try_to_convert = partial(self._try_to_convert, cv.utils.dumpInt64)\n        min_int64, max_int64 = get_limits(ctypes.c_longlong)\n        for convertible in (-10, -1, 2, int(43.2), np.uint8(15), np.int8(33), np.int16(-13),\n                            np.int32(4), np.int64(345), (23), min_int64, max_int64, np.int_(33)):\n            expected = 'int64: {0:d}'.format(convertible)\n            actual = try_to_convert(convertible)\n            self.assertEqual(expected, actual,\n                             msg=get_conversion_error_msg(convertible, expected, actual))", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "000420", "source": "def wrap_validator(value: Any, call_next: Callable[[Any], Any], info: core_schema.ValidationInfo) -> None: ...", "target": "def _gh_post_comment(\n    url: str, comment: str, dry_run: bool = False\n) -> list[dict[str, Any]]:\n    if dry_run:\n        print(comment)\n        return []\n    return gh_fetch_json_list(url, data={\"body\": comment})", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000421", "source": "def test_exclude_unset(any_serializer):\n    m = FieldsSetModel(foo=1, bar=2, spam=3, __pydantic_fields_set__={'bar', 'spam'})\n    assert any_serializer.to_python(m) == {'foo': 1, 'bar': 2, 'spam': 3}\n    assert any_serializer.to_python(m, exclude_unset=True) == {'bar': 2, 'spam': 3}\n    assert any_serializer.to_python(m, exclude=None, exclude_unset=True) == {'bar': 2, 'spam': 3}\n    assert any_serializer.to_python(m, exclude={'bar'}, exclude_unset=True) == {'spam': 3}\n    assert any_serializer.to_python(m, exclude={'bar': ...}, exclude_unset=True) == {'spam': 3}\n    assert any_serializer.to_python(m, exclude={'bar': {}}, exclude_unset=True) == {'bar': 2, 'spam': 3}\n    assert any_serializer.to_json(m, exclude=None, exclude_unset=True) == b'{\"bar\":2,\"spam\":3}'\n    assert any_serializer.to_json(m, exclude={'bar'}, exclude_unset=True) == b'{\"spam\":3}'\n    assert any_serializer.to_json(m, exclude={'bar': ...}, exclude_unset=True) == b'{\"spam\":3}'\n    assert any_serializer.to_json(m, exclude={'bar': {}}, exclude_unset=True) == b'{\"bar\":2,\"spam\":3}'\n    m2 = FieldsSetModel(foo=1, bar=2, spam=3, __pydantic_fields_set__={'bar', 'spam', 'missing'})\n    assert any_serializer.to_python(m2) == {'foo': 1, 'bar': 2, 'spam': 3}\n    assert any_serializer.to_python(m2, exclude_unset=True) == {'bar': 2, 'spam': 3}", "target": "def test_exclude_unset():\n    s = SchemaSerializer(\n        core_schema.model_schema(\n            FieldsSetModel,\n            core_schema.model_fields_schema(\n                {\n                    'foo': core_schema.model_field(core_schema.int_schema()),\n                    'bar': core_schema.model_field(core_schema.int_schema()),\n                    'spam': core_schema.model_field(core_schema.int_schema()),\n                },\n                extra_behavior='ignore',\n            ),\n        )\n    )\n    m = FieldsSetModel(foo=1, bar=2, spam=3, __pydantic_fields_set__={'bar', 'spam'})\n    assert s.to_python(m) == {'foo': 1, 'bar': 2, 'spam': 3}\n    assert s.to_python(m, exclude_unset=True) == {'bar': 2, 'spam': 3}\n    assert s.to_python(m, exclude=None, exclude_unset=True) == {'bar': 2, 'spam': 3}\n    assert s.to_python(m, exclude={'bar'}, exclude_unset=True) == {'spam': 3}\n    assert s.to_python(m, exclude={'bar': ...}, exclude_unset=True) == {'spam': 3}\n    assert s.to_python(m, exclude={'bar': {}}, exclude_unset=True) == {'bar': 2, 'spam': 3}\n    assert s.to_json(m, exclude=None, exclude_unset=True) == b'{\"bar\":2,\"spam\":3}'\n    assert s.to_json(m, exclude={'bar'}, exclude_unset=True) == b'{\"spam\":3}'\n    assert s.to_json(m, exclude={'bar': ...}, exclude_unset=True) == b'{\"spam\":3}'\n    assert s.to_json(m, exclude={'bar': {}}, exclude_unset=True) == b'{\"bar\":2,\"spam\":3}'\n    m2 = FieldsSetModel(foo=1, bar=2, spam=3, __pydantic_fields_set__={'bar', 'spam', 'missing'})\n    assert s.to_python(m2) == {'foo': 1, 'bar': 2, 'spam': 3}\n    assert s.to_python(m2, exclude_unset=True) == {'bar': 2, 'spam': 3}", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000422", "source": "def test_detect(self):\n        img = cv.imread(os.path.join(self.extraTestDataPath, 'cv/qrcode/link_ocv.jpg'))\n        self.assertFalse(img is None)\n        detector = cv.QRCodeDetector()\n        retval, points = detector.detect(img)\n        self.assertTrue(retval)\n        self.assertEqual(points.shape, (1, 4, 2))", "target": "def test_detect(self):\n        img = cv.imread(os.path.join(self.extraTestDataPath, 'cv/barcode/multiple/4_barcodes.jpg'))\n        self.assertFalse(img is None)\n        detector = cv.barcode_BarcodeDetector()\n        retval, corners = detector.detect(img)\n        self.assertTrue(retval)\n        self.assertEqual(corners.shape, (4, 4, 2))", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "000423", "source": "def make_data(self, params):\n        representation, solver, n_jobs = params\n        if Benchmark.data_size == \"large\":\n            if representation == \"sparse\":\n                data = _20newsgroups_highdim_dataset(n_samples=10000)\n            else:\n                data = _20newsgroups_lowdim_dataset(n_components=1e3)\n        else:\n            if representation == \"sparse\":\n                data = _20newsgroups_highdim_dataset(n_samples=2500)\n            else:\n                data = _20newsgroups_lowdim_dataset()\n        return data", "target": "def make_data(self, params):\n        (representation,) = params\n        if representation == \"sparse\":\n            data = _20newsgroups_highdim_dataset()\n        else:\n            data = _20newsgroups_lowdim_dataset()\n        return data", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "000424", "source": "def test_serialize_as_any_with_dataclass() -> None:\n    @dataclass\n    class Parent:\n        x: int\n    class Child(Parent):\n        y: str\n    Parent.__pydantic_core_schema__ = core_schema.dataclass_schema(\n        Parent,\n        core_schema.dataclass_args_schema(\n            'Parent',\n            [\n                core_schema.dataclass_field(name='x', schema=core_schema.int_schema()),\n            ],\n        ),\n        ['x'],\n    )\n    Parent.__pydantic_validator__ = SchemaValidator(Parent.__pydantic_core_schema__)\n    Parent.__pydantic_serializer__ = SchemaSerializer(Parent.__pydantic_core_schema__)\n    Child.__pydantic_core_schema__ = core_schema.dataclass_schema(\n        Child,\n        core_schema.dataclass_args_schema(\n            'Child',\n            [\n                core_schema.dataclass_field(name='x', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(name='y', schema=core_schema.str_schema()),\n            ],\n        ),\n        ['x', 'y'],\n    )\n    Child.__pydantic_validator__ = SchemaValidator(Child.__pydantic_core_schema__)\n    Child.__pydantic_serializer__ = SchemaSerializer(Child.__pydantic_core_schema__)\n    child = Child.__pydantic_validator__.validate_python({'x': 1, 'y': 'hopefully not a secret'})\n    assert Parent.__pydantic_serializer__.to_python(child, serialize_as_any=False) == {'x': 1}\n    assert Parent.__pydantic_serializer__.to_python(child, serialize_as_any=True) == {\n        'x': 1,\n        'y': 'hopefully not a secret',\n    }", "target": "def cvt_nv12_to_yuv(self, y, uv):\n            h,w,_ = uv.shape\n            upsample_uv = cv.resize(uv, (h * 2, w * 2))\n            return cv.merge([y, upsample_uv])", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000425", "source": "def benchmark(self):\n        for M, N in self.get_shapes():\n            print(f\"Tensor dimensions: [{M}, {N}]\")\n            torch_dtype = cutlass_torch.dtype(cutlass.BFloat16)\n            x = 0.1 * torch.randn(\n                M, N, device=\"cuda\", dtype=torch_dtype, requires_grad=True\n            )\n            dy = torch.randn(M, N, device=\"cuda\", dtype=torch_dtype)\n            self.benchmark_single_shape((x, dy), setting=f\"shape: [{M}, {N}]\")", "target": "def benchmark(self):\n        for M, N in self.get_shapes():\n            print(f\"Tensor dimensions: [{M}, {N}]\")\n            torch_dtype = cutlass_torch.dtype(cutlass.BFloat16)\n            x = torch.randn(M, N, device=\"cuda\", dtype=torch_dtype, requires_grad=True)\n            w = torch.randn(N, device=\"cuda\", dtype=torch.float32, requires_grad=True)\n            dy = torch.randn(M, N, device=\"cuda\", dtype=torch_dtype)\n            self.benchmark_single_shape((x, w, dy), setting=f\"shape: [{M}, {N}]\")", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "000426", "source": "def main():\n    result_path = sys.argv[1]\n    benchmarks = [\n        Benchmark(NestedModule),\n    ]\n    for b in benchmarks:\n        b.enable_compile_time_instruction_count().collect_all().append_results(\n            result_path\n        )", "target": "def test_complete_core_serializer_to_json(benchmark):\n    core_schema = schema()\n    v = SchemaValidator(core_schema)\n    model = v.validate_python(input_data_lax())\n    serializer = SchemaSerializer(core_schema)\n    benchmark(serializer.to_json, model)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "000427", "source": "def test_local_image_exists_without_client_param_calls_get_client_once(self):\n        with mock.patch(\"cli.lib.common.docker_helper._get_client\") as mock_get_client:\n            mock_client = MagicMock()\n            mock_get_client.return_value = mock_client\n            local_image_exists(\"repo:tag\")\n            local_image_exists(\"repo:tag2\")\n            self.assertEqual(mock_get_client.call_count, 2)\n            self.assertEqual(mock_client.images.get.call_count, 2)\n            mock_client.images.get.assert_any_call(\"repo:tag\")\n            mock_client.images.get.assert_any_call(\"repo:tag2\")", "target": "def test_datetime_key():\n    v = SchemaSerializer(core_schema.dict_schema(core_schema.datetime_schema(), core_schema.datetime_schema()))\n    assert v.to_python({datetime(2022, 12, 2, 12, 13, 14): datetime(2022, 12, 2, 12, 13, 14)}) == {\n        datetime(2022, 12, 2, 12, 13, 14): datetime(2022, 12, 2, 12, 13, 14)\n    }\n    assert v.to_python({datetime(2022, 12, 2, 12, 13, 14): datetime(2022, 12, 2, 12, 13, 14)}, mode='json') == {\n        '2022-12-02T12:13:14': '2022-12-02T12:13:14'\n    }\n    assert (\n        v.to_json({datetime(2022, 12, 2, 12, 13, 14): datetime(2022, 12, 2, 12, 13, 14)})\n        == b'{\"2022-12-02T12:13:14\":\"2022-12-02T12:13:14\"}'\n    )", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000428", "source": "def create_class_node(self, class_info, namespaces):\n            return create_class_node(self.cv_root, class_info, namespaces)", "target": "def create_class_node(self, class_info, namespaces):\n            return ClassNode()", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "000429", "source": "def fs_model_serializer_fixture():\n    return SchemaSerializer(\n        core_schema.model_schema(\n            FieldsSetModel,\n            core_schema.model_fields_schema(\n                {\n                    'a': core_schema.model_field(core_schema.int_schema()),\n                    'b': core_schema.model_field(core_schema.int_schema()),\n                    'c': core_schema.model_field(core_schema.int_schema()),\n                    'd': core_schema.model_field(core_schema.int_schema()),\n                    'e': core_schema.model_field(core_schema.int_schema()),\n                    'f': core_schema.model_field(core_schema.int_schema()),\n                    'g': core_schema.model_field(core_schema.int_schema()),\n                    'h': core_schema.model_field(core_schema.int_schema()),\n                }\n            ),\n        )\n    )", "target": "def test_default_matx_argument(self):\n        res = cv.utils.dumpVec2i()\n        self.assertEqual(res, \"Vec2i(42, 24)\",\n                         msg=\"Default argument is not properly handled\")\n        res = cv.utils.dumpVec2i((12, 21))\n        self.assertEqual(res, \"Vec2i(12, 21)\")", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000430", "source": "def benchmark(self):\n        for M, N in self.get_shapes():\n            print(f\"Tensor dimensions: [{M}, {N}]\")\n            torch_dtype = cutlass_torch.dtype(cutlass.BFloat16)\n            x = 0.1 * torch.randn(\n                M, N, device=\"cuda\", dtype=torch_dtype, requires_grad=True\n            )\n            target = torch.randint(0, N, (M,), device=\"cuda\", dtype=torch.int64)\n            dloss = torch.randn(M, device=\"cuda\", dtype=torch.float32)\n            self.benchmark_single_shape(\n                (x, target, dloss), setting=f\"shape: [{M}, {N}]\"\n            )", "target": "def benchmark(self):\n        for M, N in self.get_shapes():\n            print(f\"Tensor dimensions: [{M}, {N}]\")\n            torch_dtype = cutlass_torch.dtype(cutlass.BFloat16)\n            x = 0.1 * torch.randn(M, N, device=\"cuda\", dtype=torch_dtype)\n            self.benchmark_single_shape((x,), setting=f\"shape: [{M}, {N}]\")", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000431", "source": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "target": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000432", "source": "def test_serialize_as_any_with_nested_models() -> None:\n    class Parent:\n        x: int\n    class Other(Parent):\n        y: str\n    class Outer:\n        p: Parent\n    Parent.__pydantic_core_schema__ = core_schema.model_schema(\n        Parent,\n        core_schema.model_fields_schema(\n            {\n                'x': core_schema.model_field(core_schema.int_schema()),\n            }\n        ),\n        ref='Parent',\n    )\n    Parent.__pydantic_validator__ = SchemaValidator(Parent.__pydantic_core_schema__)\n    Parent.__pydantic_serializer__ = SchemaSerializer(Parent.__pydantic_core_schema__)\n    Other.__pydantic_core_schema__ = core_schema.model_schema(\n        Other,\n        core_schema.model_fields_schema(\n            {\n                'x': core_schema.model_field(core_schema.int_schema()),\n                'y': core_schema.model_field(core_schema.str_schema()),\n            }\n        ),\n        config=core_schema.CoreConfig(extra_fields_behavior='allow'),\n    )\n    Other.__pydantic_validator__ = SchemaValidator(Other.__pydantic_core_schema__)\n    Other.__pydantic_serializer__ = SchemaSerializer(Other.__pydantic_core_schema__)\n    Outer.__pydantic_core_schema__ = core_schema.definitions_schema(\n        core_schema.model_schema(\n            Outer,\n            core_schema.model_fields_schema(\n                {\n                    'p': core_schema.model_field(core_schema.definition_reference_schema('Parent')),\n                }\n            ),\n        ),\n        [\n            Parent.__pydantic_core_schema__,\n        ],\n    )\n    Outer.__pydantic_validator__ = SchemaValidator(Outer.__pydantic_core_schema__)\n    Outer.__pydantic_serializer__ = SchemaSerializer(Outer.__pydantic_core_schema__)\n    other = Other.__pydantic_validator__.validate_python({'x': 1, 'y': 'hopefully not a secret'})\n    outer = Outer()\n    outer.p = other\n    assert Outer.__pydantic_serializer__.to_python(outer, serialize_as_any=False) == {\n        'p': {'x': 1},\n    }\n    assert Outer.__pydantic_serializer__.to_python(outer, serialize_as_any=True) == {\n        'p': {\n            'x': 1,\n            'y': 'hopefully not a secret',\n        }\n    }\n    assert Outer.__pydantic_serializer__.to_json(outer, serialize_as_any=False) == b'{\"p\":{\"x\":1}}'\n    assert (\n        Outer.__pydantic_serializer__.to_json(outer, serialize_as_any=True)\n        == b'{\"p\":{\"x\":1,\"y\":\"hopefully not a secret\"}}'\n    )", "target": "def test_model():\n    class MyModel:\n        __slots__ = '__dict__', '__pydantic_fields_set__', '__pydantic_extra__', '__pydantic_private__'\n        field_a: int\n        field_b: date\n    v = SchemaValidator(\n        core_schema.model_schema(\n            MyModel,\n            core_schema.model_fields_schema(\n                {\n                    'field_a': core_schema.model_field(core_schema.int_schema()),\n                    'field_b': core_schema.model_field(core_schema.date_schema()),\n                }\n            ),\n        )\n    )\n    m2 = v.validate_strings({'field_a': '1', 'field_b': '2017-01-01'})\n    assert m2.__dict__ == {'field_a': 1, 'field_b': date(2017, 1, 1)}\n    m2 = v.validate_strings({'field_a': '1', 'field_b': '2017-01-01'}, strict=True)\n    assert m2.__dict__ == {'field_a': 1, 'field_b': date(2017, 1, 1)}", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000433", "source": "def test_dataclass_args_init_only_no_fields(py_and_json: PyAndJson, input_value, expected):\n    schema = core_schema.dataclass_args_schema(\n        'MyDataclass', [core_schema.dataclass_field(name='a', schema=core_schema.str_schema())], collect_init_only=True\n    )\n    v = py_and_json(schema)\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=expected.message) as exc_info:\n            v.validate_test(input_value)\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        assert v.validate_test(input_value) == expected", "target": "def test_positional_empty_extra(py_and_json: PyAndJson):\n    v = py_and_json({'type': 'tuple', 'items_schema': [{'type': 'int'}], 'variadic_item_index': 0})\n    assert v.validate_test([]) == ()\n    assert v.validate_python(()) == ()\n    assert v.validate_test([1]) == (1,)\n    assert v.validate_test(list(range(100))) == tuple(range(100))", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "000434", "source": "def get_config_update(self, name: str, arg: Expression, lax_extra: bool = False) -> ModelConfigData | None:\n        if name not in self.tracked_config_fields:\n            return None\n        if name == 'extra':\n            if isinstance(arg, StrExpr):\n                forbid_extra = arg.value == 'forbid'\n            elif isinstance(arg, MemberExpr):\n                forbid_extra = arg.name == 'forbid'\n            else:\n                if not lax_extra:\n                    error_invalid_config_value(name, self._api, arg)\n                return None\n            return ModelConfigData(forbid_extra=forbid_extra)\n        if name == 'alias_generator':\n            has_alias_generator = True\n            if isinstance(arg, NameExpr) and arg.fullname == 'builtins.None':\n                has_alias_generator = False\n            return ModelConfigData(has_alias_generator=has_alias_generator)\n        if isinstance(arg, NameExpr) and arg.fullname in ('builtins.True', 'builtins.False'):\n            return ModelConfigData(**{name: arg.fullname == 'builtins.True'})\n        error_invalid_config_value(name, self._api, arg)\n        return None", "target": "def _generate_docker_build_cmd(\n        self,\n        inputs: VllmBuildParameters,\n    ) -> str:\n        base_image_arg, final_base_image_arg, pull_flag = self._get_base_image_args(\n            inputs\n        )\n        torch_arg = self._get_torch_wheel_path_arg(inputs.torch_whls_path)\n        return textwrap.dedent(\n            f\n        ).strip()", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000435", "source": "def liger(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        from liger_kernel.transformers.cross_entropy import LigerCrossEntropyLoss\n        x, target, dloss = args\n        cross_entropy = LigerCrossEntropyLoss(reduction=\"none\")\n        loss = cross_entropy(x, target)\n        return lambda: torch.autograd.grad(\n            loss, x, grad_outputs=dloss, retain_graph=True\n        )", "target": "def liger(self, args, kwargs) -> Any:\n        from liger_kernel.ops.layer_norm import layer_norm_backward\n        x, w, dy = args\n        eps = 1e-6\n        mean, rstd = self.compute_mean_rstd(x, eps)\n        M, N = x.shape\n        return lambda: layer_norm_backward(dy, x, w, None, mean, rstd)[0:2]", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000436", "source": "def test_version():\n    assert isinstance(__version__, str)\n    assert '.' in __version__", "target": "def prepare_config(config: ConfigDict | dict[str, Any] | type[Any] | None) -> ConfigDict:\n    if config is None:\n        return ConfigDict()\n    if not isinstance(config, dict):\n        warnings.warn(DEPRECATION_MESSAGE, PydanticDeprecatedSince20, stacklevel=4)\n        config = {k: getattr(config, k) for k in dir(config) if not k.startswith('__')}\n    config_dict = cast(ConfigDict, config)\n    check_deprecated(config_dict)\n    return config_dict", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "000437", "source": "def arg_types(self) -> Sequence[TypeNode]:\n        return self.items[:-1]", "target": "def general_wrap_validator_function(*args, **kwargs):\n    warnings.warn(\n        '`general_wrap_validator_function` is deprecated, use `with_info_wrap_validator_function` instead.',\n        DeprecationWarning,\n    )\n    return with_info_wrap_validator_function(*args, **kwargs)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000438", "source": "def rm_one(d):\n    d = os.path.abspath(d)\n    if os.path.exists(d):\n        if os.path.isdir(d):\n            log.info(\"Removing dir: %s\", d)\n            shutil.rmtree(d)\n        elif os.path.isfile(d):\n            log.info(\"Removing file: %s\", d)\n            os.remove(d)", "target": "def rm_one(d):\n    d = str(d)\n    d = os.path.abspath(d)\n    if os.path.exists(d):\n        if os.path.isdir(d):\n            log.info(\"Removing dir: %s\", d)\n            shutil.rmtree(d)\n        elif os.path.isfile(d):\n            log.info(\"Removing file: %s\", d)\n            os.remove(d)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000439", "source": "def test_no_args(py_and_json: PyAndJson, input_value, expected) -> None:\n    v = py_and_json(cs.arguments_v3_schema([]))\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError) as exc_info:\n            v.validate_test(input_value)\n        error = exc_info.value.errors()[0]\n        assert error['type'] == expected.errors[0]['type']\n    else:\n        assert v.validate_test(input_value) == expected", "target": "def test_no_args(py_and_json: PyAndJson, input_value, expected):\n    v = py_and_json({'type': 'arguments', 'arguments_schema': []})\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)) as exc_info:\n            v.validate_test(input_value)\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        assert v.validate_test(input_value) == expected", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000440", "source": "def run(img1, img2, dtype):\n            return cv.add(img1, img2)", "target": "def run(img0, img1):\n                    raise Exception('Error')\n                    return img0 + img1", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|opencv/opencv", "data_split": "train", "example_id": "000441", "source": "def norm_hamming(x, y=None):\n    def norm(vec):\n        return sum(bin(i).count('1') for i in vec.flatten())\n    return norm(x) if y is None else norm(np.bitwise_xor(x, y))", "target": "def test_cuda_convertTo(self):\n        npMat_8UC4 = (np.random.random((128, 128, 4)) * 255).astype(np.uint8)\n        npMat_32FC4 = npMat_8UC4.astype(np.single)\n        new_type = cv.CV_32FC4\n        cuMat_8UC4 = cv.cuda_GpuMat(npMat_8UC4)\n        cuMat_32FC4 = cv.cuda_GpuMat(cuMat_8UC4.size(), new_type)\n        cuMat_32FC4_out = cuMat_8UC4.convertTo(new_type, cuMat_32FC4)\n        self.assertTrue(cuMat_32FC4.cudaPtr() == cuMat_32FC4_out.cudaPtr())\n        npMat_32FC4_out = cuMat_32FC4.download()\n        self.assertTrue(np.array_equal(npMat_32FC4, npMat_32FC4_out))\n        cuMat_32FC4_out = cuMat_8UC4.convertTo(new_type)\n        npMat_32FC4_out = cuMat_32FC4.download()\n        self.assertTrue(np.array_equal(npMat_32FC4, npMat_32FC4_out))\n        stream = cv.cuda.Stream()\n        cuMat_32FC4 = cv.cuda_GpuMat(cuMat_8UC4.size(), new_type)\n        cuMat_32FC4_out = cuMat_8UC4.convertTo(new_type, cuMat_32FC4)\n        cuMat_32FC4_out = cuMat_8UC4.convertTo(new_type, 1, 0, stream, cuMat_32FC4)\n        self.assertTrue(cuMat_32FC4.cudaPtr() == cuMat_32FC4_out.cudaPtr())\n        npMat_32FC4_out = cuMat_32FC4.download(stream)\n        stream.waitForCompletion()\n        self.assertTrue(np.array_equal(npMat_32FC4, npMat_32FC4_out))\n        cuMat_32FC4_out = cuMat_8UC4.convertTo(new_type, 1, 0, stream)\n        npMat_32FC4_out = cuMat_32FC4.download(stream)\n        stream.waitForCompletion()\n        self.assertTrue(np.array_equal(npMat_32FC4, npMat_32FC4_out))", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000442", "source": "def isTest(self, fullpath):\n        if os.path.isfile(fullpath):\n            if fullpath.endswith(\".apk\") or os.access(fullpath, os.X_OK):\n                return True\n        return False", "target": "def isTest(self, fullpath):\n        if fullpath in ['java', 'python2', 'python3']:\n            return self.options.mode == 'test'\n        if not os.path.isfile(fullpath):\n            return False\n        if self.cache.getOS() == \"nt\" and not fullpath.endswith(\".exe\"):\n            return False\n        return os.access(fullpath, os.X_OK)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "000443", "source": "def _get_torch_wheel_path_arg(self, torch_whl_dir: Optional[Path]) -> str:\n        if not torch_whl_dir:\n            return \"\"\n        return f\"--build-arg TORCH_WHEELS_PATH={_VLLM_TEMP_FOLDER}\"", "target": "def test_val_info_repr(config, kwargs, expected_repr):\n    def f(input_value, info: core_schema.ValidationInfo):\n        assert repr(info) == expected_repr\n        assert str(info) == expected_repr\n        return input_value\n    v = SchemaValidator(core_schema.with_info_before_validator_function(f, core_schema.str_schema()), config=config)\n    assert v.validate_python('input value', **kwargs) == 'input value'", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000444", "source": "def get_memory_bytes(self, args, kwargs) -> int:\n        x, w = args\n        M, N = x.shape\n        return 2 * M * N * x.dtype.itemsize + N * w.dtype.itemsize", "target": "def get_memory_bytes(self, args, kwargs) -> int:\n        x, w, dy = args\n        M, N = x.shape\n        return 3 * M * N * x.dtype.itemsize + 2 * N * w.dtype.itemsize", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000445", "source": "def required_definition_imports(self) -> Generator[str, None, None]:\n        yield from ()", "target": "def required_definition_imports(self) -> Generator[str, None, None]:\n        yield \"import typing as _typing\"\n        yield from super().required_definition_imports", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "000446", "source": "def test_filter_args(params):\n    s = SchemaSerializer(core_schema.dict_schema())\n    include, exclude, expected = params['include'], params['exclude'], IsStrictDict(params['expected'])\n    value = {'0': 0, '1': 1, '2': 2, '3': 3}\n    assert s.to_python(value, include=include, exclude=exclude) == expected\n    assert s.to_python(value, mode='json', include=include, exclude=exclude) == expected\n    assert json.loads(s.to_json(value, include=include, exclude=exclude)) == expected", "target": "def medium_sliced():\n    return (rand(32, 12, 64, 64)[..., ::2], rand(32, 12, 64, 64)[..., ::2])", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000447", "source": "def relative_typename(self, module: str) -> str:\n        return self.type_format.format(self.types_separator.join(\n            item.relative_typename(module) for item in self\n        ))", "target": "def relative_typename(self, module: str) -> str:\n        return '_typing.Callable[[{}], {}]'.format(\n            ', '.join(arg.relative_typename(module) for arg in self.arg_types),\n            self.ret_type.relative_typename(module)\n        )", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000448", "source": "def test_dataclass_slots_init_vars(any_serializer):\n    @dataclasses.dataclass(slots=True)\n    class Foo:\n        a: int\n        b: str\n        c: dataclasses.InitVar[int]\n        d: ClassVar[int] = 42\n    foo = Foo(1, 'a', 42)\n    assert any_serializer.to_python(foo) == IsStrictDict(a=1, b='a')\n    assert any_serializer.to_json(foo) == b'{\"a\":1,\"b\":\"a\"}'", "target": "def test_model_a(self, schema_validator: SchemaValidator):\n        m_a = schema_validator.validate_python({'a': 1, 'b': 'hello'})\n        assert isinstance(m_a, self.ModelA)\n        assert m_a.a == 1\n        assert m_a.b == 'hello'", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000449", "source": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "target": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000450", "source": "def test_validate_assignment_ignore_extra():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(fields={'field_a': core_schema.model_field(schema=core_schema.str_schema())})\n    )\n    assert v.validate_python({'field_a': 'test'}) == ({'field_a': 'test'}, None, {'field_a'})\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_assignment({'field_a': 'test'}, 'other_field', 456)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'no_such_attribute',\n            'loc': ('other_field',),\n            'msg': \"Object has no attribute 'other_field'\",\n            'input': 456,\n            'ctx': {'attribute': 'other_field'},\n        }\n    ]", "target": "def field_plain_validator_function(function: WithInfoValidatorFunction, field_name: str, **kwargs):\n    warnings.warn(\n        '`field_plain_validator_function` is deprecated, use `with_info_plain_validator_function` instead.',\n        DeprecationWarning,\n    )\n    return with_info_plain_validator_function(function, field_name=field_name, **kwargs)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000451", "source": "def test_function_wrap():\n    def f(input_value, validator, info):\n        return validator(input_value=input_value) + ' Changed'\n    v = SchemaValidator(core_schema.with_info_wrap_validator_function(f, core_schema.str_schema()))\n    assert v.validate_python('input value') == 'input value Changed'", "target": "def test_custom_timezone_utc_repr():\n    output = SchemaValidator(cs.datetime_schema()).validate_python('2022-06-08T12:13:14Z')\n    assert output == datetime(2022, 6, 8, 12, 13, 14, tzinfo=timezone(timedelta(0)))\n    assert output.tzinfo.utcoffset(output) == timedelta(0)\n    assert output.tzinfo.dst(output) is None\n    assert output.tzinfo.tzname(output) == 'UTC'\n    assert str(output.tzinfo) == 'UTC'\n    assert repr(output.tzinfo) == 'TzInfo(0)'", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000452", "source": "def predict(self, samples):\n        _ret, resp = self.model.predict(samples)\n        return resp.ravel()", "target": "def predict(self, samples):\n        _ret, resp = self.model.predict(samples)\n        return resp.argmax(-1)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "000453", "source": "def time_cuda(fn, args, iters):\n    start = torch.cuda.Event(enable_timing=True)\n    end = torch.cuda.Event(enable_timing=True)\n    start.record()\n    for _ in range(iters):\n        fn(*args)\n    end.record()\n    torch.cuda.synchronize()\n    return start.elapsed_time(end) / 1e3", "target": "def test_negative(schema_func, seq_f):\n    v = SchemaSerializer(schema_func(core_schema.any_schema()))\n    assert v.to_python(seq_f('a', 'b', 'c', 'd', 'e')) == seq_f('a', 'b', 'c', 'd', 'e')\n    assert v.to_python(seq_f('a', 'b', 'c', 'd', 'e'), include={-1, -2}) == seq_f('d', 'e')\n    assert v.to_python(seq_f('a', 'b', 'c', 'd', 'e'), include={-1: None, -2: None}) == seq_f('d', 'e')\n    assert v.to_python(seq_f('a', 'b', 'c', 'd', 'e'), include={-1, -2}, mode='json') == ['d', 'e']\n    assert v.to_python(seq_f('a', 'b', 'c', 'd', 'e'), include={-1: None, -2: None}, mode='json') == ['d', 'e']\n    assert v.to_json(seq_f('a', 'b', 'c', 'd', 'e'), include={-1, -2}) == b'[\"d\",\"e\"]'", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000454", "source": "def gen(error: bool):\n        yield 1\n        yield 2\n        if error:\n            raise RuntimeError('my error')\n        yield 3", "target": "def gen(error: bool):\n        yield 1\n        yield 2\n        if error:\n            raise RuntimeError('my error')\n        yield 3", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000455", "source": "def test_only_validate_by_name(py_and_json) -> None:\n    v = py_and_json(\n        cs.arguments_v3_schema(\n            [\n                cs.arguments_v3_parameter(\n                    name='a', schema=cs.str_schema(), alias='FieldA', mode='positional_or_keyword'\n                ),\n            ],\n            validate_by_name=True,\n            validate_by_alias=False,\n        )\n    )\n    assert v.validate_test(ArgsKwargs((), {'a': 'hello'})) == ((), {'a': 'hello'})\n    assert v.validate_test({'a': 'hello'}) == (('hello',), {})\n    with pytest.raises(ValidationError, match=r'a\\n +Missing required argument \\[type=missing_argument,'):\n        assert v.validate_test(ArgsKwargs((), {'FieldA': 'hello'}))\n    with pytest.raises(ValidationError, match=r'a\\n +Missing required argument \\[type=missing_argument,'):\n        assert v.validate_test({'FieldA': 'hello'})", "target": "def test_only_validate_by_name(py_and_json) -> None:\n    v = py_and_json(\n        {\n            'type': 'typed-dict',\n            'fields': {\n                'field_a': {'validation_alias': 'FieldA', 'type': 'typed-dict-field', 'schema': {'type': 'int'}}\n            },\n            'config': {'validate_by_name': True, 'validate_by_alias': False},\n        }\n    )\n    assert v.validate_test({'field_a': '123'}) == {'field_a': 123}\n    with pytest.raises(ValidationError, match=r'field_a\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'FieldA': '123'})", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "000456", "source": "def listIntSumReturnTuple(self, input: list[int]) -> tuple[list[int], int]:\n        sum = 0\n        for x in input:\n            sum += x\n        return (input, sum)", "target": "def test_function_before():\n    def f(input_value, _info):\n        assert isinstance(input_value, dict)\n        input_value['field_a'] += b' XX'\n        return input_value\n    v = SchemaValidator(\n        {\n            'type': 'function-before',\n            'function': {'type': 'with-info', 'function': f},\n            'schema': core_schema.model_schema(\n                cls=MyModel,\n                schema=core_schema.model_fields_schema(\n                    fields={\n                        'field_a': core_schema.model_field(schema=core_schema.str_schema()),\n                        'field_b': core_schema.model_field(schema=core_schema.int_schema()),\n                    }\n                ),\n            ),\n        }\n    )\n    m = v.validate_python({'field_a': b'321', 'field_b': '12'})\n    assert isinstance(m, MyModel)\n    assert m.field_a == '321 XX'\n    assert m.field_b == 12\n    m2 = MyModel()\n    v.validate_python({'field_a': b'321', 'field_b': '12'}, self_instance=m2)\n    assert m2.__dict__ == {'field_a': '321 XX', 'field_b': 12}\n    assert m2.__pydantic_fields_set__ == {'field_a', 'field_b'}", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000457", "source": "def benchmark(self):\n        for M, N in self.get_shapes():\n            print(f\"Tensor dimensions: [{M}, {N}]\")\n            torch_dtype = cutlass_torch.dtype(cutlass.BFloat16)\n            x = 0.1 * torch.randn(\n                M, N, device=\"cuda\", dtype=torch_dtype, requires_grad=True\n            )\n            target = torch.randint(0, N, (M,), device=\"cuda\", dtype=torch.int64)\n            dloss = torch.randn(M, device=\"cuda\", dtype=torch.float32)\n            self.benchmark_single_shape(\n                (x, target, dloss), setting=f\"shape: [{M}, {N}]\"\n            )", "target": "def benchmark(self):\n        for M, N in self.get_shapes():\n            print(f\"Tensor dimensions: [{M}, {N}]\")\n            torch_dtype = cutlass_torch.dtype(cutlass.BFloat16)\n            x = 0.1 * torch.randn(\n                M, N, device=\"cuda\", dtype=torch_dtype, requires_grad=True\n            )\n            dy = torch.randn(M, N, device=\"cuda\", dtype=torch_dtype)\n            self.benchmark_single_shape((x, dy), setting=f\"shape: [{M}, {N}]\")", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "000458", "source": "def assertLess(self, a, b, msg=None):\n            if not a < b:\n                self.fail('%s not less than %s' % (repr(a), repr(b)))", "target": "def literal_schema(\n    expected: list[Any],\n    *,\n    ref: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> LiteralSchema:\n    return _dict_not_none(type='literal', expected=expected, ref=ref, metadata=metadata, serialization=serialization)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "000459", "source": "def from_name(cls, name: str):\n        if name in transformer_configs:\n            return cls(**transformer_configs[name])\n        config = [\n            config\n            for config in transformer_configs\n            if config in str(name).upper() or config in str(name)\n        ]\n        if len(config) > 1:\n            config.sort(key=len, reverse=True)\n            assert len(config[0]) != len(config[1]), (\n                name\n            )\n        return cls(**transformer_configs[config[0]])", "target": "def test_frozen_field():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'name': core_schema.model_field(schema=core_schema.str_schema()),\n                'age': core_schema.model_field(schema=core_schema.int_schema()),\n                'is_developer': core_schema.model_field(\n                    schema=core_schema.with_default_schema(schema=core_schema.bool_schema(), default=True), frozen=True\n                ),\n            }\n        )\n    )\n    r1, model_extra, fields_set = v.validate_python({'name': 'Samuel', 'age': '36'})\n    assert r1 == {'name': 'Samuel', 'age': 36, 'is_developer': True}\n    assert model_extra is None\n    assert fields_set == {'name', 'age'}\n    v.validate_assignment(r1, 'age', '35')\n    assert r1 == {'name': 'Samuel', 'age': 35, 'is_developer': True}\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_assignment(r1, 'is_developer', False)\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'frozen_field', 'loc': ('is_developer',), 'msg': 'Field is frozen', 'input': False}\n    ]", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000460", "source": "def constrain(self: _Pipeline[_InT, _NewOutLe], constraint: annotated_types.Le) -> _Pipeline[_InT, _NewOutLe]: ...", "target": "def constrain(self: _Pipeline[_InT, _OutT], constraint: _In) -> _Pipeline[_InT, _OutT]: ...", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000461", "source": "def test_constraints_schema_validation_error(constraint: str) -> None:\n    with pytest.raises(SchemaError, match=f\"'{constraint}' must be coercible to a Decimal instance\"):\n        SchemaValidator(cs.decimal_schema(**{constraint: 'bad_value'}))", "target": "def test_constraints_schema_validation_error(constraint: str) -> None:\n    with pytest.raises(SchemaError, match=f\"'{constraint}' must be coercible to a time instance\"):\n        SchemaValidator(core_schema.time_schema(**{constraint: 'bad_value'}))", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|scikit-learn/scikit-learn", "data_split": "train", "example_id": "000462", "source": "def test_datetime_strict(input_value, expected):\n    v = SchemaValidator(cs.datetime_schema(strict=True))\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            v.validate_python(input_value)\n    else:\n        output = v.validate_python(input_value)\n        assert output == expected", "target": "def hyperparameter_baseline_similarity(self):\n        return Hyperparameter(\n            \"baseline_similarity\", \"numeric\", self.baseline_similarity_bounds\n        )", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|opencv/opencv", "data_split": "train", "example_id": "000463", "source": "def get_optimus_optimize_ctx(config, nopython, inductor_compile_mode):\n    if config == \"vertical_opt\":\n        optimus_inductor_config = {\n            \"pre_grad_fusion_options\": {\n                \"normalization_pass\": {},\n                \"merge_splits_pass\": {},\n                \"split_cat_pass\": {},\n                \"unbind_stack_pass\": {},\n                \"unbind_cat_to_view_pass\": {},\n            }\n        }\n    elif config == \"horizontal_opt\":\n        optimus_inductor_config = {\n            \"pre_grad_fusion_options\": {\n                \"normalization_pass\": {},\n                \"batch_linear\": {},\n                \"batch_layernorm\": {},\n            },\n        }\n    elif config == \"all\":\n        optimus_inductor_config = {\n            \"pre_grad_fusion_options\": {\n                \"normalization_pass\": {},\n                \"batch_linear\": {},\n                \"batch_layernorm\": {},\n                \"merge_splits_pass\": {},\n                \"split_cat_pass\": {},\n                \"unbind_stack_pass\": {},\n                \"unbind_cat_to_view_pass\": {},\n            },\n        }\n    else:\n        raise RuntimeError(f\"Unknown optimus config: {config}\")\n    def _inner(fn):\n        if \"pre_grad_fusion_options\" in optimus_inductor_config:\n            torch._inductor.config.pre_grad_fusion_options = optimus_inductor_config[\n                \"pre_grad_fusion_options\"\n            ]\n        if \"post_grad_fusion_options\" in optimus_inductor_config:\n            torch._inductor.config.post_grad_fusion_options = optimus_inductor_config[\n                \"post_grad_fusion_options\"\n            ]\n        return torch.compile(\n            fn, backend=\"inductor\", fullgraph=nopython, mode=inductor_compile_mode\n        )\n    return _inner", "target": "def types_separator(self) -> str:\n        return \"\"", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "000464", "source": "def test_aggregates_failures_and_raises(monkeypatch, patch_module):\n    run_test_plan = patch_module.module.run_test_plan\n    tests_map = {\n        \"mix\": {\n            \"title\": \"Some pass some fail\",\n            \"steps\": [\n                \"pytest test_a.py\",\n                \"pytest test_b.py\",\n                \"pytest test_c.py\",\n            ],\n        }\n    }\n    patch_module.run_command.side_effect = [0, 1, 2]\n    with pytest.raises(RuntimeError) as ei:\n        run_test_plan(\"mix\", \"cpu\", tests_map)\n    msg = str(ei.value)\n    assert \"2 pytest runs failed\" in msg\n    patch_module.logger.error.assert_called_once()\n    assert patch_module.run_command.call_count == 3", "target": "def test_schema_serializer_not_reused_when_unpickling() -> None:\n    s = SchemaSerializer(\n        core_schema.model_schema(\n            cls=Model,\n            schema=core_schema.model_fields_schema(fields={}, model_name='Model'),\n            config={'title': 'Model'},\n            ref='Model:123',\n        )\n    )\n    Model.__pydantic_serializer__ = s\n    assert 'Prebuilt' not in str(Model.__pydantic_serializer__)\n    reconstructed = pickle.loads(pickle.dumps(Model.__pydantic_serializer__))\n    assert 'Prebuilt' not in str(reconstructed)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000465", "source": "def quack(self, args, kwargs=None) -> Any:\n        from quack.softmax import softmax\n        assert kwargs is None\n        x, dy = args\n        y = softmax(x)\n        return lambda: torch.autograd.grad(y, x, grad_outputs=dy, retain_graph=True)", "target": "def quack(self, args, kwargs=None) -> Any:\n        from quack.rmsnorm import _rmsnorm_fwd\n        x, w = args\n        y = torch.empty_like(x)\n        def quack_fwd():\n            _rmsnorm_fwd(\n                x,\n                w,\n                out=y,\n                bias=None,\n                rstd=None,\n                residual=None,\n                residual_out=None,\n                eps=1e-6,\n            )\n            return y\n        return quack_fwd", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000466", "source": "def area(self) -> float:\n            return self.side**2", "target": "def area(self, area: float) -> None:\n            self.side = area**0.5", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000467", "source": "def test_union_datetime_downcasts_correctly():\n    serialization_schema = core_schema.plain_serializer_function_ser_schema(lambda v: None)\n    json_validation_schema = core_schema.no_info_plain_validator_function(\n        function=lambda v: v, serialization=serialization_schema\n    )\n    test_custom_ser_schema = core_schema.json_schema(\n        schema=json_validation_schema,\n        serialization=serialization_schema,\n    )\n    s = SchemaSerializer(core_schema.union_schema(choices=[core_schema.datetime_schema(), test_custom_ser_schema]))\n    assert s.to_python('foo') is None", "target": "def test_isinstance(py_and_json: PyAndJson):\n    def f(input_value, validator, info):\n        if 'error' in info.context:\n            raise ValueError('wrong')\n        return validator(input_value)\n    v = py_and_json(core_schema.with_info_wrap_validator_function(f, core_schema.str_schema()))\n    assert v.validate_python('foobar', None, {}) == 'foobar'\n    with pytest.raises(TypeError):\n        v.validate_test('foobar')\n    with pytest.raises(TypeError):\n        v.isinstance_test('foobar')\n    with pytest.raises(ValidationError, match=r'Value error, wrong \\[type=value_error,'):\n        v.validate_test('foobar', None, {'error'})\n    assert v.isinstance_test('foobar', None, {}) is True\n    with pytest.raises(TypeError):\n        v.isinstance_test('foobar')\n    assert v.isinstance_test('foobar', None, {'error'}) is False", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000468", "source": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "target": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|opencv/opencv", "data_split": "train", "example_id": "000469", "source": "def mish(a):\n    return a * ((a * 1.0).exp().log1p() / 1.0).tanh()", "target": "def test_arithm_op_with_saturation(self):\n        np.random.seed(4231568)\n        src = np.random.randint(20, 40, 4 * 8 * 4).astype(np.uint8).reshape(4, 8, 4)\n        operations = get_ocv_arithm_op_table(apply_saturation=True)\n        for ocv_op, numpy_op in operations.items():\n            for val in (10, 4, (40, ), (15, 12), (25., 41., 15.),\n                        np.uint8([1, 2, 20]), np.float64([50, 21, 64, 30]),):\n                dst = ocv_op(src, val)\n                expected = numpy_op(src, val)\n                self.assertLess(np.max(np.abs(dst - expected)), 2,\n                  msg=\"Saturated Operation '{}' is failed for {}\".format(ocv_op.__name__, val ) )", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "000470", "source": "def benchmark_using_throughput_benchmark(config, module):\n    print(\"Benchmarking via ThroughputBenchmark\")\n    bench = ThroughputBenchmark(module.module)\n    bench.add_input(*module.tensor_inputs)\n    stats = bench.benchmark(1, config.num_warmup_iters, config.num_iters)\n    return stats.latency_avg_ms / NUM_LOOP_ITERS", "target": "def time_schema(\n    *,\n    strict: bool | None = None,\n    le: time | None = None,\n    ge: time | None = None,\n    lt: time | None = None,\n    gt: time | None = None,\n    tz_constraint: Literal['aware', 'naive'] | int | None = None,\n    microseconds_precision: Literal['truncate', 'error'] = 'truncate',\n    ref: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> TimeSchema:\n    return _dict_not_none(\n        type='time',\n        strict=strict,\n        le=le,\n        ge=ge,\n        lt=lt,\n        gt=gt,\n        tz_constraint=tz_constraint,\n        microseconds_precision=microseconds_precision,\n        ref=ref,\n        metadata=metadata,\n        serialization=serialization,\n    )", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000471", "source": "def dynamic_rnn(\n        input: Tensor,\n        hidden: tuple[Tensor, Tensor],\n        w_ih: Tensor,\n        w_hh: Tensor,\n        b_ih: Tensor,\n        b_hh: Tensor,\n    ) -> tuple[Tensor, tuple[Tensor, Tensor]]:\n        hx, cx = hidden\n        outputs = []\n        inputs = input.unbind(0)\n        hy, cy = hx[0], cx[0]\n        for seq_idx in range(len(inputs)):\n            hy, cy = cell(inputs[seq_idx], (hy, cy), w_ih, w_hh, b_ih, b_hh)\n            outputs += [hy]\n        return torch.stack(outputs), (hy.unsqueeze(0), cy.unsqueeze(0))", "target": "def dynamic_rnn(\n        input: Tensor,\n        hidden: tuple[Tensor, Tensor],\n        w_ih: Tensor,\n        w_hh: Tensor,\n        b_ih: Tensor,\n        b_hh: Tensor,\n    ) -> tuple[Tensor, tuple[Tensor, Tensor]]:\n        hx, cx = hidden\n        outputs = []\n        inpSize = input.size()\n        inpSize = input.size()\n        inputs = torch.mm(input.view(-1, inpSize[2]), w_ih.t()) + b_ih\n        inputs = inputs.view(inpSize[0], inpSize[1], -1).unbind(0)\n        hy, cy = hx[0], cx[0]\n        for seq_idx in range(len(inputs)):\n            hy, cy = premul_cell(inputs[seq_idx], (hy, cy), w_hh, b_hh)\n            outputs += [hy]\n        return torch.stack(outputs), (hy.unsqueeze(0), cy.unsqueeze(0))", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "000472", "source": "def test_gc_schema_validator() -> None:\n    class BaseModel:\n        __validator__: SchemaValidator\n        def __init_subclass__(cls) -> None:\n            cls.__validator__ = SchemaValidator(\n                schema=core_schema.model_schema(cls, GC_TEST_SCHEMA_INNER),\n                config=core_schema.CoreConfig(extra_fields_behavior='allow'),\n            )\n    cache: WeakValueDictionary[int, Any] = WeakValueDictionary()\n    for _ in range(10_000):\n        class MyModel(BaseModel):\n            pass\n        cache[id(MyModel)] = MyModel\n        del MyModel\n    assert_gc(lambda: len(cache) == 0)", "target": "def test_nested_function_availability(self):\n        self.assertTrue(hasattr(cv.utils, \"nested\"),\n                        msg=\"Module is not generated for nested namespace\")\n        self.assertTrue(hasattr(cv.utils.nested, \"testEchoBooleanFunction\"),\n                        msg=\"Function in nested module is not available\")\n        if sys.version_info[0] < 3:\n            expected_ref_count = 2\n        else:\n            expected_ref_count = 3\n        actual_ref_count = sys.getrefcount(cv.utils.nested) - 1\n        self.assertEqual(actual_ref_count, expected_ref_count,\n                         msg=\"Nested submodule reference counter has wrong value\\n\"\n                         \"Expected: {}. Actual: {}\".format(expected_ref_count, actual_ref_count))\n        for flag in (True, False):\n            self.assertEqual(flag, cv.utils.nested.testEchoBooleanFunction(flag),\n                             msg=\"Function in nested module returns wrong result\")", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "000473", "source": "def test_split3(self):\n            img_path = self.find_file('cv/face/david2.jpg', [os.environ.get('OPENCV_TEST_DATA_PATH')])\n            in_mat = cv.imread(img_path)\n            expected = cv.split(in_mat)\n            g_in = cv.GMat()\n            b, g, r = cv.gapi.split3(g_in)\n            comp = cv.GComputation(cv.GIn(g_in), cv.GOut(b, g, r))\n            for pkg_name, pkg in pkgs:\n                actual = comp.apply(cv.gin(in_mat), args=cv.gapi.compile_args(pkg))\n                for e, a in zip(expected, actual):\n                    self.assertEqual(0.0, cv.norm(e, a, cv.NORM_INF),\n                                     'Failed on ' + pkg_name + ' backend')\n                    self.assertEqual(e.dtype, a.dtype, 'Failed on ' + pkg_name + ' backend')", "target": "def test_on_error_default_factory(self, py_and_json: PyAndJson):\n        v = py_and_json(\n            {\n                'type': 'model-fields',\n                'fields': {\n                    'x': {\n                        'type': 'model-field',\n                        'schema': {\n                            'type': 'default',\n                            'schema': {'type': 'str'},\n                            'on_error': 'default',\n                            'default_factory': lambda: 'pika',\n                        },\n                    }\n                },\n            }\n        )\n        assert v.validate_test({'x': 'foo'}) == ({'x': 'foo'}, None, {'x'})\n        assert v.validate_test({'x': ['foo']}) == ({'x': 'pika'}, None, {'x'})", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "000474", "source": "def element_norm(element):\n            binary_str = bin(element).split('b')[-1]\n            if len(binary_str) % 2 == 1:\n                binary_str = '0' + binary_str\n            gen = filter(lambda p: p != '00',\n                         (binary_str[i:i+2]\n                          for i in range(0, len(binary_str), 2)))\n            return sum(1 for _ in gen)", "target": "def input_data_wrong():\n    return {\n        'field_str': ['fo'],\n        'field_str_con': 'f',\n        'field_int': 1.5,\n        'field_int_con': 11,\n        'field_float': False,\n        'field_float_con': 10.1,\n        'field_decimal': 'wrong',\n        'field_bool': 4,\n        'field_bytes': 42,\n        'field_bytes_con': b'foo',\n        'field_date': 'wrong',\n        'field_date_con': '2000-01-01',\n        'field_time': 'boom',\n        'field_time_con': '23:00:00',\n        'field_datetime': b'smash',\n        'field_datetime_con': '1900-01-01T00:00:00',\n        'field_uuid': '12345678-1234-5678-1234-567812345678',\n        'field_list_any': {1: 2, 3: 4},\n        'field_list_str': [(i,) for i in range(100)],\n        'field_list_str_con': ['a', 'b'],\n        'field_set_any': {'a': b'b', True: 1.0, None: 5},\n        'field_set_int': {f'x{i}' for i in range(100)},\n        'field_set_int_con': {i for i in range(40)},\n        'field_frozenset_any': 'wrong',\n        'field_frozenset_bytes': frozenset([i for i in range(100)]),\n        'field_frozenset_bytes_con': frozenset({b'a', b'b'}),\n        'field_tuple_var_len_any': b'wrong',\n        'field_tuple_var_len_float': tuple(f'x{i}' for i in range(100)),\n        'field_tuple_var_len_float_con': (1.0, 2.0),\n        'field_tuple_fix_len': ('a', 1, 1.0, True, 'more'),\n        'field_dict_any': {'a', 'b', 1, True, 1.0, 2.0},\n        'field_dict_str_float': {(i,): f'x{i}' for i in range(100)},\n        'field_literal_1_int': 2,\n        'field_literal_1_str': 'bat',\n        'field_literal_mult_int': 42,\n        'field_literal_mult_str': 'wrong',\n        'field_literal_assorted': 'wrong',\n        'field_list_nullable_int': [f'x{i}' for i in range(100)],\n        'field_union': {'field_str': ('foo',), 'field_int': 'x', 'field_float': b'y'},\n        'field_functions_model': {'field_before': 1, 'field_after': 1, 'field_wrap': 1, 'field_plain': 1},\n        'field_recursive': {'name': 'foo', 'sub_branch': {'name': 'bar', 'sub_branch': {}}},\n    }", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000475", "source": "def relative_typename(self, root: str) -> Optional[str]:\n            return self.type_node.relative_typename(root)", "target": "def relative_typename(self, module: str) -> str:\n        return '_typing.Callable[[{}], {}]'.format(\n            ', '.join(arg.relative_typename(module) for arg in self.arg_types),\n            self.ret_type.relative_typename(module)\n        )", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "000476", "source": "def import_path():\n    import sys\n    if sys.version_info[0] < 3 or sys.version_info[1] < 6:\n        raise unittest.SkipTest('Python 3.6+ required')\n    from pathlib import Path\n    return Path", "target": "def wrapper1(value: Any, info: core_schema.ValidationInfo) -> Any:\n            return val1(value, values=info.data)", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|pydantic/pydantic", "data_split": "train", "example_id": "000477", "source": "def construct_grids(batch):\n    xmin = batch.x_left_lower_corner + batch.grid_size\n    xmax = xmin + (batch.Nx * batch.grid_size)\n    ymin = batch.y_left_lower_corner + batch.grid_size\n    ymax = ymin + (batch.Ny * batch.grid_size)\n    xgrid = np.arange(xmin, xmax, batch.grid_size)\n    ygrid = np.arange(ymin, ymax, batch.grid_size)\n    return (xgrid, ygrid)", "target": "def test_serialize_as_any_with_nested_models() -> None:\n    class Parent:\n        x: int\n    class Other(Parent):\n        y: str\n    class Outer:\n        p: Parent\n    Parent.__pydantic_core_schema__ = core_schema.model_schema(\n        Parent,\n        core_schema.model_fields_schema(\n            {\n                'x': core_schema.model_field(core_schema.int_schema()),\n            }\n        ),\n        ref='Parent',\n    )\n    Parent.__pydantic_validator__ = SchemaValidator(Parent.__pydantic_core_schema__)\n    Parent.__pydantic_serializer__ = SchemaSerializer(Parent.__pydantic_core_schema__)\n    Other.__pydantic_core_schema__ = core_schema.model_schema(\n        Other,\n        core_schema.model_fields_schema(\n            {\n                'x': core_schema.model_field(core_schema.int_schema()),\n                'y': core_schema.model_field(core_schema.str_schema()),\n            }\n        ),\n        config=core_schema.CoreConfig(extra_fields_behavior='allow'),\n    )\n    Other.__pydantic_validator__ = SchemaValidator(Other.__pydantic_core_schema__)\n    Other.__pydantic_serializer__ = SchemaSerializer(Other.__pydantic_core_schema__)\n    Outer.__pydantic_core_schema__ = core_schema.definitions_schema(\n        core_schema.model_schema(\n            Outer,\n            core_schema.model_fields_schema(\n                {\n                    'p': core_schema.model_field(core_schema.definition_reference_schema('Parent')),\n                }\n            ),\n        ),\n        [\n            Parent.__pydantic_core_schema__,\n        ],\n    )\n    Outer.__pydantic_validator__ = SchemaValidator(Outer.__pydantic_core_schema__)\n    Outer.__pydantic_serializer__ = SchemaSerializer(Outer.__pydantic_core_schema__)\n    other = Other.__pydantic_validator__.validate_python({'x': 1, 'y': 'hopefully not a secret'})\n    outer = Outer()\n    outer.p = other\n    assert Outer.__pydantic_serializer__.to_python(outer, serialize_as_any=False) == {\n        'p': {'x': 1},\n    }\n    assert Outer.__pydantic_serializer__.to_python(outer, serialize_as_any=True) == {\n        'p': {\n            'x': 1,\n            'y': 'hopefully not a secret',\n        }\n    }\n    assert Outer.__pydantic_serializer__.to_json(outer, serialize_as_any=False) == b'{\"p\":{\"x\":1}}'\n    assert (\n        Outer.__pydantic_serializer__.to_json(outer, serialize_as_any=True)\n        == b'{\"p\":{\"x\":1,\"y\":\"hopefully not a secret\"}}'\n    )", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000478", "source": "def test_by_alias_and_name_config_interaction(\n    config_by_alias: bool | None,\n    config_by_name: bool | None,\n    runtime_by_alias: bool | None,\n    runtime_by_name: bool | None,\n) -> None:\n    if config_by_alias is False and config_by_name is False and runtime_by_alias is False and runtime_by_name is False:\n        pytest.skip(\"Can't have both by_alias and by_name as effectively False\")\n    schema = cs.arguments_v3_schema(\n        arguments=[\n            cs.arguments_v3_parameter(name='my_field', schema=cs.int_schema(), alias='my_alias'),\n        ],\n        **({'validate_by_alias': config_by_alias} if config_by_alias is not None else {}),\n        **({'validate_by_name': config_by_name} if config_by_name is not None else {}),\n    )\n    s = SchemaValidator(schema)\n    alias_allowed = next(x for x in (runtime_by_alias, config_by_alias, True) if x is not None)\n    name_allowed = next(x for x in (runtime_by_name, config_by_name, False) if x is not None)\n    if alias_allowed:\n        assert s.validate_python(\n            ArgsKwargs((), {'my_alias': 1}), by_alias=runtime_by_alias, by_name=runtime_by_name\n        ) == (\n            (),\n            {'my_field': 1},\n        )\n    if name_allowed:\n        assert s.validate_python(\n            ArgsKwargs((), {'my_field': 1}), by_alias=runtime_by_alias, by_name=runtime_by_name\n        ) == (\n            (),\n            {'my_field': 1},\n        )", "target": "def test_by_alias_and_name_config_interaction(\n    config_by_alias: Union[bool, None],\n    config_by_name: Union[bool, None],\n    runtime_by_alias: Union[bool, None],\n    runtime_by_name: Union[bool, None],\n) -> None:\n    if config_by_alias is False and config_by_name is False and runtime_by_alias is False and runtime_by_name is False:\n        pytest.skip(\"Can't have both by_alias and by_name as effectively False\")\n    schema = core_schema.arguments_schema(\n        arguments=[\n            core_schema.arguments_parameter(name='my_field', schema=core_schema.int_schema(), alias='my_alias'),\n        ],\n        **({'validate_by_alias': config_by_alias} if config_by_alias is not None else {}),\n        **({'validate_by_name': config_by_name} if config_by_name is not None else {}),\n    )\n    s = SchemaValidator(schema)\n    alias_allowed = next(x for x in (runtime_by_alias, config_by_alias, True) if x is not None)\n    name_allowed = next(x for x in (runtime_by_name, config_by_name, False) if x is not None)\n    if alias_allowed:\n        assert s.validate_python(\n            ArgsKwargs((), {'my_alias': 1}), by_alias=runtime_by_alias, by_name=runtime_by_name\n        ) == (\n            (),\n            {'my_field': 1},\n        )\n    if name_allowed:\n        assert s.validate_python(\n            ArgsKwargs((), {'my_field': 1}), by_alias=runtime_by_alias, by_name=runtime_by_name\n        ) == (\n            (),\n            {'my_field': 1},\n        )", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "train", "example_id": "000479", "source": "def find_multiple(n: int, k: int) -> int:\n    if n % k == 0:\n        return n\n    return n + k - (n % k)", "target": "def test_metavar_lists_targets(self):\n        specs: dict[str, TargetSpec] = {\n            \"foo\": {\"runner\": FooRunner, \"add_arguments\": add_foo_args},\n            \"bar\": {\"runner\": BarRunner},\n        }\n        parser = build_parser(specs)\n        subparsers_action = next(\n            a\n            for a in parser._subparsers._group_actions\n            if isinstance(a, argparse._SubParsersAction)\n        )\n        self.assertEqual(subparsers_action.metavar, \"{foo,bar}\")", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000480", "source": "def eager(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, dy = args\n        y = F.softmax(x, dim=-1)\n        return lambda: torch.autograd.grad(y, x, grad_outputs=dy, retain_graph=True)", "target": "def eager(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, w = args\n        return lambda: self.layernorm_ref(x, w)", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|opencv/opencv", "data_split": "train", "example_id": "000481", "source": "def outMeta(arr_desc0, arr_desc1, arr_desc2):\n        return cv.empty_array_desc(), cv.empty_array_desc(), \\\n               cv.empty_array_desc(), cv.empty_array_desc()", "target": "def intersection(surface, rect):\n    l_x = max(surface[0], rect[0])\n    l_y = max(surface[1], rect[1])\n    width = min(surface[0] + surface[2], rect[0] + rect[2]) - l_x\n    height = min(surface[1] + surface[3], rect[1] + rect[3]) - l_y\n    if width < 0 or height < 0:\n        return (0, 0, 0, 0)\n    return (l_x, l_y, width, height)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "000482", "source": "def with_info_plain_validator_function(\n    function: WithInfoValidatorFunction,\n    *,\n    field_name: str | None = None,\n    ref: str | None = None,\n    json_schema_input_schema: CoreSchema | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> PlainValidatorFunctionSchema:\n    if field_name is not None:\n        warnings.warn(\n            'The `field_name` argument on `with_info_plain_validator_function` is deprecated, it will be passed to the function through `ValidationState` instead.',\n            DeprecationWarning,\n            stacklevel=2,\n        )\n    return _dict_not_none(\n        type='function-plain',\n        function=_dict_not_none(type='with-info', function=function, field_name=field_name),\n        ref=ref,\n        json_schema_input_schema=json_schema_input_schema,\n        metadata=metadata,\n        serialization=serialization,\n    )", "target": "def test_state_in_class(self):\n            @cv.gapi.op('custom.sum', in_types=[cv.GArray.Int], out_types=[cv.GOpaque.Int])\n            class GSum:\n                @staticmethod\n                def outMeta(arr_desc):\n                    return cv.empty_gopaque_desc()\n            @cv.gapi.kernel(GSum)\n            class GSumImpl:\n                last_result = 0\n                @staticmethod\n                def run(arr):\n                    GSumImpl.last_result = sum(arr)\n                    return GSumImpl.last_result\n            g_in  = cv.GArray.Int()\n            comp  = cv.GComputation(cv.GIn(g_in), cv.GOut(GSum.on(g_in)))\n            s = comp.apply(cv.gin([1, 2, 3, 4]), args=cv.gapi.compile_args(cv.gapi.kernels(GSumImpl)))\n            self.assertEqual(10, s)\n            s = comp.apply(cv.gin([1, 2, 8, 7]), args=cv.gapi.compile_args(cv.gapi.kernels(GSumImpl)))\n            self.assertEqual(18, s)\n            self.assertEqual(18, GSumImpl.last_result)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000483", "source": "def f(value, serializer):\n        if value == 42:\n            return 42\n        return f'result={serializer(value)}'", "target": "def f(value, serializer):\n        return f'result={serializer(len(value))} repr={serializer!r}'", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000484", "source": "def test_time_kwargs(kwargs: dict[str, Any], input_value, expected):\n    v = SchemaValidator(core_schema.time_schema(**kwargs))\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)) as exc_info:\n            v.validate_python(input_value)\n        errors = exc_info.value.errors(include_url=False)\n        assert len(errors) == 1\n        if len(kwargs) == 1:\n            key = list(kwargs.keys())[0]\n            assert key in errors[0]['ctx']\n    else:\n        output = v.validate_python(input_value)\n        assert output == expected", "target": "def test_json_error():\n    v = SchemaValidator(\n        core_schema.typed_dict_schema(\n            fields={\n                'field_a': core_schema.typed_dict_field(\n                    schema=core_schema.list_schema(items_schema=core_schema.int_schema())\n                )\n            }\n        )\n    )\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_json('{\"field_a\": [123, \"wrong\"]}')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('field_a', 1),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'wrong',\n        }\n    ]", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000485", "source": "def test_aliases_debug():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'field_a': core_schema.model_field(\n                    validation_alias=[['foo', 'bar', 'bat'], ['foo', 3]], schema=core_schema.int_schema()\n                )\n            }\n        )\n    )\n    print(repr(v))\n    assert repr(v).startswith('SchemaValidator(title=\"model-fields\", validator=ModelFields(')\n    assert 'PathChoices(' in repr(v)", "target": "def test_aliases_debug():\n    v = SchemaValidator(\n        core_schema.typed_dict_schema(\n            fields={\n                'field_a': core_schema.typed_dict_field(\n                    validation_alias=[['foo', 'bar', 'bat'], ['foo', 3]], schema=core_schema.int_schema()\n                )\n            }\n        )\n    )\n    print(repr(v))\n    assert repr(v).startswith('SchemaValidator(title=\"typed-dict\", validator=TypedDict(')\n    assert 'PathChoices(' in repr(v)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000486", "source": "async def main(tests_zip: str, tag_name: str):\n    print(f'Using pyodide version: {pyodide.__version__}')\n    print(f'Extracting test files (size: {len(tests_zip):,})...')\n    pydantic_core_wheel = (\n        'https://githubproxy.samuelcolvin.workers.dev/pydantic/pydantic-core/releases/'\n        f'download/{tag_name}/pydantic_core-{tag_name.lstrip(\"v\")}-cp312-cp312-emscripten_3_1_58_wasm32.whl'\n    )\n    zip_file = ZipFile(BytesIO(base64.b64decode(tests_zip)))\n    count = 0\n    for name in zip_file.namelist():\n        if name.endswith('.py'):\n            path, subs = re.subn(r'^pydantic-core-.+?/tests/', 'tests/', name)\n            if subs:\n                count += 1\n                path = Path(path)\n                path.parent.mkdir(parents=True, exist_ok=True)\n                with zip_file.open(name, 'r') as f:\n                    path.write_bytes(f.read())\n    print(f'Mounted {count} test files, installing dependencies...')\n    await micropip.install(\n        [\n            'dirty-equals',\n            'hypothesis',\n            'pytest-speed',\n            'pytest-mock',\n            'tzdata',\n            'inline-snapshot<0.21',\n            'typing-extensions>=4.14.1',\n            'typing-inspection',\n            pydantic_core_wheel,\n        ]\n    )\n    importlib.invalidate_caches()\n    print('Running tests...')\n    pytest.main()", "target": "def general_plain_validator_function(*args, **kwargs):\n    warnings.warn(\n        '`general_plain_validator_function` is deprecated, use `with_info_plain_validator_function` instead.',\n        DeprecationWarning,\n    )\n    return with_info_plain_validator_function(*args, **kwargs)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pytorch/pytorch", "data_split": "train", "example_id": "000487", "source": "def test_imread_to_buffer(self):\n        path = self.extraTestDataPath + '/cv/shared/lena.png'\n        ref = cv.imread(path)\n        img = np.zeros_like(ref)\n        cv.imread(path, img)\n        self.assertEqual(cv.norm(ref, img, cv.NORM_INF), 0.0)", "target": "def liger(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        from liger_kernel.transformers.cross_entropy import LigerCrossEntropyLoss\n        x, target, dloss = args\n        cross_entropy = LigerCrossEntropyLoss(reduction=\"none\")\n        loss = cross_entropy(x, target)\n        return lambda: torch.autograd.grad(\n            loss, x, grad_outputs=dloss, retain_graph=True\n        )", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000488", "source": "def constrain(\n        self: _Pipeline[_InT, _NewOutT], constraint: annotated_types.MultipleOf\n    ) -> _Pipeline[_InT, _NewOutT]: ...", "target": "def arguments_schema(\n    arguments: list[ArgumentsParameter],\n    *,\n    validate_by_name: bool | None = None,\n    validate_by_alias: bool | None = None,\n    var_args_schema: CoreSchema | None = None,\n    var_kwargs_mode: VarKwargsMode | None = None,\n    var_kwargs_schema: CoreSchema | None = None,\n    ref: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> ArgumentsSchema:\n    return _dict_not_none(\n        type='arguments',\n        arguments_schema=arguments,\n        validate_by_name=validate_by_name,\n        validate_by_alias=validate_by_alias,\n        var_args_schema=var_args_schema,\n        var_kwargs_mode=var_kwargs_mode,\n        var_kwargs_schema=var_kwargs_schema,\n        ref=ref,\n        metadata=metadata,\n        serialization=serialization,\n    )", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000489", "source": "def format_ser_schema(formatting_string: str, *, when_used: WhenUsed = 'json-unless-none') -> FormatSerSchema:\n    if when_used == 'json-unless-none':\n        when_used = None\n    return _dict_not_none(type='format', formatting_string=formatting_string, when_used=when_used)", "target": "def input_data_strict():\n    from datetime import date, datetime, time\n    from uuid import UUID\n    input_data = input_data_lax()\n    input_data.update(\n        field_date=date(2010, 2, 3),\n        field_date_con=date(2020, 1, 1),\n        field_time=time(12, 0, 0),\n        field_time_con=time(12, 0, 0),\n        field_datetime=datetime(2020, 1, 1, 12, 13, 14),\n        field_datetime_con=datetime(2020, 1, 1),\n        field_uuid=UUID('12345678-1234-5678-1234-567812345678'),\n        field_decimal=Decimal('42.0'),\n    )\n    return input_data", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000490", "source": "def test_function_wrap_field_serializer_to_python():\n    @dataclasses.dataclass\n    class Model:\n        x: int\n        def ser_x(self, v: Any, serializer: core_schema.SerializerFunctionWrapHandler, _) -> str:\n            x = serializer(v)\n            assert self.x == 1_000\n            return f'{x:_}'\n    s = SchemaSerializer(\n        core_schema.model_schema(\n            Model,\n            core_schema.model_fields_schema(\n                {\n                    'x': core_schema.model_field(\n                        core_schema.int_schema(\n                            serialization=core_schema.wrap_serializer_function_ser_schema(\n                                Model.ser_x, is_field_serializer=True, info_arg=True, schema=core_schema.any_schema()\n                            )\n                        )\n                    )\n                }\n            ),\n        )\n    )\n    assert s.to_python(Model(x=1000)) == {'x': '1_000'}", "target": "def test_function_wrap_field_serializer_to_python():\n    class Model(TypedDict):\n        x: int\n    def ser_x(data: Model, v: Any, serializer: core_schema.SerializerFunctionWrapHandler, _) -> str:\n        x = serializer(v)\n        assert data['x'] == 1_000\n        return f'{x:_}'\n    s = SchemaSerializer(\n        core_schema.typed_dict_schema(\n            {\n                'x': core_schema.typed_dict_field(\n                    core_schema.int_schema(\n                        serialization=core_schema.wrap_serializer_function_ser_schema(\n                            ser_x, is_field_serializer=True, info_arg=True, schema=core_schema.any_schema()\n                        )\n                    )\n                )\n            }\n        )\n    )\n    assert s.to_python(Model(x=1000)) == {'x': '1_000'}", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "000491", "source": "def run(arr0, arr1):\n                    return arr0 + arr1", "target": "def test_aliases_path_multiple(py_and_json: PyAndJson, input_value, expected):\n    v = py_and_json(\n        {\n            'type': 'typed-dict',\n            'fields': {\n                'field_a': {\n                    'validation_alias': [['foo', 'bar', 'bat'], ['foo', 3], ['spam']],\n                    'type': 'typed-dict-field',\n                    'schema': {'type': 'int'},\n                }\n            },\n            'config': {'loc_by_alias': False},\n        }\n    )\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=expected.message):\n            val = v.validate_test(input_value)\n            print(f'UNEXPECTED OUTPUT: {val!r}')\n    else:\n        output = v.validate_test(input_value)\n        assert output == expected", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000492", "source": "def ser_x(self, v: Any, serializer: core_schema.SerializerFunctionWrapHandler, _) -> str:\n            assert self.x == 1_000\n            x = serializer(v)\n            return f'{x:_}'", "target": "def ser_x(data: Model, v: Any, serializer: core_schema.SerializerFunctionWrapHandler, _) -> str:\n        x = serializer(v)\n        assert data['x'] == 1_000\n        return f'{x:_}'", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|scikit-learn/scikit-learn", "data_split": "train", "example_id": "000493", "source": "def _install_test_dependencies(self):\n        logger.info(\"generate test.txt from requirements/test.in with local torch whls\")\n        preprocess_test_in()\n        copy(\"requirements/test.txt\", \"snapshot_constraint.txt\")\n        run_command(\n            f\"{sys.executable} -m uv pip compile requirements/test.in \"\n            \"-o test.txt \"\n            \"--index-strategy unsafe-best-match \"\n            \"--constraint snapshot_constraint.txt \"\n            \"--torch-backend cu128\"\n        )\n        pip_install_packages(requirements=\"test.txt\", prefer_uv=True)\n        logger.info(\"Done. installed requirements for test dependencies\")", "target": "def make_estimator(self, params):\n        representation, init = params\n        max_iter = 5 if representation == \"sparse\" else 2\n        estimator = MiniBatchKMeans(\n            n_clusters=20,\n            init=init,\n            n_init=1,\n            max_iter=max_iter,\n            batch_size=1000,\n            max_no_improvement=None,\n            compute_labels=False,\n            random_state=0,\n        )\n        return estimator", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "000494", "source": "def gh_fetch_json_dict(\n    url: str,\n    params: Optional[dict[str, Any]] = None,\n    data: Optional[dict[str, Any]] = None,\n) -> dict[str, Any]:\n    return cast(dict[str, Any], _gh_fetch_json_any(url, params, data))", "target": "def test_extra():\n    class RootModel:\n        __slots__ = '__dict__', '__pydantic_fields_set__', '__pydantic_extra__', '__pydantic_private__'\n        root: int\n    v = SchemaValidator(core_schema.model_schema(RootModel, core_schema.int_schema(), root_model=True))\n    m = v.validate_python(1)\n    with pytest.raises(AttributeError):\n        m.__pydantic_extra__", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|opencv/opencv", "data_split": "train", "example_id": "000495", "source": "def benchmark_influence(conf):\n    prediction_times = []\n    prediction_powers = []\n    complexities = []\n    for param_value in conf[\"changing_param_values\"]:\n        conf[\"tuned_params\"][conf[\"changing_param\"]] = param_value\n        estimator = conf[\"estimator\"](**conf[\"tuned_params\"])\n        print(\"Benchmarking %s\" % estimator)\n        estimator.fit(conf[\"data\"][\"X_train\"], conf[\"data\"][\"y_train\"])\n        conf[\"postfit_hook\"](estimator)\n        complexity = conf[\"complexity_computer\"](estimator)\n        complexities.append(complexity)\n        start_time = time.time()\n        for _ in range(conf[\"n_samples\"]):\n            y_pred = estimator.predict(conf[\"data\"][\"X_test\"])\n        elapsed_time = (time.time() - start_time) / float(conf[\"n_samples\"])\n        prediction_times.append(elapsed_time)\n        pred_score = conf[\"prediction_performance_computer\"](\n            conf[\"data\"][\"y_test\"], y_pred\n        )\n        prediction_powers.append(pred_score)\n        print(\n            \"Complexity: %d | %s: %.4f | Pred. Time: %fs\\n\"\n            % (\n                complexity,\n                conf[\"prediction_performance_label\"],\n                pred_score,\n                elapsed_time,\n            )\n        )\n    return prediction_powers, prediction_times, complexities", "target": "def test_parse_to_rotated_rect_not_convertible(self):\n        for not_convertible in ([], (), np.array([]), (123, (45, 34), 1), {1: 2, 3: 4}, 123,\n                                np.array([[123, 123, 14], [1, 3], 56], dtype=object), '123'):\n            with self.assertRaises((TypeError), msg=get_no_exception_msg(not_convertible)):\n                _ = cv.utils.dumpRotatedRect(not_convertible)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000496", "source": "def test_fields_required_by_default():\n    v = SchemaValidator(\n        core_schema.typed_dict_schema(\n            fields={\n                'x': core_schema.typed_dict_field(schema=core_schema.str_schema()),\n                'y': core_schema.typed_dict_field(schema=core_schema.str_schema()),\n            }\n        )\n    )\n    assert v.validate_python({'x': 'pika', 'y': 'chu'}) == {'x': 'pika', 'y': 'chu'}\n    with pytest.raises(ValidationError) as exc_info:\n        assert v.validate_python({'x': 'pika'})\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'missing', 'loc': ('y',), 'msg': 'Field required', 'input': {'x': 'pika'}}\n    ]", "target": "def test_bool_key(py_and_json: PyAndJson):\n    v = py_and_json({'type': 'dict', 'keys_schema': {'type': 'bool'}, 'values_schema': {'type': 'int'}})\n    assert v.validate_test({True: 1, False: 2}) == {True: 1, False: 2}\n    assert v.validate_test({'true': 1, 'off': 2}) == {True: 1, False: 2}\n    assert v.validate_test({'true': 1, 'off': 2}, strict=False) == {True: 1, False: 2}\n    with pytest.raises(ValidationError, match='Input should be a valid boolean'):\n        v.validate_python({'true': 1, 'off': 2}, strict=True)\n    assert v.validate_json('{\"true\": 1, \"off\": 2}', strict=True) == {True: 1, False: 2}", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "000497", "source": "def cvt_nv12_to_yuv(self, y, uv):\n            h,w,_ = uv.shape\n            upsample_uv = cv.resize(uv, (h * 2, w * 2))\n            return cv.merge([y, upsample_uv])", "target": "def decimal_schema(\n    *,\n    allow_inf_nan: bool | None = None,\n    multiple_of: Decimal | None = None,\n    le: Decimal | None = None,\n    ge: Decimal | None = None,\n    lt: Decimal | None = None,\n    gt: Decimal | None = None,\n    max_digits: int | None = None,\n    decimal_places: int | None = None,\n    strict: bool | None = None,\n    ref: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> DecimalSchema:\n    return _dict_not_none(\n        type='decimal',\n        gt=gt,\n        ge=ge,\n        lt=lt,\n        le=le,\n        max_digits=max_digits,\n        decimal_places=decimal_places,\n        multiple_of=multiple_of,\n        allow_inf_nan=allow_inf_nan,\n        strict=strict,\n        ref=ref,\n        metadata=metadata,\n        serialization=serialization,\n    )", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000498", "source": "def test_only_validate_by_name(py_and_json) -> None:\n    v = py_and_json(\n        cs.arguments_v3_schema(\n            [\n                cs.arguments_v3_parameter(\n                    name='a', schema=cs.str_schema(), alias='FieldA', mode='positional_or_keyword'\n                ),\n            ],\n            validate_by_name=True,\n            validate_by_alias=False,\n        )\n    )\n    assert v.validate_test(ArgsKwargs((), {'a': 'hello'})) == ((), {'a': 'hello'})\n    assert v.validate_test({'a': 'hello'}) == (('hello',), {})\n    with pytest.raises(ValidationError, match=r'a\\n +Missing required argument \\[type=missing_argument,'):\n        assert v.validate_test(ArgsKwargs((), {'FieldA': 'hello'}))\n    with pytest.raises(ValidationError, match=r'a\\n +Missing required argument \\[type=missing_argument,'):\n        assert v.validate_test({'FieldA': 'hello'})", "target": "def test_only_validate_by_name(py_and_json) -> None:\n    schema = core_schema.dataclass_schema(\n        BasicDataclass,\n        core_schema.dataclass_args_schema(\n            'BasicDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), validation_alias='FieldA'),\n            ],\n        ),\n        ['a'],\n        config=core_schema.CoreConfig(validate_by_name=True, validate_by_alias=False),\n    )\n    v = py_and_json(schema)\n    assert v.validate_test({'a': 'hello'}) == BasicDataclass(a='hello')\n    with pytest.raises(ValidationError, match=r'a\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'FieldA': 'hello'})", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000499", "source": "def get_memory_bytes(self, args, kwargs) -> int:\n        x, dy = args\n        M, N = x.shape\n        return 3 * M * N * x.dtype.itemsize", "target": "def get_memory_bytes(self, args, kwargs) -> int:\n        x, w, dy = args\n        M, N = x.shape\n        return (\n            2 * M * N * x.dtype.itemsize\n            + 2 * N * w.dtype.itemsize\n            + M * N * dy.dtype.itemsize\n        )", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000500", "source": "def liger(self, args, kwargs=None) -> Any:\n        from liger_kernel.transformers.rms_norm import LigerRMSNorm\n        x, w, dy = args\n        M, N = x.shape\n        liger_rmsnorm = LigerRMSNorm(\n            hidden_size=N, eps=1e-6, casting_mode=\"gemma\"\n        ).cuda()\n        liger_rmsnorm.weight.data.copy_(w)\n        y = liger_rmsnorm(x)\n        return lambda: torch.autograd.grad(\n            y, [x, liger_rmsnorm.weight], grad_outputs=dy, retain_graph=True\n        )", "target": "def liger(self, args, kwargs) -> Any:\n        from liger_kernel.ops.layer_norm import layer_norm_backward\n        x, w, dy = args\n        eps = 1e-6\n        mean, rstd = self.compute_mean_rstd(x, eps)\n        M, N = x.shape\n        return lambda: layer_norm_backward(dy, x, w, None, mean, rstd)[0:2]", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "000501", "source": "def test_fallback_cycle_same(any_serializer: SchemaSerializer):\n    def fallback_func(obj):\n        return obj\n    f = Foobar()\n    assert any_serializer.to_python(f) == f\n    with pytest.raises(ValueError, match=r'Circular reference detected \\(id repeated\\)'):\n        any_serializer.to_python(f, mode='json', fallback=fallback_func)\n    assert any_serializer.to_python(f, fallback=fallback_func) == f\n    with pytest.raises(ValueError, match=r'Circular reference detected \\(id repeated\\)'):\n        any_serializer.to_json(f, fallback=fallback_func)", "target": "def contains_tensor_types(type):\n    return type.isSubtypeOf(tensor_type) or any(\n        contains_tensor_types(e) for e in type.containedTypes()\n    )", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000502", "source": "def test_more_specific_data_matches_subclass(self, choices) -> None:\n        validator = SchemaValidator(core_schema.union_schema(choices))\n        assert isinstance(validator.validate_python({'a': 1}), self.ModelA)\n        assert isinstance(validator.validate_python({'a': 1, 'b': 2}), self.ModelB)\n        assert isinstance(validator.validate_python({'a': 1, 'b': 2}), self.ModelB)\n        assert isinstance(validator.validate_python({'a': '1', 'b': '2'}), self.ModelB)\n        assert isinstance(validator.validate_python({'a': '1', 'b': 2}), self.ModelB)\n        assert isinstance(validator.validate_python({'a': 1, 'b': '2'}), self.ModelB)", "target": "def test_model_field_plain_validator() -> None:\n    class Model:\n        x: str\n    def f(input_value: Any, info: core_schema.ValidationInfo) -> Any:\n        assert info.field_name == 'x'\n        assert info.data == {}\n        assert isinstance(input_value, bytes)\n        return f'input: {input_value.decode()}'\n    v = SchemaValidator(\n        core_schema.model_schema(\n            Model,\n            core_schema.model_fields_schema(\n                {'x': core_schema.model_field(core_schema.with_info_plain_validator_function(f))}\n            ),\n        )\n    )\n    assert v.validate_python({'x': b'foo'}).x == 'input: foo'", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000503", "source": "def test_too_long(py_and_json: PyAndJson):\n    v = py_and_json({'type': 'generator', 'items_schema': {'type': 'int'}, 'max_length': 2})\n    assert list(v.validate_test([1])) == [1]\n    assert list(v.validate_test([1, 2])) == [1, 2]\n    with pytest.raises(ValidationError) as exc_info:\n        list(v.validate_test([1, 2, 3]))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'too_long',\n            'loc': (),\n            'msg': 'Generator should have at most 2 items after validation, not more',\n            'input': [1, 2, 3],\n            'ctx': {'field_type': 'Generator', 'max_length': 2, 'actual_length': None},\n        }\n    ]", "target": "def test_allow_inf_nan_true_json() -> None:\n    v = SchemaValidator(core_schema.float_schema())\n    assert v.validate_json('123') == 123\n    assert v.validate_json('NaN') == IsFloatNan()\n    assert v.validate_json('Infinity') == float('inf')\n    assert v.validate_json('-Infinity') == float('-inf')\n    assert v.validate_json('\"NaN\"') == IsFloatNan()\n    assert v.validate_json('\"Infinity\"') == float('inf')\n    assert v.validate_json('\"-Infinity\"') == float('-inf')", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000504", "source": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "target": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "000505", "source": "def test_validation_error_subclassable():\n    class CustomValidationError(ValidationError):\n        pass\n    with pytest.raises(ValidationError) as exception_info:\n        raise CustomValidationError.from_exception_data(\n            'My CustomError',\n            [\n                InitErrorDetails(\n                    type='value_error',\n                    loc=('myField',),\n                    msg='This is my custom error.',\n                    input='something invalid',\n                    ctx={\n                        'myField': 'something invalid',\n                        'error': \"'something invalid' is not a valid value for 'myField'\",\n                    },\n                )\n            ],\n        )\n    assert isinstance(exception_info.value, CustomValidationError)", "target": "def parse_file(self, fname, prefix = \"\"):\n        istest = fname.endswith(\"Test.java\")\n        clsname = os.path.basename(fname).replace(\"Test\", \"\").replace(\".java\", \"\")\n        clsname = prefix + clsname[0].upper() + clsname[1:]\n        for cls in classes_ignore_list:\n            if re.match(cls, clsname):\n                return\n        f = open(fname, \"rt\")\n        linenum = 0\n        for line in f:\n            linenum += 1\n            m1 = self.r1.match(line)\n            m2 = self.r2.match(line)\n            m3 = self.r3.match(line)\n            func = ''\n            args_str = ''\n            if m1:\n                func = m1.group(1)\n                args_str = m1.group(2)\n            elif m2:\n                if \"public\" not in line:\n                    continue\n                func = m2.group(1)\n                args_str = m2.group(2)\n            elif m3:\n                self.empty_stubs_cnt += 1\n                continue\n            else:\n                continue\n            d = (self.mdict, self.tdict)[istest]\n            w = (self.mwhere, self.twhere)[istest]\n            func = re.sub(r\"^test\", \"\", func)\n            func = clsname + \"--\" + func[0].upper() + func[1:]\n            args_str = args_str.replace(\"[]\", \"Array\").replace(\"...\", \"Array \")\n            args_str = re.sub(r\"List<(\\w+)>\", \"ListOf\\g<1>\", args_str)\n            args_str = re.sub(r\"List<(\\w+)>\", \"ListOf\\g<1>\", args_str)\n            args = [a.split()[0] for a in args_str.split(\",\") if a]\n            func_ex = func + \"\".join([a[0].upper() + a[1:] for a in args])\n            func_loc = fname + \" (line: \" + str(linenum)  + \")\"\n            skip = False\n            for fi in funcs_ignore_list:\n                if re.match(fi, func_ex):\n                    skip = True\n                    break\n            if skip:\n                continue\n            if func in d:\n                d[func].append(func_ex)\n            else:\n                d[func] = [func_ex]\n            w[func_ex] = func_loc\n            w[func] = func_loc\n        f.close()\n        return", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000506", "source": "def field_serializer(\n    field: str,\n    /,\n    *fields: str,\n    mode: Literal['wrap'],\n    return_type: Any = ...,\n    when_used: WhenUsed = ...,\n    check_fields: bool | None = ...,\n) -> Callable[[_FieldWrapSerializerT], _FieldWrapSerializerT]: ...", "target": "def field_serializer(\n    field: str,\n    /,\n    *fields: str,\n    mode: Literal['plain'] = ...,\n    return_type: Any = ...,\n    when_used: WhenUsed = ...,\n    check_fields: bool | None = ...,\n) -> Callable[[_FieldPlainSerializerT], _FieldPlainSerializerT]: ...", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000507", "source": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "target": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000508", "source": "def test_constraints_schema_validation_error(constraint: str) -> None:\n    with pytest.raises(SchemaError, match=f\"'{constraint}' must be coercible to a date instance\"):\n        SchemaValidator(cs.date_schema(**{constraint: 'bad_value'}))", "target": "def test_constraints_schema_validation_error(constraint: str) -> None:\n    with pytest.raises(SchemaError, match=f\"'{constraint}' must be coercible to a timedelta instance\"):\n        SchemaValidator(core_schema.timedelta_schema(**{constraint: 'bad_value'}))", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "000509", "source": "def test_export_inner_class_of_class_exported_with_different_name(self):\n        if not hasattr(cv.utils.nested, \"ExportClassName\"):\n            raise unittest.SkipTest(\n                \"Outer class with export alias is not registered in the submodule\")\n        self.assertTrue(hasattr(cv.utils.nested.ExportClassName, \"Params\"),\n                        msg=\"Inner class with export alias is not registered in \"\n                        \"the outer class\")\n        self.assertTrue(hasattr(cv, \"utils_nested_ExportClassName_Params\"),\n                        msg=\"Inner class with export alias is not registered in \"\n                        \"global module\")\n        params = cv.utils.nested.ExportClassName.Params()\n        params.int_value = 45\n        params.float_value = 4.5\n        instance = cv.utils.nested.ExportClassName.create(params)\n        self.assertTrue(isinstance(instance, cv.utils.nested.ExportClassName),\n                        msg=\"Factory function returns wrong class instance: {}\".format(type(instance)))\n        self.assertEqual(\n            params.int_value, instance.getIntParam(),\n            msg=\"Class initialized with wrong integer parameter. Expected: {}. Actual: {}\".format(\n                params.int_value, instance.getIntParam()\n            )\n        )\n        self.assertEqual(\n            params.float_value, instance.getFloatParam(),\n            msg=\"Class initialized with wrong integer parameter. Expected: {}. Actual: {}\".format(\n                params.float_value, instance.getFloatParam()\n            )\n        )", "target": "def test_alias(py_and_json: PyAndJson):\n    v = py_and_json(\n        {\n            'type': 'typed-dict',\n            'fields': {\n                'field_a': {'validation_alias': 'FieldA', 'type': 'typed-dict-field', 'schema': {'type': 'int'}}\n            },\n        }\n    )\n    assert v.validate_test({'FieldA': '123'}) == {'field_a': 123}\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'foobar': '123'})\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'field_a': '123'})", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "000510", "source": "def test_paths_allow_by_name(py_and_json: PyAndJson, input_value):\n    v = py_and_json(\n        {\n            'type': 'typed-dict',\n            'fields': {\n                'field_a': {\n                    'validation_alias': [['foo', 'bar'], ['foo']],\n                    'type': 'typed-dict-field',\n                    'schema': {'type': 'int'},\n                }\n            },\n            'config': {'validate_by_name': True},\n        },\n    )\n    assert v.validate_test(input_value) == {'field_a': 42}", "target": "def test_charuco_detector(self):\n        aruco_dict = cv.aruco.getPredefinedDictionary(cv.aruco.DICT_4X4_250)\n        board_size = (3, 3)\n        board = cv.aruco.CharucoBoard(board_size, 1.0, .8, aruco_dict)\n        charuco_detector = cv.aruco.CharucoDetector(board)\n        cell_size = 100\n        image = board.generateImage((cell_size*board_size[0], cell_size*board_size[1]))\n        list_gold_corners = []\n        for i in range(1, board_size[0]):\n            for j in range(1, board_size[1]):\n                list_gold_corners.append((j*cell_size, i*cell_size))\n        gold_corners = np.array(list_gold_corners, dtype=np.float32)\n        charucoCorners, charucoIds, markerCorners, markerIds = charuco_detector.detectBoard(image)\n        self.assertEqual(len(charucoIds), 4)\n        for i in range(0, 4):\n            self.assertEqual(charucoIds[i], i)\n        np.testing.assert_allclose(gold_corners, charucoCorners.reshape(-1, 2), 0.01, 0.1)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "000511", "source": "def reverse(lst: list[Tensor]) -> list[Tensor]:\n    return lst[::-1]", "target": "def test_extra_ignore(py_and_json: PyAndJson, input_value) -> None:\n    v = py_and_json(\n        cs.arguments_v3_schema(\n            [\n                cs.arguments_v3_parameter(name='a', schema=cs.int_schema(), mode='keyword_only'),\n                cs.arguments_v3_parameter(name='b', schema=cs.int_schema(), alias='c', mode='keyword_only'),\n            ],\n            extra_behavior='ignore',\n        ),\n    )\n    assert v.validate_test(input_value) == ((), {'a': 1, 'b': 3})", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000512", "source": "def field_type_str(self) -> str:\n        return f'{self.field_type.__name__}' if hasattr(self.field_type, '__name__') else f'{self.field_type}'", "target": "def test_dict_fail_fast(fail_fast, expected):\n    v = SchemaValidator(\n        {'type': 'dict', 'keys_schema': {'type': 'int'}, 'values_schema': {'type': 'int'}, 'fail_fast': fail_fast}\n    )\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python({'a': 'b', 'c': 'd'})\n    assert exc_info.value.errors(include_url=False) == expected", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000513", "source": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "target": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000514", "source": "def parseSSD(detections, size):\n                h, w = size\n                bboxes = []\n                detections = detections.reshape(-1, 7)\n                for sample_id, class_id, confidence, xmin, ymin, xmax, ymax in detections:\n                    if confidence >= 0.5:\n                        x      = int(xmin * w)\n                        y      = int(ymin * h)\n                        width  = int(xmax * w - x)\n                        height = int(ymax * h - y)\n                        bboxes.append((x, y, width, height))\n                return bboxes", "target": "def parseSSD(detections, size):\n                h, w = size\n                bboxes = []\n                detections = detections.reshape(-1, 7)\n                for sample_id, class_id, confidence, xmin, ymin, xmax, ymax in detections:\n                    if confidence >= 0.5:\n                        x      = int(xmin * w)\n                        y      = int(ymin * h)\n                        width  = int(xmax * w - x)\n                        height = int(ymax * h - y)\n                        bboxes.append((x, y, width, height))\n                return bboxes", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|pydantic/pydantic", "data_split": "train", "example_id": "000515", "source": "def benchmark(\n    metrics=tuple(v for k, v in sorted(METRICS.items())),\n    formats=tuple(v for k, v in sorted(FORMATS.items())),\n    samples=1000,\n    classes=4,\n    density=0.2,\n    n_times=5,\n):\n    metrics = np.atleast_1d(metrics)\n    samples = np.atleast_1d(samples)\n    classes = np.atleast_1d(classes)\n    density = np.atleast_1d(density)\n    formats = np.atleast_1d(formats)\n    out = np.zeros(\n        (len(metrics), len(formats), len(samples), len(classes), len(density)),\n        dtype=float,\n    )\n    it = itertools.product(samples, classes, density)\n    for i, (s, c, d) in enumerate(it):\n        _, y_true = make_multilabel_classification(\n            n_samples=s, n_features=1, n_classes=c, n_labels=d * c, random_state=42\n        )\n        _, y_pred = make_multilabel_classification(\n            n_samples=s, n_features=1, n_classes=c, n_labels=d * c, random_state=84\n        )\n        for j, f in enumerate(formats):\n            f_true = f(y_true)\n            f_pred = f(y_pred)\n            for k, metric in enumerate(metrics):\n                t = timeit(partial(metric, f_true, f_pred), number=n_times)\n                out[k, j].flat[i] = t\n    return out", "target": "def test_url_pickle(value):\n    pickled = pickle.dumps(value)\n    unpickled = pickle.loads(pickled)\n    assert value == unpickled", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000516", "source": "def collect_known_metadata(annotations: Iterable[Any]) -> tuple[dict[str, Any], list[Any]]:\n    annotations = expand_grouped_metadata(annotations)\n    res: dict[str, Any] = {}\n    remaining: list[Any] = []\n    for annotation in annotations:\n        if isinstance(annotation, PydanticMetadata):\n            res.update(annotation.__dict__)\n        elif (annotation_type := type(annotation)) in (at_to_constraint_map := _get_at_to_constraint_map()):\n            constraint = at_to_constraint_map[annotation_type]\n            res[constraint] = getattr(annotation, constraint)\n        elif isinstance(annotation, type) and issubclass(annotation, PydanticMetadata):\n            res.update({k: v for k, v in vars(annotation).items() if not k.startswith('_')})\n        else:\n            remaining.append(annotation)\n    res = {k: v for k, v in res.items() if v is not None}\n    return res, remaining", "target": "def wrapper2(value: Any, _: core_schema.ValidationInfo) -> Any:\n            return val2(value)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000517", "source": "def test_custom_error():\n    v = SchemaValidator(\n        core_schema.tagged_union_schema(\n            discriminator='foo',\n            custom_error_type='snap',\n            custom_error_message='Input should be a foo or bar',\n            choices={\n                'apple': core_schema.typed_dict_schema(\n                    fields={\n                        'foo': core_schema.typed_dict_field(schema=core_schema.str_schema()),\n                        'bar': core_schema.typed_dict_field(schema=core_schema.int_schema()),\n                    }\n                ),\n                'banana': core_schema.typed_dict_schema(\n                    fields={\n                        'foo': core_schema.typed_dict_field(schema=core_schema.str_schema()),\n                        'spam': core_schema.typed_dict_field(\n                            schema=core_schema.list_schema(items_schema=core_schema.int_schema())\n                        ),\n                    }\n                ),\n            },\n        )\n    )\n    assert v.validate_python({'foo': 'apple', 'bar': '123'}) == {'foo': 'apple', 'bar': 123}\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python({'spam': 'apple', 'bar': 'Bar'})\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'snap', 'loc': (), 'msg': 'Input should be a foo or bar', 'input': {'spam': 'apple', 'bar': 'Bar'}}\n    ]\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python({'foo': 'other', 'bar': 'Bar'})\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'snap', 'loc': (), 'msg': 'Input should be a foo or bar', 'input': {'foo': 'other', 'bar': 'Bar'}}\n    ]", "target": "def test_custom_error():\n    v = SchemaValidator(\n        core_schema.union_schema(\n            choices=[core_schema.str_schema(), core_schema.bytes_schema()],\n            custom_error_type='my_error',\n            custom_error_message='Input should be a string or bytes',\n        )\n    )\n    assert v.validate_python('hello') == 'hello'\n    assert v.validate_python(b'hello') == b'hello'\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python(123)\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'my_error', 'loc': (), 'msg': 'Input should be a string or bytes', 'input': 123}\n    ]", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000518", "source": "def get_int_key():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'field_a': core_schema.model_field(\n                    validation_alias=[['foo', 3], ['spam']], schema=core_schema.int_schema()\n                )\n            }\n        )\n    )\n    assert v.validate_python({'foo': {3: 33}}) == ({'field_a': 33}, {}, {'field_a'})", "target": "def get_int_key():\n    v = SchemaValidator(\n        core_schema.typed_dict_schema(\n            fields={\n                'field_a': core_schema.typed_dict_field(\n                    validation_alias=[['foo', 3], ['spam']], schema=core_schema.int_schema()\n                )\n            }\n        )\n    )\n    assert v.validate_python({'foo': {3: 33}}) == ({'field_a': 33}, {'field_a'})", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "000519", "source": "def test_optional_union_with_members_having_defaults(self, choices) -> None:\n        class WrapModel:\n            val: Optional[Union[self.ModelA, self.ModelB]] = None\n        val = SchemaValidator(\n            schema=core_schema.model_schema(\n                WrapModel,\n                core_schema.model_fields_schema(\n                    fields={\n                        'val': core_schema.model_field(\n                            core_schema.with_default_schema(\n                                core_schema.union_schema(choices),\n                                default=None,\n                            )\n                        )\n                    }\n                ),\n            )\n        )\n        assert isinstance(val.validate_python({'val': {'a': 1}}).val, self.ModelA)\n        assert isinstance(val.validate_python({'val': {'b': 1}}).val, self.ModelB)\n        assert val.validate_python({}).val is None", "target": "def parse_file(self, fname, prefix = \"\"):\n        istest = fname.endswith(\"Test.java\")\n        clsname = os.path.basename(fname).replace(\"Test\", \"\").replace(\".java\", \"\")\n        clsname = prefix + clsname[0].upper() + clsname[1:]\n        for cls in classes_ignore_list:\n            if re.match(cls, clsname):\n                return\n        f = open(fname, \"rt\")\n        linenum = 0\n        for line in f:\n            linenum += 1\n            m1 = self.r1.match(line)\n            m2 = self.r2.match(line)\n            m3 = self.r3.match(line)\n            func = ''\n            args_str = ''\n            if m1:\n                func = m1.group(1)\n                args_str = m1.group(2)\n            elif m2:\n                if \"public\" not in line:\n                    continue\n                func = m2.group(1)\n                args_str = m2.group(2)\n            elif m3:\n                self.empty_stubs_cnt += 1\n                continue\n            else:\n                continue\n            d = (self.mdict, self.tdict)[istest]\n            w = (self.mwhere, self.twhere)[istest]\n            func = re.sub(r\"^test\", \"\", func)\n            func = clsname + \"--\" + func[0].upper() + func[1:]\n            args_str = args_str.replace(\"[]\", \"Array\").replace(\"...\", \"Array \")\n            args_str = re.sub(r\"List<(\\w+)>\", \"ListOf\\g<1>\", args_str)\n            args_str = re.sub(r\"List<(\\w+)>\", \"ListOf\\g<1>\", args_str)\n            args = [a.split()[0] for a in args_str.split(\",\") if a]\n            func_ex = func + \"\".join([a[0].upper() + a[1:] for a in args])\n            func_loc = fname + \" (line: \" + str(linenum)  + \")\"\n            skip = False\n            for fi in funcs_ignore_list:\n                if re.match(fi, func_ex):\n                    skip = True\n                    break\n            if skip:\n                continue\n            if func in d:\n                d[func].append(func_ex)\n            else:\n                d[func] = [func_ex]\n            w[func_ex] = func_loc\n            w[func] = func_loc\n        f.close()\n        return", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000520", "source": "def liger(self, args, kwargs=None) -> Any:\n        from liger_kernel.transformers.rms_norm import LigerRMSNorm\n        x, w, dy = args\n        M, N = x.shape\n        liger_rmsnorm = LigerRMSNorm(\n            hidden_size=N, eps=1e-6, casting_mode=\"gemma\"\n        ).cuda()\n        liger_rmsnorm.weight.data.copy_(w)\n        y = liger_rmsnorm(x)\n        return lambda: torch.autograd.grad(\n            y, [x, liger_rmsnorm.weight], grad_outputs=dy, retain_graph=True\n        )", "target": "def liger(self, args, kwargs) -> Any:\n        from liger_kernel.transformers.layer_norm import LigerLayerNorm\n        x, w = args\n        M, N = x.shape\n        liger_layernorm = LigerLayerNorm(hidden_size=N, eps=1e-6).cuda()\n        liger_layernorm.weight.data.copy_(w)\n        liger_layernorm.bias.data.copy_(\n            torch.zeros(N, device=\"cuda\", dtype=torch.float32)\n        )\n        return lambda: liger_layernorm(x)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "000521", "source": "def recurrent(x, scale, shift):\n    y = x\n    for i in range(100):\n        y = fn(y, scale, shift)\n    return y", "target": "def test_base64():\n    s = SchemaSerializer(core_schema.any_schema(), core_schema.CoreConfig(ser_json_bytes='base64'))\n    assert s.to_python(b'foo') == b'foo'\n    assert s.to_python(b'foo', mode='json') == 'Zm9v'\n    assert s.to_json(b'foo') == b'\"Zm9v\"'\n    assert s.to_python(bytearray(b'foo')) == b'foo'\n    assert s.to_python(bytearray(b'foo'), mode='json') == 'Zm9v'\n    assert s.to_json(bytearray(b'foo')) == b'\"Zm9v\"'", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000522", "source": "def constrain(\n        self: _Pipeline[_InT, _NewOutLen], constraint: annotated_types.Len\n    ) -> _Pipeline[_InT, _NewOutLen]: ...", "target": "def constrain(\n        self: _Pipeline[_InT, _NewOutT], constraint: annotated_types.MultipleOf\n    ) -> _Pipeline[_InT, _NewOutT]: ...", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "000523", "source": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "target": "def test_uuid_json(py_and_json: PyAndJson, input_value, expected):\n    v = py_and_json({'type': 'uuid'})\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            v.validate_test(input_value)\n    else:\n        output = v.validate_test(input_value)\n        assert output == expected\n        assert isinstance(output, UUID)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|opencv/opencv", "data_split": "train", "example_id": "000524", "source": "def forward(\n        self, input: Tensor, states: list[list[tuple[Tensor, Tensor]]]\n    ) -> tuple[Tensor, list[list[tuple[Tensor, Tensor]]]]:\n        output_states = jit.annotate(list[list[tuple[Tensor, Tensor]]], [])\n        output = input\n        i = 0\n        for rnn_layer in self.layers:\n            state = states[i]\n            output, out_state = rnn_layer(output, state)\n            output_states += [out_state]\n            i += 1\n        return output, output_states", "target": "def get_correct_answers(self, img_list, net_output_blob):\n        correct_answers = 0\n        for i in range(len(img_list)):\n            indexes = np.argsort(net_output_blob[i])[-5:]\n            correct_index = self.img_classes[img_list[i]]\n            if correct_index in indexes:\n                correct_answers += 1\n        return correct_answers", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|opencv/opencv", "data_split": "train", "example_id": "000525", "source": "def make_estimator(self, params):\n        (svd_solver,) = params\n        estimator = PCA(n_components=32, svd_solver=svd_solver, random_state=0)\n        return estimator", "target": "def str_(cls, ctype_name: Optional[str] = None,\n             required_modules: Tuple[str, ...] = ()):\n        if ctype_name is None:\n            ctype_name = \"string\"\n        return PrimitiveTypeNode(ctype_name, \"str\", required_modules=required_modules)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000526", "source": "def types_separator(self) -> str:\n        return \"\"", "target": "def types_separator(self) -> str:\n        return \", \"", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000527", "source": "def test_slots_mixed(any_serializer):\n    @dataclasses.dataclass(slots=True)\n    class Model:\n        x: int\n        y: dataclasses.InitVar[str]\n        z: ClassVar[str] = 'z-classvar'\n    @dataclasses.dataclass\n    class SubModel(Model):\n        x2: int\n        y2: dataclasses.InitVar[str]\n        z2: ClassVar[str] = 'z2-classvar'\n    dc = SubModel(x=1, y='a', x2=2, y2='b')\n    assert dataclasses.asdict(dc) == {'x': 1, 'x2': 2}\n    assert any_serializer.to_python(dc) == {'x': 1, 'x2': 2}\n    assert any_serializer.to_json(dc) == b'{\"x\":1,\"x2\":2}'", "target": "def test_on_error_raise_explicit(self, py_and_json: PyAndJson):\n        v = py_and_json(\n            {\n                'type': 'model-fields',\n                'fields': {\n                    'x': {\n                        'type': 'model-field',\n                        'schema': {'type': 'default', 'schema': {'type': 'str'}, 'on_error': 'raise'},\n                    }\n                },\n            }\n        )\n        assert v.validate_test({'x': 'foo'}) == ({'x': 'foo'}, None, {'x'})\n        with pytest.raises(ValidationError) as exc_info:\n            v.validate_test({'x': ['foo']})\n        assert exc_info.value.errors(include_url=False) == [\n            {'input': ['foo'], 'type': 'string_type', 'loc': ('x',), 'msg': 'Input should be a valid string'}\n        ]", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000528", "source": "def name(self):\n        prefix = f\"{self.category()}_{self._name}_{self.backend()}\"\n        if self.is_dynamic():\n            prefix += \"_dynamic\"\n        if self._is_gpu:\n            prefix += \"_gpu\"\n        if self._force_shape_pad:\n            prefix += \"_force_shape_pad\"\n        return prefix", "target": "def name(self) -> str:\n        prefix = f\"{self.category()}_{self.backend()}\"\n        if self.is_dynamic():\n            prefix += \"_dynamic\"\n        if self.device() == \"cuda\":\n            prefix += \"_gpu\"\n        return prefix", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "train", "example_id": "000529", "source": "def skip_accuracy_check_as_eager_non_deterministic(self):\n        if self.args.accuracy and self.args.training:\n            return SKIP_ACCURACY_CHECK_AS_EAGER_NON_DETERMINISTIC_MODELS\n        return set()", "target": "def complete_wheel(folder: str) -> str:\n    wheel_name = list_dir(f\"/{folder}/dist\")[0]\n    if \"pytorch\" in folder and not enable_cuda:\n        print(\"Repairing Wheel with AuditWheel\")\n        check_call([\"auditwheel\", \"repair\", f\"dist/{wheel_name}\"], cwd=folder)\n        repaired_wheel_name = list_dir(f\"/{folder}/wheelhouse\")[0]\n        print(f\"Moving {repaired_wheel_name} wheel to /{folder}/dist\")\n        os.rename(\n            f\"/{folder}/wheelhouse/{repaired_wheel_name}\",\n            f\"/{folder}/dist/{repaired_wheel_name}\",\n        )\n    else:\n        repaired_wheel_name = list_dir(f\"/{folder}/dist\")[0]\n    print(f\"Copying {repaired_wheel_name} to artifacts\")\n    shutil.copy2(\n        f\"/{folder}/dist/{repaired_wheel_name}\", f\"/artifacts/{repaired_wheel_name}\"\n    )\n    return repaired_wheel_name", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000530", "source": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "target": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pytorch/pytorch", "data_split": "train", "example_id": "000531", "source": "def test_custom_op_goodFeaturesToTrack(self):\n            img_path = self.find_file('cv/face/david2.jpg', [os.environ.get('OPENCV_TEST_DATA_PATH')])\n            in_mat = cv.cvtColor(cv.imread(img_path), cv.COLOR_RGB2GRAY)\n            max_corners         = 50\n            quality_lvl         = 0.01\n            min_distance        = 10.0\n            block_sz            = 3\n            use_harris_detector = True\n            k                   = 0.04\n            expected = cv.goodFeaturesToTrack(in_mat, max_corners, quality_lvl,\n                                              min_distance, mask=None,\n                                              blockSize=block_sz, useHarrisDetector=use_harris_detector, k=k)\n            g_in = cv.GMat()\n            g_out = GGoodFeatures.on(g_in, max_corners, quality_lvl,\n                                     min_distance, block_sz, use_harris_detector, k)\n            comp = cv.GComputation(cv.GIn(g_in), cv.GOut(g_out))\n            pkg = cv.gapi.kernels(GGoodFeaturesImpl)\n            actual = comp.apply(cv.gin(in_mat), args=cv.gapi.compile_args(pkg))\n            self.assertEqual(0.0, cv.norm(expected.flatten(),\n                                          np.array(actual, dtype=np.float32).flatten(), cv.NORM_INF))", "target": "def forward(self, x: Tensor) -> Tensor:\n        output = self._norm(x.float()).type_as(x)\n        return output * self.weight", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|scikit-learn/scikit-learn", "data_split": "train", "example_id": "000532", "source": "def get_transformer(device: torch.device) -> GetterReturnType:\n    N = 64\n    seq_length = 128\n    ntoken = 50\n    model = models.TransformerModel(\n        ntoken=ntoken, ninp=720, nhead=12, nhid=2048, nlayers=2\n    )\n    model.to(device)\n    if has_functorch:\n        model.eval()\n    criterion = nn.NLLLoss()\n    params, names = extract_weights(model)\n    data = torch.rand(N, seq_length + 1, device=device).mul(ntoken).long()\n    inputs = data.narrow(1, 0, seq_length)\n    targets = data.narrow(1, 1, seq_length)\n    def forward(*new_params: Tensor) -> Tensor:\n        load_weights(model, names, new_params)\n        out = model(inputs)\n        loss = criterion(\n            out.reshape(N * seq_length, ntoken), targets.reshape(N * seq_length)\n        )\n        return loss\n    return forward, params", "target": "def _linkcode_resolve(domain, info, package, url_fmt, revision):\n    if revision is None:\n        return\n    if domain not in (\"py\", \"pyx\"):\n        return\n    if not info.get(\"module\") or not info.get(\"fullname\"):\n        return\n    class_name = info[\"fullname\"].split(\".\")[0]\n    module = __import__(info[\"module\"], fromlist=[class_name])\n    obj = attrgetter(info[\"fullname\"])(module)\n    obj = inspect.unwrap(obj)\n    try:\n        fn = inspect.getsourcefile(obj)\n    except Exception:\n        fn = None\n    if not fn:\n        try:\n            fn = inspect.getsourcefile(sys.modules[obj.__module__])\n        except Exception:\n            fn = None\n    if not fn:\n        return\n    try:\n        fn = os.path.relpath(fn, start=os.path.dirname(__import__(package).__file__))\n    except ValueError:\n        return None\n    try:\n        lineno = inspect.getsourcelines(obj)[1]\n    except Exception:\n        lineno = \"\"\n    return url_fmt.format(revision=revision, package=package, path=fn, lineno=lineno)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|scikit-learn/scikit-learn", "data_split": "train", "example_id": "000533", "source": "def generate_pydantic_signature(\n    init: Callable[..., None],\n    fields: dict[str, FieldInfo],\n    validate_by_name: bool,\n    extra: ExtraValues | None,\n    is_dataclass: bool = False,\n) -> Signature:\n    merged_params = _generate_signature_parameters(init, fields, validate_by_name, extra)\n    if is_dataclass:\n        merged_params = {k: _process_param_defaults(v) for k, v in merged_params.items()}\n    return Signature(parameters=list(merged_params.values()), return_annotation=None)", "target": "def pytest_collection_modifyitems(config, items):\n    skip_doctests = False\n    if np_base_version < parse_version(\"2\"):\n        reason = \"Due to NEP 51 numpy scalar repr has changed in numpy 2\"\n        skip_doctests = True\n    if sp_version < parse_version(\"1.14\"):\n        reason = \"Scipy sparse matrix repr has changed in scipy 1.14\"\n        skip_doctests = True\n    for item in items:\n        if isinstance(item, DoctestItem):\n            item.dtest.globs = {}\n    if skip_doctests:\n        skip_marker = pytest.mark.skip(reason=reason)\n        for item in items:\n            if isinstance(item, DoctestItem):\n                item.add_marker(skip_marker)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "000534", "source": "def test_basic_schema_serializer():\n    s = SchemaSerializer(core_schema.dict_schema())\n    s = pickle.loads(pickle.dumps(s))\n    assert s.to_python({'a': 1, b'b': 2, 33: 3}) == {'a': 1, b'b': 2, 33: 3}\n    assert s.to_python({'a': 1, b'b': 2, 33: 3, True: 4}, mode='json') == {'a': 1, 'b': 2, '33': 3, 'true': 4}\n    assert s.to_json({'a': 1, b'b': 2, 33: 3, True: 4}) == b'{\"a\":1,\"b\":2,\"33\":3,\"true\":4}'\n    assert s.to_python({(1, 2): 3}) == {(1, 2): 3}\n    assert s.to_python({(1, 2): 3}, mode='json') == {'1,2': 3}\n    assert s.to_json({(1, 2): 3}) == b'{\"1,2\":3}'", "target": "def get_transformer(device: torch.device) -> GetterReturnType:\n    N = 64\n    seq_length = 128\n    ntoken = 50\n    model = models.TransformerModel(\n        ntoken=ntoken, ninp=720, nhead=12, nhid=2048, nlayers=2\n    )\n    model.to(device)\n    if has_functorch:\n        model.eval()\n    criterion = nn.NLLLoss()\n    params, names = extract_weights(model)\n    data = torch.rand(N, seq_length + 1, device=device).mul(ntoken).long()\n    inputs = data.narrow(1, 0, seq_length)\n    targets = data.narrow(1, 1, seq_length)\n    def forward(*new_params: Tensor) -> Tensor:\n        load_weights(model, names, new_params)\n        out = model(inputs)\n        loss = criterion(\n            out.reshape(N * seq_length, ntoken), targets.reshape(N * seq_length)\n        )\n        return loss\n    return forward, params", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "000535", "source": "def fn():\n        for n in g.graph.nodes:\n            pass", "target": "def test_omit(py_and_json: PyAndJson):\n    def omit(v, info):\n        if v == 'omit':\n            raise PydanticOmit\n        elif v == 'error':\n            raise ValueError('error')\n        else:\n            return v\n    v = py_and_json(core_schema.with_info_plain_validator_function(omit))\n    assert v.validate_test('foo') == 'foo'\n    if v.validator_type == 'python':\n        assert v.isinstance_test('foo') is True\n    if v.validator_type == 'python':\n        assert v.isinstance_test('error') is False\n    with pytest.raises(SchemaError, match='Uncaught Omit error, please check your usage of `default` validators.'):\n        v.validate_test('omit')", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000536", "source": "def test_include_exclude_schema():\n    s = SchemaSerializer(\n        core_schema.typed_dict_schema(\n            {\n                '0': core_schema.typed_dict_field(core_schema.int_schema(), serialization_exclude=True),\n                '1': core_schema.typed_dict_field(core_schema.int_schema()),\n                '2': core_schema.typed_dict_field(\n                    core_schema.int_schema(), serialization_exclude=True, serialization_exclude_if=lambda x: x < 0\n                ),\n                '3': core_schema.typed_dict_field(\n                    core_schema.int_schema(), serialization_exclude=False, serialization_exclude_if=lambda x: x < 0\n                ),\n            }\n        )\n    )\n    value = {'0': 0, '1': 1, '2': 2, '3': 3}\n    assert s.to_python(value) == {'1': 1, '3': 3}\n    assert s.to_python(value, mode='json') == {'1': 1, '3': 3}\n    assert json.loads(s.to_json(value)) == {'1': 1, '3': 3}\n    value = {'0': 0, '1': 1, '2': 2, '3': -3}\n    assert s.to_python(value) == {'1': 1}\n    assert s.to_python(value, mode='json') == {'1': 1}\n    assert json.loads(s.to_json(value)) == {'1': 1}", "target": "def row_as_markdown(cols: list[str]) -> str:\n        return f'| {\" | \".join(cols)} |'", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000537", "source": "def test_one_choice():\n    v = SchemaValidator(core_schema.union_schema(choices=[core_schema.str_schema()]))\n    assert (\n        plain_repr(v)\n        == 'SchemaValidator(title=\"str\",validator=Str(StrValidator{strict:false,coerce_numbers_to_str:false}),definitions=[],cache_strings=True)'\n    )\n    assert v.validate_python('hello') == 'hello'", "target": "def test_list_allowed_inputs_python(testcase: ListInputTestCase):\n    v = SchemaValidator(core_schema.list_schema(core_schema.int_schema(), strict=testcase.strict))\n    if isinstance(testcase.output, Err):\n        with pytest.raises(ValidationError, match=re.escape(testcase.output.message)):\n            v.validate_python(testcase.input)\n    else:\n        output = v.validate_python(testcase.input)\n        assert output == testcase.output\n        assert output is not testcase.input", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000538", "source": "def required_usage_imports(self) -> Generator[str, None, None]:\n        yield \"import typing as _typing\"", "target": "def required_usage_imports(self) -> Generator[str, None, None]:\n        yield \"import numpy\"\n        yield \"import typing as _typing\"", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|opencv/opencv", "data_split": "train", "example_id": "000539", "source": "def make_estimator(self, params):\n        estimator = HistGradientBoostingClassifier(\n            max_iter=100, max_leaf_nodes=15, early_stopping=False, random_state=0\n        )\n        return estimator", "target": "def build_perf(self):\n        execute([\"make\", \"-j\", str(multiprocessing.cpu_count()), \"opencv_js_perf\"])", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000540", "source": "def get_memory_bytes(self, args, kwargs) -> int:\n        x, target, dloss = args\n        M, N = x.shape\n        return (\n            2 * M * N * x.dtype.itemsize\n            + M * target.dtype.itemsize\n            + M * dloss.dtype.itemsize\n        )", "target": "def get_memory_bytes(self, args, kwargs) -> int:\n        x, w, dy = args\n        M, N = x.shape\n        return 3 * M * N * x.dtype.itemsize + 2 * N * w.dtype.itemsize", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000541", "source": "def test_exclude_default():\n    v = SchemaSerializer(\n        core_schema.typed_dict_schema(\n            {\n                'foo': core_schema.typed_dict_field(core_schema.nullable_schema(core_schema.int_schema())),\n                'bar': core_schema.typed_dict_field(\n                    core_schema.with_default_schema(core_schema.bytes_schema(), default=b'[default]')\n                ),\n            }\n        )\n    )\n    assert v.to_python({'foo': 1, 'bar': b'x'}) == {'foo': 1, 'bar': b'x'}\n    assert v.to_python({'foo': 1, 'bar': b'[default]'}) == {'foo': 1, 'bar': b'[default]'}\n    assert v.to_python({'foo': 1, 'bar': b'[default]'}, exclude_defaults=True) == {'foo': 1}\n    assert v.to_python({'foo': 1, 'bar': b'[default]'}, mode='json') == {'foo': 1, 'bar': '[default]'}\n    assert v.to_python({'foo': 1, 'bar': b'[default]'}, exclude_defaults=True, mode='json') == {'foo': 1}\n    assert v.to_json({'foo': 1, 'bar': b'[default]'}) == b'{\"foo\":1,\"bar\":\"[default]\"}'\n    assert v.to_json({'foo': 1, 'bar': b'[default]'}, exclude_defaults=True) == b'{\"foo\":1}'", "target": "def deserialize(cls, info: TypeInfo, data: JsonDict, api: SemanticAnalyzerPluginInterface) -> PydanticModelField:\n        data = data.copy()\n        typ = deserialize_and_fixup_type(data.pop('type'), api)\n        return cls(type=typ, info=info, **data)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000542", "source": "def constrain(self: _Pipeline[_InT, _NewOutGe], constraint: annotated_types.Ge) -> _Pipeline[_InT, _NewOutGe]: ...", "target": "def constrain(self: _Pipeline[_InT, _NewOutLt], constraint: annotated_types.Lt) -> _Pipeline[_InT, _NewOutLt]: ...", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000543", "source": "def liger(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        from liger_kernel.transformers.cross_entropy import LigerCrossEntropyLoss\n        x, target, dloss = args\n        cross_entropy = LigerCrossEntropyLoss(reduction=\"none\")\n        loss = cross_entropy(x, target)\n        return lambda: torch.autograd.grad(\n            loss, x, grad_outputs=dloss, retain_graph=True\n        )", "target": "def liger(self, args, kwargs=None) -> Any:\n        from liger_kernel.transformers.rms_norm import LigerRMSNorm\n        x, w, dy = args\n        M, N = x.shape\n        liger_rmsnorm = LigerRMSNorm(\n            hidden_size=N, eps=1e-6, casting_mode=\"gemma\"\n        ).cuda()\n        liger_rmsnorm.weight.data.copy_(w)\n        y = liger_rmsnorm(x)\n        return lambda: torch.autograd.grad(\n            y, [x, liger_rmsnorm.weight], grad_outputs=dy, retain_graph=True\n        )", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000544", "source": "def test_config(config: CoreConfig, input_value, expected):\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'a': core_schema.model_field(schema=core_schema.int_schema()),\n                'b': core_schema.model_field(\n                    schema=core_schema.with_default_schema(schema=core_schema.float_schema(), default=4.2)\n                ),\n            }\n        ),\n        config=config,\n    )\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            val = v.validate_python(input_value)\n            print(f'UNEXPECTED OUTPUT: {val!r}')\n    else:\n        result = v.validate_python(input_value)\n        assert result == expected", "target": "def test_config(config: CoreConfig, input_value, expected):\n    v = SchemaValidator(\n        core_schema.typed_dict_schema(\n            fields={\n                'a': core_schema.typed_dict_field(schema=core_schema.int_schema()),\n                'b': core_schema.typed_dict_field(schema=core_schema.float_schema(), required=False),\n            },\n            config=config,\n        )\n    )\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            val = v.validate_python(input_value)\n            print(f'UNEXPECTED OUTPUT: {val!r}')\n    else:\n        output_dict = v.validate_python(input_value)\n        assert output_dict == expected", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|scikit-learn/scikit-learn", "data_split": "train", "example_id": "000545", "source": "def test_function_wrong_sig():\n    def f(input_value):\n        return input_value + ' Changed'\n    v = SchemaValidator(core_schema.with_info_before_validator_function(f, core_schema.str_schema()))\n    if platform.python_implementation() == 'PyPy':\n        error_message = 'f() takes 1 positional argument but 2 were given'\n    else:\n        error_message = 'f() takes 1 positional argument but 2 were given'\n    with pytest.raises(TypeError, match=re.escape(error_message)):\n        v.validate_python('input value')", "target": "def scatter_time_vs_s(time, norm, point_labels, title):\n    plt.figure()\n    size = 100\n    for i, l in enumerate(sorted(norm.keys())):\n        if l != \"fbpca\":\n            plt.scatter(time[l], norm[l], label=l, marker=\"o\", c=\"b\", s=size)\n            for label, x, y in zip(point_labels, list(time[l]), list(norm[l])):\n                plt.annotate(\n                    label,\n                    xy=(x, y),\n                    xytext=(0, -80),\n                    textcoords=\"offset points\",\n                    ha=\"right\",\n                    arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3\"),\n                    va=\"bottom\",\n                    size=11,\n                    rotation=90,\n                )\n        else:\n            plt.scatter(time[l], norm[l], label=l, marker=\"^\", c=\"red\", s=size)\n            for label, x, y in zip(point_labels, list(time[l]), list(norm[l])):\n                plt.annotate(\n                    label,\n                    xy=(x, y),\n                    xytext=(0, 30),\n                    textcoords=\"offset points\",\n                    ha=\"right\",\n                    arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3\"),\n                    va=\"bottom\",\n                    size=11,\n                    rotation=90,\n                )\n    plt.legend(loc=\"best\")\n    plt.suptitle(title)\n    plt.ylabel(\"norm discrepancy\")\n    plt.xlabel(\"running time [s]\")", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000546", "source": "def forward(self, x):\n        return torch.mm(x, self.weight)", "target": "def forward(self, x):\n        x = self.relu_a(x)\n        x = x + self.sub_mods(x)\n        return x + self.relu_b(x) + self.a", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000547", "source": "def test_frozen_field():\n    @dataclasses.dataclass\n    class MyModel:\n        f: str\n    v = SchemaValidator(\n        core_schema.dataclass_schema(\n            MyModel,\n            core_schema.dataclass_args_schema(\n                'MyModel', [core_schema.dataclass_field('f', core_schema.str_schema(), frozen=True)]\n            ),\n            ['f'],\n        )\n    )\n    m = v.validate_python({'f': 'x'})\n    assert m.f == 'x'\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_assignment(m, 'f', 'y')\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'frozen_field', 'loc': ('f',), 'msg': 'Field is frozen', 'input': 'y'}\n    ]", "target": "def test_frozen_field():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'name': core_schema.model_field(schema=core_schema.str_schema()),\n                'age': core_schema.model_field(schema=core_schema.int_schema()),\n                'is_developer': core_schema.model_field(\n                    schema=core_schema.with_default_schema(schema=core_schema.bool_schema(), default=True), frozen=True\n                ),\n            }\n        )\n    )\n    r1, model_extra, fields_set = v.validate_python({'name': 'Samuel', 'age': '36'})\n    assert r1 == {'name': 'Samuel', 'age': 36, 'is_developer': True}\n    assert model_extra is None\n    assert fields_set == {'name', 'age'}\n    v.validate_assignment(r1, 'age', '35')\n    assert r1 == {'name': 'Samuel', 'age': 35, 'is_developer': True}\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_assignment(r1, 'is_developer', False)\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'frozen_field', 'loc': ('is_developer',), 'msg': 'Field is frozen', 'input': False}\n    ]", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|opencv/opencv", "data_split": "train", "example_id": "000548", "source": "def setup(self, *params):\n        if self.skip(params):\n            raise NotImplementedError\n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\"rb\") as f:\n            self.estimator = pickle.load(f)\n        self.make_scorers()", "target": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "000549", "source": "def _clean_schema_for_pretty_print(obj: Any, strip_metadata: bool = True) -> Any:\n    if isinstance(obj, Mapping):\n        new_dct = {}\n        for k, v in obj.items():\n            if k == 'metadata' and strip_metadata:\n                new_metadata = {}\n                for meta_k, meta_v in v.items():\n                    if meta_k in ('pydantic_js_functions', 'pydantic_js_annotation_functions'):\n                        new_metadata['js_metadata'] = '<stripped>'\n                    else:\n                        new_metadata[meta_k] = _clean_schema_for_pretty_print(meta_v, strip_metadata=strip_metadata)\n                if list(new_metadata.keys()) == ['js_metadata']:\n                    new_metadata = {'<stripped>'}\n                new_dct[k] = new_metadata\n            elif k in ('custom_init', 'root_model') and not v:\n                continue\n            else:\n                new_dct[k] = _clean_schema_for_pretty_print(v, strip_metadata=strip_metadata)\n        return new_dct\n    elif isinstance(obj, Sequence) and not isinstance(obj, str):\n        return [_clean_schema_for_pretty_print(v, strip_metadata=strip_metadata) for v in obj]\n    else:\n        return obj", "target": "def test_success_runs_all_steps_and_uses_env_and_workdir(monkeypatch, patch_module):\n    run_test_plan = patch_module.run_test_plan\n    tests_map = {\n        \"basic\": {\n            \"title\": \"Basic suite\",\n            \"package_install\": [],\n            \"working_directory\": \"tests\",\n            \"env_vars\": {\"GLOBAL_FLAG\": \"1\"},\n            \"steps\": [\n                \"export A=x && pytest -q\",\n                \"export B=y && pytest -q tests/unit\",\n            ],\n        }\n    }\n    patch_module.run_command.side_effect = [0, 0, 0]\n    run_test_plan(\"basic\", \"cpu\", tests_map)\n    calls = patch_module.run_command.call_args_list\n    cmds = [_get_cmd(c) for c in calls]\n    checks = [_get_check(c) for c in calls]\n    assert cmds == [\n        \"export A=x && pytest -q\",\n        \"export B=y && pytest -q tests/unit\",\n    ]\n    assert all(chk is False for chk in checks)\n    assert patch_module.workdir_calls == [\"tests\"]\n    assert patch_module.temp_calls == [{\"GLOBAL_FLAG\": \"1\"}]", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000550", "source": "def getCMakeArgs(self, arch, target):\n        args = Builder.getCMakeArgs(self, arch, target)\n        args = args + [\n            '-DVISIONOS_ARCH=%s' % arch\n        ]\n        return args", "target": "def getCMakeArgs(self):\n        args = TestRunner.getCMakeArgs(self)\n        args = args + [\n            '-DMACOSX_DEPLOYMENT_TARGET=%s' % os.environ['MACOSX_DEPLOYMENT_TARGET']\n        ]\n        return args", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000551", "source": "def test_branch_nullable():\n    s = SchemaSerializer(\n        core_schema.definitions_schema(\n            core_schema.definition_reference_schema('Branch'),\n            [\n                core_schema.typed_dict_schema(\n                    {\n                        'name': core_schema.typed_dict_field(core_schema.str_schema()),\n                        'sub_branch': core_schema.typed_dict_field(\n                            core_schema.nullable_schema(core_schema.definition_reference_schema('Branch'))\n                        ),\n                    },\n                    ref='Branch',\n                )\n            ],\n        )\n    )\n    assert s.to_python({'name': 'root', 'sub_branch': {'name': 'branch', 'sub_branch': None}}) == {\n        'name': 'root',\n        'sub_branch': {'name': 'branch', 'sub_branch': None},\n    }\n    assert s.to_python({'name': 'root', 'sub_branch': {'name': 'branch', 'sub_branch': None}}, exclude_none=True) == {\n        'name': 'root',\n        'sub_branch': {'name': 'branch'},\n    }", "target": "def test_branch_nullable():\n    v = SchemaValidator(\n        core_schema.definitions_schema(\n            {'type': 'definition-ref', 'schema_ref': 'Branch'},\n            [\n                {\n                    'type': 'typed-dict',\n                    'ref': 'Branch',\n                    'fields': {\n                        'name': {'type': 'typed-dict-field', 'schema': {'type': 'str'}},\n                        'sub_branch': {\n                            'type': 'typed-dict-field',\n                            'schema': {\n                                'type': 'default',\n                                'schema': {\n                                    'type': 'nullable',\n                                    'schema': {'type': 'definition-ref', 'schema_ref': 'Branch'},\n                                },\n                                'default': None,\n                            },\n                        },\n                    },\n                }\n            ],\n        )\n    )\n    assert v.validate_python({'name': 'root'}) == {'name': 'root', 'sub_branch': None}\n    assert v.validate_python({'name': 'root', 'sub_branch': {'name': 'b1'}}) == (\n        {'name': 'root', 'sub_branch': {'name': 'b1', 'sub_branch': None}}\n    )\n    assert v.validate_python({'name': 'root', 'sub_branch': {'name': 'b1', 'sub_branch': {'name': 'b2'}}}) == (\n        {'name': 'root', 'sub_branch': {'name': 'b1', 'sub_branch': {'name': 'b2', 'sub_branch': None}}}\n    )\n    assert ',definitions=[TypedDict(TypedDictValidator{' in plain_repr(v)", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|opencv/opencv", "data_split": "train", "example_id": "000552", "source": "def check_close_angles(self, a, b, angle_delta):\n        self.assertTrue(abs(a - b) <= angle_delta or\n                        abs(360 - abs(a - b)) <= angle_delta)", "target": "def get_conf_mat(gt, prob):\n    assert type(gt) is np.ndarray\n    assert type(prob) is np.ndarray\n    conf_mat = np.zeros((gt.shape[0], gt.shape[0]))\n    for ch_gt in range(conf_mat.shape[0]):\n        gt_channel = gt[ch_gt, ...]\n        for ch_pr in range(conf_mat.shape[1]):\n            prob_channel = prob[ch_pr, ...]\n            conf_mat[ch_gt][ch_pr] = np.count_nonzero(np.multiply(gt_channel, prob_channel))\n    return conf_mat", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000553", "source": "def test_include():\n    s = SchemaSerializer(core_schema.dict_schema(serialization=core_schema.filter_dict_schema(include={'a', 'c'})))\n    assert s.to_python({'a': 1, 'b': 2, 'c': 3, 'd': 4}) == {'a': 1, 'c': 3}\n    assert s.to_json({'a': 1, 'b': 2, 'c': 3, 'd': 4}) == b'{\"a\":1,\"c\":3}'\n    assert s.to_python({'a': 1, 'b': 2, 'd': 4}, include={'d'}) == {'a': 1, 'd': 4}\n    assert s.to_python({'a': 1, 'b': 2, 'd': 4}, include={'d': None}) == {'a': 1, 'd': 4}\n    assert s.to_python({'a': 1, 'b': 2, 'd': 4}, include={'d': {1}}) == {'a': 1, 'd': 4}\n    assert s.to_python({'a': 1, 'b': 2, 'd': 4, 5: 6}, include={5}) == {'a': 1, 5: 6}\n    assert s.to_python({'a': 1, 'b': 2, 'd': 4, 5: 6}, mode='json', include={5}) == {'a': 1, '5': 6}\n    assert s.to_json({'a': 1, 'b': 2, 'd': 4, 5: 6}, include={5}) == b'{\"a\":1,\"5\":6}'", "target": "def test_include():\n    v = SchemaSerializer(\n        core_schema.generator_schema(\n            core_schema.any_schema(), serialization=core_schema.filter_seq_schema(include={1, 3, 5})\n        )\n    )\n    assert v.to_python(gen_ok(0, 1, 2, 3), mode='json') == [1, 3]\n    assert list(v.to_python(gen_ok(0, 1, 2, 3))) == [1, 3]\n    assert v.to_python(gen_ok('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'), mode='json') == ['b', 'd', 'f']\n    assert v.to_python(gen_ok('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'), mode='json') == ['b', 'd', 'f']\n    assert v.to_json(gen_ok('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h')) == b'[\"b\",\"d\",\"f\"]'\n    assert v.to_python(gen_ok('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'), include={6}, mode='json') == ['b', 'd', 'f', 'g']\n    assert list(v.to_python(gen_ok('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'), include={6})) == ['b', 'd', 'f', 'g']\n    assert v.to_json(gen_ok('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'), include={6}) == b'[\"b\",\"d\",\"f\",\"g\"]'\n    assert v.to_python(gen_ok('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'), include={6: None}, mode='json') == [\n        'b',\n        'd',\n        'f',\n        'g',\n    ]\n    with pytest.raises(ValueError, match='Negative indices cannot be used to exclude items on unsized iterables'):\n        v.to_python(gen_ok('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'), include={-1: None, -2: None}, mode='json')\n    v.to_python(gen_ok('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'), include={'__all__': None}, mode='json')", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000554", "source": "def test_uuid_json(value, expected):\n    v = SchemaSerializer(core_schema.uuid_schema())\n    assert v.to_python(value, mode='json') == expected\n    assert v.to_json(value).decode() == f'\"{expected}\"'", "target": "def val2(value, handler, info):\n        nonlocal field_names\n        field_names.append(('val2', info.field_name))\n        return handler(value)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000555", "source": "def val_function(x, *args: Any):\n    return x", "target": "def len(self: _Pipeline[_InT, _NewOutLen], min_len: int, max_len: int | None = None) -> _Pipeline[_InT, _NewOutLen]:\n        return self.constrain(annotated_types.Len(min_len, max_len))", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000556", "source": "def test_slots_mixed():\n    @dataclasses.dataclass(slots=True)\n    class Model:\n        x: int\n        y: dataclasses.InitVar[str]\n        z: ClassVar[str] = 'z-classvar'\n    @dataclasses.dataclass\n    class SubModel(Model):\n        x2: int\n        y2: dataclasses.InitVar[str]\n        z2: ClassVar[str] = 'z2-classvar'\n    schema = core_schema.dataclass_schema(\n        SubModel,\n        core_schema.dataclass_args_schema(\n            'SubModel',\n            [\n                core_schema.dataclass_field(name='x', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(name='y', init_only=True, schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='x2', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(name='y2', init_only=True, schema=core_schema.str_schema()),\n            ],\n        ),\n        ['x', 'x2'],\n        slots=True,\n    )\n    dc = SubModel(x=1, y='a', x2=2, y2='b')\n    assert dataclasses.asdict(dc) == {'x': 1, 'x2': 2}\n    s = SchemaSerializer(schema)\n    assert s.to_python(dc) == {'x': 1, 'x2': 2}\n    assert s.to_json(dc) == b'{\"x\":1,\"x2\":2}'", "target": "def test_slots_mixed(any_serializer):\n    @dataclasses.dataclass(slots=True)\n    class Model:\n        x: int\n        y: dataclasses.InitVar[str]\n        z: ClassVar[str] = 'z-classvar'\n    @dataclasses.dataclass\n    class SubModel(Model):\n        x2: int\n        y2: dataclasses.InitVar[str]\n        z2: ClassVar[str] = 'z2-classvar'\n    dc = SubModel(x=1, y='a', x2=2, y2='b')\n    assert dataclasses.asdict(dc) == {'x': 1, 'x2': 2}\n    assert any_serializer.to_python(dc) == {'x': 1, 'x2': 2}\n    assert any_serializer.to_json(dc) == b'{\"x\":1,\"x2\":2}'", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000557", "source": "def eager(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, target, dloss = args\n        loss = F.cross_entropy(x, target, reduction=\"none\")\n        return lambda: torch.autograd.grad(\n            loss, x, grad_outputs=dloss, retain_graph=True\n        )", "target": "def eager(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, w = args\n        return lambda: self.rms_norm_ref(x, w)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000558", "source": "def forward(self, x):\n        return torch.mm(x, self.weight)", "target": "def forward(self, input):\n        mu, sigma = self.compute_layernorm_stats(input)\n        return (input - mu) / sigma * self.weight + self.bias", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000559", "source": "def test_literal_with_enum() -> None:\n    class SomeEnum(str, Enum):\n        CAT = 'cat'\n        DOG = 'dog'\n    @dataclass\n    class Dog:\n        name: str\n        type: Literal[SomeEnum.DOG] = SomeEnum.DOG\n    @dataclass\n    class Cat:\n        name: str\n        type: Literal[SomeEnum.CAT] = SomeEnum.CAT\n    @dataclass\n    class Yard:\n        pet: Union[Dog, Cat]\n    serializer = SchemaSerializer(\n        core_schema.model_schema(\n            cls=Yard,\n            schema=core_schema.model_fields_schema(\n                fields={\n                    'pet': core_schema.model_field(\n                        schema=core_schema.tagged_union_schema(\n                            choices={\n                                SomeEnum.DOG: core_schema.model_schema(\n                                    cls=Dog,\n                                    schema=core_schema.model_fields_schema(\n                                        fields={\n                                            'type': core_schema.model_field(\n                                                schema=core_schema.with_default_schema(\n                                                    schema=core_schema.literal_schema([SomeEnum.DOG]),\n                                                    default=SomeEnum.DOG,\n                                                )\n                                            ),\n                                            'name': core_schema.model_field(schema=core_schema.str_schema()),\n                                        },\n                                        model_name='Dog',\n                                    ),\n                                ),\n                                SomeEnum.CAT: core_schema.model_schema(\n                                    cls=Cat,\n                                    schema=core_schema.model_fields_schema(\n                                        fields={\n                                            'type': core_schema.model_field(\n                                                schema=core_schema.with_default_schema(\n                                                    schema=core_schema.literal_schema([SomeEnum.CAT]),\n                                                    default=SomeEnum.CAT,\n                                                )\n                                            ),\n                                            'name': core_schema.model_field(schema=core_schema.str_schema()),\n                                        },\n                                        model_name='Cat',\n                                    ),\n                                ),\n                            },\n                            discriminator='type',\n                            strict=False,\n                            from_attributes=True,\n                        )\n                    )\n                }\n            ),\n        )\n    )\n    yard = Yard(pet=Dog(name='Rex'))\n    assert serializer.to_python(yard, mode='json') == {'pet': {'type': 'dog', 'name': 'Rex'}}", "target": "def test_function_wrap_str():\n    def f(input_value, validator, info):\n        return plain_repr(validator)\n    v = SchemaValidator(core_schema.with_info_wrap_validator_function(f, core_schema.str_schema()))\n    assert (\n        v.validate_python('input value')\n        == 'ValidatorCallable(Str(StrValidator{strict:false,coerce_numbers_to_str:false}))'\n    )", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000560", "source": "def test_custom_name():\n    def my_function(a):\n        return a\n    v = SchemaValidator(\n        cs.call_schema(\n            function=my_function,\n            function_name='foobar',\n            arguments=cs.arguments_schema(\n                arguments=[{'name': 'a', 'mode': 'positional_or_keyword', 'schema': cs.int_schema()}]\n            ),\n        )\n    )\n    assert 'name:\"call[foobar]\"' in plain_repr(v)\n    assert v.validate_python((1,)) == 1\n    assert v.validate_python(('2',)) == 2", "target": "async def wrapper_function(*args, **kwargs):\n            return await wrapper(*args, **kwargs)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|opencv/opencv", "data_split": "train", "example_id": "000561", "source": "def lstm_inputs(\n    seqLength=100,\n    numLayers=1,\n    inputSize=512,\n    hiddenSize=512,\n    miniBatch=64,\n    dropout=0.0,\n    return_module=False,\n    device=\"cuda\",\n    seed=None,\n):\n    if seed is not None:\n        torch.manual_seed(seed)\n    x = torch.randn(seqLength, miniBatch, inputSize, device=device)\n    hx = torch.randn(numLayers, miniBatch, hiddenSize, device=device)\n    cx = torch.randn(numLayers, miniBatch, hiddenSize, device=device)\n    lstm = torch.nn.LSTM(inputSize, hiddenSize, numLayers, dropout=dropout)\n    if \"cuda\" in device:\n        lstm = lstm.cuda()\n    if return_module:\n        return x, (hx, cx), lstm.all_weights, lstm\n    else:\n        return x, (hx, cx), lstm.all_weights, None", "target": "def is_resolved(self) -> bool:\n        return self._ast_node is not None or self._module_name is not None", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "000562", "source": "def load_data(dtype=np.float32, order=\"C\", random_state=13):\n    print(\"Loading dataset...\")\n    data = fetch_covtype(\n        download_if_missing=True, shuffle=True, random_state=random_state\n    )\n    X = check_array(data[\"data\"], dtype=dtype, order=order)\n    y = (data[\"target\"] != 1).astype(int)\n    print(\"Creating train-test split...\")\n    n_train = 522911\n    X_train = X[:n_train]\n    y_train = y[:n_train]\n    X_test = X[n_train:]\n    y_test = y[n_train:]\n    mean = X_train.mean(axis=0)\n    std = X_train.std(axis=0)\n    mean[10:] = 0.0\n    std[10:] = 1.0\n    X_train = (X_train - mean) / std\n    X_test = (X_test - mean) / std\n    return X_train, X_test, y_train, y_test", "target": "def load_data(dtype=np.float32, order=\"F\"):\n    print(\"Loading dataset...\")\n    data = fetch_openml(\"mnist_784\", as_frame=True)\n    X = check_array(data[\"data\"], dtype=dtype, order=order)\n    y = data[\"target\"]\n    X = X / 255\n    print(\"Creating train-test split...\")\n    n_train = 60000\n    X_train = X[:n_train]\n    y_train = y[:n_train]\n    X_test = X[n_train:]\n    y_test = y[n_train:]\n    return X_train, X_test, y_train, y_test", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000563", "source": "def liger(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        from liger_kernel.transformers.cross_entropy import LigerCrossEntropyLoss\n        x, target = args\n        cross_entropy = LigerCrossEntropyLoss(reduction=\"none\")\n        return lambda: cross_entropy(x, target)", "target": "def liger(self, args, kwargs) -> Any:\n        from liger_kernel.ops.layer_norm import layer_norm_backward\n        x, w, dy = args\n        eps = 1e-6\n        mean, rstd = self.compute_mean_rstd(x, eps)\n        M, N = x.shape\n        return lambda: layer_norm_backward(dy, x, w, None, mean, rstd)[0:2]", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "000564", "source": "def f2(input_value, info):\n        info.context['f2'] = input_value\n        return input_value + f'| context: {info.context}'", "target": "def run(eyesl, eyesr):\n        out_l_st = [int(st) for eye_l in eyesl for st in (eye_l[:, 0] < eye_l[:, 1]).ravel()]\n        out_r_st = [int(st) for eye_r in eyesr for st in (eye_r[:, 0] < eye_r[:, 1]).ravel()]\n        return out_l_st, out_r_st", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000565", "source": "def validate_b(cls, v: bytes, info: core_schema.ValidationInfo) -> bytes:\n            assert v == b'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return b'hello world!'", "target": "def validate_b(\n            cls, v: bytes, nxt: core_schema.ValidatorFunctionWrapHandler, info: core_schema.ValidationInfo\n        ) -> str:\n            assert v == b'hello'\n            v = nxt(v)\n            assert v == 'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return 'hello world!'", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000566", "source": "def preprocess(self, img):\n        return img - self.mean_blob", "target": "def preprocess(self, img):\n        img = self.initial_preprocess(img)\n        return self.preprocess_input(img)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "000567", "source": "def torch_mm(a, b):\n    return torch.mm(a, b)", "target": "def test_uuid_json(py_and_json: PyAndJson, input_value, expected):\n    v = py_and_json({'type': 'uuid'})\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            v.validate_test(input_value)\n    else:\n        output = v.validate_test(input_value)\n        assert output == expected\n        assert isinstance(output, UUID)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000568", "source": "def my_function(a, b, c):\n        return a + b + c", "target": "def my_function(input_value, info):\n        return input_value + 'x'", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "000569", "source": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "target": "def field_name(self) -> str | None:\n        raise NotImplementedError", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000570", "source": "def area(self) -> int:\n            return self.width * self.height", "target": "def area(self) -> int:\n            return self.width * self.height", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|pydantic/pydantic", "data_split": "train", "example_id": "000571", "source": "def generate_missing_values(X, missing_fraction):\n    total_cells = X.shape[0] * X.shape[1]\n    num_missing_cells = int(total_cells * missing_fraction)\n    row_indices = rng.choice(X.shape[0], num_missing_cells, replace=True)\n    col_indices = rng.choice(X.shape[1], num_missing_cells, replace=True)\n    X_missing = X.copy()\n    X_missing.iloc[row_indices, col_indices] = np.nan\n    return X_missing", "target": "def str_lower(self: _Pipeline[_InT, str]) -> _Pipeline[_InT, str]:\n        return self.transform(str.lower)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000572", "source": "def test_fields_required_by_default_with_default():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'x': core_schema.model_field(schema=core_schema.str_schema()),\n                'y': core_schema.model_field(\n                    schema=core_schema.with_default_schema(schema=core_schema.str_schema(), default='bulbi')\n                ),\n            }\n        )\n    )\n    assert v.validate_python({'x': 'pika', 'y': 'chu'}) == ({'x': 'pika', 'y': 'chu'}, None, {'x', 'y'})\n    assert v.validate_python({'x': 'pika'}) == ({'x': 'pika', 'y': 'bulbi'}, None, {'x'})", "target": "def test_fields_required_by_default_with_default():\n    v = SchemaValidator(\n        core_schema.typed_dict_schema(\n            fields={\n                'x': core_schema.typed_dict_field(schema=core_schema.str_schema()),\n                'y': core_schema.typed_dict_field(\n                    schema=core_schema.with_default_schema(schema=core_schema.str_schema(), default='bulbi')\n                ),\n            }\n        )\n    )\n    assert v.validate_python({'x': 'pika', 'y': 'chu'}) == {'x': 'pika', 'y': 'chu'}\n    assert v.validate_python({'x': 'pika'}) == {'x': 'pika', 'y': 'bulbi'}", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000573", "source": "def test_paths_allow_by_name(py_and_json: PyAndJson, input_value):\n    v = py_and_json(\n        {\n            'type': 'model-fields',\n            'fields': {\n                'field_a': {\n                    'validation_alias': [['foo', 'bar'], ['foo']],\n                    'type': 'model-field',\n                    'schema': {'type': 'int'},\n                }\n            },\n        },\n        config=CoreConfig(validate_by_name=True),\n    )\n    assert v.validate_test(input_value) == ({'field_a': 42}, None, {'field_a'})", "target": "def test_constraints_schema_validation() -> None:\n    val = SchemaValidator(cs.decimal_schema(gt='1'))\n    with pytest.raises(ValidationError):\n        val.validate_python('0')", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000574", "source": "def test_extra_behavior_forbid(\n    config: Union[core_schema.CoreConfig, None],\n    schema_extra_behavior_kw: dict[str, Any],\n    validate_fn_extra_kw: Union[ExtraBehavior, None],\n):\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            {'f': core_schema.model_field(core_schema.str_schema())}, **schema_extra_behavior_kw\n        ),\n        config=config,\n    )\n    m, model_extra, fields_set = v.validate_python({'f': 'x'}, extra=validate_fn_extra_kw)\n    assert m == {'f': 'x'}\n    assert fields_set == {'f'}\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python({'f': 'x', 'extra_field': 123}, extra=validate_fn_extra_kw)\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'extra_forbidden', 'loc': ('extra_field',), 'msg': 'Extra inputs are not permitted', 'input': 123}\n    ]\n    v.validate_assignment(m, 'f', 'y', extra=validate_fn_extra_kw)\n    assert m['f'] == 'y'\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_assignment(m, 'not_f', 'xyz', extra=validate_fn_extra_kw)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'no_such_attribute',\n            'loc': ('not_f',),\n            'msg': \"Object has no attribute 'not_f'\",\n            'input': 'xyz',\n            'ctx': {'attribute': 'not_f'},\n        }\n    ]\n    assert 'not_f' not in m", "target": "def test_extra_behavior_forbid(\n    config: Union[core_schema.CoreConfig, None],\n    schema_extra_behavior_kw: dict[str, Any],\n    validate_fn_extra_kw: Union[ExtraBehavior, None],\n):\n    v = SchemaValidator(\n        core_schema.typed_dict_schema(\n            {'f': core_schema.typed_dict_field(core_schema.str_schema())},\n            **schema_extra_behavior_kw,\n            config=config,\n        )\n    )\n    m: dict[str, Any] = v.validate_python({'f': 'x'}, extra=validate_fn_extra_kw)\n    assert m == {'f': 'x'}\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python({'f': 'x', 'extra_field': 123}, extra=validate_fn_extra_kw)\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'extra_forbidden', 'loc': ('extra_field',), 'msg': 'Extra inputs are not permitted', 'input': 123}\n    ]", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|pydantic/pydantic", "data_split": "train", "example_id": "000575", "source": "def scalable_frobenius_norm_discrepancy(X, U, s, V):\n    if not sp.sparse.issparse(X) or (\n        X.shape[0] * X.shape[1] * X.dtype.itemsize < MAX_MEMORY\n    ):\n        A = X - U.dot(np.diag(s).dot(V))\n        return norm_diff(A, norm=\"fro\")\n    print(\"... computing fro norm by batches...\")\n    batch_size = 1000\n    Vhat = np.diag(s).dot(V)\n    cum_norm = 0.0\n    for batch in gen_batches(X.shape[0], batch_size):\n        M = X[batch, :] - U[batch, :].dot(Vhat)\n        cum_norm += norm_diff(M, norm=\"fro\", msg=False)\n    return np.sqrt(cum_norm)", "target": "def load_str_bytes(\n    b: str | bytes,\n    *,\n    content_type: str | None = None,\n    encoding: str = 'utf8',\n    proto: Protocol | None = None,\n    allow_pickle: bool = False,\n    json_loads: Callable[[str], Any] = json.loads,\n) -> Any:\n    warnings.warn('`load_str_bytes` is deprecated.', category=PydanticDeprecatedSince20, stacklevel=2)\n    if proto is None and content_type:\n        if content_type.endswith(('json', 'javascript')):\n            pass\n        elif allow_pickle and content_type.endswith('pickle'):\n            proto = Protocol.pickle\n        else:\n            raise TypeError(f'Unknown content-type: {content_type}')\n    proto = proto or Protocol.json\n    if proto == Protocol.json:\n        if isinstance(b, bytes):\n            b = b.decode(encoding)\n        return json_loads(b)\n    elif proto == Protocol.pickle:\n        if not allow_pickle:\n            raise RuntimeError('Trying to decode with pickle with allow_pickle=False')\n        bb = b if isinstance(b, bytes) else b.encode()\n        return pickle.loads(bb)\n    else:\n        raise TypeError(f'Unknown protocol: {proto}')", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000576", "source": "def attempt_rebuild_fn(attr_fn: Callable[[TypeAdapter], T]) -> Callable[[], T | None]:\n        def handler() -> T | None:\n            if adapter.rebuild(raise_errors=False, _parent_namespace_depth=5) is not False:\n                return attr_fn(adapter)\n            return None\n        return handler", "target": "def attempt_rebuild_fn(attr_fn: Callable[[type[BaseModel]], T]) -> Callable[[], T | None]:\n        def handler() -> T | None:\n            if cls.model_rebuild(raise_errors=False, _parent_namespace_depth=5) is not False:\n                return attr_fn(cls)\n            return None\n        return handler", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000577", "source": "def test_build_error():\n    with pytest.raises(SchemaError, match='SchemaError: `expected` should have length > 0'):\n        SchemaValidator(cs.literal_schema(expected=[]))", "target": "def test_union_timedelta_respects_instanceof_check():\n    serialization_schema = core_schema.plain_serializer_function_ser_schema(lambda v: None)\n    json_validation_schema = core_schema.no_info_plain_validator_function(\n        function=lambda v: v, serialization=serialization_schema\n    )\n    test_custom_ser_schema = core_schema.json_schema(\n        schema=json_validation_schema,\n        serialization=serialization_schema,\n    )\n    s = SchemaSerializer(core_schema.union_schema(choices=[core_schema.timedelta_schema(), test_custom_ser_schema]))\n    assert s.to_python('foo') is None", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "000578", "source": "def test_check_ref_used_ignores_metadata():\n    v = SchemaValidator(\n        core_schema.list_schema(\n            core_schema.int_schema(metadata={'type': 'definition-ref', 'schema_ref': 'foobar'}), ref='foobar'\n        )\n    )\n    assert v.validate_python([1, 2, 3]) == [1, 2, 3]", "target": "def simple_backward_setup(output, seed=None):\n    assert isinstance(output, torch.Tensor)\n    if seed:\n        torch.manual_seed(seed)\n    grad_output = torch.randn_like(output)\n    return output, grad_output", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000579", "source": "def discriminator_function(obj):\n        if isinstance(obj, str):\n            return 'str'\n        elif isinstance(obj, int):\n            return 'int'\n        elif obj is None:\n            return None\n        else:\n            return 'other'", "target": "def discriminator_function(obj):\n        if isinstance(obj, str):\n            return 'a'\n        elif isinstance(obj, int):\n            return 1\n        elif obj is None:\n            return None\n        else:\n            return 'other'", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "000580", "source": "def pytest_addoption(parser):\n    parser.addoption(\"--fuser\", default=\"old\", help=\"fuser to use for benchmarks\")\n    parser.addoption(\n        \"--executor\", default=\"legacy\", help=\"executor to use for benchmarks\"\n    )", "target": "def test_simple_tagged_union(py_and_json: PyAndJson, input_value, expected):\n    v = py_and_json(\n        {\n            'type': 'tagged-union',\n            'discriminator': 'foo',\n            'from_attributes': False,\n            'choices': {\n                'apple': {\n                    'type': 'typed-dict',\n                    'fields': {\n                        'foo': {'type': 'typed-dict-field', 'schema': {'type': 'str'}},\n                        'bar': {'type': 'typed-dict-field', 'schema': {'type': 'int'}},\n                    },\n                },\n                'banana': {\n                    'type': 'typed-dict',\n                    'fields': {\n                        'foo': {'type': 'typed-dict-field', 'schema': {'type': 'str'}},\n                        'spam': {\n                            'type': 'typed-dict-field',\n                            'schema': {'type': 'list', 'items_schema': {'type': 'int'}},\n                        },\n                    },\n                },\n            },\n        }\n    )\n    assert 'discriminator: LookupKey' in repr(v.validator)\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=expected.message) as exc_info:\n            v.validate_test(input_value)\n        assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        assert v.validate_test(input_value) == expected", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000581", "source": "def get_memory_bytes(self, args, kwargs) -> int:\n        (x,) = args\n        M, N = x.shape\n        return 2 * M * N * x.dtype.itemsize", "target": "def get_memory_bytes(self, args, kwargs) -> int:\n        x, w = args\n        M, N = x.shape\n        return 2 * M * N * x.dtype.itemsize + N * w.dtype.itemsize", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000582", "source": "def f(value, handler, _info):\n        return handler(value)", "target": "def _extract_source_from_frame(cls: type[Any]) -> list[str] | None:\n    frame = inspect.currentframe()\n    while frame:\n        if inspect.getmodule(frame) is inspect.getmodule(cls):\n            lnum = frame.f_lineno\n            try:\n                lines, _ = inspect.findsource(frame)\n            except OSError:\n                pass\n            else:\n                block_lines = inspect.getblock(lines[lnum - 1 :])\n                dedent_source = _dedent_source_lines(block_lines)\n                try:\n                    block_tree = ast.parse(dedent_source)\n                except SyntaxError:\n                    pass\n                else:\n                    stmt = block_tree.body[0]\n                    if isinstance(stmt, ast.FunctionDef) and stmt.name == 'dedent_workaround':\n                        stmt = stmt.body[0]\n                    if isinstance(stmt, ast.ClassDef) and stmt.name == cls.__name__:\n                        return block_lines\n        frame = frame.f_back", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000583", "source": "def typename(self) -> str:\n            return self.type_node.full_typename", "target": "def typename(self) -> str:\n        return \"None\"", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "train", "example_id": "000584", "source": "def deserialize_tensor(size, dtype, stride=None):\n    if stride is not None:\n        out = torch.empty_strided(size, stride, dtype=dtype)\n    else:\n        out = torch.empty(size, dtype=dtype)\n    try:\n        out.copy_(make_tensor(size, dtype=dtype, device=\"cpu\"))\n    except Exception as e:\n        print(e)\n        return out\n    return out", "target": "def find_pypi_package_version(package: str) -> Optional[str]:\n    from importlib import metadata\n    dists = metadata.distributions()\n    for dist in dists:\n        if dist.metadata[\"Name\"].startswith(package):\n            return dist.version\n    return None", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000585", "source": "def set_dataclass_mocks(cls: type[PydanticDataclass], undefined_name: str = 'all referenced types') -> None:\n    from ..dataclasses import rebuild_dataclass\n    undefined_type_error_message = (\n        f'`{cls.__name__}` is not fully defined; you should define {undefined_name},'\n        f' then call `pydantic.dataclasses.rebuild_dataclass({cls.__name__})`.'\n    )\n    def attempt_rebuild_fn(attr_fn: Callable[[type[PydanticDataclass]], T]) -> Callable[[], T | None]:\n        def handler() -> T | None:\n            if rebuild_dataclass(cls, raise_errors=False, _parent_namespace_depth=5) is not False:\n                return attr_fn(cls)\n            return None\n        return handler\n    cls.__pydantic_core_schema__ = MockCoreSchema(\n        undefined_type_error_message,\n        code='class-not-fully-defined',\n        attempt_rebuild=attempt_rebuild_fn(lambda c: c.__pydantic_core_schema__),\n    )\n    cls.__pydantic_validator__ = MockValSer(\n        undefined_type_error_message,\n        code='class-not-fully-defined',\n        val_or_ser='validator',\n        attempt_rebuild=attempt_rebuild_fn(lambda c: c.__pydantic_validator__),\n    )\n    cls.__pydantic_serializer__ = MockValSer(\n        undefined_type_error_message,\n        code='class-not-fully-defined',\n        val_or_ser='serializer',\n        attempt_rebuild=attempt_rebuild_fn(lambda c: c.__pydantic_serializer__),\n    )", "target": "def test_datetime(input_value, expected):\n    v = SchemaValidator(cs.datetime_schema())\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            result = v.validate_python(input_value)\n            print(f'input_value={input_value} result={result}')\n    else:\n        output = v.validate_python(input_value)\n        assert output == expected", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "000586", "source": "def test_different_output_opaque_kinds(self):\n            g_in = cv.GArray.Int()\n            g_mean, g_squares = GSquareMean.on(g_in)\n            comp = cv.GComputation(cv.GIn(g_in), cv.GOut(g_mean, g_squares))\n            pkg = cv.gapi.kernels(GSquareMeanImpl)\n            mean, squares = comp.apply(cv.gin([1,2,3]), args=cv.gapi.compile_args(pkg))\n            self.assertEqual([1,4,9], list(squares))\n            self.assertEqual(2.0, mean)", "target": "def test_ser_function_plain():\n    def f(input: Any, info: core_schema.SerializationInfo, /) -> str:\n        return str(info)\n    s = SchemaSerializer(\n        core_schema.any_schema(\n            serialization=core_schema.plain_serializer_function_ser_schema(\n                f, info_arg=True, return_schema=core_schema.str_schema()\n            )\n        )\n    )\n    assert s.to_python(123) == (\n        \"SerializationInfo(include=None, exclude=None, context=None, mode='python', by_alias=False, exclude_unset=False, \"\n        'exclude_defaults=False, exclude_none=False, exclude_computed_fields=False, round_trip=False, serialize_as_any=False)'\n    )", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000587", "source": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "target": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "000588", "source": "def make_estimator(self, params):\n        (method,) = params\n        estimator = TSNE(random_state=0, method=method)\n        return estimator", "target": "def make_estimator(self, params):\n        representation, n_jobs = params\n        n_estimators = 500 if Benchmark.data_size == \"large\" else 100\n        estimator = RandomForestClassifier(\n            n_estimators=n_estimators,\n            min_samples_split=10,\n            max_features=\"log2\",\n            n_jobs=n_jobs,\n            random_state=0,\n        )\n        return estimator", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "000589", "source": "def test_json_none():\n    v = SchemaValidator(cs.none_schema())\n    assert v.validate_json('null') is None\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_json('1')\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'none_required', 'loc': (), 'msg': 'Input should be null', 'input': 1}\n    ]", "target": "def get_memory_bytes(self, args, kwargs) -> int:\n        x, w = args\n        M, N = x.shape\n        return 2 * M * N * x.dtype.itemsize + N * w.dtype.itemsize", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000590", "source": "def test_constraints_schema_validation() -> None:\n    val = SchemaValidator(cs.datetime_schema(gt='2020-01-01T00:00:00'))\n    with pytest.raises(ValidationError):\n        val.validate_python('2019-01-01T00:00:00')", "target": "def test_constraints_schema_validation() -> None:\n    val = SchemaValidator(core_schema.timedelta_schema(gt=3))\n    with pytest.raises(ValidationError):\n        val.validate_python(1)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000591", "source": "def test_repr():\n    v = SchemaValidator(cs.union_schema(choices=[cs.int_schema(), cs.callable_schema()]))\n    assert v.isinstance_python(4) is True\n    assert v.isinstance_python(func) is True\n    assert v.isinstance_python('foo') is False\n    with pytest.raises(ValidationError, match=r'callable\\s+Input should be callable'):\n        v.validate_python('foo')", "target": "def test_repr():\n    v = SchemaValidator(cs.union_schema(choices=[cs.int_schema(), cs.is_instance_schema(cls=Foo)]))\n    assert v.isinstance_python(4) is True\n    assert v.isinstance_python(Bar()) is True\n    assert v.isinstance_python('foo') is False\n    with pytest.raises(ValidationError, match=r'is-instance\\[Foo\\]\\s+Input should be an instance of Foo'):\n        v.validate_python('foo')", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000592", "source": "def eager(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, w = args\n        return lambda: self.layernorm_ref(x, w)", "target": "def eager(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, w, dy = args\n        y = self.layernorm_ref(x, w)\n        return lambda: torch.autograd.grad(\n            y, [x, w], grad_outputs=dy, retain_graph=True\n        )", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000593", "source": "def int_(cls, ctype_name: Optional[str] = None,\n             required_modules: Tuple[str, ...] = ()):\n        if ctype_name is None:\n            ctype_name = \"int\"\n        return PrimitiveTypeNode(ctype_name, typename=\"int\", required_modules=required_modules)", "target": "def int_(cls, ctype_name: str, export_name: Optional[str] = None,\n             doc: Optional[str] = None, required_modules: Tuple[str, ...] = ()):\n        return cls(ctype_name, PrimitiveTypeNode.int_(), export_name, doc, required_modules)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000594", "source": "def test_constraints_schema_validation() -> None:\n    val = SchemaValidator(cs.int_schema(gt='1'))\n    with pytest.raises(ValidationError):\n        val.validate_python('0')", "target": "def test_constraints_schema_validation() -> None:\n    val = SchemaValidator(core_schema.timedelta_schema(gt=3))\n    with pytest.raises(ValidationError):\n        val.validate_python(1)", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pytorch/pytorch", "data_split": "train", "example_id": "000595", "source": "def test_20968(self):\n        pixel = np.uint8([[[40, 50, 200]]])\n        _ = cv.cvtColor(pixel, cv.COLOR_RGB2BGR)", "target": "def print_measurements(prefix, nelem, measurements):\n        measurements = sorted(measurements)\n        local_print(f\"{prefix:8s}:\")\n        for p in [50, 75, 90, 95]:\n            v = np.percentile(measurements, p)\n            local_print(f\"  p{p:02d}:  {v:1.3f}s  {nelem / v:6d}/s\")\n        local_print(\"\\n\")", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "000596", "source": "def make_data(self, params):\n        (representation,) = params\n        if representation == \"dense\":\n            data = _synth_regression_dataset(n_samples=100000, n_features=200)\n        else:\n            data = _synth_regression_sparse_dataset(\n                n_samples=100000, n_features=1000, density=0.01\n            )\n        return data", "target": "def make_data(self, params):\n        representation, precompute = params\n        if representation == \"dense\":\n            data = _synth_regression_dataset(n_samples=1000000, n_features=100)\n        else:\n            data = _synth_regression_sparse_dataset(\n                n_samples=50000, n_features=5000, density=0.01\n            )\n        return data", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000597", "source": "def getCMakeArgs(self, arch, target):\n        args = Builder.getCMakeArgs(self, arch, target)\n        args = args + [\n            '-DVISIONOS_ARCH=%s' % arch\n        ]\n        return args", "target": "def getCMakeArgs(self):\n        args = TestRunner.getCMakeArgs(self)\n        args = args + [\n            \"-DIOS_ARCH=%s\" % self.arch,\n            \"-DIPHONEOS_DEPLOYMENT_TARGET=%s\" % os.environ['IPHONEOS_DEPLOYMENT_TARGET'],\n        ]\n        return args", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000598", "source": "def test_only_allow_alias(py_and_json) -> None:\n    v = py_and_json(\n        cs.arguments_v3_schema(\n            [\n                cs.arguments_v3_parameter(\n                    name='a', schema=cs.str_schema(), alias='FieldA', mode='positional_or_keyword'\n                ),\n            ],\n            validate_by_name=False,\n            validate_by_alias=True,\n        )\n    )\n    assert v.validate_test(ArgsKwargs((), {'FieldA': 'hello'})) == ((), {'a': 'hello'})\n    assert v.validate_test({'FieldA': 'hello'}) == (('hello',), {})\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Missing required argument \\[type=missing_argument,'):\n        assert v.validate_test(ArgsKwargs((), {'a': 'hello'}))\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Missing required argument \\[type=missing_argument,'):\n        assert v.validate_test({'a': 'hello'})", "target": "def test_only_allow_alias(py_and_json) -> None:\n    v = py_and_json(\n        {\n            'type': 'model-fields',\n            'fields': {'field_a': {'validation_alias': 'FieldA', 'type': 'model-field', 'schema': {'type': 'int'}}},\n        },\n        config=CoreConfig(validate_by_name=False, validate_by_alias=True),\n    )\n    assert v.validate_test({'FieldA': '123'}) == ({'field_a': 123}, None, {'field_a'})\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'field_a': '123'})", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|opencv/opencv", "data_split": "train", "example_id": "000599", "source": "def barplot_neighbors(\n    Nrange=2 ** np.arange(1, 11),\n    Drange=2 ** np.arange(7),\n    krange=2 ** np.arange(10),\n    N=1000,\n    D=64,\n    k=5,\n    leaf_size=30,\n    dataset=\"digits\",\n):\n    algorithms = (\"kd_tree\", \"brute\", \"ball_tree\")\n    fiducial_values = {\"N\": N, \"D\": D, \"k\": k}\n    N_results_build = {alg: np.zeros(len(Nrange)) for alg in algorithms}\n    N_results_query = {alg: np.zeros(len(Nrange)) for alg in algorithms}\n    for i, NN in enumerate(Nrange):\n        print(\"N = %i (%i out of %i)\" % (NN, i + 1, len(Nrange)))\n        X = get_data(NN, D, dataset)\n        for algorithm in algorithms:\n            nbrs = neighbors.NearestNeighbors(\n                n_neighbors=min(NN, k), algorithm=algorithm, leaf_size=leaf_size\n            )\n            t0 = time()\n            nbrs.fit(X)\n            t1 = time()\n            nbrs.kneighbors(X)\n            t2 = time()\n            N_results_build[algorithm][i] = t1 - t0\n            N_results_query[algorithm][i] = t2 - t1\n    D_results_build = {alg: np.zeros(len(Drange)) for alg in algorithms}\n    D_results_query = {alg: np.zeros(len(Drange)) for alg in algorithms}\n    for i, DD in enumerate(Drange):\n        print(\"D = %i (%i out of %i)\" % (DD, i + 1, len(Drange)))\n        X = get_data(N, DD, dataset)\n        for algorithm in algorithms:\n            nbrs = neighbors.NearestNeighbors(\n                n_neighbors=k, algorithm=algorithm, leaf_size=leaf_size\n            )\n            t0 = time()\n            nbrs.fit(X)\n            t1 = time()\n            nbrs.kneighbors(X)\n            t2 = time()\n            D_results_build[algorithm][i] = t1 - t0\n            D_results_query[algorithm][i] = t2 - t1\n    k_results_build = {alg: np.zeros(len(krange)) for alg in algorithms}\n    k_results_query = {alg: np.zeros(len(krange)) for alg in algorithms}\n    X = get_data(N, DD, dataset)\n    for i, kk in enumerate(krange):\n        print(\"k = %i (%i out of %i)\" % (kk, i + 1, len(krange)))\n        for algorithm in algorithms:\n            nbrs = neighbors.NearestNeighbors(\n                n_neighbors=kk, algorithm=algorithm, leaf_size=leaf_size\n            )\n            t0 = time()\n            nbrs.fit(X)\n            t1 = time()\n            nbrs.kneighbors(X)\n            t2 = time()\n            k_results_build[algorithm][i] = t1 - t0\n            k_results_query[algorithm][i] = t2 - t1\n    plt.figure(figsize=(8, 11))\n    for sbplt, vals, quantity, build_time, query_time in [\n        (311, Nrange, \"N\", N_results_build, N_results_query),\n        (312, Drange, \"D\", D_results_build, D_results_query),\n        (313, krange, \"k\", k_results_build, k_results_query),\n    ]:\n        ax = plt.subplot(sbplt, yscale=\"log\")\n        plt.grid(True)\n        tick_vals = []\n        tick_labels = []\n        bottom = 10 ** np.min(\n            [min(np.floor(np.log10(build_time[alg]))) for alg in algorithms]\n        )\n        for i, alg in enumerate(algorithms):\n            xvals = 0.1 + i * (1 + len(vals)) + np.arange(len(vals))\n            width = 0.8\n            c_bar = plt.bar(xvals, build_time[alg] - bottom, width, bottom, color=\"r\")\n            q_bar = plt.bar(xvals, query_time[alg], width, build_time[alg], color=\"b\")\n            tick_vals += list(xvals + 0.5 * width)\n            tick_labels += [\"%i\" % val for val in vals]\n            plt.text(\n                (i + 0.02) / len(algorithms),\n                0.98,\n                alg,\n                transform=ax.transAxes,\n                ha=\"left\",\n                va=\"top\",\n                bbox=dict(facecolor=\"w\", edgecolor=\"w\", alpha=0.5),\n            )\n            plt.ylabel(\"Time (s)\")\n        ax.xaxis.set_major_locator(ticker.FixedLocator(tick_vals))\n        ax.xaxis.set_major_formatter(ticker.FixedFormatter(tick_labels))\n        for label in ax.get_xticklabels():\n            label.set_rotation(-90)\n            label.set_fontsize(10)\n        title_string = \"Varying %s\" % quantity\n        descr_string = \"\"\n        for s in \"NDk\":\n            if s == quantity:\n                pass\n            else:\n                descr_string += \"%s = %i, \" % (s, fiducial_values[s])\n        descr_string = descr_string[:-2]\n        plt.text(\n            1.01,\n            0.5,\n            title_string,\n            transform=ax.transAxes,\n            rotation=-90,\n            ha=\"left\",\n            va=\"center\",\n            fontsize=20,\n        )\n        plt.text(\n            0.99,\n            0.5,\n            descr_string,\n            transform=ax.transAxes,\n            rotation=-90,\n            ha=\"right\",\n            va=\"center\",\n        )\n        plt.gcf().suptitle(\"%s data set\" % dataset.capitalize(), fontsize=16)\n    plt.figlegend((c_bar, q_bar), (\"construction\", \"N-point query\"), \"upper right\")", "target": "def test_mat_wrap_channels_fail(self):\n            data = np.random.random([2, 3, 4, 520])\n            mat_data0 = cv.Mat(data)\n            assert isinstance(mat_data0, cv.Mat)\n            assert isinstance(mat_data0, np.ndarray)\n            self.assertEqual(mat_data0.wrap_channels, False)\n            res0 = cv.utils.dumpInputArray(mat_data0)\n            self.assertEqual(res0, \"InputArray: empty()=false kind=0x00010000 flags=0x01010000 total(-1)=12480 dims(-1)=4 size(-1)=[2 3 4 520] type(-1)=CV_64FC1\")\n            with self.assertRaises(cv.error):\n                mat_data1 = cv.Mat(data, wrap_channels=True)\n                res1 = cv.utils.dumpInputArray(mat_data1)\n                print(mat_data1.__dict__)\n                print(res1)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000600", "source": "def benchmark(self):\n        for M, N in self.get_shapes():\n            print(f\"Tensor dimensions: [{M}, {N}]\")\n            torch_dtype = cutlass_torch.dtype(cutlass.BFloat16)\n            x = 0.1 * torch.randn(M, N, device=\"cuda\", dtype=torch_dtype)\n            self.benchmark_single_shape((x,), setting=f\"shape: [{M}, {N}]\")", "target": "def benchmark(self):\n        for M, N in self.get_shapes():\n            print(f\"Tensor dimensions: [{M}, {N}]\")\n            torch_dtype = cutlass_torch.dtype(cutlass.BFloat16)\n            x = 0.1 * torch.randn(\n                M, N, device=\"cuda\", dtype=torch_dtype, requires_grad=True\n            )\n            dy = torch.randn(M, N, device=\"cuda\", dtype=torch_dtype)\n            self.benchmark_single_shape((x, dy), setting=f\"shape: [{M}, {N}]\")", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "train", "example_id": "000601", "source": "def run_rnn(\n    name,\n    rnn_creator,\n    nloops=5,\n    seqLength=100,\n    numLayers=1,\n    inputSize=512,\n    hiddenSize=512,\n    miniBatch=64,\n    device=\"cuda\",\n    seed=None,\n):\n    def run_iter(modeldef):\n        forward_output = modeldef.forward(*modeldef.inputs)\n        if modeldef.backward_setup is not None:\n            backward_input = modeldef.backward_setup(forward_output)\n        else:\n            backward_input = forward_output\n        if modeldef.backward is not None:\n            modeldef.backward(*backward_input)\n        if modeldef.backward is not None:\n            with torch.no_grad():\n                for param in modeldef.params:\n                    param.grad.zero_()\n        torch.cuda.synchronize()\n    assert device == \"cuda\"\n    creator_args = dict(\n        seqLength=seqLength,\n        numLayers=numLayers,\n        inputSize=inputSize,\n        hiddenSize=hiddenSize,\n        miniBatch=miniBatch,\n        device=device,\n        seed=seed,\n    )\n    modeldef = rnn_creator(**creator_args)\n    [run_iter(modeldef) for _ in range(nloops)]", "target": "def filter_requires_grad(tensors):\n    return [t for t in tensors if t.requires_grad]", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000602", "source": "def area(self) -> int:\n            return self.width * self.height", "target": "def area(self, area: float) -> None:\n            self.side = area**0.5", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000603", "source": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "target": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000604", "source": "def test_smart_union_model_field():\n    class ModelA:\n        x: int\n    class ModelB:\n        x: str\n    schema = core_schema.union_schema(\n        [\n            core_schema.model_schema(\n                ModelA, core_schema.model_fields_schema({'x': core_schema.model_field(core_schema.int_schema())})\n            ),\n            core_schema.model_schema(\n                ModelB, core_schema.model_fields_schema({'x': core_schema.model_field(core_schema.str_schema())})\n            ),\n        ]\n    )\n    validator = SchemaValidator(schema)\n    result = validator.validate_python({'x': 1})\n    assert isinstance(result, ModelA)\n    assert result.x == 1\n    result = validator.validate_python({'x': '1'})\n    assert isinstance(result, ModelB)\n    assert result.x == '1'", "target": "def test_dataclass_field_after_validator():\n    @dataclasses.dataclass\n    class Foo:\n        a: int\n        b: str\n        @classmethod\n        def validate_b(cls, v: str, info: core_schema.ValidationInfo) -> str:\n            assert v == 'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return 'hello world!'\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(\n                    name='b',\n                    schema=core_schema.with_info_after_validator_function(Foo.validate_b, core_schema.str_schema()),\n                ),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 1, 'b': b'hello'})\n    assert dataclasses.asdict(foo) == {'a': 1, 'b': 'hello world!'}", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "000605", "source": "def add_target(self, image, rect, data=None):\n        x0, y0, x1, y1 = rect\n        raw_points, raw_descrs = self.detect_features(image)\n        points, descs = [], []\n        for kp, desc in zip(raw_points, raw_descrs):\n            x, y = kp.pt\n            if x0 <= x <= x1 and y0 <= y <= y1:\n                points.append(kp)\n                descs.append(desc)\n        descs = np.uint8(descs)\n        self.matcher.add([descs])\n        target = PlanarTarget(image = image, rect=rect, keypoints = points, descrs=descs, data=data)\n        self.targets.append(target)", "target": "def schema_using_defs() -> cs.CoreSchema:\n    definitions: list[cs.CoreSchema] = [\n        {'type': 'int', 'ref': 'int'},\n        {\n            'type': 'model',\n            'cls': MyModel,\n            'schema': {\n                'type': 'model-fields',\n                'fields': {\n                    str(c): {'type': 'model-field', 'schema': {'type': 'definition-ref', 'schema_ref': 'int'}}\n                    for c in range(N)\n                },\n            },\n            'ref': f'model_{N}',\n        },\n    ]\n    level = N\n    for level in reversed(range(N)):\n        definitions.append(\n            {\n                'type': 'model',\n                'cls': MyModel,\n                'schema': {\n                    'type': 'model-fields',\n                    'fields': {\n                        str(c): {\n                            'type': 'model-field',\n                            'schema': {'type': 'definition-ref', 'schema_ref': f'model_{level + 1}'},\n                        }\n                        for c in range(N)\n                    },\n                },\n                'ref': f'model_{level}',\n            }\n        )\n    return {\n        'type': 'definitions',\n        'definitions': definitions,\n        'schema': {'type': 'definition-ref', 'schema_ref': 'model_0'},\n    }", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "000606", "source": "def hessian(model, inp, v=None, strict=None):\n        argnums = tuple(range(len(inp)))\n        return ft.hessian(model, argnums=argnums)(*inp)", "target": "def test_model_deep():\n    v = SchemaValidator(\n        core_schema.typed_dict_schema(\n            fields={\n                'field_a': core_schema.typed_dict_field(schema=core_schema.str_schema()),\n                'field_b': core_schema.typed_dict_field(\n                    schema=core_schema.typed_dict_schema(\n                        fields={\n                            'field_c': core_schema.typed_dict_field(schema=core_schema.str_schema()),\n                            'field_d': core_schema.typed_dict_field(\n                                schema=core_schema.typed_dict_schema(\n                                    fields={\n                                        'field_e': core_schema.typed_dict_field(schema=core_schema.str_schema()),\n                                        'field_f': core_schema.typed_dict_field(schema=core_schema.int_schema()),\n                                    }\n                                )\n                            ),\n                        }\n                    )\n                ),\n            }\n        )\n    )\n    output = v.validate_python({'field_a': '1', 'field_b': {'field_c': '2', 'field_d': {'field_e': '4', 'field_f': 4}}})\n    assert output == {'field_a': '1', 'field_b': ({'field_c': '2', 'field_d': {'field_e': '4', 'field_f': 4}})}\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python({'field_a': '1', 'field_b': {'field_c': '2', 'field_d': {'field_e': '4', 'field_f': 'xx'}}})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('field_b', 'field_d', 'field_f'),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'xx',\n        }\n    ]", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pytorch/pytorch", "data_split": "train", "example_id": "000607", "source": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "target": "def test_sdpa(device=\"cpu\", dtype=torch.float16) -> None:\n    print(f\"Testing SDPA on {device} using type {dtype}\")\n    k, q, v = torch.rand(3, 1, 16, 77, 64, dtype=dtype, device=device).unbind(0)\n    attn = torch.rand(1, 1, 77, 77, dtype=dtype, device=device)\n    rc = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn)\n    assert rc.isnan().any().item() is False", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000608", "source": "async def wrapper_function(*args, **kwargs):\n            return await wrapper(*args, **kwargs)", "target": "def wrapper_function(*args, **kwargs):\n            return wrapper(*args, **kwargs)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000609", "source": "def s1(v: int) -> int:\n        return v + 1", "target": "def test_positional_tuple():\n    s = SchemaSerializer({'type': 'tuple', 'items_schema': [{'type': 'int'}, {'type': 'bytes'}, {'type': 'float'}]})\n    assert s.to_python((1, b'2', 3.0)) == (1, b'2', 3.0)\n    with pytest.warns(UserWarning, match='Unexpected extra items present in tuple'):\n        assert s.to_python((1, b'2', 3.0, 123)) == (1, b'2', 3.0, 123)\n    assert s.to_python((1, b'2')) == (1, b'2')\n    assert s.to_python((1, b'2', 3.0), mode='json') == [1, '2', 3.0]\n    with pytest.warns(UserWarning, match='Unexpected extra items present in tuple'):\n        assert s.to_python((1, b'2', 3.0, 123), mode='json') == [1, '2', 3.0, 123]\n    assert s.to_python((1, b'2'), mode='json') == [1, '2']\n    assert s.to_json((1, b'2', 3.0)) == b'[1,\"2\",3.0]'\n    with pytest.warns(UserWarning, match='Unexpected extra items present in tuple'):\n        assert s.to_json((1, b'2', 3.0, 123)) == b'[1,\"2\",3.0,123]'\n    assert s.to_json((1, b'2')) == b'[1,\"2\"]'", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "000610", "source": "def one_run(n_threads, n_samples):\n    X_train = X_train_[:n_samples]\n    X_test = X_test_[:n_samples]\n    y_train = y_train_[:n_samples]\n    y_test = y_test_[:n_samples]\n    if sample_weight is not None:\n        sample_weight_train = sample_weight_train_[:n_samples]\n    else:\n        sample_weight_train = None\n    assert X_train.shape[0] == n_samples\n    assert X_test.shape[0] == n_samples\n    print(\"Fitting a sklearn model...\")\n    tic = time()\n    est = sklearn.base.clone(sklearn_est)\n    with threadpool_limits(n_threads, user_api=\"openmp\"):\n        est.fit(X_train, y_train, sample_weight=sample_weight_train)\n        sklearn_fit_duration = time() - tic\n        tic = time()\n        sklearn_score = est.score(X_test, y_test)\n        sklearn_score_duration = time() - tic\n    print(\"score: {:.4f}\".format(sklearn_score))\n    print(\"fit duration: {:.3f}s,\".format(sklearn_fit_duration))\n    print(\"score duration: {:.3f}s,\".format(sklearn_score_duration))\n    lightgbm_score = None\n    lightgbm_fit_duration = None\n    lightgbm_score_duration = None\n    if args.lightgbm:\n        print(\"Fitting a LightGBM model...\")\n        lightgbm_est = get_equivalent_estimator(\n            est, lib=\"lightgbm\", n_classes=args.n_classes\n        )\n        lightgbm_est.set_params(num_threads=n_threads)\n        tic = time()\n        lightgbm_est.fit(X_train, y_train, sample_weight=sample_weight_train)\n        lightgbm_fit_duration = time() - tic\n        tic = time()\n        lightgbm_score = lightgbm_est.score(X_test, y_test)\n        lightgbm_score_duration = time() - tic\n        print(\"score: {:.4f}\".format(lightgbm_score))\n        print(\"fit duration: {:.3f}s,\".format(lightgbm_fit_duration))\n        print(\"score duration: {:.3f}s,\".format(lightgbm_score_duration))\n    xgb_score = None\n    xgb_fit_duration = None\n    xgb_score_duration = None\n    if args.xgboost:\n        print(\"Fitting an XGBoost model...\")\n        xgb_est = get_equivalent_estimator(est, lib=\"xgboost\", n_classes=args.n_classes)\n        xgb_est.set_params(nthread=n_threads)\n        tic = time()\n        xgb_est.fit(X_train, y_train, sample_weight=sample_weight_train)\n        xgb_fit_duration = time() - tic\n        tic = time()\n        xgb_score = xgb_est.score(X_test, y_test)\n        xgb_score_duration = time() - tic\n        print(\"score: {:.4f}\".format(xgb_score))\n        print(\"fit duration: {:.3f}s,\".format(xgb_fit_duration))\n        print(\"score duration: {:.3f}s,\".format(xgb_score_duration))\n    cat_score = None\n    cat_fit_duration = None\n    cat_score_duration = None\n    if args.catboost:\n        print(\"Fitting a CatBoost model...\")\n        cat_est = get_equivalent_estimator(\n            est, lib=\"catboost\", n_classes=args.n_classes\n        )\n        cat_est.set_params(thread_count=n_threads)\n        tic = time()\n        cat_est.fit(X_train, y_train, sample_weight=sample_weight_train)\n        cat_fit_duration = time() - tic\n        tic = time()\n        cat_score = cat_est.score(X_test, y_test)\n        cat_score_duration = time() - tic\n        print(\"score: {:.4f}\".format(cat_score))\n        print(\"fit duration: {:.3f}s,\".format(cat_fit_duration))\n        print(\"score duration: {:.3f}s,\".format(cat_score_duration))\n    return (\n        sklearn_score,\n        sklearn_fit_duration,\n        sklearn_score_duration,\n        lightgbm_score,\n        lightgbm_fit_duration,\n        lightgbm_score_duration,\n        xgb_score,\n        xgb_fit_duration,\n        xgb_score_duration,\n        cat_score,\n        cat_fit_duration,\n        cat_score_duration,\n    )", "target": "def one_run(n_samples):\n    X_train = X_train_[:n_samples]\n    X_test = X_test_[:n_samples]\n    y_train = y_train_[:n_samples]\n    y_test = y_test_[:n_samples]\n    if sample_weight is not None:\n        sample_weight_train = sample_weight_train_[:n_samples]\n    else:\n        sample_weight_train = None\n    assert X_train.shape[0] == n_samples\n    assert X_test.shape[0] == n_samples\n    print(\"Data size: %d samples train, %d samples test.\" % (n_samples, n_samples))\n    print(\"Fitting a sklearn model...\")\n    tic = time()\n    est = Estimator(\n        learning_rate=lr,\n        max_iter=n_trees,\n        max_bins=max_bins,\n        max_leaf_nodes=n_leaf_nodes,\n        early_stopping=False,\n        random_state=0,\n        verbose=0,\n    )\n    loss = args.loss\n    if args.problem == \"classification\":\n        if loss == \"default\":\n            loss = \"log_loss\"\n    else:\n        if loss == \"default\":\n            loss = \"squared_error\"\n    est.set_params(loss=loss)\n    est.fit(X_train, y_train, sample_weight=sample_weight_train)\n    sklearn_fit_duration = time() - tic\n    tic = time()\n    sklearn_score = est.score(X_test, y_test)\n    sklearn_score_duration = time() - tic\n    print(\"score: {:.4f}\".format(sklearn_score))\n    print(\"fit duration: {:.3f}s,\".format(sklearn_fit_duration))\n    print(\"score duration: {:.3f}s,\".format(sklearn_score_duration))\n    lightgbm_score = None\n    lightgbm_fit_duration = None\n    lightgbm_score_duration = None\n    if args.lightgbm:\n        print(\"Fitting a LightGBM model...\")\n        lightgbm_est = get_equivalent_estimator(\n            est, lib=\"lightgbm\", n_classes=args.n_classes\n        )\n        tic = time()\n        lightgbm_est.fit(X_train, y_train, sample_weight=sample_weight_train)\n        lightgbm_fit_duration = time() - tic\n        tic = time()\n        lightgbm_score = lightgbm_est.score(X_test, y_test)\n        lightgbm_score_duration = time() - tic\n        print(\"score: {:.4f}\".format(lightgbm_score))\n        print(\"fit duration: {:.3f}s,\".format(lightgbm_fit_duration))\n        print(\"score duration: {:.3f}s,\".format(lightgbm_score_duration))\n    xgb_score = None\n    xgb_fit_duration = None\n    xgb_score_duration = None\n    if args.xgboost:\n        print(\"Fitting an XGBoost model...\")\n        xgb_est = get_equivalent_estimator(est, lib=\"xgboost\", n_classes=args.n_classes)\n        tic = time()\n        xgb_est.fit(X_train, y_train, sample_weight=sample_weight_train)\n        xgb_fit_duration = time() - tic\n        tic = time()\n        xgb_score = xgb_est.score(X_test, y_test)\n        xgb_score_duration = time() - tic\n        print(\"score: {:.4f}\".format(xgb_score))\n        print(\"fit duration: {:.3f}s,\".format(xgb_fit_duration))\n        print(\"score duration: {:.3f}s,\".format(xgb_score_duration))\n    cat_score = None\n    cat_fit_duration = None\n    cat_score_duration = None\n    if args.catboost:\n        print(\"Fitting a CatBoost model...\")\n        cat_est = get_equivalent_estimator(\n            est, lib=\"catboost\", n_classes=args.n_classes\n        )\n        tic = time()\n        cat_est.fit(X_train, y_train, sample_weight=sample_weight_train)\n        cat_fit_duration = time() - tic\n        tic = time()\n        cat_score = cat_est.score(X_test, y_test)\n        cat_score_duration = time() - tic\n        print(\"score: {:.4f}\".format(cat_score))\n        print(\"fit duration: {:.3f}s,\".format(cat_fit_duration))\n        print(\"score duration: {:.3f}s,\".format(cat_score_duration))\n    return (\n        sklearn_score,\n        sklearn_fit_duration,\n        sklearn_score_duration,\n        lightgbm_score,\n        lightgbm_fit_duration,\n        lightgbm_score_duration,\n        xgb_score,\n        xgb_fit_duration,\n        xgb_score_duration,\n        cat_score,\n        cat_fit_duration,\n        cat_score_duration,\n    )", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "000611", "source": "def fit(self, X, y=None, **params):\n        self.fit_transform(X, **params)\n        return self", "target": "def fit(self, X, y):\n        self.classes_ = sorted(set(y))\n        self._is_fitted = True\n        return self", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000612", "source": "def required_usage_imports(self) -> Generator[str, None, None]:\n        for item in self:\n            yield from item.required_usage_imports", "target": "def required_usage_imports(self) -> Generator[str, None, None]:\n        yield \"import typing as _typing\"\n        yield from super().required_usage_imports", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000613", "source": "def test_non_finite_json_values(py_and_json: PyAndJson, input_value, allow_inf_nan, expected):\n    v = py_and_json({'type': 'float', 'allow_inf_nan': allow_inf_nan})\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            v.validate_test(input_value)\n    else:\n        assert v.validate_test(input_value) == expected", "target": "def test_invalid_custom_error():\n    s = core_schema.union_schema([{'type': 'int'}, {'type': 'str'}], custom_error_type='foobar')\n    with pytest.raises(SchemaError, match=r\"KeyError: 'custom_error_message'\"):\n        SchemaValidator(s)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000614", "source": "def test_constraints_schema_validation_error(constraint: str) -> None:\n    with pytest.raises(SchemaError, match=f\"'{constraint}' must be coercible to a datetime instance\"):\n        SchemaValidator(cs.datetime_schema(**{constraint: 'bad_value'}))", "target": "def test_constraints_schema_validation_error(constraint: str) -> None:\n    with pytest.raises(SchemaError, match=f\"'{constraint}' must be coercible to an integer\"):\n        SchemaValidator(cs.int_schema(**{constraint: 'bad_value'}))", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000615", "source": "def test_missing_error(pydantic_version):\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'field_a': core_schema.model_field(schema=core_schema.str_schema()),\n                'field_b': core_schema.model_field(schema=core_schema.int_schema()),\n            }\n        )\n    )\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python({'field_a': b'abc'})\n    assert (\n        str(exc_info.value)\n        ==\n        + (\n            f'\\n    For further information visit https://errors.pydantic.dev/{pydantic_version}/v/missing'\n            if os.environ.get('PYDANTIC_ERRORS_INCLUDE_URL', '1') != 'false'\n            else ''\n        )\n    )", "target": "def test_missing_error(pydantic_version):\n    v = SchemaValidator(\n        core_schema.typed_dict_schema(\n            fields={\n                'field_a': core_schema.typed_dict_field(schema=core_schema.str_schema()),\n                'field_b': core_schema.typed_dict_field(schema=core_schema.int_schema()),\n            }\n        )\n    )\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python({'field_a': b'abc'})\n    assert str(exc_info.value) == (\n        '1 validation error for typed-dict\\n'\n        'field_b\\n'\n        \"  Field required [type=missing, input_value={'field_a': b'abc'}, input_type=dict]\"\n        + (\n            f'\\n    For further information visit https://errors.pydantic.dev/{pydantic_version}/v/missing'\n            if os.environ.get('PYDANTIC_ERRORS_INCLUDE_URL', '1') != 'false'\n            else ''\n        )\n    )", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000616", "source": "def f(value, serializer):\n        return f'result={serializer(len(value))} repr={serializer!r}'", "target": "def f(value, serializer, _info):\n        return f'result={serializer(value)}'", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "000617", "source": "def add(x, y):\n        return x + y", "target": "def test_complete_core_error(benchmark):\n    v = SchemaValidator(schema())\n    data = input_data_wrong()\n    @benchmark\n    def f():\n        try:\n            v.validate_python(data)\n        except ValueError:\n            pass\n        else:\n            raise RuntimeError('expected ValueError')", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "000618", "source": "def upload_pytest_cache(\n    pr_identifier: PRIdentifier,\n    repo: GithubRepo,\n    job_identifier: str,\n    sha: str,\n    test_config: str,\n    shard: str,\n    cache_dir: Path,\n    temp_dir: Path,\n    bucket: str = BUCKET,\n) -> None:\n    if not isinstance(pr_identifier, PRIdentifier):\n        raise ValueError(\n            f\"pr_identifier must be of type PRIdentifier, not {type(pr_identifier)}\"\n        )\n    if not bucket:\n        bucket = BUCKET\n    obj_key_prefix = _get_s3_key_prefix(\n        pr_identifier, repo, job_identifier, sha, test_config, shard\n    )\n    zip_file_path = zip_folder(cache_dir, temp_dir / ZIP_UPLOAD / obj_key_prefix)\n    obj_key = f\"{obj_key_prefix}{os.path.splitext(zip_file_path)[1]}\"\n    upload_file_to_s3(zip_file_path, bucket, obj_key)", "target": "def test_serialize_pattern():\n    ser = core_schema.plain_serializer_function_ser_schema(\n        attrgetter('pattern'), when_used='json', return_schema=core_schema.str_schema()\n    )\n    s = SchemaSerializer(core_schema.any_schema(serialization=ser))\n    pattern = re.compile('^regex$')\n    assert s.to_python(pattern) == pattern\n    assert s.to_python(pattern, mode='json') == '^regex$'\n    assert s.to_json(pattern) == b'\"^regex$\"'", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000619", "source": "def for_element(self, e: int | str) -> AbstractSetIntStr | MappingIntStrAny | None:\n        item = self._items.get(e)\n        return item if not self.is_true(item) else None", "target": "def test_to_json():\n    assert to_json([1, 2]) == b'[1,2]'\n    assert to_json([1, 2], indent=2) == b'[\\n  1,\\n  2\\n]'\n    assert to_json([1, b'x']) == b'[1,\"x\"]'\n    assert to_json(['', '']).decode('utf-8') == '[\"\",\"\"]'\n    assert to_json(['', ''], indent=2).decode('utf-8') == '[\\n  \"\",\\n  \"\"\\n]'\n    assert to_json(['', ''], indent=2, ensure_ascii=True).decode('utf-8') == '[\\n  \"\\\\u00e0\",\\n  \"\\\\u00e9\"\\n]'\n    with pytest.raises(TypeError, match=r'to_json\\(\\) takes 1 positional arguments but 2 were given'):\n        to_json([1, 2], 2)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000620", "source": "def forward(self, inputs):\n        output = self.pool(F.relu(self.conv(inputs)))\n        output = output.view(1)\n        return output", "target": "def forward(self, inputs):\n        output = self.pool(F.relu(self.conv(inputs)))\n        output = output.view(1)\n        return output", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "000621", "source": "def test_load(self):\n        k_nearest = cv.ml.KNearest_load(self.find_file(\"ml/opencv_ml_knn.xml\"))\n        self.assertFalse(k_nearest.empty())\n        self.assertTrue(k_nearest.isTrained())", "target": "def _update_fields_from_docstrings(cls: type[Any], fields: dict[str, FieldInfo], use_inspect: bool = False) -> None:\n    fields_docs = extract_docstrings_from_cls(cls, use_inspect=use_inspect)\n    for ann_name, field_info in fields.items():\n        if field_info.description is None and ann_name in fields_docs:\n            field_info.description = fields_docs[ann_name]", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000622", "source": "def name(self) -> str:\n        prefix = f\"{self.category()}\"\n        return prefix", "target": "def name(self) -> str:\n        return \"aten\"", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000623", "source": "def constrain(self: _Pipeline[_InT, _NewOutLe], constraint: annotated_types.Le) -> _Pipeline[_InT, _NewOutLe]: ...", "target": "def constrain(\n        self: _Pipeline[_InT, _NewOutDatetime], constraint: annotated_types.Timezone\n    ) -> _Pipeline[_InT, _NewOutDatetime]: ...", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|pytorch/pytorch", "data_split": "train", "example_id": "000624", "source": "def bench(clfs):\n    for (\n        name,\n        clf,\n        iter_range,\n        train_losses,\n        train_scores,\n        test_scores,\n        durations,\n    ) in clfs:\n        print(\"training %s\" % name)\n        clf_type = type(clf)\n        clf_params = clf.get_params()\n        for n_iter in iter_range:\n            gc.collect()\n            train_loss, train_score, test_score, duration = bench_one(\n                name, clf_type, clf_params, n_iter\n            )\n            train_losses.append(train_loss)\n            train_scores.append(train_score)\n            test_scores.append(test_score)\n            durations.append(duration)\n            print(\"classifier: %s\" % name)\n            print(\"train_loss: %.8f\" % train_loss)\n            print(\"train_score: %.8f\" % train_score)\n            print(\"test_score: %.8f\" % test_score)\n            print(\"time for fit: %.8f seconds\" % duration)\n            print(\"\")\n        print(\"\")\n    return clfs", "target": "def gh_post_pr_comment(\n    org: str, repo: str, pr_num: int, comment: str, dry_run: bool = False\n) -> list[dict[str, Any]]:\n    return _gh_post_comment(\n        f\"{GITHUB_API_URL}/repos/{org}/{repo}/issues/{pr_num}/comments\",\n        comment,\n        dry_run,\n    )", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000625", "source": "def ser_x(data: Model, v: Any, serializer: core_schema.SerializerFunctionWrapHandler, _) -> str:\n        x = serializer(v)\n        assert data['x'] == 1_000\n        return f'{x:_}'", "target": "def ser_x(data: Model, v: Any, serializer: core_schema.SerializerFunctionWrapHandler) -> str:\n        assert data['x'] == 1_000\n        x = serializer(v)\n        return f'{x:_}'", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|opencv/opencv", "data_split": "train", "example_id": "000626", "source": "def parse_file(self, fname, prefix = \"\"):\n        istest = fname.endswith(\"Test.java\")\n        clsname = os.path.basename(fname).replace(\"Test\", \"\").replace(\".java\", \"\")\n        clsname = prefix + clsname[0].upper() + clsname[1:]\n        for cls in classes_ignore_list:\n            if re.match(cls, clsname):\n                return\n        f = open(fname, \"rt\")\n        linenum = 0\n        for line in f:\n            linenum += 1\n            m1 = self.r1.match(line)\n            m2 = self.r2.match(line)\n            m3 = self.r3.match(line)\n            func = ''\n            args_str = ''\n            if m1:\n                func = m1.group(1)\n                args_str = m1.group(2)\n            elif m2:\n                if \"public\" not in line:\n                    continue\n                func = m2.group(1)\n                args_str = m2.group(2)\n            elif m3:\n                self.empty_stubs_cnt += 1\n                continue\n            else:\n                continue\n            d = (self.mdict, self.tdict)[istest]\n            w = (self.mwhere, self.twhere)[istest]\n            func = re.sub(r\"^test\", \"\", func)\n            func = clsname + \"--\" + func[0].upper() + func[1:]\n            args_str = args_str.replace(\"[]\", \"Array\").replace(\"...\", \"Array \")\n            args_str = re.sub(r\"List<(\\w+)>\", \"ListOf\\g<1>\", args_str)\n            args_str = re.sub(r\"List<(\\w+)>\", \"ListOf\\g<1>\", args_str)\n            args = [a.split()[0] for a in args_str.split(\",\") if a]\n            func_ex = func + \"\".join([a[0].upper() + a[1:] for a in args])\n            func_loc = fname + \" (line: \" + str(linenum)  + \")\"\n            skip = False\n            for fi in funcs_ignore_list:\n                if re.match(fi, func_ex):\n                    skip = True\n                    break\n            if skip:\n                continue\n            if func in d:\n                d[func].append(func_ex)\n            else:\n                d[func] = [func_ex]\n            w[func_ex] = func_loc\n            w[func] = func_loc\n        f.close()\n        return", "target": "def test_python_986(self):\n        cntls = []\n        img = np.zeros((100,100,3), dtype=np.uint8)\n        color = (0,0,0)\n        cnts = np.array(cntls, dtype=np.int32).reshape((1, -1, 2))\n        try:\n            cv.fillPoly(img, cnts, color)\n            assert False\n        except:\n            assert True", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000627", "source": "def outMeta(arr_desc0, arr_desc1):\n        return cv.empty_array_desc(), cv.empty_array_desc()", "target": "def outMeta(desc):\n            out_desc = desc.withType(desc.depth, 1)\n            return out_desc, out_desc, out_desc", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000628", "source": "def test_smart_union_validator_function_one_arm():\n    schema = core_schema.union_schema(\n        [\n            core_schema.float_schema(),\n            core_schema.no_info_after_validator_function(lambda v: v * 2, core_schema.int_schema()),\n        ]\n    )\n    validator = SchemaValidator(schema)\n    assert repr(validator.validate_python(1)) == '2'\n    assert repr(validator.validate_python(1.0)) == '1.0'\n    schema = core_schema.union_schema(\n        [\n            core_schema.float_schema(),\n            core_schema.no_info_wrap_validator_function(lambda v, handler: handler(v) * 2, core_schema.int_schema()),\n        ]\n    )\n    validator = SchemaValidator(schema)\n    assert repr(validator.validate_python(1)) == '2'\n    assert repr(validator.validate_python(1.0)) == '1.0'", "target": "def test_frozenset_ints_both(py_and_json: PyAndJson, input_value, expected):\n    v = py_and_json({'type': 'frozenset', 'items_schema': {'type': 'int'}})\n    output = v.validate_test(input_value)\n    assert output == expected\n    assert isinstance(output, frozenset)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000629", "source": "def test_frozen():\n    @dataclasses.dataclass\n    class MyModel:\n        f: str\n    v = SchemaValidator(\n        core_schema.dataclass_schema(\n            MyModel,\n            core_schema.dataclass_args_schema('MyModel', [core_schema.dataclass_field('f', core_schema.str_schema())]),\n            ['f'],\n            frozen=True,\n        )\n    )\n    m = v.validate_python({'f': 'x'})\n    assert m.f == 'x'\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_assignment(m, 'f', 'y')\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'frozen_instance', 'loc': (), 'msg': 'Instance is frozen', 'input': 'y'}\n    ]", "target": "def y_formatted(self) -> str:\n            return f'{self.y:_}'", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "000630", "source": "def test_dataclass_serialization_python(benchmark):\n    s = SchemaSerializer(dataclass_schema)\n    dc = Foo(a='hello', b=b'more', c=123, d=1.23)\n    assert s.to_python(dc) == {'a': 'hello', 'b': b'more', 'c': 123, 'd': 1.23}\n    benchmark(s.to_python, dc)", "target": "def test_umat_merge_mertens(self):\n        if self.extraTestDataPath is None:\n            self.fail('Test data is not available')\n        test_data_path = os.path.join(self.extraTestDataPath, 'cv', 'hdr')\n        images, _ = load_exposure_seq(os.path.join(test_data_path, 'exposures'))\n        num_threads = cv.getNumThreads()\n        cv.setNumThreads(1)\n        merge = cv.createMergeMertens()\n        mat_result = merge.process(images)\n        umat_images = [cv.UMat(img) for img in images]\n        umat_result = merge.process(umat_images)\n        cv.setNumThreads(num_threads)\n        self.assertTrue(np.allclose(umat_result.get(), mat_result))", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "000631", "source": "def test_extra_arguments(py_and_json: PyAndJson):\n    v = py_and_json({'type': 'tuple', 'items_schema': [{'type': 'int'}, {'type': 'int'}]})\n    assert v.validate_test([1, 2]) == (1, 2)\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_test([1, 2, 3, 4])\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'too_long',\n            'loc': (),\n            'msg': 'Tuple should have at most 2 items after validation, not 4',\n            'input': [1, 2, 3, 4],\n            'ctx': {'field_type': 'Tuple', 'max_length': 2, 'actual_length': 4},\n        }\n    ]", "target": "def symbolic_convert_overhead_stress_test(x, y, n):\n    while n > 0:\n        n -= 1\n        x, y = y, x\n    return x + y", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000632", "source": "def _prepare_once(self) -> None:\n        self.mesh = torch.distributed.device_mesh.init_device_mesh(\n            \"cuda\", (self.world_size,), mesh_dim_names=(\"dp\",)\n        )\n        self.a = DTensor.from_local(\n            torch.ones(10, 10, device=self.device()), self.mesh, [Replicate()]\n        )\n        self.b = DTensor.from_local(\n            torch.ones(10, 10, device=self.device()), self.mesh, [Replicate()]\n        )", "target": "def _prepare_once(self):\n        self.m = self.ModuleClass()\n        torch.set_float32_matmul_precision(\"high\")\n        self.input = torch.ones(10, device=self.device())", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "train", "example_id": "000633", "source": "def recurrent(x, scale, shift):\n    y = x\n    for i in range(100):\n        y = fn(y, scale, shift)\n    return y", "target": "def rev_parse(self, name: str) -> str:\n        return self._run_git(\"rev-parse\", \"--verify\", name).strip()", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000634", "source": "def resolve(self, root: ASTNode):\n        try:\n            self.value.resolve(root)\n        except TypeResolutionError as e:\n            raise TypeResolutionError(\n                'Failed to resolve alias \"{}\" exposed as \"{}\"'.format(\n                    self.ctype_name, self.typename\n                )\n            ) from e", "target": "def resolve(self, root: ASTNode) -> None:\n        errors = []\n        for item in filter(lambda item: not item.is_resolved, self):\n            try:\n                item.resolve(root)\n            except TypeResolutionError as e:\n                errors.append(str(e))\n        if len(errors) > 0:\n            raise TypeResolutionError(\n                'Failed to resolve one of \"{}\" items. Errors: {}'.format(\n                    self.full_typename, errors\n                )\n            )", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pytorch/pytorch", "data_split": "train", "example_id": "000635", "source": "def test_age_gender_infer_image(self):\n            skip_if_openvino_not_available()\n            root_path  = '/omz_intel_models/intel/age-gender-recognition-retail-0013/FP32/age-gender-recognition-retail-0013'\n            model_path = self.find_file(root_path + '.xml',   [os.environ.get('OPENCV_DNN_TEST_DATA_PATH')], required=False)\n            bin_path   = self.find_file(root_path + '.bin',   [os.environ.get('OPENCV_DNN_TEST_DATA_PATH')], required=False)\n            device_id  = 'CPU'\n            img_path = self.find_file('cv/face/david2.jpg', [os.environ.get('OPENCV_TEST_DATA_PATH')])\n            img = cv.imread(img_path)\n            def preproc(ppp):\n                ppp.input().model().set_layout(Layout(\"NCHW\"))\n                ppp.input().tensor().set_element_type(Type.u8)                            \\\n                                    .set_spatial_static_shape(img.shape[0], img.shape[1]) \\\n                                    .set_layout(Layout(\"NHWC\"))\n                ppp.input().preprocess().resize(ResizeAlgorithm.RESIZE_LINEAR)\n            ref = AgeGenderOV(model_path, bin_path, device_id)\n            ref.cfgPrePostProcessing(preproc)\n            ov_age, ov_gender = ref.apply(np.expand_dims(img, 0))\n            comp = AgeGenderGAPI(model_path, bin_path, device_id)\n            gapi_age, gapi_gender = comp.apply(img)\n            self.assertEqual(0.0, cv.norm(ov_gender, gapi_gender, cv.NORM_INF))\n            self.assertEqual(0.0, cv.norm(ov_age, gapi_age, cv.NORM_INF))", "target": "def env_str_field(\n    name: str,\n    default: str = \"\",\n) -> str:\n    return field(default_factory=lambda: get_env(name, default))", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "train", "example_id": "000636", "source": "def testNonContiguous(self):\n        x = torch.tensor([100, 200, 300])[::2]\n        assert not x.is_contiguous()\n        assert x[0] == 100\n        assert x[1] == 300\n        return x", "target": "def register_test_commands(subparsers: argparse._SubParsersAction) -> None:\n    build_parser = subparsers.add_parser(\n        \"test\",\n        help=\"test related commands\",\n        formatter_class=RichHelp,\n    )\n    build_subparsers = build_parser.add_subparsers(dest=\"test_command\", required=True)\n    overview = \"\\n\".join(\n        f\"  {name:12} {spec.get('help', '')}\" for name, spec in _TARGETS.items()\n    )\n    external_parser = build_subparsers.add_parser(\n        \"external\",\n        help=\"Test external targets\",\n        description=\"Test third-party targets.\\n\\nAvailable targets:\\n\" + overview,\n        formatter_class=RichHelp,\n    )\n    register_targets(external_parser, _TARGETS, common_args=common_args)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000637", "source": "def as_dataclass_field(pydantic_field: FieldInfo) -> dataclasses.Field[Any]:\n    field_args: dict[str, Any] = {'default': pydantic_field}\n    if sys.version_info >= (3, 14) and pydantic_field.description is not None:\n        field_args['doc'] = pydantic_field.description\n    if sys.version_info >= (3, 10) and pydantic_field.kw_only:\n        field_args['kw_only'] = True\n    if pydantic_field.repr is not True:\n        field_args['repr'] = pydantic_field.repr\n    return dataclasses.field(**field_args)", "target": "def get_type_ref(type_: Any, args_override: tuple[type[Any], ...] | None = None) -> str:\n    origin = get_origin(type_) or type_\n    args = get_args(type_) if is_generic_alias(type_) else (args_override or ())\n    generic_metadata = getattr(type_, '__pydantic_generic_metadata__', None)\n    if generic_metadata:\n        origin = generic_metadata['origin'] or origin\n        args = generic_metadata['args'] or args\n    module_name = getattr(origin, '__module__', '<No __module__>')\n    if typing_objects.is_typealiastype(origin):\n        type_ref = f'{module_name}.{origin.__name__}:{id(origin)}'\n    else:\n        try:\n            qualname = getattr(origin, '__qualname__', f'<No __qualname__: {origin}>')\n        except Exception:\n            qualname = getattr(origin, '__qualname__', '<No __qualname__>')\n        type_ref = f'{module_name}.{qualname}:{id(origin)}'\n    arg_refs: list[str] = []\n    for arg in args:\n        if isinstance(arg, str):\n            arg_ref = f'{arg}:str-{id(arg)}'\n        else:\n            arg_ref = f'{_repr.display_as_type(arg)}:{id(arg)}'\n        arg_refs.append(arg_ref)\n    if arg_refs:\n        type_ref = f'{type_ref}[{\",\".join(arg_refs)}]'\n    return type_ref", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000638", "source": "def f(value, serializer, _info):\n        return f'result={serializer(len(value))}'", "target": "def f(value, serializer):\n        return f'result={serializer(len(value))} repr={serializer!r}'", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000639", "source": "def _work(self):\n        with (\n            fresh_cache(),\n            torch._inductor.config.patch(force_shape_pad=self._force_shape_pad),\n        ):\n            opt_m = torch.compile(backend=self.backend(), dynamic=self.is_dynamic())(\n                self.m.cuda() if self._is_gpu else self.m\n            )\n            opt_m(self.input)", "target": "def _work(self):\n        with (\n            fresh_cache(),\n        ):\n            opt_m = torch.compile(backend=self.backend(), dynamic=self.is_dynamic())(\n                self.m.cuda() if self._is_gpu else self.m\n            )\n            opt_m(self.input)", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000640", "source": "def typename(self) -> Optional[str]:\n            return getattr(self.type_node, \"full_typename\", None)", "target": "def typename(self) -> str:\n        return \"\"", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000641", "source": "def getToolchain(self, arch, target):\n        toolchain = os.path.join(self.opencv, \"platforms\", \"ios\", \"cmake\", \"Toolchains\", \"Toolchain-%s_Xcode.cmake\" % target)\n        return toolchain", "target": "def getToolchain(self):\n        toolchain = os.path.join(self.script_dir, \"cmake\", \"Toolchains\", \"Toolchain-%s_Xcode.cmake\" % self.target)\n        return toolchain", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000642", "source": "def volume(self) -> int:\n            return self.area * self.height", "target": "def volume(self) -> None:\n            return None", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|opencv/opencv", "data_split": "train", "example_id": "000643", "source": "def _decorator(cls: type) -> type:\n        block = generate_dataclass_help(params_cls)\n        cls.__doc__ = (cls.__doc__ or \"\") + f\"\\n\\n{title}:\\n{block}\"\n        return cls", "target": "def test_custom_op_goodFeaturesToTrack(self):\n            img_path = self.find_file('cv/face/david2.jpg', [os.environ.get('OPENCV_TEST_DATA_PATH')])\n            in_mat = cv.cvtColor(cv.imread(img_path), cv.COLOR_RGB2GRAY)\n            max_corners         = 50\n            quality_lvl         = 0.01\n            min_distance        = 10.0\n            block_sz            = 3\n            use_harris_detector = True\n            k                   = 0.04\n            expected = cv.goodFeaturesToTrack(in_mat, max_corners, quality_lvl,\n                                              min_distance, mask=None,\n                                              blockSize=block_sz, useHarrisDetector=use_harris_detector, k=k)\n            g_in = cv.GMat()\n            g_out = GGoodFeatures.on(g_in, max_corners, quality_lvl,\n                                     min_distance, block_sz, use_harris_detector, k)\n            comp = cv.GComputation(cv.GIn(g_in), cv.GOut(g_out))\n            pkg = cv.gapi.kernels(GGoodFeaturesImpl)\n            actual = comp.apply(cv.gin(in_mat), args=cv.gapi.compile_args(pkg))\n            self.assertEqual(0.0, cv.norm(expected.flatten(),\n                                          np.array(actual, dtype=np.float32).flatten(), cv.NORM_INF))", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000644", "source": "def ser_root(self, v: Any, _) -> str:\n            assert self.root == 1_000\n            return f'{v:_}'", "target": "def ser_root(self, v: Any, serializer: core_schema.SerializerFunctionWrapHandler, _) -> str:\n            root = serializer(v)\n            assert self.root == 1_000\n            return f'{root:_}'", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "train", "example_id": "000645", "source": "def allgather_run(cmd):\n    proc = subprocess.run(shlex.split(cmd), capture_output=True)\n    assert proc.returncode == 0\n    return allgather_object(proc.stdout.decode(\"utf-8\"))", "target": "def local_print(msg):\n        if dist.get_rank() == 0:\n            print(msg, end=\"\", flush=True)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000646", "source": "def constrain(self: _Pipeline[_InT, _OutT], constraint: annotated_types.Predicate) -> _Pipeline[_InT, _OutT]: ...", "target": "def constrain(self: _Pipeline[_InT, _OutT], constraint: _NotEq) -> _Pipeline[_InT, _OutT]: ...", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "000647", "source": "def test_config_datetime(\n    dt: datetime, expected_to_python, expected_to_json, expected_to_python_dict, expected_to_json_dict, mode\n):\n    s = SchemaSerializer(core_schema.datetime_schema(), config={'ser_json_temporal': mode})\n    assert s.to_python(dt) == dt\n    assert s.to_python(dt, mode='json') == expected_to_python\n    assert s.to_json(dt) == expected_to_json\n    with pytest.warns(\n        UserWarning,\n        match=(\n            r'Expected `datetime` - serialized value may not be as expected '\n            r\"\\[input_value=\\{datetime\\.datetime\\([^)]*\\): 'foo'\\}, input_type=dict\\]\"\n        ),\n    ):\n        assert s.to_python({dt: 'foo'}) == {dt: 'foo'}\n    with pytest.warns(\n        UserWarning,\n        match=(\n            r'Expected `datetime` - serialized value may not be as expected '\n            r\"\\[input_value=\\{datetime\\.datetime\\([^)]*\\): 'foo'\\}, input_type=dict\\]\"\n        ),\n    ):\n        assert s.to_python({dt: 'foo'}, mode='json') == expected_to_python_dict\n    with pytest.warns(\n        UserWarning,\n        match=(\n            r'Expected `datetime` - serialized value may not be as expected '\n            r\"\\[input_value=\\{datetime\\.datetime\\([^)]*\\): 'foo'\\}, input_type=dict\\]\"\n        ),\n    ):\n        assert s.to_json({dt: 'foo'}) == expected_to_json_dict", "target": "def test_houghlines(self):\n        fn = \"/samples/data/pic1.png\"\n        src = self.get_sample(fn)\n        dst = cv.Canny(src, 50, 200)\n        lines = cv.HoughLinesP(dst, 1, math.pi/180.0, 40, np.array([]), 50, 10)[:,0,:]\n        eps = 5\n        testLines = [\n             [ 232,  25, 43, 25],\n             [ 43, 129, 232, 129],\n             [ 43, 129,  43,  25],\n             [232, 129, 232,  25],\n             [251,  86, 314, 183],\n             [252,  86, 323,  40],\n             [315, 183, 386, 137],\n             [324,  40, 386, 136],\n             [245, 205, 377, 205],\n             [244, 206, 305, 278],\n             [306, 279, 377, 205],\n             [153, 177, 196, 177],\n             [153, 277, 153, 179],\n             [153, 277, 196, 277],\n             [196, 177, 196, 277]]\n        matches_counter = 0\n        for i in range(len(testLines)):\n            for j in range(len(lines)):\n                if linesDiff(testLines[i], lines[j]) < eps:\n                    matches_counter += 1\n        self.assertGreater(float(matches_counter) / len(testLines), .7)\n        lines_acc = cv.HoughLinesWithAccumulator(dst, rho=1, theta=np.pi / 180, threshold=150, srn=0, stn=0)\n        self.assertEqual(lines_acc[0,0,2], 192.0)\n        self.assertEqual(lines_acc[1,0,2], 187.0)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pytorch/pytorch", "data_split": "train", "example_id": "000648", "source": "def get(self, name, units=\"ms\"):\n        if name == \"classname\":\n            return self.fixture\n        if name == \"name\":\n            return self.name\n        if name == \"fullname\":\n            return self.__str__()\n        if name == \"value_param\":\n            return self.value_param\n        if name == \"type_param\":\n            return self.type_param\n        if name == \"status\":\n            return self.status\n        val = self.metrix.get(name, None)\n        if not val:\n            return val\n        if name == \"time\":\n            return self.metrix.get(\"time\")\n        if name in [\"gmean\", \"min\", \"mean\", \"median\", \"stddev\"]:\n            scale = 1.0\n            frequency = self.metrix.get(\"frequency\", 1.0) or 1.0\n            if units == \"ms\":\n                scale = 1000.0\n            if units == \"us\" or units == \"mks\":\n                scale = 1000000.0\n            if units == \"ns\":\n                scale = 1000000000.0\n            if units == \"ticks\":\n                frequency = long(1)\n                scale = long(1)\n            return val * scale / frequency\n        return val", "target": "def medium_transpose():\n    return (\n        rand(32, 12, 64, 64).transpose(-1, -2),\n        rand(32, 12, 64, 64).transpose(-1, -2),\n    )", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000649", "source": "def test_constraints_schema_validation_error(constraint: str) -> None:\n    with pytest.raises(SchemaError, match=f\"'{constraint}' must be coercible to a Decimal instance\"):\n        SchemaValidator(cs.decimal_schema(**{constraint: 'bad_value'}))", "target": "def test_constraints_schema_validation_error(constraint: str) -> None:\n    with pytest.raises(SchemaError, match=f\"'{constraint}' must be coercible to an integer\"):\n        SchemaValidator(cs.int_schema(**{constraint: 'bad_value'}))", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000650", "source": "def outMeta(arr_desc0, arr_desc1, arr_desc2):\n        return cv.empty_array_desc(), cv.empty_array_desc(), \\\n               cv.empty_array_desc(), cv.empty_array_desc()", "target": "def outMeta(desc1, desc2, depth):\n            return desc1", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000651", "source": "def dataclass(\n        *,\n        init: Literal[False] = False,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool = False,\n        config: ConfigDict | type[object] | None = None,\n        validate_on_init: bool | None = None,\n        kw_only: bool = ...,\n        slots: bool = ...,\n    ) -> Callable[[type[_T]], type[PydanticDataclass]]:\n        ...", "target": "def dataclass(\n        _cls: type[_T],\n        *,\n        init: Literal[False] = False,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool | None = None,\n        config: ConfigDict | type[object] | None = None,\n        validate_on_init: bool | None = None,\n    ) -> type[PydanticDataclass]: ...", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "000652", "source": "def quack(self, args, kwargs=None) -> Any:\n        from quack.cross_entropy import cross_entropy\n        assert kwargs is None\n        x, target, dloss = args\n        loss = cross_entropy(x, target)\n        return lambda: torch.autograd.grad(\n            loss, x, grad_outputs=dloss, retain_graph=True\n        )", "target": "def generate_repeats():\n    for i in 1, 2, 3:\n        yield i\n        yield i", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000653", "source": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "target": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "000654", "source": "def make_data(self, params):\n        (method,) = params\n        n_samples = 500 if method == \"exact\" else None\n        return _digits_dataset(n_samples=n_samples)", "target": "def make_data(self, params):\n        data = _synth_classification_dataset(n_samples=10000, n_features=100)\n        return data", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000655", "source": "def test_include_exclude_args(params):\n    s = SchemaSerializer(\n        core_schema.typed_dict_schema(\n            {\n                '0': core_schema.typed_dict_field(core_schema.int_schema()),\n                '1': core_schema.typed_dict_field(core_schema.int_schema()),\n                '2': core_schema.typed_dict_field(core_schema.int_schema()),\n                '3': core_schema.typed_dict_field(core_schema.int_schema()),\n            }\n        )\n    )\n    include, exclude, expected = params['include'], params['exclude'], IsStrictDict(params['expected'])\n    value = {'0': 0, '1': 1, '2': 2, '3': 3}\n    assert s.to_python(value, include=include, exclude=exclude) == expected\n    assert s.to_python(value, mode='json', include=include, exclude=exclude) == expected\n    assert json.loads(s.to_json(value, include=include, exclude=exclude)) == expected", "target": "def field_name(self) -> str | None:\n        ...", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "000656", "source": "def test_stateful_kernel_multiple_instances(self):\n            g_in   = cv.GOpaque.Int()\n            g_out0 = GStatefulCounter.on(g_in)\n            g_out1 = GStatefulCounter.on(g_in)\n            comp   = cv.GComputation(cv.GIn(g_in), cv.GOut(g_out0, g_out1))\n            pkg    = cv.gapi.kernels(GStatefulCounterImpl)\n            nums = [i for i in range(10)]\n            acc0 = acc1 = 0\n            for v in nums:\n                acc0, acc1 = comp.apply(cv.gin(v), args=cv.gapi.compile_args(pkg))\n            ref = sum(nums)\n            self.assertEqual(ref, acc0)\n            self.assertEqual(ref, acc1)", "target": "def test_class_with_validator():\n    class Foobar:\n        a: int\n        def __init__(self, a):\n            self.a = a\n        @classmethod\n        def __validate__(cls, input_value, info):\n            return Foobar(input_value * 2)\n    v = SchemaValidator(\n        {\n            'type': 'function-after',\n            'function': {'type': 'with-info', 'function': Foobar.__validate__},\n            'schema': cs.str_schema(),\n        }\n    )\n    f = v.validate_python('foo')\n    assert isinstance(f, Foobar)\n    assert f.a == 'foofoo'\n    f = v.validate_python(b'a')\n    assert isinstance(f, Foobar)\n    assert f.a == 'aa'\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python(True)\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'string_type', 'loc': (), 'msg': 'Input should be a valid string', 'input': True}\n    ]", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "000657", "source": "def eqDictFloatKeyIntValue(self, input: dict[float, int]) -> dict[float, int]:\n        return input", "target": "def test_smart_union_does_nested_dataclass_field_counting() -> None:\n    @dataclass\n    class SubModelA:\n        x: int = 1\n    @dataclass\n    class SubModelB:\n        y: int = 2\n    @dataclass\n    class ModelA:\n        sub: SubModelA\n    @dataclass\n    class ModelB:\n        sub: SubModelB\n    dc_a_schema = core_schema.dataclass_schema(\n        ModelA,\n        core_schema.dataclass_args_schema(\n            'ModelA',\n            [\n                core_schema.dataclass_field(\n                    'sub',\n                    core_schema.with_default_schema(\n                        core_schema.dataclass_schema(\n                            SubModelA,\n                            core_schema.dataclass_args_schema(\n                                'SubModelA',\n                                [\n                                    core_schema.dataclass_field(\n                                        'x', core_schema.with_default_schema(core_schema.int_schema(), default=1)\n                                    )\n                                ],\n                            ),\n                            ['x'],\n                        ),\n                        default=SubModelA(),\n                    ),\n                )\n            ],\n        ),\n        ['sub'],\n    )\n    dc_b_schema = core_schema.dataclass_schema(\n        ModelB,\n        core_schema.dataclass_args_schema(\n            'ModelB',\n            [\n                core_schema.dataclass_field(\n                    'sub',\n                    core_schema.with_default_schema(\n                        core_schema.dataclass_schema(\n                            SubModelB,\n                            core_schema.dataclass_args_schema(\n                                'SubModelB',\n                                [\n                                    core_schema.dataclass_field(\n                                        'y', core_schema.with_default_schema(core_schema.int_schema(), default=2)\n                                    )\n                                ],\n                            ),\n                            ['y'],\n                        ),\n                        default=SubModelB(),\n                    ),\n                )\n            ],\n        ),\n        ['sub'],\n    )\n    for choices in permute_choices([dc_a_schema, dc_b_schema]):\n        validator = SchemaValidator(core_schema.union_schema(choices=choices))\n        assert isinstance(validator.validate_python({'sub': {'x': 1}}), ModelA)\n        assert isinstance(validator.validate_python({'sub': {'y': 3}}), ModelB)\n        assert isinstance(validator.validate_python({'sub': {}}), choices[0]['cls'])", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000658", "source": "def name(self) -> str:\n        prefix = f\"{self.category()}\"\n        return prefix", "target": "def name(self) -> str:\n        if self.enable_persistent_tma_matmul:\n            return \"triton_persistent_tma\"\n        else:\n            return \"triton\"", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000659", "source": "def ser_x(self, v: Any, _) -> str:\n            assert self.x == 1_000\n            return f'{v:_}'", "target": "def ser_x(data: Model, v: Any) -> str:\n        assert data['x'] == 1_000\n        return f'{v:_}'", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000660", "source": "def test_ser_json_inf_nan_with_any() -> None:\n    s = SchemaSerializer(core_schema.any_schema(), core_schema.CoreConfig(ser_json_inf_nan='constants'))\n    assert isinf(s.to_python(inf))\n    assert isinf(s.to_python(inf, mode='json'))\n    assert s.to_json(inf) == b'Infinity'\n    assert isnan(s.to_python(nan))\n    assert isnan(s.to_python(nan, mode='json'))\n    assert s.to_json(nan) == b'NaN'\n    s = SchemaSerializer(core_schema.any_schema(), core_schema.CoreConfig(ser_json_inf_nan='null'))\n    assert isinf(s.to_python(inf))\n    assert s.to_python(inf, mode='json') is None\n    assert s.to_json(inf) == b'null'\n    assert isnan(s.to_python(nan))\n    assert s.to_python(nan, mode='json') is None\n    assert s.to_json(nan) == b'null'\n    s = SchemaSerializer(core_schema.any_schema(), core_schema.CoreConfig(ser_json_inf_nan='strings'))\n    assert isinf(s.to_python(inf))\n    assert isinf(s.to_python(inf, mode='json'))\n    assert s.to_json(inf) == b'\"Infinity\"'\n    assert isnan(s.to_python(nan))\n    assert isnan(s.to_python(nan, mode='json'))\n    assert s.to_json(nan) == b'\"NaN\"'", "target": "def func(*args: Any) -> Any:\n        calls.append(args)\n        return args[0]", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "000661", "source": "def test_multi_dict_arucodetector(self):\n        aruco_params = cv.aruco.DetectorParameters()\n        aruco_dicts = [\n                cv.aruco.getPredefinedDictionary(cv.aruco.DICT_4X4_250),\n                cv.aruco.getPredefinedDictionary(cv.aruco.DICT_5X5_250)\n            ]\n        aruco_detector = cv.aruco.ArucoDetector(aruco_dicts, aruco_params)\n        id = 2\n        marker_size = 100\n        offset = 10\n        img_marker1 = cv.aruco.generateImageMarker(aruco_dicts[0], id, marker_size, aruco_params.markerBorderBits)\n        img_marker1 = np.pad(img_marker1, pad_width=offset, mode='constant', constant_values=255)\n        img_marker2 = cv.aruco.generateImageMarker(aruco_dicts[1], id, marker_size, aruco_params.markerBorderBits)\n        img_marker2 = np.pad(img_marker2, pad_width=offset, mode='constant', constant_values=255)\n        img_markers = np.concatenate((img_marker1, img_marker2), axis=1)\n        corners, ids, rejected, dictIndices = aruco_detector.detectMarkersMultiDict(img_markers)\n        self.assertEqual(2, len(ids))\n        self.assertEqual(id, ids[0])\n        self.assertEqual(id, ids[1])\n        self.assertEqual(2, len(dictIndices))\n        self.assertEqual(0, dictIndices[0])\n        self.assertEqual(1, dictIndices[1])", "target": "def test_alias_error_loc_field_names(py_and_json: PyAndJson):\n    v = py_and_json(\n        {\n            'type': 'typed-dict',\n            'fields': {\n                'field_a': {\n                    'type': 'typed-dict-field',\n                    'schema': {'type': 'int'},\n                    'validation_alias': [['foo'], ['bar', 1, -1]],\n                }\n            },\n            'config': {'loc_by_alias': False},\n        }\n    )\n    assert v.validate_test({'foo': 42}) == {'field_a': 42}\n    assert v.validate_test({'bar': ['x', [1, 2, 42]]}) == {'field_a': 42}\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_test({'foo': 'not_int'})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('field_a',),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'not_int',\n        }\n    ]\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_test({'bar': ['x', [1, 2, 'not_int']]})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('field_a',),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'not_int',\n        }\n    ]\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_test({})\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'missing', 'loc': ('field_a',), 'msg': 'Field required', 'input': {}}\n    ]", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000662", "source": "def intersectionRate(s1, s2):\n    area, _intersection = cv.intersectConvexConvex(np.array(s1), np.array(s2))\n    return 2 * area / (cv.contourArea(np.array(s1)) + cv.contourArea(np.array(s2)))", "target": "def intersectionRate(s1, s2):\n    x1, y1, x2, y2 = s1\n    s1 = np.array([[x1, y1], [x2,y1], [x2, y2], [x1, y2]])\n    x1, y1, x2, y2 = s2\n    s2 = np.array([[x1, y1], [x2,y1], [x2, y2], [x1, y2]])\n    area, _intersection = cv.intersectConvexConvex(s1, s2)\n    return 2 * area / (cv.contourArea(s1) + cv.contourArea(s2))", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000663", "source": "def fn():\n        @dataclasses.dataclass\n        class Dataclass:\n            a: int\n            @classmethod\n            def _validator(cls, v, info):\n                return v\n            @classmethod\n            def _wrap_validator(cls, v, validator, info):\n                return validator(v)\n        field_schema = core_schema.int_schema()\n        if validator == 'field':\n            field_schema = core_schema.with_info_before_validator_function(Dataclass._validator, field_schema)\n            field_schema = core_schema.with_info_wrap_validator_function(Dataclass._wrap_validator, field_schema)\n            field_schema = core_schema.with_info_after_validator_function(Dataclass._validator, field_schema)\n        dataclass_schema = core_schema.dataclass_schema(\n            Dataclass,\n            core_schema.dataclass_args_schema('Dataclass', [core_schema.dataclass_field('a', field_schema)]),\n            ['a'],\n        )\n        if validator == 'dataclass':\n            dataclass_schema = core_schema.with_info_before_validator_function(Dataclass._validator, dataclass_schema)\n            dataclass_schema = core_schema.with_info_wrap_validator_function(\n                Dataclass._wrap_validator, dataclass_schema\n            )\n            dataclass_schema = core_schema.with_info_after_validator_function(Dataclass._validator, dataclass_schema)\n        Dataclass.__pydantic_validator__ = SchemaValidator(dataclass_schema)\n        return Dataclass", "target": "def fn():\n        class Model:\n            a: int\n            @classmethod\n            def _validator(cls, v, info):\n                return v\n            @classmethod\n            def _wrap_validator(cls, v, validator, info):\n                return validator(v)\n        field_schema = core_schema.int_schema()\n        if validator == 'field':\n            field_schema = core_schema.with_info_before_validator_function(Model._validator, field_schema)\n            field_schema = core_schema.with_info_wrap_validator_function(Model._wrap_validator, field_schema)\n            field_schema = core_schema.with_info_after_validator_function(Model._validator, field_schema)\n        model_schema = core_schema.model_schema(\n            Model, core_schema.model_fields_schema({'a': core_schema.model_field(field_schema)})\n        )\n        if validator == 'model':\n            model_schema = core_schema.with_info_before_validator_function(Model._validator, model_schema)\n            model_schema = core_schema.with_info_wrap_validator_function(Model._wrap_validator, model_schema)\n            model_schema = core_schema.with_info_after_validator_function(Model._validator, model_schema)\n        Model.__pydantic_validator__ = SchemaValidator(model_schema)\n        return Model", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000664", "source": "def f(input_value, validator, info):\n        return plain_repr(validator)", "target": "def ids_function(val):\n    if callable(val):\n        return val.__name__\n    elif isinstance(val, tuple) and len(val) == 2:\n        return '({})'.format(', '.join([repr(a) for a in val[0]] + [f'{k}={v!r}' for k, v in val[1].items()]))\n    else:\n        return repr(val)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000665", "source": "def test_on_error_raise_explicit(self, py_and_json: PyAndJson):\n        v = py_and_json(\n            {\n                'type': 'model-fields',\n                'fields': {\n                    'x': {\n                        'type': 'model-field',\n                        'schema': {'type': 'default', 'schema': {'type': 'str'}, 'on_error': 'raise'},\n                    }\n                },\n            }\n        )\n        assert v.validate_test({'x': 'foo'}) == ({'x': 'foo'}, None, {'x'})\n        with pytest.raises(ValidationError) as exc_info:\n            v.validate_test({'x': ['foo']})\n        assert exc_info.value.errors(include_url=False) == [\n            {'input': ['foo'], 'type': 'string_type', 'loc': ('x',), 'msg': 'Input should be a valid string'}\n        ]", "target": "def test_on_error_raise_explicit(self, py_and_json: PyAndJson):\n        v = py_and_json(\n            {\n                'type': 'typed-dict',\n                'fields': {\n                    'x': {\n                        'type': 'typed-dict-field',\n                        'schema': {'type': 'default', 'schema': {'type': 'str'}, 'on_error': 'raise'},\n                    }\n                },\n            }\n        )\n        assert v.validate_test({'x': 'foo'}) == {'x': 'foo'}\n        with pytest.raises(ValidationError) as exc_info:\n            v.validate_test({'x': ['foo']})\n        assert exc_info.value.errors(include_url=False) == [\n            {'input': ['foo'], 'type': 'string_type', 'loc': ('x',), 'msg': 'Input should be a valid string'}\n        ]", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "000666", "source": "def test_function_error():\n    def raise_error(value, _info):\n        raise TypeError('foo')\n    s = SchemaSerializer(\n        core_schema.any_schema(\n            serialization=core_schema.plain_serializer_function_ser_schema(raise_error, info_arg=True)\n        )\n    )\n    msg = 'Error calling function `raise_error`: TypeError: foo$'\n    with pytest.raises(PydanticSerializationError, match=msg) as exc_info:\n        s.to_python('abc')\n    assert isinstance(exc_info.value.__cause__, TypeError)\n    with pytest.raises(PydanticSerializationError, match=msg) as exc_info:\n        s.to_python('abc', mode='json')\n    assert isinstance(exc_info.value.__cause__, TypeError)\n    with pytest.raises(PydanticSerializationError, match=msg):\n        s.to_json('foo')", "target": "def checkout(self, branch: str) -> None:\n        self._run_git(\"checkout\", branch)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000667", "source": "def test_function_plain_field_serializer_to_json():\n    class Model(RootModel):\n        def ser_root(self, v: Any, _) -> str:\n            assert self.root == 1_000\n            return f'{v:_}'\n    s = SchemaSerializer(\n        core_schema.model_schema(\n            Model,\n            core_schema.int_schema(\n                serialization=core_schema.plain_serializer_function_ser_schema(\n                    Model.ser_root, is_field_serializer=True, info_arg=True\n                )\n            ),\n            root_model=True,\n        )\n    )\n    assert json.loads(s.to_json(Model(1000))) == '1_000'", "target": "def test_function_plain_field_serializer_to_json():\n    class Model(TypedDict):\n        x: int\n    def ser_x(data: Model, v: Any, info: core_schema.FieldSerializationInfo) -> str:\n        assert data['x'] == 1_000\n        return f'{v:_}-{info.field_name}'\n    s = SchemaSerializer(\n        core_schema.typed_dict_schema(\n            {\n                'x': core_schema.typed_dict_field(\n                    core_schema.int_schema(\n                        serialization=core_schema.plain_serializer_function_ser_schema(\n                            ser_x, is_field_serializer=True, info_arg=True\n                        )\n                    )\n                )\n            }\n        )\n    )\n    assert json.loads(s.to_json(Model(x=1000))) == {'x': '1_000-x'}", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|opencv/opencv", "data_split": "train", "example_id": "000668", "source": "def main():\n    result_path = sys.argv[1]\n    all = [\n        Benchmark(training=False, subclass=False),\n        Benchmark(training=True, subclass=False),\n        Benchmark(training=False, subclass=True),\n        Benchmark(training=True, subclass=True),\n    ]\n    for benchmark in all:\n        benchmark.enable_compile_time_instruction_count().collect_all().append_results(\n            result_path\n        )", "target": "def test_async_simple(self):\n        m = np.array([[1,2],[3,4],[5,6]])\n        async_result = cv.utils.testAsyncArray(m)\n        self.assertTrue(async_result.valid())\n        ret, result = async_result.get(timeoutNs=10**6)\n        self.assertTrue(ret)\n        self.assertFalse(async_result.valid())\n        self.assertEqual(cv.norm(m, result, cv.NORM_INF), 0)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000669", "source": "def test_raise_assertion_error_plain():\n    def f(input_value, info):\n        raise AssertionError\n    v = SchemaValidator(core_schema.with_info_before_validator_function(f, core_schema.str_schema()))\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python('input value')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'assertion_error',\n            'loc': (),\n            'msg': 'Assertion failed, ',\n            'input': 'input value',\n            'ctx': {'error': HasRepr(repr(AssertionError()))},\n        }\n    ]", "target": "def test_alias(py_and_json: PyAndJson, input_value, expected):\n    v = py_and_json(\n        {\n            'type': 'arguments',\n            'arguments_schema': [\n                {'name': 'a', 'mode': 'positional_or_keyword', 'schema': {'type': 'int'}, 'alias': 'Foo'}\n            ],\n        }\n    )\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            v.validate_test(input_value)\n    else:\n        assert v.validate_test(input_value) == expected", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "000670", "source": "def test_wrong_function_signature() -> None:\n    def wrong_validator(value: Any) -> Any:\n        return value\n    v = SchemaValidator(core_schema.with_info_plain_validator_function(wrong_validator))\n    try:\n        v.validate_python(1)\n    except TypeError as exc:\n        assert 'takes 1 positional argument but 2 were given' in str(exc)\n    else:\n        raise AssertionError('v.validate_python(1) did not raise TypeError')", "target": "def _make_round_rect(x, y, diam, corners=(\"right\", \"right\", \"right\", \"right\")):\n        rad = diam / 2\n        cw_point = ((0, 0), (diam, 0), (diam, diam), (0, diam))\n        mid_cw_point = ((0, rad), (rad, 0), (diam, rad), (rad, diam))\n        res_str = \"M{},{} \".format(x + mid_cw_point[0][0], y + mid_cw_point[0][1])\n        n = len(cw_point)\n        for i in range(n):\n            if corners[i] == \"right\":\n                res_str += \"L{},{} L{},{} \".format(x + cw_point[i][0], y + cw_point[i][1],\n                                                   x + mid_cw_point[(i + 1) % n][0], y + mid_cw_point[(i + 1) % n][1])\n            elif corners[i] == \"round\":\n                res_str += \"A{},{} 0,0,1 {},{} \".format(rad, rad, x + mid_cw_point[(i + 1) % n][0],\n                                                        y + mid_cw_point[(i + 1) % n][1])\n            else:\n                raise TypeError(\"unknown corner type\")\n        return res_str", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000671", "source": "def constrain(self: _Pipeline[_InT, _NewOutLt], constraint: annotated_types.Lt) -> _Pipeline[_InT, _NewOutLt]: ...", "target": "def constrain(\n        self: _Pipeline[_InT, _NewOutInterval], constraint: annotated_types.Interval\n    ) -> _Pipeline[_InT, _NewOutInterval]: ...", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000672", "source": "def constrain(\n        self: _Pipeline[_InT, _NewOutLen], constraint: annotated_types.Len\n    ) -> _Pipeline[_InT, _NewOutLen]: ...", "target": "def constrain(self: _Pipeline[_InT, _OutT], constraint: _In) -> _Pipeline[_InT, _OutT]: ...", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|pydantic/pydantic", "data_split": "train", "example_id": "000673", "source": "def fixed_batch_size_comparison(data):\n    all_features = [\n        i.astype(int) for i in np.linspace(data.shape[1] // 10, data.shape[1], num=5)\n    ]\n    batch_size = 1000\n    all_times = defaultdict(list)\n    all_errors = defaultdict(list)\n    for n_components in all_features:\n        pca = PCA(n_components=n_components)\n        ipca = IncrementalPCA(n_components=n_components, batch_size=batch_size)\n        results_dict = {\n            k: benchmark(est, data) for k, est in [(\"pca\", pca), (\"ipca\", ipca)]\n        }\n        for k in sorted(results_dict.keys()):\n            all_times[k].append(results_dict[k][\"time\"])\n            all_errors[k].append(results_dict[k][\"error\"])\n    plot_feature_times(all_times, batch_size, all_features, data)\n    plot_feature_errors(all_errors, batch_size, all_features, data)", "target": "def test_custom_invalid_tz():\n    class CustomTz(tzinfo):\n        def tzname(self, _dt):\n            return 'CustomTZ'\n    schema = SchemaValidator(cs.datetime_schema(gt=datetime(2022, 1, 1, 15, 0, 0)))\n    dt = datetime(2022, 1, 1, 16, 0, 0, tzinfo=CustomTz())\n    with pytest.raises(ValidationError) as excinfo:\n        schema.validate_python(dt)\n    if platform.python_implementation() in ('PyPy', 'GraalVM'):\n        error_message = 'NotImplementedError: tzinfo subclass must override utcoffset()'\n    else:\n        error_message = 'NotImplementedError: a tzinfo subclass must implement utcoffset()'\n    assert excinfo.value.errors(include_url=False) == [\n        {\n            'type': 'datetime_object_invalid',\n            'loc': (),\n            'msg': f'Invalid datetime object, got {error_message}',\n            'input': dt,\n            'ctx': {'error': error_message},\n        }\n    ]", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000674", "source": "def wrap_function(input_value, validator, info):\n            try:\n                return validator(input_value)\n            except ValidationError:\n                if isinstance(input_value, list):\n                    return str(len(input_value))\n                else:\n                    return repr(input_value)", "target": "def f(input_value, _info):\n        assert isinstance(input_value, MyModel)\n        input_value.field_a += ' Changed'\n        return input_value", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "000675", "source": "def test_any_model():\n    @dataclasses.dataclass\n    class Foo:\n        a: str\n        b: bytes\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo', [core_schema.dataclass_field(name='a', schema=core_schema.str_schema())]\n        ),\n        ['a'],\n    )\n    Foo.__pydantic_validator__ = SchemaValidator(schema)\n    Foo.__pydantic_serializer__ = SchemaSerializer(schema)\n    s = SchemaSerializer(core_schema.any_schema())\n    assert s.to_python(Foo(a='hello', b=b'more')) == IsStrictDict(a='hello')\n    assert s.to_python(Foo(a='hello', b=b'more'), mode='json') == IsStrictDict(a='hello')\n    j = s.to_json(Foo(a='hello', b=b'more'))\n    if on_pypy:\n        assert json.loads(j) == {'a': 'hello'}\n    else:\n        assert j == b'{\"a\":\"hello\"}'\n    assert s.to_python(Foo(a='hello', b=b'more'), exclude={'a'}) == IsStrictDict()\n    assert s.to_json(Foo(a='hello', b=b'more'), exclude={'a'}) == b'{}'\n    assert s.to_python(Foo) == Foo\n    with pytest.raises(PydanticSerializationError, match=r\"Unable to serialize unknown type: <class 'type'>\"):\n        s.to_python(Foo, mode='json')\n    with pytest.raises(PydanticSerializationError, match=r\"Unable to serialize unknown type: <class 'type'>\"):\n        s.to_json(Foo)\n    assert s.to_python(Foo, mode='json', fallback=lambda x: x.__name__) == 'Foo'\n    assert s.to_json(Foo, fallback=lambda x: x.__name__) == b'\"Foo\"'", "target": "def test_cuda_runtime_errors_captured() -> None:\n    cuda_exception_missed = True\n    try:\n        print(\"Testing test_cuda_runtime_errors_captured\")\n        torch._assert_async(torch.tensor(0, device=\"cuda\"))\n        torch._assert_async(torch.tensor(0 + 0j, device=\"cuda\"))\n    except RuntimeError as e:\n        if re.search(\"CUDA\", f\"{e}\"):\n            print(f\"Caught CUDA exception with success: {e}\")\n            cuda_exception_missed = False\n        else:\n            raise e\n    if cuda_exception_missed:\n        raise RuntimeError(\"Expected CUDA RuntimeError but have not received!\")", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000676", "source": "def with_config(*, config: ConfigDict) -> Callable[[_TypeT], _TypeT]: ...", "target": "def with_config(config: ConfigDict, /) -> Callable[[_TypeT], _TypeT]: ...", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000677", "source": "def test_alias(py_and_json: PyAndJson, input_value, expected):\n    v = py_and_json(\n        {\n            'type': 'arguments',\n            'arguments_schema': [\n                {'name': 'a', 'mode': 'positional_or_keyword', 'schema': {'type': 'int'}, 'alias': 'Foo'}\n            ],\n        }\n    )\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            v.validate_test(input_value)\n    else:\n        assert v.validate_test(input_value) == expected", "target": "def test_alias(py_and_json: PyAndJson):\n    v = py_and_json(\n        {\n            'type': 'model-fields',\n            'fields': {'field_a': {'validation_alias': 'FieldA', 'type': 'model-field', 'schema': {'type': 'int'}}},\n        }\n    )\n    assert v.validate_test({'FieldA': '123'}) == ({'field_a': 123}, None, {'field_a'})\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'foobar': '123'})\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'field_a': '123'})", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "000678", "source": "def make_data(self, params):\n        data = _synth_classification_dataset(n_samples=10000, n_features=100)\n        return data", "target": "def make_data(self, params):\n        representation, precompute = params\n        if representation == \"dense\":\n            data = _synth_regression_dataset(n_samples=1000000, n_features=100)\n        else:\n            data = _synth_regression_sparse_dataset(\n                n_samples=50000, n_features=5000, density=0.01\n            )\n        return data", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000679", "source": "def test_float_no_remainder():\n    v = SchemaValidator(core_schema.int_schema())\n    assert v.validate_json('123.0') == 123", "target": "def ns_for_function(obj: Callable[..., Any], parent_namespace: MappingNamespace | None = None) -> NamespacesTuple:\n    locals_list: list[MappingNamespace] = []\n    if parent_namespace is not None:\n        locals_list.append(parent_namespace)\n    type_params: tuple[_TypeVarLike, ...] = getattr(obj, '__type_params__', ())\n    if parent_namespace is not None:\n        type_params += parent_namespace.get('__type_params__', ())\n    locals_list.append({t.__name__: t for t in type_params})\n    globalns = get_module_ns_of(obj)\n    return NamespacesTuple(globalns, LazyLocalNamespace(*locals_list))", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|pydantic/pydantic", "data_split": "train", "example_id": "000680", "source": "def heldout_score(clf, X_test, y_test):\n    score = np.zeros((n_estimators,), dtype=np.float64)\n    for i, y_proba in enumerate(clf.staged_predict_proba(X_test)):\n        score[i] = 2 * log_loss(y_test, y_proba[:, 1])\n    return score", "target": "def test_datetime_future_timezone():\n    v = SchemaValidator(core_schema.datetime_schema(now_utc_offset=0, now_op='future'))\n    now_utc = datetime.now(timezone.utc)\n    soon_utc = now_utc + timedelta(minutes=1)\n    assert v.isinstance_python(soon_utc)\n    assert v.isinstance_python(soon_utc.astimezone(zoneinfo.ZoneInfo('Europe/Istanbul')))\n    assert v.isinstance_python(soon_utc.astimezone(zoneinfo.ZoneInfo('America/Los_Angeles')))\n    past_utc = now_utc - timedelta(minutes=1)\n    assert not v.isinstance_python(past_utc)\n    assert not v.isinstance_python(past_utc.astimezone(zoneinfo.ZoneInfo('Europe/Istanbul')))\n    assert not v.isinstance_python(past_utc.astimezone(zoneinfo.ZoneInfo('America/Los_Angeles')))", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000681", "source": "def get_memory_bytes(self, args, kwargs) -> int:\n        x, target, dloss = args\n        M, N = x.shape\n        return (\n            2 * M * N * x.dtype.itemsize\n            + M * target.dtype.itemsize\n            + M * dloss.dtype.itemsize\n        )", "target": "def get_memory_bytes(self, args, kwargs) -> int:\n        x, dy = args\n        M, N = x.shape\n        return 3 * M * N * x.dtype.itemsize", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "000682", "source": "def non_compute_operator(op):\n    schema = op._schema\n    if not any(contains_tensor_types(arg.type) for arg in schema.arguments):\n        return True\n    if \"_like\" in op.name():\n        return True\n    if schema.is_mutable:\n        return False\n    tensor_inps = [arg for arg in schema.arguments if arg.type is tensor_type]\n    tensor_outputs = [ret for ret in schema.returns if ret.type is tensor_type]\n    if len(tensor_outputs) != 1:\n        return False\n    for inp in tensor_inps:\n        if inp.alias_info and tensor_outputs[0].alias_info:\n            if inp.alias_info.before_set.intersection(\n                tensor_outputs[0].alias_info.after_set\n            ):\n                return True\n    return False", "target": "def test_any_uuid_key():\n    v = SchemaSerializer(core_schema.dict_schema())\n    input_value = {UUID('12345678-1234-5678-1234-567812345678'): 1}\n    assert v.to_python(input_value, mode='json') == {'12345678-1234-5678-1234-567812345678': 1}\n    assert v.to_json(input_value) == b'{\"12345678-1234-5678-1234-567812345678\":1}'", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "000683", "source": "def make_data(self, params):\n        representation, solver, n_jobs = params\n        if Benchmark.data_size == \"large\":\n            if representation == \"sparse\":\n                data = _20newsgroups_highdim_dataset(n_samples=10000)\n            else:\n                data = _20newsgroups_lowdim_dataset(n_components=1e3)\n        else:\n            if representation == \"sparse\":\n                data = _20newsgroups_highdim_dataset(n_samples=2500)\n            else:\n                data = _20newsgroups_lowdim_dataset()\n        return data", "target": "def make_data(self, params):\n        algorithm, dimension, n_jobs = params\n        if Benchmark.data_size == \"large\":\n            n_components = 40 if dimension == \"low\" else 200\n        else:\n            n_components = 10 if dimension == \"low\" else 50\n        data = _20newsgroups_lowdim_dataset(n_components=n_components)\n        return data", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000684", "source": "def constrain(\n        self: _Pipeline[_InT, _NewOutT], constraint: annotated_types.MultipleOf\n    ) -> _Pipeline[_InT, _NewOutT]: ...", "target": "def constrain(self: _Pipeline[_InT, _OutT], constraint: annotated_types.Predicate) -> _Pipeline[_InT, _OutT]: ...", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000685", "source": "def generate_arguments_schema(\n    func: Callable[..., Any],\n    schema_type: Literal['arguments', 'arguments-v3'] = 'arguments-v3',\n    parameters_callback: Callable[[int, str, Any], Literal['skip'] | None] | None = None,\n    config: ConfigDict | None = None,\n) -> CoreSchema:\n    generate_schema = _generate_schema.GenerateSchema(\n        _config.ConfigWrapper(config),\n        ns_resolver=_namespace_utils.NsResolver(namespaces_tuple=_namespace_utils.ns_for_function(func)),\n    )\n    if schema_type == 'arguments':\n        schema = generate_schema._arguments_schema(func, parameters_callback)\n    else:\n        schema = generate_schema._arguments_v3_schema(func, parameters_callback)\n    return generate_schema.clean_schema(schema)", "target": "def test_format_fallback():\n    s = SchemaSerializer(core_schema.any_schema(serialization=core_schema.format_ser_schema('^5s')))\n    assert s.to_python('abc') == 'abc'\n    assert s.to_python('abc', mode='json') == ' abc '\n    assert s.to_json('abc') == b'\" abc \"'\n    assert s.to_python(None) is None\n    assert s.to_python(None, mode='json') is None\n    assert s.to_json(None) == b'null'", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000686", "source": "def forward(self, x, y):\n        return (x + y,)", "target": "def forward(self, x):\n        x = self.relu_a(x)\n        x = x + self.sub_mods(x)\n        return x + self.relu_b(x) + self.a", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "000687", "source": "def jacrev(model, inp, v=None, strict=None):\n        argnums = tuple(range(len(inp)))\n        return ft.jacrev(model, argnums)(*inp)", "target": "def add_initializer(\n        self, fields: list[PydanticModelField], config: ModelConfigData, is_settings: bool, is_root_model: bool\n    ) -> None:\n        if '__init__' in self._cls.info.names and not self._cls.info.names['__init__'].plugin_generated:\n            return\n        typed = self.plugin_config.init_typed\n        model_strict = bool(config.strict)\n        use_alias = not (config.validate_by_name or config.populate_by_name) and config.validate_by_alias is not False\n        requires_dynamic_aliases = bool(config.has_alias_generator and not config.validate_by_name)\n        args = self.get_field_arguments(\n            fields,\n            typed=typed,\n            model_strict=model_strict,\n            requires_dynamic_aliases=requires_dynamic_aliases,\n            use_alias=use_alias,\n            is_settings=is_settings,\n            is_root_model=is_root_model,\n            force_typevars_invariant=True,\n        )\n        if is_settings:\n            base_settings_node = self._api.lookup_fully_qualified(BASESETTINGS_FULLNAME).node\n            assert isinstance(base_settings_node, TypeInfo)\n            if '__init__' in base_settings_node.names:\n                base_settings_init_node = base_settings_node.names['__init__'].node\n                assert isinstance(base_settings_init_node, FuncDef)\n                if base_settings_init_node is not None and base_settings_init_node.type is not None:\n                    func_type = base_settings_init_node.type\n                    assert isinstance(func_type, CallableType)\n                    for arg_idx, arg_name in enumerate(func_type.arg_names):\n                        if arg_name is None or arg_name.startswith('__') or not arg_name.startswith('_'):\n                            continue\n                        analyzed_variable_type = self._api.anal_type(func_type.arg_types[arg_idx])\n                        if analyzed_variable_type is not None and arg_name == '_cli_settings_source':\n                            analyzed_variable_type = analyzed_variable_type.accept(\n                                ChangeExplicitTypeOfAny(TypeOfAny.from_omitted_generics)\n                            )\n                        variable = Var(arg_name, analyzed_variable_type)\n                        args.append(Argument(variable, analyzed_variable_type, None, ARG_OPT))\n        if not self.should_init_forbid_extra(fields, config):\n            var = Var('kwargs')\n            args.append(Argument(var, AnyType(TypeOfAny.explicit), None, ARG_STAR2))\n        add_method(self._api, self._cls, '__init__', args=args, return_type=NoneType())", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000688", "source": "def name(self):\n        prefix = f\"{self.category()}_{self.mode()}\"\n        if self._subclass:\n            prefix += \"_subclass\"\n        else:\n            prefix += \"_nosubclass\"\n        if self.device() == \"cpu\":\n            prefix += \"_cpu\"\n        return prefix", "target": "def name(self) -> str:\n        if self.enable_persistent_tma_matmul:\n            return \"triton_persistent_tma\"\n        else:\n            return \"triton\"", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|opencv/opencv", "data_split": "train", "example_id": "000689", "source": "def _get_torch_wheel_path_arg(self, torch_whl_dir: Optional[Path]) -> str:\n        if not torch_whl_dir:\n            return \"\"\n        return f\"--build-arg TORCH_WHEELS_PATH={_VLLM_TEMP_FOLDER}\"", "target": "def types_separator(self) -> str:\n        return \", \"", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000690", "source": "def test_int_not_coerced_to_enum():\n    class BinaryEnum(IntEnum):\n        ZERO = 0\n        ONE = 1\n    enum_schema = core_schema.lax_or_strict_schema(\n        core_schema.no_info_after_validator_function(BinaryEnum, core_schema.int_schema()),\n        core_schema.is_instance_schema(BinaryEnum),\n    )\n    schema = core_schema.union_schema([enum_schema, core_schema.int_schema()])\n    validator = SchemaValidator(schema)\n    assert validator.validate_python(0) is not BinaryEnum.ZERO\n    assert validator.validate_python(1) is not BinaryEnum.ONE\n    assert validator.validate_python(BinaryEnum.ZERO) is BinaryEnum.ZERO\n    assert validator.validate_python(BinaryEnum.ONE) is BinaryEnum.ONE", "target": "def test_python_none():\n    v = SchemaValidator(cs.none_schema())\n    assert v.validate_python(None) is None\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python(1)\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'none_required', 'loc': (), 'msg': 'Input should be None', 'input': 1}\n    ]", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "000691", "source": "def run(img0, img1):\n                    return img0 + img1", "target": "def deep_update(mapping: dict[KeyType, Any], *updating_mappings: dict[KeyType, Any]) -> dict[KeyType, Any]:\n    updated_mapping = mapping.copy()\n    for updating_mapping in updating_mappings:\n        for k, v in updating_mapping.items():\n            if k in updated_mapping and isinstance(updated_mapping[k], dict) and isinstance(v, dict):\n                updated_mapping[k] = deep_update(updated_mapping[k], v)\n            else:\n                updated_mapping[k] = v\n    return updated_mapping", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000692", "source": "def ser_x(self, v: Any, serializer: core_schema.SerializerFunctionWrapHandler, _) -> str:\n            x = serializer(v)\n            assert self.x == 1_000\n            return f'{x:_}'", "target": "def ser_x(data: Model, v: Any, serializer: core_schema.SerializerFunctionWrapHandler) -> str:\n        assert data['x'] == 1_000\n        x = serializer(v)\n        return f'{x:_}'", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000693", "source": "def required_definition_imports(self) -> Generator[str, None, None]:\n        yield from ()", "target": "def required_definition_imports(self) -> Generator[str, None, None]:\n        for item in self:\n            yield from item.required_definition_imports", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000694", "source": "def test_only_allow_alias(py_and_json) -> None:\n    schema = core_schema.arguments_schema(\n        [\n            core_schema.arguments_parameter(name='a', schema=core_schema.str_schema(), alias='FieldA'),\n        ],\n        validate_by_name=False,\n        validate_by_alias=True,\n    )\n    v = py_and_json(schema)\n    assert v.validate_test(ArgsKwargs((), {'FieldA': 'hello'})) == ((), {'a': 'hello'})\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Missing required argument \\[type=missing_argument,'):\n        assert v.validate_test(ArgsKwargs((), {'a': 'hello'}))", "target": "def test_only_allow_alias(py_and_json) -> None:\n    schema = core_schema.dataclass_schema(\n        BasicDataclass,\n        core_schema.dataclass_args_schema(\n            'BasicDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), validation_alias='FieldA'),\n            ],\n        ),\n        ['a'],\n        config=core_schema.CoreConfig(validate_by_name=False, validate_by_alias=True),\n    )\n    v = py_and_json(schema)\n    assert v.validate_test({'FieldA': 'hello'}) == BasicDataclass(a='hello')\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'a': 'hello'})", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000695", "source": "def types_separator(self) -> str:\n        return \", \"", "target": "def types_separator(self) -> str:\n        return \", \"", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000696", "source": "def dynamic_rnn(\n        input: Tensor,\n        hidden: tuple[Tensor, Tensor],\n        w_ih: Tensor,\n        w_hh: Tensor,\n        b_ih: Tensor,\n        b_hh: Tensor,\n    ) -> tuple[Tensor, tuple[Tensor, Tensor]]:\n        hx, cx = hidden\n        outputs = []\n        inpSize = input.size()\n        inpSize = input.size()\n        inputs = torch.mm(input.view(-1, inpSize[2]), w_ih.t()) + b_ih\n        inputs = inputs.view(inpSize[0], inpSize[1], -1).unbind(0)\n        hy, cy = hx[0], cx[0]\n        for seq_idx in range(len(inputs)):\n            hy, cy = premul_cell(inputs[seq_idx], (hy, cy), w_hh, b_hh)\n            outputs += [hy]\n        return torch.stack(outputs), (hy.unsqueeze(0), cy.unsqueeze(0))", "target": "def dynamic_rnn(\n        input: Tensor, hidden: tuple[Tensor, Tensor], params: list[Tensor]\n    ) -> tuple[Tensor, tuple[Tensor, Tensor]]:\n        params_stride = 4\n        hx, cx = hidden\n        hy, cy = hidden\n        inputs, outputs = input.unbind(0), []\n        for layer in range(hx.size(0)):\n            hy = hx[layer]\n            cy = cx[layer]\n            base_idx = layer * params_stride\n            w_ih = params[base_idx]\n            w_hh = params[base_idx + 1]\n            b_ih = params[base_idx + 2]\n            b_hh = params[base_idx + 3]\n            for seq_idx in range(len(inputs)):\n                hy, cy = cell(inputs[seq_idx], (hy, cy), w_ih, w_hh, b_ih, b_hh)\n                outputs += [hy]\n            inputs, outputs = outputs, []\n        return torch.stack(inputs), (hy.unsqueeze(0), cy.unsqueeze(0))", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000697", "source": "def area(self) -> int:\n            return self.width * self.height", "target": "def area(self) -> float:\n            return self.side**2", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "000698", "source": "def relative_typename(self, module: str) -> str:\n        return self.type_format.format(self.types_separator.join(\n            item.relative_typename(module) for item in self\n        ))", "target": "def test_exclude(schema_func, seq_f):\n    v = SchemaSerializer(\n        schema_func(core_schema.any_schema(), serialization=core_schema.filter_seq_schema(exclude={1, 3, 5}))\n    )\n    assert v.to_python(seq_f(0, 1, 2, 3)) == seq_f(0, 2)\n    assert v.to_python(seq_f('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h')) == seq_f('a', 'c', 'e', 'g', 'h')\n    assert v.to_python(seq_f('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'), mode='json') == ['a', 'c', 'e', 'g', 'h']\n    assert v.to_json(seq_f('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h')) == b'[\"a\",\"c\",\"e\",\"g\",\"h\"]'\n    assert v.to_python(seq_f('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'), exclude={6}) == seq_f('a', 'c', 'e', 'h')\n    assert v.to_python(seq_f('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'), exclude={-1, -2}) == seq_f('a', 'c', 'e')\n    assert v.to_json(seq_f('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'), exclude={6}) == b'[\"a\",\"c\",\"e\",\"h\"]'\n    assert v.to_json(seq_f('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'), exclude={-1, -2}) == b'[\"a\",\"c\",\"e\"]'", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "000699", "source": "def getTestWideName(sname, indexes, lists, x, y):\n    name = sname + \"::(\"\n    for i in range(len(indexes)):\n        if i > 0:\n            name += \", \"\n        if i == x:\n            name += \"X\"\n        elif i == y:\n            name += \"Y\"\n        else:\n            name += lists[i][indexes[i]]\n    return str(name + \")\")", "target": "def test_strict_union_member_level() -> None:\n    v = SchemaValidator(\n        core_schema.union_schema(choices=[core_schema.bool_schema(strict=True), core_schema.int_schema(strict=True)])\n    )\n    assert v.validate_python(1) == 1\n    assert v.validate_python(123) == 123\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python('123')\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'bool_type', 'loc': ('bool',), 'msg': 'Input should be a valid boolean', 'input': '123'},\n        {'type': 'int_type', 'loc': ('int',), 'msg': 'Input should be a valid integer', 'input': '123'},\n    ]", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "train", "example_id": "000700", "source": "def print_measurements(prefix, nelem, measurements):\n        measurements = sorted(measurements)\n        local_print(f\"{prefix:8s}:\")\n        for p in [50, 75, 90, 95]:\n            v = np.percentile(measurements, p)\n            local_print(f\"  p{p:02d}:  {v:1.3f}s  {nelem / v:6d}/s\")\n        local_print(\"\\n\")", "target": "def eqBool(self, input: bool) -> bool:\n        return input", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000701", "source": "def test_constraints_schema_validation_error(constraint: str) -> None:\n    with pytest.raises(SchemaError, match=f\"'{constraint}' must be coercible to a Decimal instance\"):\n        SchemaValidator(cs.decimal_schema(**{constraint: 'bad_value'}))", "target": "def test_constraints_schema_validation_error(constraint: str) -> None:\n    with pytest.raises(SchemaError, match=f\"'{constraint}' must be coercible to a timedelta instance\"):\n        SchemaValidator(core_schema.timedelta_schema(**{constraint: 'bad_value'}))", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000702", "source": "def eager(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, target, dloss = args\n        loss = F.cross_entropy(x, target, reduction=\"none\")\n        return lambda: torch.autograd.grad(\n            loss, x, grad_outputs=dloss, retain_graph=True\n        )", "target": "def eager(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, w, dy = args\n        y = self.rms_norm_ref(x, w)\n        return lambda: torch.autograd.grad(\n            y, [x, w], grad_outputs=dy, retain_graph=True\n        )", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "000703", "source": "def test_merged_lastfailed_content_without_overlap(self) -> None:\n        last_failed_source = {\n            \"tools/tests/test_foo.py::test_num1\": True,\n            \"tools/tests/test_foo.py::test_num2\": True,\n            \"tools/tests/test_bar.py::test_num1\": True,\n        }\n        last_failed_dest = {\n            \"tools/tests/test_car.py::test_num1\": True,\n            \"tools/tests/test_car.py::test_num2\": True,\n        }\n        last_failed_merged = {\n            \"tools/tests/test_foo.py::test_num1\": True,\n            \"tools/tests/test_foo.py::test_num2\": True,\n            \"tools/tests/test_bar.py::test_num1\": True,\n            \"tools/tests/test_car.py::test_num1\": True,\n            \"tools/tests/test_car.py::test_num2\": True,\n        }\n        merged = _merged_lastfailed_content(last_failed_source, last_failed_dest)\n        self.assertEqual(merged, last_failed_merged)", "target": "def test_function_call_arguments(py_and_json: PyAndJson, input_value, expected):\n    def my_function(a, b, c):\n        return a + b + c\n    v = py_and_json(\n        {\n            'type': 'call',\n            'function': my_function,\n            'arguments_schema': {\n                'type': 'arguments',\n                'arguments_schema': [\n                    {'name': 'a', 'mode': 'positional_or_keyword', 'schema': {'type': 'int'}},\n                    {'name': 'b', 'mode': 'positional_or_keyword', 'schema': {'type': 'int'}},\n                    {'name': 'c', 'mode': 'positional_or_keyword', 'schema': {'type': 'int'}},\n                ],\n            },\n            'return_schema': {'type': 'int', 'le': 10},\n        }\n    )\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)) as exc_info:\n            v.validate_python(input_value)\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        assert v.validate_python(input_value) == expected", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|scikit-learn/scikit-learn", "data_split": "train", "example_id": "000704", "source": "def create_class_node(root: NamespaceNode, class_info,\n                      namespaces: Sequence[str]) -> ClassNode:\n    symbol_name = SymbolName.parse(class_info.full_original_name, namespaces)\n    scope = find_scope(root, symbol_name)\n    return create_class_node_in_scope(scope, symbol_name, class_info)", "target": "def benchmark(estimator, data):\n    gc.collect()\n    print(\"Benching %s\" % estimator)\n    t0 = time()\n    estimator.fit(data)\n    training_time = time() - t0\n    data_t = estimator.transform(data)\n    data_r = estimator.inverse_transform(data_t)\n    reconstruction_error = np.mean(np.abs(data - data_r))\n    return {\"time\": training_time, \"error\": reconstruction_error}", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000705", "source": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "target": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000706", "source": "def _get_types_namespace(self) -> NamespacesTuple:\n        raise NotImplementedError", "target": "def test_utcoffset(self):\n        dummy = self.DT\n        for h in [0, 1.5, 12]:\n            offset = h * HOUR\n            self.assertEqual(timedelta(seconds=offset), TzInfo(offset).utcoffset(dummy))\n            self.assertEqual(timedelta(seconds=-offset), TzInfo(-offset).utcoffset(dummy))\n        self.assertEqual(self.EST.utcoffset(''), timedelta(hours=-5))\n        self.assertEqual(self.EST.utcoffset(5), timedelta(hours=-5))", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000707", "source": "def typename(self) -> str:\n        return self.type_node.full_typename", "target": "def typename(self) -> str:\n        return \"None\"", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000708", "source": "def constrain(\n        self: _Pipeline[_InT, _NewOutT], constraint: annotated_types.MultipleOf\n    ) -> _Pipeline[_InT, _NewOutT]: ...", "target": "def constrain(\n        self: _Pipeline[_InT, _NewOutInterval], constraint: annotated_types.Interval\n    ) -> _Pipeline[_InT, _NewOutInterval]: ...", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000709", "source": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "target": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000710", "source": "def test_dataclass_slots(any_serializer):\n    @dataclasses.dataclass(slots=True)\n    class Foo:\n        a: int\n        b: str\n    foo = Foo(1, 'a')\n    assert any_serializer.to_python(foo) == IsStrictDict(a=1, b='a')\n    assert any_serializer.to_json(foo) == b'{\"a\":1,\"b\":\"a\"}'\n    @dataclasses.dataclass(slots=True)\n    class Foo2(Foo):\n        pass\n    foo2 = Foo2(2, 'b')\n    assert any_serializer.to_python(foo2) == IsStrictDict(a=2, b='b')\n    assert any_serializer.to_json(foo2) == b'{\"a\":2,\"b\":\"b\"}'", "target": "def json_schema(\n    schema: CoreSchema | None = None,\n    *,\n    ref: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> JsonSchema:\n    return _dict_not_none(type='json', schema=schema, ref=ref, metadata=metadata, serialization=serialization)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000711", "source": "def dynamic_rnn(\n        input: Tensor,\n        hidden: tuple[Tensor, Tensor],\n        w_ih: Tensor,\n        w_hh: Tensor,\n        b_ih: Tensor,\n        b_hh: Tensor,\n    ) -> tuple[Tensor, tuple[Tensor, Tensor]]:\n        hx, cx = hidden\n        outputs = []\n        inputs = torch.matmul(input, w_ih.t()).unbind(0)\n        hy, cy = hx[0], cx[0]\n        for seq_idx in range(len(inputs)):\n            hy, cy = premul_cell(inputs[seq_idx], (hy, cy), w_hh, b_ih, b_hh)\n            outputs += [hy]\n        return torch.stack(outputs), (hy.unsqueeze(0), cy.unsqueeze(0))", "target": "def dynamic_rnn(\n        input: Tensor,\n        hidden: tuple[Tensor, Tensor],\n        w_ih: Tensor,\n        w_hh: Tensor,\n        b_ih: Tensor,\n        b_hh: Tensor,\n    ) -> tuple[Tensor, tuple[Tensor, Tensor]]:\n        hx, cx = hidden\n        outputs = []\n        inpSize = input.size()\n        inpSize = input.size()\n        inputs = torch.mm(input.view(-1, inpSize[2]), w_ih.t()) + b_ih\n        inputs = inputs.view(inpSize[0], inpSize[1], -1).unbind(0)\n        hy, cy = hx[0], cx[0]\n        for seq_idx in range(len(inputs)):\n            hy, cy = premul_cell(inputs[seq_idx], (hy, cy), w_hh, b_hh)\n            outputs += [hy]\n        return torch.stack(outputs), (hy.unsqueeze(0), cy.unsqueeze(0))", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "000712", "source": "def my_function(input_value, info):\n        return input_value + 'x'", "target": "def compute_speedups(\n    operator, models, example_inputs, repeats, accuracy_checking=False, device=\"cuda\"\n):\n    expected = models[0](*example_inputs)\n    if accuracy_checking:\n        for model in models[1:]:\n            actual = model(*example_inputs)\n            try:\n                same(actual, expected, cos_similarity=True, equal_nan=True)\n            except AssertionError as e:\n                print(e)\n                print(f\"Accuracy check failed: {operator}\")\n                print((expected[0] - actual[0]).abs().max())\n    timings = np.zeros((repeats, len(models)), np.float64)\n    for rep in range(repeats):\n        with maybe_record_function(f\"rep_{rep}\"):\n            for m, model in enumerate(models):\n                with maybe_record_function(f\"model_{m}\"):\n                    if device == \"cuda\":\n                        model(*example_inputs)\n                        timings[rep, m] = benchmarker.benchmark_gpu(\n                            lambda: model(*example_inputs)\n                        )\n                    else:\n                        from torch._inductor.utils import timed\n                        timings[rep, m] = timed(model, example_inputs)\n    return np.median(timings, axis=0)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "000713", "source": "def test_gh_get_labels(\n        self,\n        mock_request_for_labels: Any,\n        mock_get_last_page_num_from_header: Any,\n    ) -> None:\n        res = gh_get_labels(\"mock_org\", \"mock_repo\")\n        mock_get_last_page_num_from_header.assert_called_once()\n        self.assertEqual(res, [\"foo\"] * 3)", "target": "def sequence_like(v: Any) -> bool:\n    return isinstance(v, (list, tuple, set, frozenset, GeneratorType, deque))", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000714", "source": "def typename(self) -> str:\n        return \"None\"", "target": "def typename(self) -> str:\n        return \"_typing.Any\"", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000715", "source": "def train(self, samples, responses):\n        self.model.train(samples, cv.ml.ROW_SAMPLE, responses)", "target": "def train(self, samples, responses):\n        self.model.train(samples, cv.ml.ROW_SAMPLE, responses)", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000716", "source": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "target": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "000717", "source": "def description(self):\n        return \"partitioner benchmark 1 input and 100 weights, mix of recompute and non-recompute ops\"", "target": "def parse_toml(config_file: str) -> dict[str, Any] | None:\n    if not config_file.endswith('.toml'):\n        return None\n    if sys.version_info >= (3, 11):\n        import tomllib as toml_\n    else:\n        try:\n            import tomli as toml_\n        except ImportError:\n            import warnings\n            warnings.warn('No TOML parser installed, cannot read configuration from `pyproject.toml`.', stacklevel=2)\n            return None\n    with open(config_file, 'rb') as rf:\n        return toml_.load(rf)", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|pytorch/pytorch", "data_split": "train", "example_id": "000718", "source": "def compute_bench(samples_range, features_range):\n    it = 0\n    results = defaultdict(lambda: [])\n    chunk = 100\n    max_it = len(samples_range) * len(features_range)\n    for n_samples in samples_range:\n        for n_features in features_range:\n            it += 1\n            print(\"==============================\")\n            print(\"Iteration %03d of %03d\" % (it, max_it))\n            print(\"==============================\")\n            print()\n            data = nr.randint(-50, 51, (n_samples, n_features))\n            print(\"K-Means\")\n            tstart = time()\n            kmeans = KMeans(init=\"k-means++\", n_clusters=10).fit(data)\n            delta = time() - tstart\n            print(\"Speed: %0.3fs\" % delta)\n            print(\"Inertia: %0.5f\" % kmeans.inertia_)\n            print()\n            results[\"kmeans_speed\"].append(delta)\n            results[\"kmeans_quality\"].append(kmeans.inertia_)\n            print(\"Fast K-Means\")\n            mbkmeans = MiniBatchKMeans(\n                init=\"k-means++\", n_clusters=10, batch_size=chunk\n            )\n            tstart = time()\n            mbkmeans.fit(data)\n            delta = time() - tstart\n            print(\"Speed: %0.3fs\" % delta)\n            print(\"Inertia: %f\" % mbkmeans.inertia_)\n            print()\n            print()\n            results[\"MiniBatchKMeans Speed\"].append(delta)\n            results[\"MiniBatchKMeans Quality\"].append(mbkmeans.inertia_)\n    return results", "target": "def _prepare_once(self):\n        self.a = torch.ones(1000, device=self.device())\n        self.b = torch.torch.ones(1000, device=self.device())", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000719", "source": "def test_validate_strings_forbid_extra_fn_override():\n    v = SchemaValidator(\n        core_schema.typed_dict_schema(\n            {\n                'f': core_schema.typed_dict_field(core_schema.int_schema()),\n            }\n        )\n    )\n    with pytest.raises(ValidationError, match='Extra inputs are not permitted'):\n        v.validate_strings({'f': '1', 'extra_field': '123'}, extra='forbid')", "target": "def f(input_value: Any, info: core_schema.ValidationInfo) -> Any:\n        assert info.field_name == 'x'\n        assert info.data == {}\n        assert isinstance(input_value, bytes)\n        return f'input: {input_value.decode()}'", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "000720", "source": "def get_memory_bytes(self, args, kwargs) -> int:\n        (x,) = args\n        M, N = x.shape\n        return 2 * M * N * x.dtype.itemsize", "target": "def test_wrong_return_type():\n    s = SchemaSerializer(\n        core_schema.any_schema(\n            serialization=core_schema.plain_serializer_function_ser_schema(\n                repr_function, info_arg=True, return_schema=core_schema.int_schema()\n            )\n        )\n    )\n    with pytest.warns(\n        UserWarning,\n        match=r\"Expected `int` - serialized value may not be as expected \\[input_value='123', input_type=str\\]\",\n    ):\n        assert s.to_python(123) == '123'\n    with pytest.warns(\n        UserWarning,\n        match=r\"Expected `int` - serialized value may not be as expected \\[input_value='123', input_type=str\\]\",\n    ):\n        assert s.to_python(123, mode='json') == '123'\n    with pytest.warns(\n        UserWarning,\n        match=r\"Expected `int` - serialized value may not be as expected \\[input_value='123', input_type=str\\]\",\n    ):\n        assert s.to_json(123) == b'\"123\"'", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000721", "source": "def validate_b(cls, v: bytes, info: core_schema.ValidationInfo) -> bytes:\n            assert v == b'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return b'hello world!'", "target": "def test_isinstance(py_and_json: PyAndJson):\n    def f(input_value, validator, info):\n        if 'error' in info.context:\n            raise ValueError('wrong')\n        return validator(input_value)\n    v = py_and_json(core_schema.with_info_wrap_validator_function(f, core_schema.str_schema()))\n    assert v.validate_python('foobar', None, {}) == 'foobar'\n    with pytest.raises(TypeError):\n        v.validate_test('foobar')\n    with pytest.raises(TypeError):\n        v.isinstance_test('foobar')\n    with pytest.raises(ValidationError, match=r'Value error, wrong \\[type=value_error,'):\n        v.validate_test('foobar', None, {'error'})\n    assert v.isinstance_test('foobar', None, {}) is True\n    with pytest.raises(TypeError):\n        v.isinstance_test('foobar')\n    assert v.isinstance_test('foobar', None, {'error'}) is False", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000722", "source": "def field_name(self) -> str | None:\n        return self._generate_schema.field_name_stack.get()", "target": "def test_format_when_used_json():\n    s = SchemaSerializer(core_schema.any_schema(serialization=core_schema.format_ser_schema('0.1f', when_used='json')))\n    assert s.to_python(42.12345) == 42.12345\n    assert s.to_python(None) is None\n    assert s.to_python(42.12345, mode='json') == '42.1'\n    assert s.to_json(42.12345) == b'\"42.1\"'\n    with pytest.raises(PydanticSerializationError, match=r'Error calling `format\\(value, \\'0.1f\\'\\)`: TypeError:'):\n        s.to_json(None)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000723", "source": "def compiled(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        (x,) = args\n        torch._dynamo.mark_dynamic(x, 0)\n        compiled_softmax = torch.compile(\n            lambda x: F.softmax(x, dim=-1), mode=self.compile_mode, fullgraph=True\n        )\n        return lambda: compiled_softmax(x)", "target": "def compiled(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, w = args\n        torch._dynamo.mark_dynamic(x, 0)\n        compiled_layernorm = torch.compile(\n            self.layernorm_ref, mode=self.compile_mode, fullgraph=True\n        )\n        return lambda: compiled_layernorm(x, w, eps=1e-6)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000724", "source": "def constrain(self: _Pipeline[_InT, _OutT], constraint: annotated_types.Predicate) -> _Pipeline[_InT, _OutT]: ...", "target": "def constrain(self: _Pipeline[_InT, _OutT], constraint: _In) -> _Pipeline[_InT, _OutT]: ...", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000725", "source": "def types_separator(self) -> str:\n        return \", \"", "target": "def types_separator(self) -> str:\n        if TypeNode.compatible_to_runtime_usage:\n            return \", \"\n        return \" | \"", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000726", "source": "def validate_b(\n            cls, v: bytes, nxt: core_schema.ValidatorFunctionWrapHandler, info: core_schema.ValidationInfo\n        ) -> str:\n            assert v == b'hello'\n            v = nxt(v)\n            assert v == 'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return 'hello world!'", "target": "def validate_b(\n            cls, v: bytes, nxt: core_schema.ValidatorFunctionWrapHandler, info: core_schema.ValidationInfo\n        ) -> bytes:\n            assert v == b'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return nxt(b'hello world!')", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "000727", "source": "def prepare_config(config: ConfigDict | dict[str, Any] | type[Any] | None) -> ConfigDict:\n    if config is None:\n        return ConfigDict()\n    if not isinstance(config, dict):\n        warnings.warn(DEPRECATION_MESSAGE, PydanticDeprecatedSince20, stacklevel=4)\n        config = {k: getattr(config, k) for k in dir(config) if not k.startswith('__')}\n    config_dict = cast(ConfigDict, config)\n    check_deprecated(config_dict)\n    return config_dict", "target": "def insert_python_fn_signature(soup, table, variants, symbol):\n    description = create_python_fn_description(soup, variants)\n    description['class'] = 'python_language'\n    soup = insert_or_replace(table, description, 'table', 'python_language')\n    return soup", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|scikit-learn/scikit-learn", "data_split": "train", "example_id": "000728", "source": "def test_dict_value_error(py_and_json: PyAndJson):\n    v = py_and_json({'type': 'dict', 'values_schema': {'type': 'int'}})\n    assert v.validate_test({'a': 2, 'b': '4'}) == {'a': 2, 'b': 4}\n    with pytest.raises(ValidationError, match='Input should be a valid integer') as exc_info:\n        v.validate_test({'a': 2, 'b': 'wrong'})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('b',),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'wrong',\n        }\n    ]", "target": "def rmse(a, b):\n    return np.sqrt(np.mean((a - b) ** 2))", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000729", "source": "def is_resolved(self) -> bool:\n        return self._ast_node is not None or self._module_name is not None", "target": "def is_resolved(self) -> bool:\n        return all(item.is_resolved for item in self.items)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000730", "source": "def get_memory_bytes(self, args, kwargs) -> int:\n        x, target, dloss = args\n        M, N = x.shape\n        return (\n            2 * M * N * x.dtype.itemsize\n            + M * target.dtype.itemsize\n            + M * dloss.dtype.itemsize\n        )", "target": "def get_memory_bytes(self, args, kwargs) -> int:\n        x, w, dy = args\n        M, N = x.shape\n        return (\n            2 * M * N * x.dtype.itemsize\n            + 2 * N * w.dtype.itemsize\n            + M * N * dy.dtype.itemsize\n        )", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|opencv/opencv", "data_split": "train", "example_id": "000731", "source": "def preprocess_simple(digits):\n    return np.float32(digits).reshape(-1, SZ*SZ) / 255.0", "target": "def test_multi_dict_arucodetector(self):\n        aruco_params = cv.aruco.DetectorParameters()\n        aruco_dicts = [\n                cv.aruco.getPredefinedDictionary(cv.aruco.DICT_4X4_250),\n                cv.aruco.getPredefinedDictionary(cv.aruco.DICT_5X5_250)\n            ]\n        aruco_detector = cv.aruco.ArucoDetector(aruco_dicts, aruco_params)\n        id = 2\n        marker_size = 100\n        offset = 10\n        img_marker1 = cv.aruco.generateImageMarker(aruco_dicts[0], id, marker_size, aruco_params.markerBorderBits)\n        img_marker1 = np.pad(img_marker1, pad_width=offset, mode='constant', constant_values=255)\n        img_marker2 = cv.aruco.generateImageMarker(aruco_dicts[1], id, marker_size, aruco_params.markerBorderBits)\n        img_marker2 = np.pad(img_marker2, pad_width=offset, mode='constant', constant_values=255)\n        img_markers = np.concatenate((img_marker1, img_marker2), axis=1)\n        corners, ids, rejected, dictIndices = aruco_detector.detectMarkersMultiDict(img_markers)\n        self.assertEqual(2, len(ids))\n        self.assertEqual(id, ids[0])\n        self.assertEqual(id, ids[1])\n        self.assertEqual(2, len(dictIndices))\n        self.assertEqual(0, dictIndices[0])\n        self.assertEqual(1, dictIndices[1])", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "000732", "source": "def test_gh_get_labels_raises_with_no_pages(\n        self,\n        mock_request_for_labels: Any,\n        get_last_page_num_from_header: Any,\n    ) -> None:\n        with self.assertRaises(AssertionError) as err:\n            gh_get_labels(\"foo\", \"bar\")\n        self.assertIn(\"number of pages of labels\", str(err.exception))", "target": "def ip_v4_address_validator(input_value: Any, /) -> IPv4Address:\n    if isinstance(input_value, IPv4Address):\n        return input_value\n    try:\n        return IPv4Address(input_value)\n    except ValueError:\n        raise PydanticCustomError('ip_v4_address', 'Input is not a valid IPv4 address')", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000733", "source": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "target": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000734", "source": "def to_options(self) -> dict[str, Any]:\n        return {\n            \"max_autotune\": self.max_autotune,\n            \"coordinate_descent_tuning\": self.coordinate_descent_tuning,\n            \"max_autotune_gemm_backends\": self.max_autotune_gemm_backends,\n        }", "target": "def to_options(self) -> dict[str, Any]:\n        return {\n            **super().to_options(),\n            \"cuda.cutlass_instantiation_level\": self.cutlass_instantiation_level,\n        }", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "000735", "source": "def have_git() -> bool:\n    try:\n        subprocess.check_output(['git', '--help'])\n        return True\n    except subprocess.CalledProcessError:\n        return False\n    except OSError:\n        return False", "target": "def test_age_gender_infer2_roi(self):\n            if not cv.dnn.DNN_TARGET_CPU in cv.dnn.getAvailableTargets(cv.dnn.DNN_BACKEND_INFERENCE_ENGINE):\n                return\n            root_path    = '/omz_intel_models/intel/age-gender-recognition-retail-0013/FP32/age-gender-recognition-retail-0013'\n            model_path   = self.find_file(root_path + '.xml',   [os.environ.get('OPENCV_DNN_TEST_DATA_PATH')], required=False)\n            weights_path = self.find_file(root_path + '.bin',   [os.environ.get('OPENCV_DNN_TEST_DATA_PATH')], required=False)\n            device_id    = 'CPU'\n            rois = [(10, 15, 62, 62), (23, 50, 62, 62), (14, 100, 62, 62), (80, 50, 62, 62)]\n            img_path = self.find_file('cv/face/david2.jpg', [os.environ.get('OPENCV_TEST_DATA_PATH')])\n            img = cv.imread(img_path)\n            dnn_age_list    = []\n            dnn_gender_list = []\n            for roi in rois:\n                age, gender = self.infer_reference_network(model_path,\n                                                           weights_path,\n                                                           self.make_roi(img, roi))\n                dnn_age_list.append(age)\n                dnn_gender_list.append(gender)\n            g_in   = cv.GMat()\n            g_rois = cv.GArrayT(cv.gapi.CV_RECT)\n            inputs = cv.GInferListInputs()\n            inputs.setInput('data', g_rois)\n            outputs  = cv.gapi.infer2(\"net\", g_in, inputs)\n            age_g    = outputs.at(\"age_conv3\")\n            gender_g = outputs.at(\"prob\")\n            comp = cv.GComputation(cv.GIn(g_in, g_rois), cv.GOut(age_g, gender_g))\n            pp = cv.gapi.ie.params(\"net\", model_path, weights_path, device_id)\n            gapi_age_list, gapi_gender_list = comp.apply(cv.gin(img, rois),\n                                                         args=cv.gapi.compile_args(cv.gapi.networks(pp)))\n            for gapi_age, gapi_gender, dnn_age, dnn_gender in zip(gapi_age_list,\n                                                                  gapi_gender_list,\n                                                                  dnn_age_list,\n                                                                  dnn_gender_list):\n                self.assertEqual(0.0, cv.norm(dnn_gender, gapi_gender, cv.NORM_INF))\n                self.assertEqual(0.0, cv.norm(dnn_age, gapi_age, cv.NORM_INF))", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000736", "source": "def test_dict():\n    v = SchemaValidator(core_schema.dict_schema(core_schema.int_schema(), core_schema.int_schema()))\n    assert v.validate_python({'1': 2, 3: '4'}) == snapshot({1: 2, 3: 4})\n    assert v.validate_python({'1': 2, 3: '4'}, allow_partial=True) == snapshot({1: 2, 3: 4})\n    assert v.validate_python(MyMapping({'1': 2, 3: '4'}), allow_partial=True) == snapshot({1: 2, 3: 4})\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python({'1': 2, 3: 'wrong'})\n    assert exc_info.value.errors(include_url=False) == snapshot(\n        [\n            {\n                'type': 'int_parsing',\n                'loc': (3,),\n                'msg': 'Input should be a valid integer, unable to parse string as an integer',\n                'input': 'wrong',\n            }\n        ]\n    )\n    assert v.validate_python({'1': 2, 3: 'x'}, allow_partial=True) == snapshot({1: 2})\n    assert v.validate_python(MyMapping({'1': 2, 3: 'x'}), allow_partial=True) == snapshot({1: 2})\n    assert v.validate_python({'1': 2, 3: 4, 5: '6', 7: 'x'}, allow_partial=True) == snapshot({1: 2, 3: 4, 5: 6})\n    with pytest.raises(ValidationError, match='Input should be a valid integer'):\n        v.validate_python({'1': 2, 3: 4, 5: 'x', 7: '8'})\n    with pytest.raises(ValidationError, match='Input should be a valid integer'):\n        v.validate_python({'1': 2, 3: 4, 5: 'x', 7: 'x'})\n    with pytest.raises(ValidationError, match='Input should be a valid integer'):\n        v.validate_python({'1': 2, 3: 4, 'x': 6})", "target": "def test_core_schema_import_missing():\n    with pytest.raises(AttributeError, match=\"module 'pydantic_core' has no attribute 'foobar'\"):\n        core_schema.foobar", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|scikit-learn/scikit-learn", "data_split": "train", "example_id": "000737", "source": "def eager(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        (x,) = args\n        return lambda: F.softmax(x, dim=-1)", "target": "def fit_transform(self, X, y=None, W=None, H=None):\n        W, H, self.n_iter = self._fit_transform(X, W=W, H=H, update_H=True)\n        self.components_ = H\n        return W", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000738", "source": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "target": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000739", "source": "def name(self) -> str:\n        prefix = f\"{self.category()}\"\n        return prefix", "target": "def name(self):\n        prefix = f\"{self.category()}_{self._name}_{self.backend()}\"\n        return prefix", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000740", "source": "def name(self):\n        prefix = f\"{self.category()}_{self._name}_{self.backend()}\"\n        if self.is_dynamic():\n            prefix += \"_dynamic\"\n        if self._is_gpu:\n            prefix += \"_gpu\"\n        if self._force_shape_pad:\n            prefix += \"_force_shape_pad\"\n        return prefix", "target": "def name(self):\n        prefix = f\"{self.category()}_{self._name}_{self.backend()}\"\n        return prefix", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000741", "source": "def test_model_field_wrap_validator() -> None:\n    class Model:\n        x: str\n    def f(input_value: Any, val: core_schema.ValidatorFunctionWrapHandler, info: core_schema.ValidationInfo) -> Any:\n        assert info.field_name == 'x'\n        assert info.data == {}\n        assert isinstance(input_value, bytes)\n        return f'input: {val(input_value)}'\n    v = SchemaValidator(\n        core_schema.model_schema(\n            Model,\n            core_schema.model_fields_schema(\n                {\n                    'x': core_schema.model_field(\n                        core_schema.with_info_wrap_validator_function(f, core_schema.str_schema())\n                    )\n                }\n            ),\n        )\n    )\n    assert v.validate_python({'x': b'foo'}).x == 'input: foo'", "target": "def multiple_of(self: _Pipeline[_InT, _NewOutDiv], multiple_of: _NewOutDiv) -> _Pipeline[_InT, _NewOutDiv]: ...", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "000742", "source": "def test_any_iter():\n    s = SchemaSerializer(core_schema.any_schema())\n    gen = s.to_python(gen_ok('a', b'b', 3))\n    assert repr(gen) == IsStr(regex=r'SerializationIterator\\(index=0, iterator=<generator object gen_ok at 0x\\w+>\\)')\n    assert str(gen) == repr(gen)\n    assert next(gen) == 'a'\n    assert repr(gen) == IsStr(regex=r'SerializationIterator\\(index=1, iterator=<generator object gen_ok at 0x\\w+>\\)')\n    assert next(gen) == b'b'\n    assert next(gen) == 3\n    with pytest.raises(StopIteration):\n        next(gen)", "target": "def test_parse_to_bool_not_convertible_extra(self):\n        for not_convertible in (np.array([False]), np.array([True])):\n            with self.assertRaises((TypeError, OverflowError),\n                                   msg=get_no_exception_msg(not_convertible)):\n                _ = cv.utils.dumpBool(not_convertible)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000743", "source": "def bench(name, fn):\n    torch._dynamo.reset()\n    inps = [[torch.randn(i) for _ in range(100)] for i in range(10, 101, 10)]\n    def run_fn():\n        for inp in inps:\n            fn(*inp)\n    start = time.perf_counter()\n    for _ in range(3):\n        run_fn()\n    end = time.perf_counter()\n    results = timeit.repeat(lambda: run_fn(), number=1000, repeat=10)\n    print(f\"{name} {np.median(results) * 1000:.1f}us (warmup={end - start:.1f}s)\")", "target": "def bench(name, fn, requires_grad):\n    torch._dynamo.reset()\n    x = torch.randn(1, requires_grad=requires_grad)\n    start = time.perf_counter()\n    for _ in range(3):\n        fn(x)\n    end = time.perf_counter()\n    results = timeit.repeat(lambda: fn(x), number=1000, repeat=1000)\n    print(f\"{name} {np.median(results) * 1000:.1f}us (warmup={end - start:.1f}s)\")", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000744", "source": "def eager(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, target = args\n        return lambda: F.cross_entropy(x, target, reduction=\"none\")", "target": "def eager(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, w, dy = args\n        y = self.layernorm_ref(x, w)\n        return lambda: torch.autograd.grad(\n            y, [x, w], grad_outputs=dy, retain_graph=True\n        )", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000745", "source": "def required_usage_imports(self) -> Generator[str, None, None]:\n        yield \"import typing as _typing\"", "target": "def required_usage_imports(self) -> Generator[str, None, None]:\n        if TypeNode.compatible_to_runtime_usage:\n            yield \"import typing as _typing\"\n        yield from super().required_usage_imports", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000746", "source": "def liger(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        from liger_kernel.transformers.cross_entropy import LigerCrossEntropyLoss\n        x, target = args\n        cross_entropy = LigerCrossEntropyLoss(reduction=\"none\")\n        return lambda: cross_entropy(x, target)", "target": "def liger(self, args, kwargs) -> Any:\n        from liger_kernel.transformers.layer_norm import LigerLayerNorm\n        x, w = args\n        M, N = x.shape\n        liger_layernorm = LigerLayerNorm(hidden_size=N, eps=1e-6).cuda()\n        liger_layernorm.weight.data.copy_(w)\n        liger_layernorm.bias.data.copy_(\n            torch.zeros(N, device=\"cuda\", dtype=torch.float32)\n        )\n        return lambda: liger_layernorm(x)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000747", "source": "def benchmark(self):\n        for M, N in self.get_shapes():\n            print(f\"Tensor dimensions: [{M}, {N}]\")\n            torch_dtype = cutlass_torch.dtype(cutlass.BFloat16)\n            x = 0.1 * torch.randn(M, N, device=\"cuda\", dtype=torch_dtype)\n            self.benchmark_single_shape((x,), setting=f\"shape: [{M}, {N}]\")", "target": "def benchmark(self):\n        for M, N in self.get_shapes():\n            print(f\"Tensor dimensions: [{M}, {N}]\")\n            torch_dtype = cutlass_torch.dtype(cutlass.BFloat16)\n            x = torch.randn(M, N, device=\"cuda\", dtype=torch_dtype, requires_grad=True)\n            w = torch.randn(N, device=\"cuda\", dtype=torch.float32, requires_grad=True)\n            dy = torch.randn(M, N, device=\"cuda\", dtype=torch_dtype)\n            self.benchmark_single_shape((x, w, dy), setting=f\"shape: [{M}, {N}]\")", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "train", "example_id": "000748", "source": "def _install_wheels(self, params: VllmTestParameters):\n        logger.info(\"Running vllm test with inputs: %s\", params)\n        if not pkg_exists(\"torch\"):\n            torch_p = f\"{str(params.torch_whls_path)}/{self.TORCH_WHL_PATH_REGEX}\"\n            pip_install_first_match(torch_p, self.TORCH_WHL_EXTRA)\n        torch_whls_path = [\n            f\"{str(params.torch_whls_path)}/{whl_path}\"\n            for whl_path in self.TORCH_ADDITIONAL_WHLS_REGEX\n        ]\n        for torch_whl in torch_whls_path:\n            pip_install_first_match(torch_whl)\n        logger.info(\"Done. Installed torch and other torch-related wheels \")\n        logger.info(\"Installing vllm wheels\")\n        vllm_whls_path = [\n            f\"{str(params.vllm_whls_path)}/{whl_path}\"\n            for whl_path in self.VLLM_TEST_WHLS_REGEX\n        ]\n        for vllm_whl in vllm_whls_path:\n            pip_install_first_match(vllm_whl)\n        logger.info(\"Done. Installed vllm wheels\")", "target": "def lstm_multilayer_creator(script=True, **kwargs):\n    input, hidden, params, _ = lstm_inputs(return_module=False, **kwargs)\n    inputs = [input, hidden, flatten_list(params)]\n    return ModelDef(\n        inputs=inputs,\n        params=flatten_list(params),\n        forward=lstm_factory_multilayer(lstm_cell, script),\n        backward_setup=lstm_backward_setup,\n        backward=simple_backward,\n    )", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "000749", "source": "def test_repr():\n    v = SchemaValidator(cs.union_schema(choices=[cs.int_schema(), cs.callable_schema()]))\n    assert v.isinstance_python(4) is True\n    assert v.isinstance_python(func) is True\n    assert v.isinstance_python('foo') is False\n    with pytest.raises(ValidationError, match=r'callable\\s+Input should be callable'):\n        v.validate_python('foo')", "target": "def find_function_node(root: NamespaceNode, function_symbol: SymbolName,\n                       create_missing_namespaces: bool = False) -> FunctionNode:\n    scope = find_scope(root, function_symbol, create_missing_namespaces)\n    if function_symbol.name not in scope.functions:\n        raise SymbolNotFoundError(\n            \"Can't find {} in its scope\".format(function_symbol)\n        )\n    return scope.functions[function_symbol.name]", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000750", "source": "def intersectionRate(s1, s2):\n    x1, y1, x2, y2 = s1\n    s1 = np.array([[x1, y1], [x2,y1], [x2, y2], [x1, y2]])\n    area, _intersection = cv.intersectConvexConvex(s1, np.array(s2))\n    return 2 * area / (cv.contourArea(s1) + cv.contourArea(np.array(s2)))", "target": "def intersectionRate(s1, s2):\n    area, _intersection = cv.intersectConvexConvex(np.array(s1), np.array(s2))\n    return 2 * area / (cv.contourArea(np.array(s1)) + cv.contourArea(np.array(s2)))", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000751", "source": "def full_typename(self) -> str:\n        return \"cv2.typing.\" + self.typename", "target": "def full_typename(self) -> str:\n        return self.type_format.format(self.types_separator.join(\n            item.full_typename for item in self\n        ))", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000752", "source": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "target": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000753", "source": "def area(self) -> int:\n            return self.width * self.height", "target": "def area(self) -> None:\n            self.side = 0.0", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000754", "source": "def types_separator(self) -> str:\n        return \"\"", "target": "def types_separator(self) -> str:\n        return \", \"", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pytorch/pytorch", "data_split": "train", "example_id": "000755", "source": "def getObjcTarget(self, target):\n        if target == \"Catalyst\":\n            return 'ios'\n        else:\n            return 'osx'", "target": "def jacrev(model, inp, v=None, strict=None):\n        argnums = tuple(range(len(inp)))\n        return ft.jacrev(model, argnums)(*inp)", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|pytorch/pytorch", "data_split": "train", "example_id": "000756", "source": "def get_meson_info():\n    build_path = Path(\"build/introspect\")\n    subprocess.check_call([\"meson\", \"setup\", build_path, \"--reconfigure\"])\n    json_out = subprocess.check_output(\n        [\"meson\", \"introspect\", build_path, \"--targets\"], text=True\n    )\n    target_list = json.loads(json_out)\n    meson_targets = [target for target in target_list if has_openmp_flags(target)]\n    return [get_canonical_name_meson(each, build_path) for each in meson_targets]", "target": "def get_memory_bytes(self, args, kwargs) -> int:\n        x, w, dy = args\n        M, N = x.shape\n        return (\n            2 * M * N * x.dtype.itemsize\n            + 2 * N * w.dtype.itemsize\n            + M * N * dy.dtype.itemsize\n        )", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000757", "source": "def test_typed_dict():\n    v = SchemaValidator(\n        core_schema.typed_dict_schema(\n            fields={\n                'field_a': core_schema.typed_dict_field(schema=core_schema.str_schema()),\n                'field_b': core_schema.typed_dict_field(schema=core_schema.int_schema()),\n            }\n        )\n    )\n    input_str = '{\"field_a\": \"abc\", \"field_b\": 1}'\n    assert v.validate_json(input_str) == {'field_a': 'abc', 'field_b': 1}\n    input_str = '{\"field_a\": \"a\", \"field_a\": \"b\", \"field_b\": 1}'\n    assert v.validate_json(input_str) == {'field_a': 'b', 'field_b': 1}\n    assert v.validate_json(input_str) == {'field_a': 'b', 'field_b': 1}", "target": "def test_typed_dict():\n    v = SchemaValidator(\n        core_schema.typed_dict_schema(\n            {\n                'field_a': core_schema.typed_dict_field(core_schema.int_schema()),\n                'field_b': core_schema.typed_dict_field(core_schema.date_schema()),\n            }\n        )\n    )\n    m2 = v.validate_strings({'field_a': '1', 'field_b': '2017-01-01'})\n    assert m2 == {'field_a': 1, 'field_b': date(2017, 1, 1)}\n    m2 = v.validate_strings({'field_a': '1', 'field_b': '2017-01-01'}, strict=True)\n    assert m2 == {'field_a': 1, 'field_b': date(2017, 1, 1)}", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "train", "example_id": "000758", "source": "def test_params_missing_torch_whls_raises(self, _is_path):\n        with tempfile.TemporaryDirectory() as td:\n            os.chdir(td)\n            with self.assertRaises(ValueError) as cm:\n                vllm_build.VllmBuildParameters(\n                    use_local_base_image=False,\n                    use_local_dockerfile=False,\n                )\n        err = cm.exception\n        self.assertIn(\"TORCH_WHEELS_PATH\", str(err))", "target": "def update(self, input_pos, k_val, v_val):\n        assert input_pos.shape[0] == k_val.shape[2]\n        k_out = self.k_cache\n        v_out = self.v_cache\n        k_out[:, :, input_pos] = k_val\n        v_out[:, :, input_pos] = v_val\n        return k_out, v_out", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000759", "source": "def test_allow_extra_wrong():\n    with pytest.raises(SchemaError, match='Invalid extra_behavior: `wrong`'):\n        SchemaValidator(\n            schema=core_schema.model_fields_schema(fields={}), config=CoreConfig(extra_fields_behavior='wrong')\n        )", "target": "def test_cyclic_recursion():\n    s = SchemaSerializer(\n        core_schema.definitions_schema(\n            core_schema.definition_reference_schema('Branch'),\n            [\n                core_schema.typed_dict_schema(\n                    {\n                        'name': core_schema.typed_dict_field(core_schema.str_schema()),\n                        'sub_branch': core_schema.typed_dict_field(\n                            core_schema.nullable_schema(core_schema.definition_reference_schema('Branch'))\n                        ),\n                    },\n                    ref='Branch',\n                )\n            ],\n        )\n    )\n    v = {'name': 'root'}\n    v['sub_branch'] = v\n    with pytest.raises(ValueError, match=r'Circular reference detected \\(id repeated\\)'):\n        s.to_python(v)\n    with pytest.raises(ValueError, match=r'Circular reference detected \\(id repeated\\)'):\n        s.to_python(v, mode='json')\n    with pytest.raises(ValueError, match=r'Circular reference detected \\(id repeated\\)'):\n        s.to_json(v)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000760", "source": "def types_separator(self) -> str:\n        return \", \"", "target": "def types_separator(self) -> str:\n        return \", \"", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000761", "source": "def forward(self, x, y):\n        return (x + y,)", "target": "def forward(self, x):\n                total = sum(t.item() for t in x)\n                return total // 2", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000762", "source": "def test_tuple_validate_iterator(variadic_item_index, items):\n    v = SchemaValidator(core_schema.tuple_schema(items_schema=items, variadic_item_index=variadic_item_index))\n    assert v.validate_python(x for x in [1, 2, '3']) == (1, 2, 3)", "target": "def ser_x(data: Model, v: Any, _) -> str:\n        assert data['x'] == 1_000\n        return f'{v:_}'", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000763", "source": "def eager(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, dy = args\n        y = F.softmax(x, dim=-1)\n        return lambda: torch.autograd.grad(y, x, grad_outputs=dy, retain_graph=True)", "target": "def eager(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, w, dy = args\n        y = self.rms_norm_ref(x, w)\n        return lambda: torch.autograd.grad(\n            y, [x, w], grad_outputs=dy, retain_graph=True\n        )", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000764", "source": "def constants(self) -> Dict[str, ConstantNode]:\n        return self._children[ASTNodeType.Constant]", "target": "def constants(self) -> Dict[str, ConstantNode]:\n        return self._children[ASTNodeType.Constant]", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "train", "example_id": "000765", "source": "def patterns_to_regex(allowed_patterns: list[str]) -> Any:\n    rc = \"(\"\n    for idx, pattern in enumerate(allowed_patterns):\n        if idx > 0:\n            rc += \"|\"\n        pattern_ = PeekableIterator(pattern)\n        assert not any(c in pattern for c in \"{}()[]\\\\\")\n        for c in pattern_:\n            if c == \".\":\n                rc += \"\\\\.\"\n            elif c == \"+\":\n                rc += \"\\\\+\"\n            elif c == \"*\":\n                if pattern_.peek() == \"*\":\n                    next(pattern_)\n                    rc += \".*\"\n                else:\n                    rc += \"[^/]*\"\n            else:\n                rc += c\n    rc += \")\"\n    return re.compile(rc)", "target": "def check_accuracy(self, args, kwargs) -> None:\n        res = {}\n        for backend in self.available_backends:\n            args_ref, kwargs_ref = self.clone_inputs(args, kwargs)\n            res[backend] = getattr(self, backend)(args_ref, kwargs_ref)()\n        gold = res[\"eager\"]\n        for backend in self.available_backends:\n            if backend == \"eager\":\n                continue\n            if backend == \"quack\":\n                res[backend] = res[backend].to(gold.dtype)\n            try:\n                torch.testing.assert_close(res[backend], gold)\n                print(\n                    f\"Accuracy check \\033[92m succeed\\033[0m for {backend} backend on {self.name} kernel\"\n                )\n            except Exception as e:\n                print(\n                    f\"Accuracy check \\033[91m failed\\033[0m for {backend} backend on {self.name} kernel. Error {e}\"\n                )", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "train", "example_id": "000766", "source": "def main(args):\n    options = parser.parse_args(args)\n    build = confu.Build.from_options(options)\n    build.export_cpath(\"include\", [\"clog.h\"])\n    with build.options(source_dir=\"src\", extra_include_dirs=\"src\"):\n        build.static_library(\"clog\", build.cc(\"clog.c\"))\n    with build.options(\n        source_dir=\"test\",\n        deps={(build, build.deps.googletest): all, \"log\": build.target.is_android},\n    ):\n        build.unittest(\"clog-test\", build.cxx(\"clog.cc\"))\n    return build", "target": "def test_local_image_exists_api_error_false(self):\n        mock_client = MagicMock()\n        mock_client.images.get.side_effect = derr.APIError(\"boom\", None)\n        ok = local_image_exists(\"broken:tag\", client=mock_client)\n        self.assertFalse(ok)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "000767", "source": "def get_arch_name() -> str:\n    if torch.cuda.is_available():\n        return torch.cuda.get_device_name()\n    else:\n        return platform.machine()", "target": "def is_core_schema_field(\n    schema: CoreSchemaOrField,\n) -> TypeGuard[CoreSchemaField]:\n    return schema['type'] in _CORE_SCHEMA_FIELD_TYPES", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pytorch/pytorch", "data_split": "train", "example_id": "000768", "source": "def preproc(ppp):\n                ppp.input().model().set_layout(Layout(\"NCHW\"))\n                ppp.input().tensor().set_element_type(Type.u8)                              \\\n                                    .set_spatial_static_shape(img1.shape[0], img2.shape[1]) \\\n                                    .set_layout(Layout(\"NHWC\"))\n                ppp.input().preprocess().resize(ResizeAlgorithm.RESIZE_LINEAR)", "target": "def _get_torch_wheel_path_arg(self, torch_whl_dir: Optional[Path]) -> str:\n        if not torch_whl_dir:\n            return \"\"\n        return f\"--build-arg TORCH_WHEELS_PATH={_VLLM_TEMP_FOLDER}\"", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000769", "source": "def test_extra_behavior_forbid(\n    config: Union[core_schema.CoreConfig, None],\n    schema_extra_behavior_kw: dict[str, Any],\n    validate_fn_extra_kw: Union[ExtraBehavior, None],\n):\n    @dataclasses.dataclass\n    class MyModel:\n        f: str\n    v = SchemaValidator(\n        core_schema.dataclass_schema(\n            MyModel,\n            core_schema.dataclass_args_schema(\n                'MyModel', [core_schema.dataclass_field('f', core_schema.str_schema())], **schema_extra_behavior_kw\n            ),\n            ['f'],\n        ),\n        config=config,\n    )\n    m: MyModel = v.validate_python({'f': 'x'}, extra=validate_fn_extra_kw)\n    assert m.f == 'x'\n    v.validate_assignment(m, 'f', 'y', extra=validate_fn_extra_kw)\n    assert m.f == 'y'\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_assignment(m, 'not_f', 'xyz', extra=validate_fn_extra_kw)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'no_such_attribute',\n            'loc': ('not_f',),\n            'msg': \"Object has no attribute 'not_f'\",\n            'input': 'xyz',\n            'ctx': {'attribute': 'not_f'},\n        }\n    ]\n    assert not hasattr(m, 'not_f')", "target": "def test_extra_behavior_forbid(\n    config: Union[core_schema.CoreConfig, None],\n    schema_extra_behavior_kw: dict[str, Any],\n    validate_fn_extra_kw: Union[ExtraBehavior, None],\n):\n    v = SchemaValidator(\n        core_schema.typed_dict_schema(\n            {'f': core_schema.typed_dict_field(core_schema.str_schema())},\n            **schema_extra_behavior_kw,\n            config=config,\n        )\n    )\n    m: dict[str, Any] = v.validate_python({'f': 'x'}, extra=validate_fn_extra_kw)\n    assert m == {'f': 'x'}\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python({'f': 'x', 'extra_field': 123}, extra=validate_fn_extra_kw)\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'extra_forbidden', 'loc': ('extra_field',), 'msg': 'Extra inputs are not permitted', 'input': 123}\n    ]", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "000770", "source": "def test_include_dict(schema_func, seq_f):\n    v = SchemaSerializer(\n        schema_func(core_schema.any_schema(), serialization=core_schema.filter_seq_schema(include={1, 3, 5}))\n    )\n    assert v.to_python(seq_f(0, 1, 2, 3, 4)) == seq_f(1, 3)\n    assert v.to_python(seq_f('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h')) == seq_f('b', 'd', 'f')\n    assert v.to_python(seq_f(0, 1, 2, 3, 4), include={2: None}) == seq_f(1, 2, 3)\n    assert v.to_python(seq_f(0, 1, 2, 3, 4), include={2: {1, 2}}) == seq_f(1, 2, 3)\n    assert v.to_python(seq_f(0, 1, 2, 3, 4), include={2}) == seq_f(1, 2, 3)", "target": "def pytorch_lstm_creator(**kwargs):\n    input, hidden, _, module = lstm_inputs(return_module=True, **kwargs)\n    return ModelDef(\n        inputs=[input, hidden],\n        params=flatten_list(module.all_weights),\n        forward=module,\n        backward_setup=lstm_backward_setup,\n        backward=simple_backward,\n    )", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000771", "source": "def test_dataclass_validate_assignment():\n    schema = core_schema.dataclass_schema(\n        FooDataclass,\n        core_schema.dataclass_args_schema(\n            'FooDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), kw_only=False),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), kw_only=False),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 'hello', 'b': 'True'})\n    assert dataclasses.asdict(foo) == {'a': 'hello', 'b': True}\n    v.validate_assignment(foo, 'a', b'world')\n    assert dataclasses.asdict(foo) == {'a': 'world', 'b': True}\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_assignment(foo, 'a', 123)\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'string_type', 'loc': ('a',), 'msg': 'Input should be a valid string', 'input': 123}\n    ]\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_assignment(foo, 'c', '123')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'no_such_attribute',\n            'loc': ('c',),\n            'msg': \"Object has no attribute 'c'\",\n            'input': '123',\n            'ctx': {'attribute': 'c'},\n        }\n    ]\n    assert not hasattr(foo, 'c')\n    with pytest.raises(AttributeError, match=\"'str' object has no attribute 'a'\"):\n        v.validate_assignment('field_a', 'c', 123)", "target": "def f(input_value, validator, info):\n        return validator(input_value) + f'| context: {info.context}'", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "000772", "source": "def test_dataclass_slots(any_serializer):\n    @dataclasses.dataclass(slots=True)\n    class Foo:\n        a: int\n        b: str\n    foo = Foo(1, 'a')\n    assert any_serializer.to_python(foo) == IsStrictDict(a=1, b='a')\n    assert any_serializer.to_json(foo) == b'{\"a\":1,\"b\":\"a\"}'\n    @dataclasses.dataclass(slots=True)\n    class Foo2(Foo):\n        pass\n    foo2 = Foo2(2, 'b')\n    assert any_serializer.to_python(foo2) == IsStrictDict(a=2, b='b')\n    assert any_serializer.to_json(foo2) == b'{\"a\":2,\"b\":\"b\"}'", "target": "def move_tensor(maybe_tensor):\n        if torch.is_tensor(maybe_tensor):\n            return maybe_tensor.to(dev_rank)\n        return maybe_tensor", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "000773", "source": "def get_output(self, input_blob):\n        self.net.setInput(input_blob, self.in_blob_name)\n        return self.net.forward(self.out_blob_name)", "target": "def get_config_update(self, name: str, arg: Expression, lax_extra: bool = False) -> ModelConfigData | None:\n        if name not in self.tracked_config_fields:\n            return None\n        if name == 'extra':\n            if isinstance(arg, StrExpr):\n                forbid_extra = arg.value == 'forbid'\n            elif isinstance(arg, MemberExpr):\n                forbid_extra = arg.name == 'forbid'\n            else:\n                if not lax_extra:\n                    error_invalid_config_value(name, self._api, arg)\n                return None\n            return ModelConfigData(forbid_extra=forbid_extra)\n        if name == 'alias_generator':\n            has_alias_generator = True\n            if isinstance(arg, NameExpr) and arg.fullname == 'builtins.None':\n                has_alias_generator = False\n            return ModelConfigData(has_alias_generator=has_alias_generator)\n        if isinstance(arg, NameExpr) and arg.fullname in ('builtins.True', 'builtins.False'):\n            return ModelConfigData(**{name: arg.fullname == 'builtins.True'})\n        error_invalid_config_value(name, self._api, arg)\n        return None", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000774", "source": "def area(self) -> bytes:\n            a = self.width * self.height\n            return b'%d' % a", "target": "def area(self) -> float:\n            return self.side**2", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000775", "source": "def val_func(v: Any) -> Any:\n                    predicate_satisfied = annotation.func(v)\n                    if not predicate_satisfied:\n                        raise PydanticCustomError(\n                            'predicate_failed',\n                            f'Predicate {predicate_name}failed',\n                        )\n                    return v", "target": "def val_func(v: Any) -> Any:\n                    predicate_satisfied = annotation.func(v)\n                    if predicate_satisfied:\n                        raise PydanticCustomError(\n                            'not_operation_failed',\n                            f'Not of {predicate_name}failed',\n                        )\n                    return v", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000776", "source": "def test_any_config_timedelta_float():\n    s = SchemaSerializer(core_schema.any_schema(), config={'ser_json_timedelta': 'float'})\n    h2 = timedelta(hours=2)\n    assert s.to_python(h2) == h2\n    assert s.to_python(h2, mode='json') == 7200.0\n    assert s.to_json(h2) == b'7200.0'\n    assert s.to_python({h2: 'foo'}) == {h2: 'foo'}\n    assert s.to_python({h2: 'foo'}, mode='json') == {'7200': 'foo'}\n    assert s.to_json({h2: 'foo'}) == b'{\"7200\":\"foo\"}'", "target": "def test_extra_behavior_allow_keys_validation() -> None:\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            {}, extra_behavior='allow', extras_keys_schema=core_schema.str_schema(max_length=3)\n        )\n    )\n    m, model_extra, fields_set = v.validate_python({'ext': 123})\n    assert m == {}\n    assert model_extra == {'ext': 123}\n    assert fields_set == {'ext'}\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python({'extra_too_long': 123})\n    assert exc_info.value.errors()[0]['type'] == 'string_too_long'", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "000777", "source": "def test_none_fallback(schema_type):\n    s = SchemaSerializer({'type': schema_type})\n    assert s.to_python(None) is None\n    assert s.to_python(None, mode='json') is None\n    assert s.to_json(None) == b'null'", "target": "def types_separator(self) -> str:\n        if TypeNode.compatible_to_runtime_usage:\n            return \", \"\n        return \" | \"", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000778", "source": "def my_function(a, b, c):\n        return a + b + c", "target": "def my_function(input_value, info):\n        return input_value + 'x'", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "000779", "source": "def make_estimator(self, params):\n        (representation,) = params\n        max_iter = 60 if representation == \"dense\" else 300\n        estimator = SGDRegressor(max_iter=max_iter, tol=None, random_state=0)\n        return estimator", "target": "def make_estimator(self, params):\n        (representation,) = params\n        n_estimators = 100 if Benchmark.data_size == \"large\" else 10\n        estimator = GradientBoostingClassifier(\n            n_estimators=n_estimators,\n            max_features=\"log2\",\n            subsample=0.5,\n            random_state=0,\n        )\n        return estimator", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|scikit-learn/scikit-learn", "data_split": "train", "example_id": "000780", "source": "def expand_typevar_from_subtype(self, sub_type: TypeInfo, api: SemanticAnalyzerPluginInterface) -> None:\n        if self.type is not None:\n            with state.strict_optional_set(api.options.strict_optional):\n                self.type = map_type_from_supertype(self.type, sub_type, self.info)", "target": "def fit(est, data_train, target_train, libname, **fit_params):\n    print(f\"Fitting a {libname} model...\")\n    tic = time()\n    est.fit(data_train, target_train, **fit_params)\n    toc = time()\n    print(f\"fitted in {toc - tic:.3f}s\")", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000781", "source": "def test_union_bool_int(input_value, expected_value):\n    v = SchemaValidator(core_schema.union_schema(choices=[core_schema.bool_schema(), core_schema.int_schema()]))\n    assert v.validate_python(input_value) == expected_value", "target": "def pydantic_encoder(obj: Any) -> Any:\n    warnings.warn(\n        '`pydantic_encoder` is deprecated, use `pydantic_core.to_jsonable_python` instead.',\n        category=PydanticDeprecatedSince20,\n        stacklevel=2,\n    )\n    from dataclasses import asdict, is_dataclass\n    BaseModel = import_cached_base_model()\n    if isinstance(obj, BaseModel):\n        return obj.model_dump()\n    elif is_dataclass(obj):\n        return asdict(obj)\n    for base in obj.__class__.__mro__[:-1]:\n        try:\n            encoder = ENCODERS_BY_TYPE[base]\n        except KeyError:\n            continue\n        return encoder(obj)\n    else:\n        raise TypeError(f\"Object of type '{obj.__class__.__name__}' is not JSON serializable\")", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pytorch/pytorch", "data_split": "train", "example_id": "000782", "source": "def add_constant(self, name: str, value: str) -> ConstantNode:\n        return self._add_child(ConstantNode, name, value=value)", "target": "def patch_init_py(\n    path: Path, *, version: str, expected_version: Optional[str] = None\n) -> None:\n    if not expected_version:\n        expected_version = read_triton_version()\n    with open(path) as f:\n        orig = f.read()\n    orig = check_and_replace(\n        orig, f\"__version__ = '{expected_version}'\", f'__version__ = \"{version}\"'\n    )\n    with open(path, \"w\") as f:\n        f.write(orig)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000783", "source": "def test_only_validate_by_name(py_and_json) -> None:\n    schema = core_schema.dataclass_schema(\n        BasicDataclass,\n        core_schema.dataclass_args_schema(\n            'BasicDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), validation_alias='FieldA'),\n            ],\n        ),\n        ['a'],\n        config=core_schema.CoreConfig(validate_by_name=True, validate_by_alias=False),\n    )\n    v = py_and_json(schema)\n    assert v.validate_test({'a': 'hello'}) == BasicDataclass(a='hello')\n    with pytest.raises(ValidationError, match=r'a\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'FieldA': 'hello'})", "target": "def test_only_validate_by_name(py_and_json) -> None:\n    v = py_and_json(\n        {\n            'type': 'typed-dict',\n            'fields': {\n                'field_a': {'validation_alias': 'FieldA', 'type': 'typed-dict-field', 'schema': {'type': 'int'}}\n            },\n            'config': {'validate_by_name': True, 'validate_by_alias': False},\n        }\n    )\n    assert v.validate_test({'field_a': '123'}) == {'field_a': 123}\n    with pytest.raises(ValidationError, match=r'field_a\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'FieldA': '123'})", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "000784", "source": "def test_by_alias_and_name_config_interaction(\n    config_by_alias: Union[bool, None],\n    config_by_name: Union[bool, None],\n    runtime_by_alias: Union[bool, None],\n    runtime_by_name: Union[bool, None],\n) -> None:\n    if config_by_alias is False and config_by_name is False and runtime_by_alias is False and runtime_by_name is False:\n        pytest.skip(\"Can't have both by_alias and by_name as effectively False\")\n    core_config = {\n        **({'validate_by_alias': config_by_alias} if config_by_alias is not None else {}),\n        **({'validate_by_name': config_by_name} if config_by_name is not None else {}),\n    }\n    schema = core_schema.typed_dict_schema(\n        fields={\n            'my_field': core_schema.typed_dict_field(schema=core_schema.int_schema(), validation_alias='my_alias'),\n        },\n        config=core_schema.CoreConfig(**core_config),\n    )\n    s = SchemaValidator(schema)\n    alias_allowed = next(x for x in (runtime_by_alias, config_by_alias, True) if x is not None)\n    name_allowed = next(x for x in (runtime_by_name, config_by_name, False) if x is not None)\n    if alias_allowed:\n        assert s.validate_python({'my_alias': 1}, by_alias=runtime_by_alias, by_name=runtime_by_name) == {'my_field': 1}\n    if name_allowed:\n        assert s.validate_python({'my_field': 1}, by_alias=runtime_by_alias, by_name=runtime_by_name) == {'my_field': 1}", "target": "def hessian_revrev(model, inp, v=None, strict=None):\n        argnums = tuple(range(len(inp)))\n        return ft.jacrev(ft.jacrev(model, argnums=argnums), argnums=argnums)(*inp)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000785", "source": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "target": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000786", "source": "def type_format(self) -> str:\n        if TypeNode.compatible_to_runtime_usage:\n            return \"_typing.Union[{}]\"\n        return \"{}\"", "target": "def type_format(self) -> str:\n        if TypeNode.compatible_to_runtime_usage:\n            return \"_typing.Optional[{}]\"\n        return \"{} | None\"", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000787", "source": "def test_pandas():\n    v = SchemaSerializer(core_schema.timedelta_schema())\n    d = pandas.Timestamp('2023-01-01T02:00:00Z') - pandas.Timestamp('2023-01-01T00:00:00Z')\n    assert v.to_python(d) == d\n    assert v.to_python(d, mode='json') == 'PT2H'\n    assert v.to_json(d) == b'\"PT2H\"'", "target": "def test_pandas():\n    v = SchemaValidator(core_schema.timedelta_schema(ge=timedelta(hours=2)))\n    two_hours = pandas.Timestamp('2023-01-01T02:00:00Z') - pandas.Timestamp('2023-01-01T00:00:00Z')\n    assert v.validate_python(two_hours) == two_hours\n    assert v.validate_python(two_hours.to_pytimedelta()) == two_hours\n    one_55 = pandas.Timestamp('2023-01-01T01:55:00Z') - pandas.Timestamp('2023-01-01T00:00:00Z')\n    msg = r'Input should be greater than or equal to 2 hours'\n    with pytest.raises(ValidationError, match=msg):\n        v.validate_python(one_55)\n    with pytest.raises(ValidationError, match=msg):\n        v.validate_python(one_55.to_pytimedelta())", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000788", "source": "def full_typename(self) -> str:\n        return self.type_format.format(self.types_separator.join(\n            item.full_typename for item in self\n        ))", "target": "def full_typename(self) -> str:\n        return '_typing.Callable[[{}], {}]'.format(\n            ', '.join(arg.full_typename for arg in self.arg_types),\n            self.ret_type.full_typename\n        )", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "000789", "source": "def make_estimator(self, params):\n        (representation,) = params\n        max_iter = 60 if representation == \"dense\" else 300\n        estimator = SGDRegressor(max_iter=max_iter, tol=None, random_state=0)\n        return estimator", "target": "def make_estimator(self, params):\n        representation, n_jobs = params\n        n_estimators = 500 if Benchmark.data_size == \"large\" else 100\n        estimator = RandomForestClassifier(\n            n_estimators=n_estimators,\n            min_samples_split=10,\n            max_features=\"log2\",\n            n_jobs=n_jobs,\n            random_state=0,\n        )\n        return estimator", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000790", "source": "def constants(self) -> Dict[str, ConstantNode]:\n        return self._children[ASTNodeType.Constant]", "target": "def constants(self) -> Dict[str, ConstantNode]:\n        return self._children[ASTNodeType.Constant]", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000791", "source": "def eager(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, target = args\n        return lambda: F.cross_entropy(x, target, reduction=\"none\")", "target": "def eager(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, dy = args\n        y = F.softmax(x, dim=-1)\n        return lambda: torch.autograd.grad(y, x, grad_outputs=dy, retain_graph=True)", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000792", "source": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "target": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "000793", "source": "def lstm_cell(\n    input: Tensor,\n    hidden: tuple[Tensor, Tensor],\n    w_ih: Tensor,\n    w_hh: Tensor,\n    b_ih: Tensor,\n    b_hh: Tensor,\n) -> tuple[Tensor, Tensor]:\n    hx, cx = hidden\n    gates = torch.mm(input, w_ih.t()) + torch.mm(hx, w_hh.t()) + b_ih + b_hh\n    ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)\n    ingate = torch.sigmoid(ingate)\n    forgetgate = torch.sigmoid(forgetgate)\n    cellgate = torch.tanh(cellgate)\n    outgate = torch.sigmoid(outgate)\n    cy = (forgetgate * cx) + (ingate * cellgate)\n    hy = outgate * torch.tanh(cy)\n    return hy, cy", "target": "def _get_at_to_constraint_map() -> dict[type, str]:\n    import annotated_types as at\n    return {\n        at.Gt: 'gt',\n        at.Ge: 'ge',\n        at.Lt: 'lt',\n        at.Le: 'le',\n        at.MultipleOf: 'multiple_of',\n        at.MinLen: 'min_length',\n        at.MaxLen: 'max_length',\n    }", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "train", "example_id": "000794", "source": "def main():\n    parser = argparse.ArgumentParser(\n        \"Main script to compare results from the benchmarks\"\n    )\n    parser.add_argument(\n        \"--before\",\n        type=str,\n        default=\"before.txt\",\n        help=\"Text file containing the times to use as base\",\n    )\n    parser.add_argument(\n        \"--after\",\n        type=str,\n        default=\"after.txt\",\n        help=\"Text file containing the times to use as new version\",\n    )\n    parser.add_argument(\n        \"--output\", type=str, default=\"\", help=\"Text file where to write the output\"\n    )\n    args = parser.parse_args()\n    with open(args.before) as f:\n        content = f.read()\n    res_before = from_markdown_table(content)\n    with open(args.after) as f:\n        content = f.read()\n    res_after = from_markdown_table(content)\n    diff = defaultdict(defaultdict)\n    for model in res_before:\n        for task in res_before[model]:\n            mean_before, var_before = res_before[model][task]\n            if task not in res_after[model]:\n                diff[model][task] = (None, mean_before, var_before, None, None)\n            else:\n                mean_after, var_after = res_after[model][task]\n                diff[model][task] = (\n                    mean_before / mean_after,\n                    mean_before,\n                    var_before,\n                    mean_after,\n                    var_after,\n                )\n    for model in res_after:\n        for task in res_after[model]:\n            if task not in res_before[model]:\n                mean_after, var_after = res_after[model][task]\n                diff[model][task] = (None, None, None, mean_after, var_after)\n    header = (\n        \"model\",\n        \"task\",\n        \"speedup\",\n        \"mean (before)\",\n        \"var (before)\",\n        \"mean (after)\",\n        \"var (after)\",\n    )\n    out = to_markdown_table(diff, header=header)\n    print(out)\n    if args.output:\n        with open(args.output, \"w\") as f:\n            f.write(out)", "target": "def get_memory_bytes(self, args, kwargs) -> int:\n        x, w, dy = args\n        M, N = x.shape\n        return 3 * M * N * x.dtype.itemsize + 2 * N * w.dtype.itemsize", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|opencv/opencv", "data_split": "train", "example_id": "000795", "source": "def str_(cls, ctype_name: Optional[str] = None,\n             required_modules: Tuple[str, ...] = ()):\n        if ctype_name is None:\n            ctype_name = \"string\"\n        return PrimitiveTypeNode(ctype_name, \"str\", required_modules=required_modules)", "target": "def test_20968(self):\n        pixel = np.uint8([[[40, 50, 200]]])\n        _ = cv.cvtColor(pixel, cv.COLOR_RGB2BGR)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|opencv/opencv", "data_split": "train", "example_id": "000796", "source": "def forward(self, x: Tensor) -> Tensor:\n        output = self._norm(x.float()).type_as(x)\n        return output * self.weight", "target": "def next(self):\n        if self.i < len(self.segm_files):\n            segm_file = self.segm_files[self.i]\n            segm = cv.imread(segm_file, cv.IMREAD_COLOR)[:, :, ::-1]\n            segm = cv.resize(segm, (1024, 512), interpolation=cv.INTER_NEAREST)\n            img_file = self.rreplace(self.img_dir + segm_file[len(self.segm_dir):], 'gtFine_color', 'leftImg8bit')\n            assert os.path.exists(img_file)\n            img = cv.imread(img_file, cv.IMREAD_COLOR)[:, :, ::-1]\n            img = cv.resize(img, (1024, 512))\n            self.i += 1\n            gt = self.color_to_gt(segm, self.colors)\n            img = self.data_prepoc.process(img)\n            return img, gt\n        else:\n            self.i = 0\n            raise StopIteration", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "000797", "source": "def torchbench_main():\n    original_dir = setup_torchbench_cwd()\n    logging.basicConfig(level=logging.WARNING)\n    warnings.filterwarnings(\"ignore\")\n    main(TorchBenchmarkRunner(), original_dir)", "target": "def test_to_string_when_used_always():\n    s = SchemaSerializer(core_schema.any_schema(serialization=core_schema.format_ser_schema('', when_used='always')))\n    assert s.to_python(123) == '123'\n    assert s.to_python(None) == 'None'\n    assert s.to_python(123, mode='json') == '123'\n    assert s.to_python(None, mode='json') == 'None'\n    assert s.to_json(None) == b'\"None\"'\n    assert s.to_json(123) == b'\"123\"'", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "000798", "source": "def test_date_format(benchmark):\n    serializer = SchemaSerializer(\n        {'type': 'any', 'serialization': {'type': 'format', 'formatting_string': '%Y-%m-%d', 'when_used': 'always'}}\n    )\n    d = date(2022, 11, 20)\n    assert serializer.to_python(d) == '2022-11-20'\n    benchmark(serializer.to_python, d)", "target": "def filter_ciflow_tags(tags: set[str]) -> list[str]:\n    \"Return sorted list of ciflow tags\"\n    return sorted(\n        tag[:-2] for tag in tags if tag.startswith(\"ciflow/\") and tag.endswith(\"/*\")\n    )", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "000799", "source": "def get_enum_module_and_export_name(enum_node: EnumerationNode) -> Tuple[str, str]:\n    enum_export_name = enum_node.export_name\n    def update_full_export_name(class_node: ClassNode) -> None:\n        nonlocal enum_export_name\n        enum_export_name = class_node.export_name + \"_\" + enum_export_name\n    namespace_node = get_enclosing_namespace(enum_node,\n                                             update_full_export_name)\n    return enum_export_name, namespace_node.full_export_name", "target": "def callable_schema(\n    *, ref: str | None = None, metadata: dict[str, Any] | None = None, serialization: SerSchema | None = None\n) -> CallableSchema:\n    return _dict_not_none(type='callable', ref=ref, metadata=metadata, serialization=serialization)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "train", "example_id": "000800", "source": "def rnn_tanh_cell(input, hidden, w_ih, w_hh, b_ih, b_hh):\n    igates = torch.mm(input, w_ih.t()) + b_ih\n    hgates = torch.mm(hidden, w_hh.t()) + b_hh\n    return torch.tanh(igates + hgates)", "target": "def copy_and_patch_library(\n    src_path: str,\n    folder: str,\n    use_nvidia_pypi_libs: bool = False,\n    desired_cuda: str = \"\",\n) -> None:\n    if os.path.exists(src_path):\n        lib_name = os.path.basename(src_path)\n        shutil.copy2(src_path, f\"{folder}/tmp/torch/lib/{lib_name}\")\n        patch_library_rpath(folder, lib_name, use_nvidia_pypi_libs, desired_cuda)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "000801", "source": "def test_filter_list_of_dicts():\n    s = SchemaSerializer(core_schema.list_schema(core_schema.dict_schema()))\n    v = [{'a': 1, 'b': 2}, {'a': 3, 'b': 4}]\n    assert s.to_python(v) == v\n    assert s.to_python(v, exclude={0: {'a'}}) == [{'b': 2}, {'a': 3, 'b': 4}]\n    assert s.to_python(v, exclude={0: {'__all__'}}) == [{}, {'a': 3, 'b': 4}]\n    assert s.to_python(v, exclude={'__all__': {'a'}}) == [{'b': 2}, {'b': 4}]\n    assert s.to_json(v) == b'[{\"a\":1,\"b\":2},{\"a\":3,\"b\":4}]'\n    assert s.to_json(v, exclude={0: {'a'}}) == b'[{\"b\":2},{\"a\":3,\"b\":4}]'\n    assert s.to_json(v, exclude={0: {'__all__'}}) == b'[{},{\"a\":3,\"b\":4}]'\n    assert s.to_json(v, exclude={'__all__': {'a'}}) == b'[{\"b\":2},{\"b\":4}]'\n    assert s.to_python(v, include={0: {'a'}, 1: None}) == [{'a': 1}, {'a': 3, 'b': 4}]\n    assert s.to_python(v, include={'__all__': {'a'}}) == [{'a': 1}, {'a': 3}]\n    assert s.to_json(v, include={0: {'a'}, 1: None}) == b'[{\"a\":1},{\"a\":3,\"b\":4}]'\n    assert s.to_json(v, include={'__all__': {'a'}}) == b'[{\"a\":1},{\"a\":3}]'", "target": "def run_mixtral_8x7b_autoquant_v2(device: str = \"cuda\"):\n    model = GPTModelConfig(\n        \"Mixtral-8x7B-v0.1\",\n        MixtralMoE,\n        \"autoquant_v2\",\n        None,\n        175,\n        1130,\n        133,\n        6,\n    )\n    token_per_sec, memory_bandwidth, compilation_time = run_experiment(\n        model, device=device\n    )\n    return [\n        Experiment(\n            model.name,\n            \"token_per_sec\",\n            model.token_per_sec,\n            f\"{token_per_sec:.02f}\",\n            model.mode,\n            device,\n            get_arch_name(),\n            True,\n        ),\n        Experiment(\n            model.name,\n            \"memory_bandwidth(GB/s)\",\n            model.memory_bandwidth,\n            f\"{memory_bandwidth:.02f}\",\n            model.mode,\n            device,\n            get_arch_name(),\n            True,\n        ),\n        Experiment(\n            model.name,\n            \"compilation_time(s)\",\n            model.compilation_time,\n            f\"{compilation_time:.02f}\",\n            model.mode,\n            device,\n            get_arch_name(),\n            True,\n        ),\n    ]", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000802", "source": "def run(img, sc, dtype):\n            return img + np.array(sc, dtype=np.uint8)[:-1]", "target": "def run(img0, img1):\n                    return img0 + img1", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000803", "source": "def load_tests(loader, tests, pattern):\n    cwd = os.getcwd()\n    config_file = 'opencv_apps_python_tests.cfg'\n    locations = [cwd, basedir]\n    if os.path.exists(config_file):\n        with open(config_file, 'r') as f:\n            locations += [str(s).strip() for s in f.readlines()]\n    else:\n        print('WARNING: OpenCV tests config file ({}) is missing, running subset of tests'.format(config_file))\n    tests_pattern = os.environ.get('OPENCV_APPS_TEST_FILTER', 'test_*') + '.py'\n    if tests_pattern != 'test_*.py':\n        print('Tests filter: {}'.format(tests_pattern))\n    processed = set()\n    for l in locations:\n        if not os.path.isabs(l):\n            l = os.path.normpath(os.path.join(cwd, l))\n        if l in processed:\n            continue\n        processed.add(l)\n        print('Discovering python tests from: {}'.format(l))\n        sys_path_modify = l not in sys.path\n        if sys_path_modify:\n            sys.path.append(l)\n        discovered_tests = loader.discover(l, pattern=tests_pattern, top_level_dir=l)\n        print('    found {} tests'.format(discovered_tests.countTestCases()))\n        tests.addTests(loader.discover(l, pattern=tests_pattern))\n        if sys_path_modify:\n            sys.path.remove(l)\n    return tests", "target": "def load_tests(loader, tests, pattern):\n    cwd = os.getcwd()\n    config_file = 'opencv_python_tests.cfg'\n    locations = [cwd, basedir]\n    if os.path.exists(config_file):\n        with open(config_file, 'r') as f:\n            locations += [str(s).strip() for s in f.readlines()]\n    else:\n        print('WARNING: OpenCV tests config file ({}) is missing, running subset of tests'.format(config_file))\n    tests_pattern = os.environ.get('OPENCV_PYTEST_FILTER', 'test_*') + '.py'\n    if tests_pattern != 'test_*.py':\n        print('Tests filter: {}'.format(tests_pattern))\n    processed = set()\n    for l in locations:\n        if not os.path.isabs(l):\n            l = os.path.normpath(os.path.join(cwd, l))\n        if l in processed:\n            continue\n        processed.add(l)\n        print('Discovering python tests from: {}'.format(l))\n        sys_path_modify = l not in sys.path\n        if sys_path_modify:\n            sys.path.append(l)\n        discovered_tests = loader.discover(l, pattern=tests_pattern, top_level_dir=l)\n        print('    found {} tests'.format(discovered_tests.countTestCases()))\n        tests.addTests(loader.discover(l, pattern=tests_pattern))\n        if sys_path_modify:\n            sys.path.remove(l)\n    return tests", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000804", "source": "def types_separator(self) -> str:\n        return \", \"", "target": "def types_separator(self) -> str:\n        return \", \"", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "000805", "source": "def _work(self) -> None:\n        @torch.compile(\n            backend=\"inductor\",\n            fullgraph=True,\n            dynamic=self._dynamic,\n        )\n        def f(a, b):\n            z = torch.mm(a, b)\n            for i in range(200):\n                z = torch.mm(z, b)\n            return z\n        with fresh_cache(), torch._inductor.config.patch(max_autotune=True):\n            f(self.a, self.b)", "target": "def test_plain_enum():\n    class MyEnum(Enum):\n        a = 1\n        b = 2\n    v = SchemaSerializer(core_schema.enum_schema(MyEnum, list(MyEnum.__members__.values())))\n    assert v.to_python(MyEnum.a) is MyEnum.a\n    assert v.to_python(MyEnum.a, mode='json') == 1\n    assert v.to_json(MyEnum.a) == b'1'\n    with pytest.warns(\n        UserWarning,\n        match=r'Expected `enum` - serialized value may not be as expected \\[input_value=1, input_type=int\\]',\n    ):\n        assert v.to_python(1) == 1\n    with pytest.warns(\n        UserWarning,\n        match=r'Expected `enum` - serialized value may not be as expected \\[input_value=1, input_type=int\\]',\n    ):\n        assert v.to_json(1) == b'1'", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000806", "source": "def test_by_alias_and_name_config_interaction(\n    config_by_alias: bool | None,\n    config_by_name: bool | None,\n    runtime_by_alias: bool | None,\n    runtime_by_name: bool | None,\n) -> None:\n    if config_by_alias is False and config_by_name is False and runtime_by_alias is False and runtime_by_name is False:\n        pytest.skip(\"Can't have both by_alias and by_name as effectively False\")\n    schema = cs.arguments_v3_schema(\n        arguments=[\n            cs.arguments_v3_parameter(name='my_field', schema=cs.int_schema(), alias='my_alias'),\n        ],\n        **({'validate_by_alias': config_by_alias} if config_by_alias is not None else {}),\n        **({'validate_by_name': config_by_name} if config_by_name is not None else {}),\n    )\n    s = SchemaValidator(schema)\n    alias_allowed = next(x for x in (runtime_by_alias, config_by_alias, True) if x is not None)\n    name_allowed = next(x for x in (runtime_by_name, config_by_name, False) if x is not None)\n    if alias_allowed:\n        assert s.validate_python(\n            ArgsKwargs((), {'my_alias': 1}), by_alias=runtime_by_alias, by_name=runtime_by_name\n        ) == (\n            (),\n            {'my_field': 1},\n        )\n    if name_allowed:\n        assert s.validate_python(\n            ArgsKwargs((), {'my_field': 1}), by_alias=runtime_by_alias, by_name=runtime_by_name\n        ) == (\n            (),\n            {'my_field': 1},\n        )", "target": "def test_by_alias_and_name_config_interaction(\n    config_by_alias: Union[bool, None],\n    config_by_name: Union[bool, None],\n    runtime_by_alias: Union[bool, None],\n    runtime_by_name: Union[bool, None],\n) -> None:\n    if config_by_alias is False and config_by_name is False and runtime_by_alias is False and runtime_by_name is False:\n        pytest.skip(\"Can't have both by_alias and by_name as effectively False\")\n    class Model:\n        def __init__(self, my_field: int) -> None:\n            self.my_field = my_field\n    core_config = {\n        **({'validate_by_alias': config_by_alias} if config_by_alias is not None else {}),\n        **({'validate_by_name': config_by_name} if config_by_name is not None else {}),\n    }\n    schema = core_schema.model_schema(\n        Model,\n        core_schema.model_fields_schema(\n            {\n                'my_field': core_schema.model_field(core_schema.int_schema(), validation_alias='my_alias'),\n            }\n        ),\n        config=core_schema.CoreConfig(**core_config),\n    )\n    s = SchemaValidator(schema)\n    alias_allowed = next(x for x in (runtime_by_alias, config_by_alias, True) if x is not None)\n    name_allowed = next(x for x in (runtime_by_name, config_by_name, False) if x is not None)\n    if alias_allowed:\n        assert s.validate_python({'my_alias': 1}, by_alias=runtime_by_alias, by_name=runtime_by_name).my_field == 1\n    if name_allowed:\n        assert s.validate_python({'my_field': 1}, by_alias=runtime_by_alias, by_name=runtime_by_name).my_field == 1", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|scikit-learn/scikit-learn", "data_split": "train", "example_id": "000807", "source": "def test_model_allow_extra():\n    s = SchemaSerializer(\n        core_schema.model_schema(\n            BasicModel,\n            core_schema.model_fields_schema(\n                {\n                    'foo': core_schema.model_field(core_schema.int_schema()),\n                    'bar': core_schema.model_field(core_schema.bytes_schema()),\n                },\n                extra_behavior='allow',\n            ),\n            extra_behavior='allow',\n        )\n    )\n    assert s.to_python(BasicModel(foo=1, bar=b'more', __pydantic_extra__={})) == IsStrictDict(foo=1, bar=b'more')\n    assert s.to_python(BasicModel(bar=b'more', foo=1, __pydantic_extra__={})) == IsStrictDict(bar=b'more', foo=1)\n    assert s.to_python(BasicModel(foo=1, __pydantic_extra__=dict(c=3), bar=b'more')) == IsStrictDict(\n        foo=1, bar=b'more', c=3\n    )\n    assert s.to_python(BasicModel(bar=b'more', __pydantic_extra__=dict(c=3, foo=1)), mode='json') == IsStrictDict(\n        bar='more', c=3, foo=1\n    )\n    j = s.to_json(BasicModel(bar=b'more', foo=1, __pydantic_extra__=dict(c=3)))\n    if on_pypy:\n        assert j == IsJson({'bar': 'more', 'foo': 1, 'c': 3})\n    else:\n        assert j == b'{\"bar\":\"more\",\"foo\":1,\"c\":3}'", "target": "def hyperparameter_baseline_similarity(self):\n        return Hyperparameter(\n            \"baseline_similarity\", \"numeric\", self.baseline_similarity_bounds\n        )", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000808", "source": "def test_only_allow_alias(py_and_json) -> None:\n    schema = core_schema.dataclass_schema(\n        BasicDataclass,\n        core_schema.dataclass_args_schema(\n            'BasicDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), validation_alias='FieldA'),\n            ],\n        ),\n        ['a'],\n        config=core_schema.CoreConfig(validate_by_name=False, validate_by_alias=True),\n    )\n    v = py_and_json(schema)\n    assert v.validate_test({'FieldA': 'hello'}) == BasicDataclass(a='hello')\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'a': 'hello'})", "target": "def test_only_allow_alias(py_and_json) -> None:\n    v = py_and_json(\n        {\n            'type': 'model-fields',\n            'fields': {'field_a': {'validation_alias': 'FieldA', 'type': 'model-field', 'schema': {'type': 'int'}}},\n        },\n        config=CoreConfig(validate_by_name=False, validate_by_alias=True),\n    )\n    assert v.validate_test({'FieldA': '123'}) == ({'field_a': 123}, None, {'field_a'})\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'field_a': '123'})", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000809", "source": "def test_config_date(\n    dt: date, expected_to_python, expected_to_json, expected_to_python_dict, expected_to_json_dict, mode\n):\n    s = SchemaSerializer(core_schema.date_schema(), config={'ser_json_temporal': mode})\n    assert s.to_python(dt) == dt\n    assert s.to_python(dt, mode='json') == expected_to_python\n    assert s.to_json(dt) == expected_to_json\n    with pytest.warns(\n        UserWarning,\n        match=(\n            r'Expected `date` - serialized value may not be as expected '\n            r\"\\[input_value=\\{datetime\\.date\\([^)]*\\): 'foo'\\}, input_type=dict\\]\"\n        ),\n    ):\n        assert s.to_python({dt: 'foo'}) == {dt: 'foo'}\n    with pytest.warns(\n        UserWarning,\n        match=(\n            r'Expected `date` - serialized value may not be as expected '\n            r\"\\[input_value=\\{datetime\\.date\\([^)]*\\): 'foo'\\}, input_type=dict\\]\"\n        ),\n    ):\n        assert s.to_python({dt: 'foo'}, mode='json') == expected_to_python_dict\n    with pytest.warns(\n        UserWarning,\n        match=(\n            r'Expected `date` - serialized value may not be as expected '\n            r\"\\[input_value=\\{datetime\\.date\\([^)]*\\): 'foo'\\}, input_type=dict\\]\"\n        ),\n    ):\n        assert s.to_json({dt: 'foo'}) == expected_to_json_dict", "target": "def test_union_frozenset_list(input_value, expected):\n    v = SchemaValidator(cs.union_schema(choices=[cs.frozenset_schema(), cs.list_schema()]))\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            v.validate_python(input_value)\n    else:\n        v.validate_python(input_value)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "000810", "source": "def string_or_pathlike_(ctype_name: str = \"string\") -> UnionTypeNode:\n        return UnionTypeNode(\n            ctype_name,\n            items=(\n                PrimitiveTypeNode.str_(ctype_name),\n                PathLikeTypeNode(ctype_name)\n            )\n        )", "target": "def test_model_a(self, schema_validator: SchemaValidator):\n        m_a = schema_validator.validate_python({'a': 1, 'b': 'hello'})\n        assert isinstance(m_a, self.ModelA)\n        assert m_a.a == 1\n        assert m_a.b == 'hello'", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000811", "source": "def test_on_error_default(self, py_and_json: PyAndJson):\n        v = py_and_json(\n            {\n                'type': 'model-fields',\n                'fields': {\n                    'x': {\n                        'type': 'model-field',\n                        'schema': {\n                            'type': 'default',\n                            'schema': {'type': 'str'},\n                            'on_error': 'default',\n                            'default': 'pika',\n                        },\n                    }\n                },\n            }\n        )\n        assert v.validate_test({'x': 'foo'}) == ({'x': 'foo'}, None, {'x'})\n        assert v.validate_test({'x': ['foo']}) == ({'x': 'pika'}, None, {'x'})", "target": "def test_on_error_default(self, py_and_json: PyAndJson):\n        v = py_and_json(\n            {\n                'type': 'typed-dict',\n                'fields': {\n                    'x': {\n                        'type': 'typed-dict-field',\n                        'schema': {\n                            'type': 'default',\n                            'schema': {'type': 'str'},\n                            'on_error': 'default',\n                            'default': 'pika',\n                        },\n                    }\n                },\n            }\n        )\n        assert v.validate_test({'x': 'foo'}) == {'x': 'foo'}\n        assert v.validate_test({'x': ['foo']}) == {'x': 'pika'}", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000812", "source": "def test_dict(py_and_json: PyAndJson):\n    v = py_and_json({'type': 'dict', 'keys_schema': {'type': 'int'}, 'values_schema': {'type': 'int'}})\n    assert v.validate_test({'1': 2, '3': 4}) == {1: 2, 3: 4}\n    v = py_and_json({'type': 'dict', 'strict': True, 'keys_schema': {'type': 'int'}, 'values_schema': {'type': 'int'}})\n    assert v.validate_test({'1': 2, '3': 4}) == {1: 2, 3: 4}\n    assert v.validate_test({}) == {}\n    with pytest.raises(ValidationError, match=re.escape('[type=dict_type, input_value=[], input_type=list]')):\n        v.validate_test([])", "target": "def test_fields_set():\n    assert core_schema.PydanticUndefined is PydanticUndefined\n    class RootModel:\n        __slots__ = '__dict__', '__pydantic_fields_set__', '__pydantic_extra__', '__pydantic_private__'\n        root: int = 42\n    v = SchemaValidator(\n        core_schema.model_schema(\n            RootModel, core_schema.with_default_schema(core_schema.int_schema(), default=42), root_model=True\n        )\n    )\n    m = RootModel()\n    v.validate_python(1, self_instance=m)\n    assert m.root == 1\n    assert m.__pydantic_fields_set__ == {'root'}\n    v.validate_python(PydanticUndefined, self_instance=m)\n    assert m.root == 42\n    assert m.__pydantic_fields_set__ == set()", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000813", "source": "def get_shapes(self) -> tuple[tuple[int, ...], ...]:\n        return (\n            (32768, 256),\n            (32768, 512),\n            (32768, 1024),\n            (32768, 2048),\n            (32768, 4096),\n            (32768, 8192),\n            (32768, 16384),\n            (32768, 32768),\n            (32768, 65536),\n            (16384, 131072),\n            (8192, 262144),\n        )", "target": "def get_shapes(self) -> tuple[tuple[int, ...], ...]:\n        return (\n            (32768, 256),\n            (32768, 512),\n            (32768, 1024),\n            (32768, 2048),\n            (32768, 4096),\n            (32768, 8192),\n            (32768, 16384),\n            (32768, 32768),\n            (32768, 65536),\n            (16384, 131072),\n            (8192, 262144),\n        ) + extra_shapes_for_norm", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000814", "source": "def node_type(self) -> ASTNodeType:\n        return ASTNodeType.Enumeration", "target": "def node_type(self) -> ASTNodeType:\n        return ASTNodeType.Class", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000815", "source": "def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = F.max_pool2d(x, 2)\n        x = torch.flatten(x, 1)\n        output = self.fc1(x)\n        return output", "target": "def forward(self, x):\n        x = self.relu_a(x)\n        x = x + self.sub_mods(x)\n        return x + self.relu_b(x) + self.a", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000816", "source": "def test_json_bytes_base64_no_padding():\n    v = SchemaValidator(core_schema.bytes_schema(), config=CoreConfig(val_json_bytes='base64'))\n    base_64_without_padding = 'bm8tcGFkZGluZw'\n    assert v.validate_json(json.dumps(base_64_without_padding)) == b'no-padding'", "target": "def test_union_error():\n    s = SchemaSerializer(core_schema.union_schema([core_schema.bool_schema(), core_schema.int_schema()]))\n    messages = [\n        \"Expected `bool` - serialized value may not be as expected [input_value='a string', input_type=str]\",\n        \"Expected `int` - serialized value may not be as expected [input_value='a string', input_type=str]\",\n    ]\n    with warnings.catch_warnings(record=True) as w:\n        warnings.simplefilter('always')\n        s.to_python('a string')\n        for m in messages:\n            assert m in str(w[0].message)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|opencv/opencv", "data_split": "train", "example_id": "000817", "source": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "target": "def exec_file_wrapper(fpath, g_vars, l_vars):\n        with open(fpath) as f:\n            code = compile(f.read(), os.path.basename(fpath), 'exec')\n            exec(code, g_vars, l_vars)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000818", "source": "def test_constraints_schema_validation_error(constraint: str) -> None:\n    with pytest.raises(SchemaError, match=f\"'{constraint}' must be coercible to a time instance\"):\n        SchemaValidator(core_schema.time_schema(**{constraint: 'bad_value'}))", "target": "def test_constraints_schema_validation_error(constraint: str) -> None:\n    with pytest.raises(SchemaError, match=f\"'{constraint}' must be coercible to a timedelta instance\"):\n        SchemaValidator(core_schema.timedelta_schema(**{constraint: 'bad_value'}))", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000819", "source": "def constrain(\n        self: _Pipeline[_InT, _NewOutDatetime], constraint: annotated_types.Timezone\n    ) -> _Pipeline[_InT, _NewOutDatetime]: ...", "target": "def constrain(self: _Pipeline[_InT, _OutT], constraint: _In) -> _Pipeline[_InT, _OutT]: ...", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000820", "source": "def types_separator(self) -> str:\n        if TypeNode.compatible_to_runtime_usage:\n            return \", \"\n        return \" | \"", "target": "def types_separator(self) -> str:\n        return \", \"", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000821", "source": "def _work(self):\n        @torch.compile(backend=self.backend(), fullgraph=True)\n        def f(inp, *weights):\n            x = inp\n            for w in weights:\n                x = torch.matmul(w, x).sin().sin()\n            return x\n        f(self.inp, *self.weights)", "target": "def _work(self) -> None:\n        @torch.compile(\n            backend=\"inductor\",\n            fullgraph=True,\n            dynamic=self._dynamic,\n        )\n        def f(a, b):\n            z = torch.mm(a, b)\n            for i in range(200):\n                z = torch.mm(z, b)\n            return z\n        with fresh_cache(), torch._inductor.config.patch(max_autotune=True):\n            f(self.a, self.b)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "000822", "source": "def test_frozenset_no_validators_both(py_and_json: PyAndJson, input_value, expected):\n    v = py_and_json({'type': 'frozenset'})\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            v.validate_test(input_value)\n    else:\n        output = v.validate_test(input_value)\n        assert output == expected\n        assert isinstance(output, frozenset)", "target": "def construct_name(fwd_bwd, test_name):\n    bwd = \"backward\" in fwd_bwd\n    suite_name = fwd_bwd.replace(\"-backward\", \"\")\n    return f\"{suite_name}[{test_name}]:{'bwd' if bwd else 'fwd'}\"", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pytorch/pytorch", "data_split": "train", "example_id": "000823", "source": "def run(img0, img1):\n                    raise Exception('Error')\n                    return img0 + img1", "target": "def training_iter_fn(batch, model, optimizer):\n    outputs = model(**batch)\n    loss = outputs.loss\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n    return loss", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000824", "source": "def test_list_from_dict_items(input_value, items_schema, expected):\n    v = SchemaValidator(cs.list_schema(items_schema=items_schema))\n    output = v.validate_python(input_value)\n    assert isinstance(output, list)\n    assert output == expected", "target": "def test_many_uses_of_ref():\n    v = SchemaValidator(\n        core_schema.definitions_schema(\n            core_schema.definition_reference_schema('Branch'),\n            [\n                core_schema.typed_dict_schema(\n                    {\n                        'name': core_schema.typed_dict_field(core_schema.definition_reference_schema('limited-string')),\n                        'other_names': core_schema.typed_dict_field(\n                            core_schema.list_schema(core_schema.definition_reference_schema('limited-string'))\n                        ),\n                    },\n                    ref='Branch',\n                ),\n                core_schema.str_schema(max_length=8, ref='limited-string'),\n            ],\n        )\n    )\n    assert v.validate_python({'name': 'Anne', 'other_names': ['Bob', 'Charlie']}) == {\n        'name': 'Anne',\n        'other_names': ['Bob', 'Charlie'],\n    }\n    with pytest.raises(ValidationError, match=r'other_names.2\\s+String should have at most 8 characters'):\n        v.validate_python({'name': 'Anne', 'other_names': ['Bob', 'Charlie', 'Daveeeeee']})\n    long_input = {'name': 'Anne', 'other_names': [f'p-{i}' for i in range(300)]}\n    assert v.validate_python(long_input) == long_input", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|opencv/opencv", "data_split": "train", "example_id": "000825", "source": "def test_get_cache_dir(self):\n        path = cv.utils.fs.getCacheDirectoryForDownloads()\n        self.assertTrue(os.path.exists(path))\n        self.assertTrue(os.path.isdir(path))", "target": "def test_aruco_detector(self):\n        aruco_params = cv.aruco.DetectorParameters()\n        aruco_dict = cv.aruco.getPredefinedDictionary(cv.aruco.DICT_4X4_250)\n        aruco_detector = cv.aruco.ArucoDetector(aruco_dict, aruco_params)\n        id = 2\n        marker_size = 100\n        offset = 10\n        img_marker = cv.aruco.generateImageMarker(aruco_dict, id, marker_size, aruco_params.markerBorderBits)\n        img_marker = np.pad(img_marker, pad_width=offset, mode='constant', constant_values=255)\n        gold_corners = np.array([[offset, offset],[marker_size+offset-1.0,offset],\n                                 [marker_size+offset-1.0,marker_size+offset-1.0],\n                                 [offset, marker_size+offset-1.0]], dtype=np.float32)\n        corners, ids, rejected = aruco_detector.detectMarkers(img_marker)\n        self.assertEqual(1, len(ids))\n        self.assertEqual(id, ids[0])\n        for i in range(0, len(corners)):\n            np.testing.assert_array_equal(gold_corners, corners[i].reshape(4, 2))", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000826", "source": "def f(input_value, validator, info):\n        if 'error' in info.context:\n            raise ValueError('wrong')\n        return validator(input_value)", "target": "def multiple_tuple_schema() -> SchemaValidator:\n    return SchemaValidator(\n        core_schema.definitions_schema(\n            core_schema.typed_dict_schema(\n                {\n                    'f1': core_schema.typed_dict_field(core_schema.definition_reference_schema('t')),\n                    'f2': core_schema.typed_dict_field(\n                        core_schema.with_default_schema(\n                            core_schema.nullable_schema(core_schema.definition_reference_schema('t')), default=None\n                        )\n                    ),\n                }\n            ),\n            [\n                core_schema.tuple_schema(\n                    [\n                        core_schema.int_schema(),\n                        core_schema.nullable_schema(core_schema.definition_reference_schema('t')),\n                    ],\n                    ref='t',\n                )\n            ],\n        )\n    )", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "000827", "source": "def getRectInTime(self, time):\n        if self.foreground is not None:\n            tmp = np.array(self.center) + np.array((self.getXOffset(time), self.getYOffset(time)))\n            x0, y0 = tmp\n            x1, y1 = tmp + self.foreground.shape[0:2]\n            return np.array([y0, x0, y1, x1])\n        else:\n            x0, y0 = self.initialRect[0] + np.array((self.getXOffset(time), self.getYOffset(time)))\n            x1, y1 = self.initialRect[2] + np.array((self.getXOffset(time), self.getYOffset(time)))\n            return np.array([y0, x0, y1, x1])", "target": "def test_prebuilt_val_and_ser_used() -> None:\n    class InnerModel:\n        x: int\n    inner_schema = core_schema.model_schema(\n        InnerModel,\n        schema=core_schema.model_fields_schema(\n            {'x': core_schema.model_field(schema=core_schema.int_schema())},\n        ),\n    )\n    inner_validator = SchemaValidator(inner_schema)\n    inner_serializer = SchemaSerializer(inner_schema)\n    InnerModel.__pydantic_complete__ = True\n    InnerModel.__pydantic_validator__ = inner_validator\n    InnerModel.__pydantic_serializer__ = inner_serializer\n    class OuterModel:\n        inner: InnerModel\n    outer_schema = core_schema.model_schema(\n        OuterModel,\n        schema=core_schema.model_fields_schema(\n            {\n                'inner': core_schema.model_field(\n                    schema=core_schema.model_schema(\n                        InnerModel,\n                        schema=core_schema.model_fields_schema(\n                            {'x': core_schema.model_field(schema=core_schema.str_schema())},\n                        ),\n                    )\n                )\n            }\n        ),\n    )\n    outer_validator = SchemaValidator(outer_schema)\n    outer_serializer = SchemaSerializer(outer_schema)\n    result = outer_validator.validate_python({'inner': {'x': 1}})\n    assert result.inner.x == 1\n    assert outer_serializer.to_python(result) == {'inner': {'x': 1}}", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "000828", "source": "def load_data(dtype=np.float32, order=\"F\"):\n    print(\"Loading dataset...\")\n    data = fetch_openml(\"mnist_784\", as_frame=True)\n    X = check_array(data[\"data\"], dtype=dtype, order=order)\n    y = data[\"target\"]\n    X = X / 255\n    print(\"Creating train-test split...\")\n    n_train = 60000\n    X_train = X[:n_train]\n    y_train = y[:n_train]\n    X_test = X[n_train:]\n    y_test = y[n_train:]\n    return X_train, X_test, y_train, y_test", "target": "def load_data(dtype=np.float32, order=\"C\", shuffle=True, seed=0):\n    print(\"Loading dataset...\")\n    data = fetch_openml(\"mnist_784\", as_frame=True)\n    X = check_array(data[\"data\"], dtype=dtype, order=order)\n    y = data[\"target\"]\n    if shuffle:\n        X, y = _shuffle(X, y, random_state=seed)\n    X /= 255\n    return X, y", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000829", "source": "def add(a, b):\n        return (a + b,)", "target": "def add(a, b):\n    return 3 * a + b", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000830", "source": "def wrap_function(input_value, validator, info):\n        return f'Input {validator(input_value)} Changed'", "target": "def wrap_function(input_value, validator, info):\n            try:\n                return validator(input_value)\n            except ValidationError:\n                if isinstance(input_value, list):\n                    return str(len(input_value))\n                else:\n                    return repr(input_value)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "000831", "source": "def validator(\n    __field: str,\n    *fields: str,\n    pre: bool = False,\n    each_item: bool = False,\n    always: bool = False,\n    check_fields: bool | None = None,\n    allow_reuse: bool = False,\n) -> Callable[[_V1ValidatorType], _V1ValidatorType]:\n    warn(\n        'Pydantic V1 style `@validator` validators are deprecated.'\n        ' You should migrate to Pydantic V2 style `@field_validator` validators,'\n        ' see the migration guide for more details',\n        DeprecationWarning,\n        stacklevel=2,\n    )\n    if allow_reuse is True:\n        warn(_ALLOW_REUSE_WARNING_MESSAGE, DeprecationWarning, stacklevel=2)\n    fields = __field, *fields\n    if isinstance(fields[0], FunctionType):\n        raise PydanticUserError(\n            '`@validator` should be used with fields and keyword arguments, not bare. '\n            \"E.g. usage should be `@validator('<field_name>', ...)`\",\n            code='validator-no-fields',\n        )\n    elif not all(isinstance(field, str) for field in fields):\n        raise PydanticUserError(\n            '`@validator` fields should be passed as separate string args. '\n            \"E.g. usage should be `@validator('<field_name_1>', '<field_name_2>', ...)`\",\n            code='validator-invalid-fields',\n        )\n    mode: Literal['before', 'after'] = 'before' if pre is True else 'after'\n    def dec(f: Any) -> _decorators.PydanticDescriptorProxy[Any]:\n        if _decorators.is_instance_method_from_sig(f):\n            raise PydanticUserError(\n                '`@validator` cannot be applied to instance methods', code='validator-instance-method'\n            )\n        f = _decorators.ensure_classmethod_based_on_signature(f)\n        wrap = _decorators_v1.make_generic_v1_field_validator\n        validator_wrapper_info = _decorators.ValidatorDecoratorInfo(\n            fields=fields,\n            mode=mode,\n            each_item=each_item,\n            always=always,\n            check_fields=check_fields,\n        )\n        return _decorators.PydanticDescriptorProxy(f, validator_wrapper_info, shim=wrap)\n    return dec", "target": "def microbenchmark(args, model, example_inputs):\n    compiled_fn = compile_fx(torch.fx.symbolic_trace(model), example_inputs)\n    cudagraphs_eager = cudagraphs_inner(model, example_inputs, copy_outputs=False)\n    cudagraphs_jit = cudagraphs_inner(\n        torch.jit.trace(model, example_inputs), example_inputs, copy_outputs=False\n    )\n    return compute_speedups(\n        args,\n        [cudagraphs_eager, cudagraphs_jit, compiled_fn],\n        example_inputs,\n    )", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|scikit-learn/scikit-learn", "data_split": "train", "example_id": "000832", "source": "def test_date():\n    v = SchemaSerializer(core_schema.date_schema())\n    assert v.to_python(date(2022, 12, 2)) == date(2022, 12, 2)\n    assert v.to_python(date(2022, 12, 2), mode='json') == '2022-12-02'\n    assert v.to_json(date(2022, 12, 2)) == b'\"2022-12-02\"'", "target": "def bench_one(\n    name, X, W0, H0, X_shape, clf_type, clf_params, init, n_components, random_state\n):\n    W = W0.copy()\n    H = H0.copy()\n    clf = clf_type(**clf_params)\n    st = time()\n    W = clf.fit_transform(X, W=W, H=H)\n    end = time()\n    H = clf.components_\n    this_loss = _beta_divergence(X, W, H, 2.0, True)\n    duration = end - st\n    return this_loss, duration", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "000833", "source": "def test_serialize_as_any_with_dataclass() -> None:\n    @dataclass\n    class Parent:\n        x: int\n    class Child(Parent):\n        y: str\n    Parent.__pydantic_core_schema__ = core_schema.dataclass_schema(\n        Parent,\n        core_schema.dataclass_args_schema(\n            'Parent',\n            [\n                core_schema.dataclass_field(name='x', schema=core_schema.int_schema()),\n            ],\n        ),\n        ['x'],\n    )\n    Parent.__pydantic_validator__ = SchemaValidator(Parent.__pydantic_core_schema__)\n    Parent.__pydantic_serializer__ = SchemaSerializer(Parent.__pydantic_core_schema__)\n    Child.__pydantic_core_schema__ = core_schema.dataclass_schema(\n        Child,\n        core_schema.dataclass_args_schema(\n            'Child',\n            [\n                core_schema.dataclass_field(name='x', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(name='y', schema=core_schema.str_schema()),\n            ],\n        ),\n        ['x', 'y'],\n    )\n    Child.__pydantic_validator__ = SchemaValidator(Child.__pydantic_core_schema__)\n    Child.__pydantic_serializer__ = SchemaSerializer(Child.__pydantic_core_schema__)\n    child = Child.__pydantic_validator__.validate_python({'x': 1, 'y': 'hopefully not a secret'})\n    assert Parent.__pydantic_serializer__.to_python(child, serialize_as_any=False) == {'x': 1}\n    assert Parent.__pydantic_serializer__.to_python(child, serialize_as_any=True) == {\n        'x': 1,\n        'y': 'hopefully not a secret',\n    }", "target": "def runTest(self):\n        cmakecmd = self.makeCMakeCmd()\n        execute(cmakecmd, cwd = self.build_dir)\n        buildcmd = self.getTestCommand()\n        execute(buildcmd, cwd = self.build_dir)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000834", "source": "def constrain(self: _Pipeline[_InT, _NewOutLe], constraint: annotated_types.Le) -> _Pipeline[_InT, _NewOutLe]: ...", "target": "def constrain(self: _Pipeline[_InT, _OutT], constraint: annotated_types.Predicate) -> _Pipeline[_InT, _OutT]: ...", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "000835", "source": "def test_lax_or_strict_custom_ser():\n    s = SchemaSerializer(\n        core_schema.lax_or_strict_schema(\n            core_schema.int_schema(),\n            core_schema.str_schema(),\n            serialization=core_schema.format_ser_schema('^5s', when_used='always'),\n        )\n    )\n    assert s.to_python('abc') == ' abc '\n    assert s.to_python('abc', mode='json') == ' abc '\n    assert s.to_json('abc') == b'\" abc \"'", "target": "def fn():\n            result = compiled_fn(a)\n            assert counters[\"inductor\"][\"fxgraph_cache_miss\"] == 0\n            assert counters[\"inductor\"][\"fxgraph_cache_hit\"] == 1\n            return result", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000836", "source": "def benchmark(self):\n        for M, N in self.get_shapes():\n            print(f\"Tensor dimensions: [{M}, {N}]\")\n            torch_dtype = cutlass_torch.dtype(cutlass.BFloat16)\n            x = 0.1 * torch.randn(\n                M, N, device=\"cuda\", dtype=torch_dtype, requires_grad=True\n            )\n            dy = torch.randn(M, N, device=\"cuda\", dtype=torch_dtype)\n            self.benchmark_single_shape((x, dy), setting=f\"shape: [{M}, {N}]\")", "target": "def benchmark(self):\n        for M, N in self.get_shapes():\n            print(f\"Tensor dimensions: [{M}, {N}]\")\n            torch_dtype = cutlass_torch.dtype(cutlass.BFloat16)\n            x = torch.randn(M, N, device=\"cuda\", dtype=torch_dtype)\n            w = torch.randn(N, device=\"cuda\", dtype=torch.float32)\n            self.benchmark_single_shape((x, w), setting=f\"shape: [{M}, {N}]\")", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000837", "source": "def liger(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        from liger_kernel.transformers.cross_entropy import LigerCrossEntropyLoss\n        x, target = args\n        cross_entropy = LigerCrossEntropyLoss(reduction=\"none\")\n        return lambda: cross_entropy(x, target)", "target": "def liger(self, args, kwargs=None) -> Any:\n        from liger_kernel.transformers.rms_norm import LigerRMSNorm\n        x, w, dy = args\n        M, N = x.shape\n        liger_rmsnorm = LigerRMSNorm(\n            hidden_size=N, eps=1e-6, casting_mode=\"gemma\"\n        ).cuda()\n        liger_rmsnorm.weight.data.copy_(w)\n        y = liger_rmsnorm(x)\n        return lambda: torch.autograd.grad(\n            y, [x, liger_rmsnorm.weight], grad_outputs=dy, retain_graph=True\n        )", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "000838", "source": "def wrap_schema_in_root_model(schema: CoreSchema) -> CoreSchema:\n    class MyRootModel:\n        __slots__ = '__dict__', '__pydantic_fields_set__', '__pydantic_extra__', '__pydantic_private__'\n    return {\n        'type': 'model',\n        'cls': MyRootModel,\n        'config': {},\n        'schema': {\n            'type': 'model-fields',\n            'fields': {\n                'root': {'type': 'model-field', 'schema': schema},\n            },\n        },\n        'root_model': True,\n    }", "target": "def typename(self) -> str:\n        return self.type_format.format(self.types_separator.join(\n            item.typename for item in self\n        ))", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "000839", "source": "def iter_model_names(self, args):\n        model_names = sorted(TIMM_MODELS.keys())\n        start, end = self.get_benchmark_indices(len(model_names))\n        for index, model_name in enumerate(model_names):\n            if index < start or index >= end:\n                continue\n            if (\n                not re.search(\"|\".join(args.filter), model_name, re.IGNORECASE)\n                or re.search(\"|\".join(args.exclude), model_name, re.IGNORECASE)\n                or model_name in args.exclude_exact\n                or model_name in self.skip_models\n            ):\n                continue\n            yield model_name", "target": "def test_function_before_raise():\n    def f(input_value, info):\n        raise ValueError('foobar')\n    v = SchemaValidator(core_schema.with_info_before_validator_function(f, core_schema.str_schema()))\n    with pytest.raises(ValidationError) as exc_info:\n        assert v.validate_python('input value') == 'input value Changed'\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'value_error',\n            'loc': (),\n            'msg': 'Value error, foobar',\n            'input': 'input value',\n            'ctx': {'error': HasRepr(repr(ValueError('foobar')))},\n        }\n    ]", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000840", "source": "def eager(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, target = args\n        return lambda: F.cross_entropy(x, target, reduction=\"none\")", "target": "def eager(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, w = args\n        return lambda: self.layernorm_ref(x, w)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000841", "source": "def wrap_function(input_value, validator, info):\n            try:\n                return validator(input_value)\n            except ValidationError:\n                if isinstance(input_value, list):\n                    return str(len(input_value))\n                else:\n                    return repr(input_value)", "target": "def handler() -> T | None:\n            if rebuild_dataclass(cls, raise_errors=False, _parent_namespace_depth=5) is not False:\n                return attr_fn(cls)\n            return None", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "000842", "source": "def test_strict_reference():\n    v = SchemaValidator(\n        core_schema.definitions_schema(\n            core_schema.definition_reference_schema(schema_ref='tuple-ref'),\n            [\n                core_schema.tuple_positional_schema(\n                    [\n                        core_schema.float_schema(),\n                        core_schema.union_schema(\n                            [core_schema.int_schema(), core_schema.definition_reference_schema('tuple-ref')]\n                        ),\n                    ],\n                    ref='tuple-ref',\n                )\n            ],\n        )\n    )\n    assert repr(v.validate_python((1, 2))) == '(1.0, 2)'\n    assert repr(v.validate_python((1.0, (2.0, 3)))) == '(1.0, (2.0, 3))'", "target": "def test_cli_run_build_external(self, mock_init, mock_run):\n        from cli.run import main\n        test_args = [\"cli.run\", \"build\", \"external\", \"vllm\"]\n        with patch.object(sys, \"argv\", test_args):\n            try:\n                main()\n            except SystemExit:\n                pass\n        mock_init.assert_called_once()\n        mock_run.assert_called_once_with()", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000843", "source": "def test_non_model_field_wrap_validator_field_info() -> None:\n    class Model:\n        __slots__ = '__dict__', '__pydantic_fields_set__', '__pydantic_extra__', '__pydantic_private__'\n        x: str\n    def f(input_value: Any, val: core_schema.ValidatorFunctionWrapHandler, info: core_schema.ValidationInfo) -> Any:\n        assert info.field_name == 'x'\n        assert info.data == {}\n        return f'input: {val(input_value)}'\n    v = SchemaValidator(\n        core_schema.model_schema(\n            Model,\n            core_schema.model_fields_schema(\n                {\n                    'x': core_schema.model_field(\n                        core_schema.with_info_wrap_validator_function(f, core_schema.str_schema())\n                    )\n                }\n            ),\n        )\n    )\n    assert v.validate_python({'x': b'foo'}).x == 'input: foo'", "target": "def transform(self) -> bool:\n        info = self._cls.info\n        is_a_root_model = is_root_model(info)\n        config = self.collect_config()\n        fields, class_vars = self.collect_fields_and_class_vars(config, is_a_root_model)\n        if fields is None or class_vars is None:\n            return False\n        for field in fields:\n            if field.type is None:\n                return False\n        is_settings = info.has_base(BASESETTINGS_FULLNAME)\n        self.add_initializer(fields, config, is_settings, is_a_root_model)\n        self.add_model_construct_method(fields, config, is_settings, is_a_root_model)\n        self.set_frozen(fields, self._api, frozen=config.frozen is True)\n        self.adjust_decorator_signatures()\n        info.metadata[METADATA_KEY] = {\n            'fields': {field.name: field.serialize() for field in fields},\n            'class_vars': {class_var.name: class_var.serialize() for class_var in class_vars},\n            'config': config.get_values_dict(),\n        }\n        return True", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "000844", "source": "def test_repr():\n    v = SchemaValidator(\n        cs.arguments_schema(\n            arguments=[\n                {'name': 'b', 'mode': 'positional_or_keyword', 'schema': cs.int_schema()},\n                {\n                    'name': 'a',\n                    'mode': 'keyword_only',\n                    'schema': cs.with_default_schema(schema=cs.int_schema(), default_factory=lambda: 42),\n                },\n            ]\n        )\n    )\n    assert 'positional_params_count:1,' in plain_repr(v)", "target": "def _generate_docker_build_cmd(\n        self,\n        inputs: VllmBuildParameters,\n    ) -> str:\n        base_image_arg, final_base_image_arg, pull_flag = self._get_base_image_args(\n            inputs\n        )\n        torch_arg = self._get_torch_wheel_path_arg(inputs.torch_whls_path)\n        return textwrap.dedent(\n            f\n        ).strip()", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "000845", "source": "def make_estimator(self, params):\n        (method,) = params\n        estimator = TSNE(random_state=0, method=method)\n        return estimator", "target": "def make_estimator(self, params):\n        (kernel,) = params\n        estimator = SVC(\n            max_iter=100, tol=1e-16, kernel=kernel, random_state=0, gamma=\"scale\"\n        )\n        return estimator", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|opencv/opencv", "data_split": "train", "example_id": "000846", "source": "def gh_post_commit_comment(\n    org: str, repo: str, sha: str, comment: str, dry_run: bool = False\n) -> list[dict[str, Any]]:\n    return _gh_post_comment(\n        f\"{GITHUB_API_URL}/repos/{org}/{repo}/commits/{sha}/comments\",\n        comment,\n        dry_run,\n    )", "target": "def getToolchain(self, arch, target):\n        return None", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pytorch/pytorch", "data_split": "train", "example_id": "000847", "source": "def test_arithm_op_with_saturation(self):\n        np.random.seed(4231568)\n        src = np.random.randint(20, 40, 4 * 8 * 4).astype(np.uint8).reshape(4, 8, 4)\n        operations = get_ocv_arithm_op_table(apply_saturation=True)\n        for ocv_op, numpy_op in operations.items():\n            for val in (10, 4, (40, ), (15, 12), (25., 41., 15.),\n                        np.uint8([1, 2, 20]), np.float64([50, 21, 64, 30]),):\n                dst = ocv_op(src, val)\n                expected = numpy_op(src, val)\n                self.assertLess(np.max(np.abs(dst - expected)), 2,\n                  msg=\"Saturated Operation '{}' is failed for {}\".format(ocv_op.__name__, val ) )", "target": "def lstm_factory_multilayer(cell, script):\n    def dynamic_rnn(\n        input: Tensor, hidden: tuple[Tensor, Tensor], params: list[Tensor]\n    ) -> tuple[Tensor, tuple[Tensor, Tensor]]:\n        params_stride = 4\n        hx, cx = hidden\n        hy, cy = hidden\n        inputs, outputs = input.unbind(0), []\n        for layer in range(hx.size(0)):\n            hy = hx[layer]\n            cy = cx[layer]\n            base_idx = layer * params_stride\n            w_ih = params[base_idx]\n            w_hh = params[base_idx + 1]\n            b_ih = params[base_idx + 2]\n            b_hh = params[base_idx + 3]\n            for seq_idx in range(len(inputs)):\n                hy, cy = cell(inputs[seq_idx], (hy, cy), w_ih, w_hh, b_ih, b_hh)\n                outputs += [hy]\n            inputs, outputs = outputs, []\n        return torch.stack(inputs), (hy.unsqueeze(0), cy.unsqueeze(0))\n    if script:\n        cell = torch.jit.script(cell)\n        dynamic_rnn = torch.jit.script(dynamic_rnn)\n    return dynamic_rnn", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000848", "source": "def full_typename(self) -> str:\n        return self.typename", "target": "def full_typename(self) -> str:\n        return \"cv2.typing.\" + self.typename", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "000849", "source": "def make_data(self, params):\n        (representation,) = params\n        if representation == \"dense\":\n            data = _synth_regression_dataset(n_samples=1000000, n_features=100)\n        else:\n            data = _synth_regression_sparse_dataset(\n                n_samples=10000, n_features=100000, density=0.01\n            )\n        return data", "target": "def make_data(self, params):\n        representation, precompute = params\n        if representation == \"dense\":\n            data = _synth_regression_dataset(n_samples=1000000, n_features=100)\n        else:\n            data = _synth_regression_sparse_dataset(\n                n_samples=50000, n_features=5000, density=0.01\n            )\n        return data", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "train", "example_id": "000850", "source": "def skip_accuracy_check_as_eager_non_deterministic(self):\n        if self.args.accuracy and self.args.training:\n            return self._accuracy[\"skip\"][\"eager_not_deterministic\"]\n        return set()", "target": "def forward(self, x):\n        t, n = x.size(0), x.size(1)\n        x = x.view(t * n, -1)\n        x = self.module(x)\n        x = x.view(t, n, -1)\n        return x", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000851", "source": "def resolve_ref_schema(self, maybe_ref_json_schema: JsonSchemaValue, /) -> JsonSchemaValue:\n        raise NotImplementedError", "target": "def resolve_ref_schema(self, maybe_ref_schema: core_schema.CoreSchema, /) -> core_schema.CoreSchema:\n        raise NotImplementedError", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "000852", "source": "def test_exclude(schema_func, seq_f):\n    v = SchemaSerializer(\n        schema_func(core_schema.any_schema(), serialization=core_schema.filter_seq_schema(exclude={1, 3, 5}))\n    )\n    assert v.to_python(seq_f(0, 1, 2, 3)) == seq_f(0, 2)\n    assert v.to_python(seq_f('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h')) == seq_f('a', 'c', 'e', 'g', 'h')\n    assert v.to_python(seq_f('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'), mode='json') == ['a', 'c', 'e', 'g', 'h']\n    assert v.to_json(seq_f('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h')) == b'[\"a\",\"c\",\"e\",\"g\",\"h\"]'\n    assert v.to_python(seq_f('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'), exclude={6}) == seq_f('a', 'c', 'e', 'h')\n    assert v.to_python(seq_f('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'), exclude={-1, -2}) == seq_f('a', 'c', 'e')\n    assert v.to_json(seq_f('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'), exclude={6}) == b'[\"a\",\"c\",\"e\",\"h\"]'\n    assert v.to_json(seq_f('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'), exclude={-1, -2}) == b'[\"a\",\"c\",\"e\"]'", "target": "def run_model(args, model, inputs, key):\n    rank = int(os.getenv(\"RANK\", 0))\n    world_size = int(os.getenv(\"WORLD_SIZE\", 1))\n    setup(rank, world_size)\n    if args.device == \"cuda\":\n        torch.cuda.set_device(rank)\n    dev_rank = f\"{args.device}:{rank}\"\n    model = model.to(dev_rank)\n    def move_tensor(maybe_tensor):\n        if torch.is_tensor(maybe_tensor):\n            return maybe_tensor.to(dev_rank)\n        return maybe_tensor\n    inputs = pytree.tree_map(move_tensor, inputs)\n    if args.fsdp:\n        model = apply_fsdp(\n            args,\n            model,\n            use_checkpointing=args.fsdp_checkpoint,\n            use_wrap_policy=args.fsdp_wrap,\n        )\n    elif args.ddp:\n        model = DDP(model)\n    if args.verbose:\n        print(model)\n    if args.dynamo:\n        dynamo.reset()\n        if args.verbose:\n            dynamo.config.verbose = True\n            dynamo.config.log_level = logging.DEBUG\n        if args.dynamo_no_optimize_ddp:\n            dynamo.config.optimize_ddp = False\n        if args.dynamo == \"inductor\" and args.fsdp:\n            torch._inductor.config.triton.cudagraphs = False\n            log.warning(\"disabling inductor cudagraphs for compatibility with FSDP\")\n        def print_compile(gm, ex):\n            print(\n                f\"print_compile:\\n{str(gm.graph)}\\n-----------------------------------------\"\n            )\n            return gm\n        dynamo_ctx = dynamo.optimize(\n            print_compile if args.dynamo == \"print\" else args.dynamo\n        )\n        model = dynamo_ctx(model)\n    _ = timed(model, model_iter_fn, inputs, times=3, return_result=False)\n    t_total = timed(\n        model, model_iter_fn, inputs, times=args.repeat, return_result=False\n    )\n    if args.torchviz:\n        torchviz_model(args, model, inputs, rank)\n    if args.profile:\n        profile_model(args, model, inputs, rank)\n    cleanup()\n    return t_total", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000853", "source": "def _prepare_once(self):\n        self.x = torch.randn(4, 4, requires_grad=True)", "target": "def _prepare_once(self):\n        torch._dynamo.config.capture_scalar_outputs = True\n        torch.manual_seed(0)\n        self.splits = torch.randint(10, (self.N,))", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000854", "source": "def test_alias_error_loc_alias(py_and_json: PyAndJson):\n    v = py_and_json(\n        {\n            'type': 'model-fields',\n            'fields': {\n                'field_a': {\n                    'type': 'model-field',\n                    'schema': {'type': 'int'},\n                    'validation_alias': [['foo', 'x'], ['bar', 1, -1]],\n                }\n            },\n        },\n        {'loc_by_alias': True},\n    )\n    assert v.validate_test({'foo': {'x': 42}}) == ({'field_a': 42}, None, {'field_a'})\n    assert v.validate_python({'bar': ['x', {-1: 42}]}) == ({'field_a': 42}, None, {'field_a'})\n    assert v.validate_test({'bar': ['x', [1, 2, 42]]}) == ({'field_a': 42}, None, {'field_a'})\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_test({'foo': {'x': 'not_int'}})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('foo', 'x'),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'not_int',\n        }\n    ]\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_test({'bar': ['x', [1, 2, 'not_int']]})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('bar', 1, -1),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'not_int',\n        }\n    ]\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_test({})\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'missing', 'loc': ('foo', 'x'), 'msg': 'Field required', 'input': {}}\n    ]", "target": "def test_alias_error_loc_alias(py_and_json: PyAndJson):\n    v = py_and_json(\n        {\n            'type': 'typed-dict',\n            'fields': {\n                'field_a': {\n                    'type': 'typed-dict-field',\n                    'schema': {'type': 'int'},\n                    'validation_alias': [['foo', 'x'], ['bar', 1, -1]],\n                }\n            },\n        },\n        {'loc_by_alias': True},\n    )\n    assert v.validate_test({'foo': {'x': 42}}) == {'field_a': 42}\n    assert v.validate_python({'bar': ['x', {-1: 42}]}) == {'field_a': 42}\n    assert v.validate_test({'bar': ['x', [1, 2, 42]]}) == {'field_a': 42}\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_test({'foo': {'x': 'not_int'}})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('foo', 'x'),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'not_int',\n        }\n    ]\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_test({'bar': ['x', [1, 2, 'not_int']]})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('bar', 1, -1),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'not_int',\n        }\n    ]\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_test({})\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'missing', 'loc': ('foo', 'x'), 'msg': 'Field required', 'input': {}}\n    ]", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000855", "source": "def is_resolved(self) -> bool:\n        return self.positive_branch_type.is_resolved \\\n                and self.negative_branch_type.is_resolved", "target": "def is_resolved(self) -> bool:\n        return self._ast_node is not None or self._module_name is not None", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000856", "source": "def typename(self) -> str:\n        return \"_typing.Any\"", "target": "def typename(self) -> str:\n        return self.alias_export_name", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pytorch/pytorch", "data_split": "train", "example_id": "000857", "source": "def type_format(self) -> str:\n        if TypeNode.compatible_to_runtime_usage:\n            return \"_typing.Optional[{}]\"\n        return \"{} | None\"", "target": "def listBoolDisjunction(self, input: list[bool]) -> bool:\n        res = False\n        for x in input:\n            res = res or x\n        return res", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "train", "example_id": "000858", "source": "def jacfwd(model, inp, v=None, strict=None):\n        argnums = tuple(range(len(inp)))\n        return ft.jacfwd(model, argnums)(*inp)", "target": "def large_transpose():\n    return (rand(8192, 8192).transpose(0, 1), rand(8192, 8192).transpose(0, 1))", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "000859", "source": "def is_union_type(typename: str) -> bool:\n    return typename.startswith('util_variant')", "target": "def _import_string_logic(dotted_path: str) -> Any:\n    from importlib import import_module\n    components = dotted_path.strip().split(':')\n    if len(components) > 2:\n        raise ImportError(f\"Import strings should have at most one ':'; received {dotted_path!r}\")\n    module_path = components[0]\n    if not module_path:\n        raise ImportError(f'Import strings should have a nonempty module name; received {dotted_path!r}')\n    try:\n        module = import_module(module_path)\n    except ModuleNotFoundError as e:\n        if '.' in module_path:\n            maybe_module_path, maybe_attribute = dotted_path.strip().rsplit('.', 1)\n            try:\n                return _import_string_logic(f'{maybe_module_path}:{maybe_attribute}')\n            except ImportError:\n                pass\n            raise ImportError(f'No module named {module_path!r}') from e\n        raise e\n    if len(components) > 1:\n        attribute = components[1]\n        try:\n            return getattr(module, attribute)\n        except AttributeError as e:\n            raise ImportError(f'cannot import name {attribute!r} from {module_path!r}') from e\n    else:\n        return module", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pytorch/pytorch", "data_split": "train", "example_id": "000860", "source": "def setup(desc):\n                    raise Exception('Throw from setup method')", "target": "def lstm_premul_bias_creator(script=True, **kwargs):\n    input, hidden, params, _ = lstm_inputs(return_module=False, **kwargs)\n    inputs = [input, hidden] + params[0]\n    return ModelDef(\n        inputs=inputs,\n        params=flatten_list(params),\n        forward=lstm_factory_premul_bias(premul_lstm_cell_no_bias, script),\n        backward_setup=lstm_backward_setup,\n        backward=simple_backward,\n    )", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "000861", "source": "def rnn_tanh_cell(input, hidden, w_ih, w_hh, b_ih, b_hh):\n    igates = torch.mm(input, w_ih.t()) + b_ih\n    hgates = torch.mm(hidden, w_hh.t()) + b_hh\n    return torch.tanh(igates + hgates)", "target": "def my_function(a, b, c):\n        return a + b + c", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000862", "source": "def validate_b(cls, v: str, info: core_schema.ValidationInfo) -> str:\n            assert v == 'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return 'hello world!'", "target": "def validate_b(\n            cls, v: bytes, nxt: core_schema.ValidatorFunctionWrapHandler, info: core_schema.ValidationInfo\n        ) -> str:\n            assert v == b'hello'\n            v = nxt(v)\n            assert v == 'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return 'hello world!'", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|pytorch/pytorch", "data_split": "train", "example_id": "000863", "source": "def target_generator(X, add_noise=False):\n    target = 0.5 + np.sin(3 * X)\n    if add_noise:\n        rng = np.random.RandomState(1)\n        target += rng.normal(0, 0.3, size=target.shape)\n    return target.squeeze()", "target": "def apply_rotary_emb(x: Tensor, freqs_cis: Tensor) -> Tensor:\n    xshaped = x.float().reshape(*x.shape[:-1], -1, 2)\n    freqs_cis = freqs_cis.view(1, xshaped.size(1), 1, xshaped.size(3), 2)\n    x_out2 = torch.stack(\n        [\n            xshaped[..., 0] * freqs_cis[..., 0] - xshaped[..., 1] * freqs_cis[..., 1],\n            xshaped[..., 1] * freqs_cis[..., 0] + xshaped[..., 0] * freqs_cis[..., 1],\n        ],\n        -1,\n    )\n    x_out2 = x_out2.flatten(3)\n    return x_out2.type_as(x)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000864", "source": "def test_err_on_invalid() -> None:\n    with pytest.raises(SchemaError, match='Cannot construct schema with `InvalidSchema` member.'):\n        SchemaValidator(core_schema.invalid_schema())", "target": "def test_union_decimal_simple(py_and_json: PyAndJson):\n    v = py_and_json({'type': 'union', 'choices': [{'type': 'decimal'}, {'type': 'list'}]})\n    assert v.validate_test('5') == 5\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_test('xxx')\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'decimal_parsing', 'loc': ('decimal',), 'msg': 'Input should be a valid decimal', 'input': 'xxx'},\n        {\n            'type': 'list_type',\n            'loc': ('list[any]',),\n            'msg': IsStr(regex='Input should be a valid (list|array)'),\n            'input': 'xxx',\n        },\n    ]", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000865", "source": "def eager(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, target = args\n        return lambda: F.cross_entropy(x, target, reduction=\"none\")", "target": "def eager(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        (x,) = args\n        return lambda: F.softmax(x, dim=-1)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "000866", "source": "def _call_initvar(*args: Any, **kwargs: Any) -> NoReturn:\n        raise TypeError(\"'InitVar' object is not callable\")", "target": "def constants(self) -> Dict[str, ConstantNode]:\n        return self._children[ASTNodeType.Constant]", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000867", "source": "def outMeta(desc1, desc2, depth):\n            return desc1", "target": "def outMeta(desc0, desc1):\n                    raise NotImplementedError(\"outMeta isn't implemented\")", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000868", "source": "def test_on_error_default(self, py_and_json: PyAndJson):\n        v = py_and_json(\n            {\n                'type': 'typed-dict',\n                'fields': {\n                    'x': {\n                        'type': 'typed-dict-field',\n                        'schema': {\n                            'type': 'default',\n                            'schema': {'type': 'str'},\n                            'on_error': 'default',\n                            'default': 'pika',\n                        },\n                    }\n                },\n            }\n        )\n        assert v.validate_test({'x': 'foo'}) == {'x': 'foo'}\n        assert v.validate_test({'x': ['foo']}) == {'x': 'pika'}", "target": "def test_smart_union_json_string_types(schema: core_schema.CoreSchema, input_value: str, expected_value: Any):\n    validator = SchemaValidator(core_schema.union_schema([schema, core_schema.str_schema()]))\n    assert validator.validate_json(f'\"{input_value}\"') == expected_value\n    assert validator.validate_python(input_value) == input_value", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000869", "source": "def test_property_other_error():\n    @dataclasses.dataclass\n    class Model:\n        width: int\n        @property\n        def area(self) -> int:\n            raise ValueError('xxx')\n    s = SchemaSerializer(\n        core_schema.model_schema(\n            Model,\n            core_schema.model_fields_schema(\n                {'width': core_schema.model_field(core_schema.int_schema())},\n                computed_fields=[core_schema.computed_field('area', core_schema.bytes_schema())],\n            ),\n        )\n    )\n    with pytest.raises(ValueError, match='^xxx$'):\n        s.to_python(Model(3))\n    with pytest.raises(ValueError, match='^xxx$'):\n        s.to_python(Model(3), mode='json')\n    e = '^Error serializing to JSON: ValueError: xxx$'\n    with pytest.raises(PydanticSerializationError, match=e):\n        s.to_json(Model(3))", "target": "def test_mixed_union_models_and_other_types() -> None:\n    s = SchemaSerializer(\n        core_schema.union_schema(\n            [\n                core_schema.tagged_union_schema(\n                    discriminator='type_',\n                    choices={\n                        'cat': core_schema.model_schema(\n                            cls=ModelCat,\n                            schema=core_schema.model_fields_schema(\n                                fields={\n                                    'type_': core_schema.model_field(core_schema.literal_schema(['cat'])),\n                                },\n                            ),\n                        ),\n                        'dog': core_schema.model_schema(\n                            cls=ModelDog,\n                            schema=core_schema.model_fields_schema(\n                                fields={\n                                    'type_': core_schema.model_field(core_schema.literal_schema(['dog'])),\n                                },\n                            ),\n                        ),\n                    },\n                ),\n                core_schema.str_schema(),\n            ]\n        )\n    )\n    assert s.to_python(ModelCat(type_='cat'), warnings='error') == {'type_': 'cat'}\n    assert s.to_python(ModelDog(type_='dog'), warnings='error') == {'type_': 'dog'}\n    assert s.to_python('a string', warnings='error') == 'a string'", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "000870", "source": "def add(a, b):\n    return 3 * a + b", "target": "def test_from_attributes_by_name():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={'a': core_schema.model_field(schema=core_schema.int_schema(), validation_alias='a_alias')},\n            from_attributes=True,\n        ),\n        config=CoreConfig(validate_by_name=True),\n    )\n    assert v.validate_python(Cls(a_alias=1)) == ({'a': 1}, None, {'a'})\n    assert v.validate_python(Cls(a=1)) == ({'a': 1}, None, {'a'})", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|pydantic/pydantic", "data_split": "train", "example_id": "000871", "source": "def make_estimator(self, params):\n        representation, init = params\n        max_iter = 5 if representation == \"sparse\" else 2\n        estimator = MiniBatchKMeans(\n            n_clusters=20,\n            init=init,\n            n_init=1,\n            max_iter=max_iter,\n            batch_size=1000,\n            max_no_improvement=None,\n            compute_labels=False,\n            random_state=0,\n        )\n        return estimator", "target": "def test_validate_assignment_allow_extra_validate():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={'field_a': core_schema.model_field(schema=core_schema.str_schema())},\n            extras_schema=core_schema.int_schema(),\n            extra_behavior='allow',\n        )\n    )\n    assert v.validate_assignment({'field_a': 'test'}, 'other_field', '456') == (\n        {'field_a': 'test'},\n        {'other_field': 456},\n        {'other_field'},\n    )\n    with pytest.raises(ValidationError) as exc_info:\n        assert v.validate_assignment({'field_a': 'test'}, 'other_field', 'xyz')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('other_field',),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'xyz',\n        }\n    ]", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000872", "source": "def benchmark(self):\n        for M, N in self.get_shapes():\n            print(f\"\\n Tensor dimensions: [{M}, {N}]\")\n            torch_dtype = cutlass_torch.dtype(cutlass.BFloat16)\n            x = 0.1 * torch.randn(M, N, device=\"cuda\", dtype=torch_dtype)\n            target = torch.randint(0, N, (M,), device=\"cuda\", dtype=torch.int64)\n            self.benchmark_single_shape((x, target), setting=f\"shape: [{M}, {N}]\")", "target": "def benchmark(self):\n        for M, N in self.get_shapes():\n            print(f\"Tensor dimensions: [{M}, {N}]\")\n            torch_dtype = cutlass_torch.dtype(cutlass.BFloat16)\n            x = 0.1 * torch.randn(\n                M, N, device=\"cuda\", dtype=torch_dtype, requires_grad=True\n            )\n            dy = torch.randn(M, N, device=\"cuda\", dtype=torch_dtype)\n            self.benchmark_single_shape((x, dy), setting=f\"shape: [{M}, {N}]\")", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000873", "source": "def my_function(a, b, c):\n        return a + b + c", "target": "def my_function(input_value, info):\n        return input_value + 'x'", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|scikit-learn/scikit-learn", "data_split": "train", "example_id": "000874", "source": "def test_repr() -> None:\n    v = SchemaValidator(\n        cs.arguments_v3_schema(\n            [\n                cs.arguments_v3_parameter(name='a', schema=cs.int_schema(), mode='positional_or_keyword'),\n                cs.arguments_v3_parameter(\n                    name='b',\n                    schema=cs.with_default_schema(schema=cs.int_schema(), default_factory=lambda: 42),\n                    mode='keyword_only',\n                ),\n            ]\n        )\n    )\n    assert 'positional_params_count:1,' in plain_repr(v)", "target": "def type_auto_or_int(val):\n    if val == \"auto\":\n        return \"auto\"\n    else:\n        return int(val)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "000875", "source": "def test_function_plain_model():\n    calls = 0\n    def wrap_function(value, _info):\n        nonlocal calls\n        calls += 1\n        return value.__dict__\n    class MyModel:\n        def __init__(self, **kwargs):\n            self.__dict__.update(kwargs)\n    s = SchemaSerializer(\n        core_schema.model_schema(\n            MyModel,\n            core_schema.typed_dict_schema(\n                {\n                    'a': core_schema.typed_dict_field(\n                        core_schema.any_schema(), serialization_exclude_if=lambda x: x == 100\n                    ),\n                    'b': core_schema.typed_dict_field(core_schema.any_schema()),\n                    'c': core_schema.typed_dict_field(core_schema.any_schema(), serialization_exclude=True),\n                }\n            ),\n            serialization=core_schema.plain_serializer_function_ser_schema(wrap_function, info_arg=True),\n        )\n    )\n    m = MyModel(a=1, b=b'foobar', c='not excluded')\n    assert calls == 0\n    assert s.to_python(m) == {'a': 1, 'b': b'foobar', 'c': 'not excluded'}\n    assert calls == 1\n    assert s.to_python(m, mode='json') == {'a': 1, 'b': 'foobar', 'c': 'not excluded'}\n    assert calls == 2\n    assert s.to_json(m) == b'{\"a\":1,\"b\":\"foobar\",\"c\":\"not excluded\"}'\n    assert calls == 3\n    assert s.to_python(m, exclude={'b'}) == {'a': 1, 'b': b'foobar', 'c': 'not excluded'}\n    assert calls == 4\n    assert s.to_python(m, mode='json', exclude={'b'}) == {'a': 1, 'b': 'foobar', 'c': 'not excluded'}\n    assert calls == 5\n    assert s.to_json(m, exclude={'b'}) == b'{\"a\":1,\"b\":\"foobar\",\"c\":\"not excluded\"}'\n    assert calls == 6", "target": "def getCMakeArgs(self):\n        args = TestRunner.getCMakeArgs(self)\n        args = args + [\n            \"-DIOS_ARCH=%s\" % self.arch,\n            \"-DIPHONEOS_DEPLOYMENT_TARGET=%s\" % os.environ['IPHONEOS_DEPLOYMENT_TARGET'],\n        ]\n        return args", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "train", "example_id": "000876", "source": "def compiled(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, w = args\n        torch._dynamo.mark_dynamic(x, 0)\n        compiled_layernorm = torch.compile(\n            self.layernorm_ref, mode=self.compile_mode, fullgraph=True\n        )\n        return lambda: compiled_layernorm(x, w, eps=1e-6)", "target": "def cherry_pick_commits(self, from_branch: str, to_branch: str) -> None:\n        orig_branch = self.current_branch()\n        assert orig_branch is not None, \"Must be on a branch\"\n        self.checkout(to_branch)\n        from_commits, to_commits = self.compute_branch_diffs(from_branch, to_branch)\n        if len(from_commits) == 0:\n            print(\"Nothing to do\")\n            self.checkout(orig_branch)\n            return\n        for commit in reversed(from_commits):\n            print(f\"Cherry picking commit {commit}\")\n            self.cherry_pick(commit)\n        self.checkout(orig_branch)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000877", "source": "def test_function_wrap_field_serializer_to_python():\n    class Model(RootModel):\n        def ser_root(self, v: Any, serializer: core_schema.SerializerFunctionWrapHandler, _) -> str:\n            root = serializer(v)\n            assert self.root == 1_000\n            return f'{root:_}'\n    s = SchemaSerializer(\n        core_schema.model_schema(\n            Model,\n            core_schema.int_schema(\n                serialization=core_schema.wrap_serializer_function_ser_schema(\n                    Model.ser_root, is_field_serializer=True, info_arg=True, schema=core_schema.any_schema()\n                )\n            ),\n            root_model=True,\n        )\n    )\n    assert s.to_python(Model(1000)) == '1_000'", "target": "def test_function_wrap_field_serializer_to_python():\n    class Model(TypedDict):\n        x: int\n    def ser_x(data: Model, v: Any, serializer: core_schema.SerializerFunctionWrapHandler, _) -> str:\n        x = serializer(v)\n        assert data['x'] == 1_000\n        return f'{x:_}'\n    s = SchemaSerializer(\n        core_schema.typed_dict_schema(\n            {\n                'x': core_schema.typed_dict_field(\n                    core_schema.int_schema(\n                        serialization=core_schema.wrap_serializer_function_ser_schema(\n                            ser_x, is_field_serializer=True, info_arg=True, schema=core_schema.any_schema()\n                        )\n                    )\n                )\n            }\n        )\n    )\n    assert s.to_python(Model(x=1000)) == {'x': '1_000'}", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000878", "source": "def compile_pattern(pattern: PatternType) -> re.Pattern[PatternType]:\n    try:\n        return re.compile(pattern)\n    except re.error:\n        raise PydanticCustomError('pattern_regex', 'Input should be a valid regular expression')", "target": "def serialize_deque(value, serializer, info: core_schema.SerializationInfo):\n        items = []\n        for index, item in enumerate(value):\n            try:\n                v = serializer(item, index)\n            except PydanticOmit:\n                pass\n            else:\n                items.append(v)\n        if info.mode_is_json():\n            return items\n        else:\n            return deque(items)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000879", "source": "def _prepare_once(self):\n        self.m = self.ModuleClass()\n        torch.set_float32_matmul_precision(\"high\")\n        self.input = torch.ones(10, device=self.device())", "target": "def _prepare_once(self) -> None:\n        self.a = torch.ones(10, 10, device=self.device())\n        self.b = torch.torch.ones(10, 10, device=self.device())", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|opencv/opencv", "data_split": "train", "example_id": "000880", "source": "def test_env_path_optional_unset_returns_default_path_no_resolve(self):\n        d = Path(\"z\")\n        with patch.dict(os.environ, {}, clear=True):\n            p = m.env_path_optional(\"P\", default=d, resolve=False)\n            self.assertEqual(p, d)", "target": "def required_usage_imports(self) -> Generator[str, None, None]:\n        yield \"import typing as _typing\"\n        yield from super().required_usage_imports", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000881", "source": "def preproc(ppp):\n                ppp.input().model().set_layout(Layout(\"NCHW\"))\n                ppp.input().tensor().set_element_type(Type.u8)                            \\\n                                    .set_spatial_static_shape(img.shape[0], img.shape[1]) \\\n                                    .set_layout(Layout(\"NHWC\"))\n                ppp.input().preprocess().resize(ResizeAlgorithm.RESIZE_LINEAR)", "target": "def preproc(ppp):\n                ppp.input().tensor().set_element_type(Type.u8)                            \\\n                                    .set_spatial_static_shape(img.shape[0], img.shape[1])\n                ppp.input().preprocess().resize(ResizeAlgorithm.RESIZE_LINEAR)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000882", "source": "def extract_function_name(func: ValidateCallSupportedTypes) -> str:\n    return f'partial({func.func.__name__})' if isinstance(func, functools.partial) else func.__name__", "target": "def test_function_change_id(strict: bool):\n    def f(input_value, info):\n        _, count = input_value.split('-')\n        return f'f-{int(count) + 1}'\n    v = SchemaValidator(\n        core_schema.definitions_schema(\n            core_schema.definition_reference_schema('root-schema'),\n            [\n                core_schema.union_schema(\n                    [\n                        core_schema.with_info_before_validator_function(\n                            f, core_schema.definition_reference_schema('root-schema')\n                        )\n                    ],\n                    auto_collapse=False,\n                    ref='root-schema',\n                )\n            ],\n        ),\n        config=CoreConfig(strict=strict),\n    )\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python('start-0')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'recursion_loop',\n            'loc': IsTuple(length=(1, 255)),\n            'msg': 'Recursion error - cyclic reference detected',\n            'input': IsStr(regex=r'f-\\d+'),\n        }\n    ]", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pytorch/pytorch", "data_split": "train", "example_id": "000883", "source": "def check_close_pairs(self, a, b, delta):\n        self.assertLessEqual(abs(a[0] - b[0]), delta)\n        self.assertLessEqual(abs(a[1] - b[1]), delta)", "target": "def _prepare_once(self):\n        torch._dynamo.config.capture_scalar_outputs = True\n        torch.manual_seed(0)\n        self.splits = torch.randint(10, (self.N,))", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000884", "source": "def test_utcoffset(self):\n        dummy = self.DT\n        for h in [0, 1.5, 12]:\n            offset = h * HOUR\n            self.assertEqual(timedelta(seconds=offset), TzInfo(offset).utcoffset(dummy))\n            self.assertEqual(timedelta(seconds=-offset), TzInfo(-offset).utcoffset(dummy))\n        self.assertEqual(self.EST.utcoffset(''), timedelta(hours=-5))\n        self.assertEqual(self.EST.utcoffset(5), timedelta(hours=-5))", "target": "def test_union_time_respects_downcasts_correctly():\n    serialization_schema = core_schema.plain_serializer_function_ser_schema(lambda v: None)\n    json_validation_schema = core_schema.no_info_plain_validator_function(\n        function=lambda v: v, serialization=serialization_schema\n    )\n    test_custom_ser_schema = core_schema.json_schema(\n        schema=json_validation_schema,\n        serialization=serialization_schema,\n    )\n    s = SchemaSerializer(core_schema.union_schema(choices=[core_schema.time_schema(), test_custom_ser_schema]))\n    assert s.to_python('foo') is None", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000885", "source": "def check_len(v: Any) -> bool:\n            if max_len is not None:\n                return (min_len <= len(v)) and (len(v) <= max_len)\n            return min_len <= len(v)", "target": "def get_values_dict(self) -> dict[str, Any]:\n        return {k: v for k, v in self.__dict__.items() if v is not None}", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "000886", "source": "def my_validator(value: Any, info: Any) -> str:\n        return str(value)", "target": "def f(a):\n            xs = a.tolist()\n            y = 0\n            if self.use_loop:\n                for i in xs:\n                    y += i\n            else:\n                y = sum(xs)\n            return torch.tensor(y)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000887", "source": "def get_shapes(self) -> tuple[tuple[int, ...], ...]:\n        return (\n            (32768, 256),\n            (32768, 512),\n            (32768, 1024),\n            (32768, 2048),\n            (32768, 4096),\n            (32768, 8192),\n            (32768, 16384),\n            (32768, 32768),\n            (32768, 65536),\n            (16384, 131072),\n            (8192, 262144),\n        )", "target": "def get_shapes(self) -> tuple[tuple[int, ...], ...]:\n        return (\n            (32768, 256),\n            (32768, 512),\n            (32768, 1024),\n            (32768, 2048),\n            (32768, 4096),\n            (32768, 8192),\n            (32768, 16384),\n        ) + extra_shapes_for_norm", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "000888", "source": "def test_config_date(\n    dt: date, expected_to_python, expected_to_json, expected_to_python_dict, expected_to_json_dict, mode\n):\n    s = SchemaSerializer(core_schema.date_schema(), config={'ser_json_temporal': mode})\n    assert s.to_python(dt) == dt\n    assert s.to_python(dt, mode='json') == expected_to_python\n    assert s.to_json(dt) == expected_to_json\n    with pytest.warns(\n        UserWarning,\n        match=(\n            r'Expected `date` - serialized value may not be as expected '\n            r\"\\[input_value=\\{datetime\\.date\\([^)]*\\): 'foo'\\}, input_type=dict\\]\"\n        ),\n    ):\n        assert s.to_python({dt: 'foo'}) == {dt: 'foo'}\n    with pytest.warns(\n        UserWarning,\n        match=(\n            r'Expected `date` - serialized value may not be as expected '\n            r\"\\[input_value=\\{datetime\\.date\\([^)]*\\): 'foo'\\}, input_type=dict\\]\"\n        ),\n    ):\n        assert s.to_python({dt: 'foo'}, mode='json') == expected_to_python_dict\n    with pytest.warns(\n        UserWarning,\n        match=(\n            r'Expected `date` - serialized value may not be as expected '\n            r\"\\[input_value=\\{datetime\\.date\\([^)]*\\): 'foo'\\}, input_type=dict\\]\"\n        ),\n    ):\n        assert s.to_json({dt: 'foo'}) == expected_to_json_dict", "target": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000889", "source": "def gt(self: _Pipeline[_InT, _NewOutGt], gt: _NewOutGt) -> _Pipeline[_InT, _NewOutGt]:\n        return self.constrain(annotated_types.Gt(gt))", "target": "def list_schema(\n    items_schema: CoreSchema | None = None,\n    *,\n    min_length: int | None = None,\n    max_length: int | None = None,\n    fail_fast: bool | None = None,\n    strict: bool | None = None,\n    ref: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: IncExSeqOrElseSerSchema | None = None,\n) -> ListSchema:\n    return _dict_not_none(\n        type='list',\n        items_schema=items_schema,\n        min_length=min_length,\n        max_length=max_length,\n        fail_fast=fail_fast,\n        strict=strict,\n        ref=ref,\n        metadata=metadata,\n        serialization=serialization,\n    )", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "000890", "source": "def make_scorers(self):\n        self.train_scorer = lambda _, __: self.estimator.kl_divergence_\n        self.test_scorer = lambda _, __: self.estimator.kl_divergence_", "target": "def make_scorers(self):\n        self.train_scorer = lambda _, __: neg_mean_inertia(\n            self.X, self.estimator.predict(self.X), self.estimator.cluster_centers_\n        )\n        self.test_scorer = lambda _, __: neg_mean_inertia(\n            self.X_val,\n            self.estimator.predict(self.X_val),\n            self.estimator.cluster_centers_,\n        )", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000891", "source": "def test_custom_error_type():\n    v = SchemaValidator(\n        core_schema.tagged_union_schema(\n            discriminator='foo',\n            custom_error_type='finite_number',\n            choices={\n                'apple': core_schema.typed_dict_schema(\n                    fields={\n                        'foo': core_schema.typed_dict_field(schema=core_schema.str_schema()),\n                        'bar': core_schema.typed_dict_field(schema=core_schema.int_schema()),\n                    }\n                ),\n                'banana': core_schema.typed_dict_schema(\n                    fields={\n                        'foo': core_schema.typed_dict_field(schema=core_schema.str_schema()),\n                        'spam': core_schema.typed_dict_field(\n                            schema=core_schema.list_schema(items_schema=core_schema.int_schema())\n                        ),\n                    }\n                ),\n            },\n        )\n    )\n    assert v.validate_python({'foo': 'apple', 'bar': '123'}) == {'foo': 'apple', 'bar': 123}\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python({'spam': 'apple', 'bar': 'Bar'})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'finite_number',\n            'loc': (),\n            'msg': 'Input should be a finite number',\n            'input': {'spam': 'apple', 'bar': 'Bar'},\n        }\n    ]\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python({'foo': 'other', 'bar': 'Bar'})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'finite_number',\n            'loc': (),\n            'msg': 'Input should be a finite number',\n            'input': {'foo': 'other', 'bar': 'Bar'},\n        }\n    ]", "target": "def display_as_type(obj: Any) -> str:\n    if isinstance(obj, (types.FunctionType, types.BuiltinFunctionType)):\n        return obj.__name__\n    elif obj is ...:\n        return '...'\n    elif isinstance(obj, Representation):\n        return repr(obj)\n    elif isinstance(obj, ForwardRef) or typing_objects.is_typealiastype(obj):\n        return str(obj)\n    if not isinstance(obj, (_typing_extra.typing_base, _typing_extra.WithArgsTypes, type)):\n        obj = obj.__class__\n    if is_union_origin(typing_extensions.get_origin(obj)):\n        args = ', '.join(map(display_as_type, typing_extensions.get_args(obj)))\n        return f'Union[{args}]'\n    elif isinstance(obj, _typing_extra.WithArgsTypes):\n        if typing_objects.is_literal(typing_extensions.get_origin(obj)):\n            args = ', '.join(map(repr, typing_extensions.get_args(obj)))\n        else:\n            args = ', '.join(map(display_as_type, typing_extensions.get_args(obj)))\n        try:\n            return f'{obj.__qualname__}[{args}]'\n        except AttributeError:\n            return str(obj).replace('typing.', '').replace('typing_extensions.', '')\n    elif isinstance(obj, type):\n        return obj.__qualname__\n    else:\n        return repr(obj).replace('typing.', '').replace('typing_extensions.', '')", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000892", "source": "def test_simple(self):\n        img1 = self.get_sample('stitching/a1.png')\n        img2 = self.get_sample('stitching/a2.png')\n        stitcher = cv.Stitcher.create(cv.Stitcher_PANORAMA)\n        stitcher.estimateTransform((img1, img2))\n        result, _ = stitcher.composePanorama((img1, img2))\n        assert result == 0", "target": "def test_simple(self):\n        finder = cv.ORB.create()\n        img1 = self.get_sample('stitching/a1.png')\n        img2 = self.get_sample('stitching/a2.png')\n        img_feat1 = cv.detail.computeImageFeatures2(finder, img1)\n        img_feat2 = cv.detail.computeImageFeatures2(finder, img2)\n        matcher = cv.detail.BestOf2NearestMatcher_create()\n        matches_info = matcher.apply(img_feat1, img_feat2)\n        self.assertIsNotNone(matches_info.matches)\n        self.assertIsNotNone(matches_info.inliers_mask)", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000893", "source": "def required_usage_imports(self) -> Generator[str, None, None]:\n        yield \"import typing as _typing\"", "target": "def required_usage_imports(self) -> Generator[str, None, None]:\n        for item in self:\n            yield from item.required_usage_imports", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000894", "source": "def validate_b(cls, v: bytes, info: core_schema.ValidationInfo) -> bytes:\n            assert v == b'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return b'hello world!'", "target": "def validate_b(cls, v: bytes, info: core_schema.ValidationInfo) -> bytes:\n            assert v == b'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return b'hello world!'", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000895", "source": "def required_usage_imports(self) -> Generator[str, None, None]:\n        yield \"import cv2.typing\"", "target": "def required_usage_imports(self) -> Generator[str, None, None]:\n        if TypeNode.compatible_to_runtime_usage:\n            yield \"import typing as _typing\"\n        yield from super().required_usage_imports", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000896", "source": "def eager(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, dy = args\n        y = F.softmax(x, dim=-1)\n        return lambda: torch.autograd.grad(y, x, grad_outputs=dy, retain_graph=True)", "target": "def eager(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, w, dy = args\n        y = self.layernorm_ref(x, w)\n        return lambda: torch.autograd.grad(\n            y, [x, w], grad_outputs=dy, retain_graph=True\n        )", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "000897", "source": "def model_dump(\n            self,\n            *,\n            mode: Literal['json', 'python'] | str = 'python',\n            include: Any = None,\n            exclude: Any = None,\n            context: dict[str, Any] | None = None,\n            by_alias: bool | None = None,\n            exclude_unset: bool = False,\n            exclude_defaults: bool = False,\n            exclude_none: bool = False,\n            exclude_computed_fields: bool = False,\n            round_trip: bool = False,\n            warnings: bool | Literal['none', 'warn', 'error'] = True,\n            serialize_as_any: bool = False,\n        ) -> Any:\n            ...", "target": "def required_definition_imports(self) -> Generator[str, None, None]:\n        yield \"import typing as _typing\"\n        yield from super().required_definition_imports", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000898", "source": "def from_name(cls, name: str):\n        return cls(ModelArgs.from_name(name))", "target": "def from_name(cls, name: str):\n        return cls(ModelArgs.from_name(name))", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|scikit-learn/scikit-learn", "data_split": "train", "example_id": "000899", "source": "def sanitize_for_s3(text: str) -> str:\n    return re.sub(r\"[^a-zA-Z0-9_-]\", \"_\", text)", "target": "def make_pca_scorers(caller):\n    caller.train_scorer = lambda _, __: caller.estimator.explained_variance_ratio_.sum()\n    caller.test_scorer = lambda _, __: (\n        explained_variance_ratio(caller.estimator.transform(caller.X_val), caller.X_val)\n    )", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "000900", "source": "def test_int_kwargs(py_and_json: PyAndJson, kwargs: dict[str, Any], input_value, expected):\n    v = py_and_json({'type': 'int', **kwargs})\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)) as exc_info:\n            v.validate_test(input_value)\n        errors = exc_info.value.errors(include_url=False)\n        assert len(errors) == 1\n        if 'ctx' in errors[0]:\n            assert errors[0]['ctx'] == kwargs\n    else:\n        output = v.validate_test(input_value)\n        assert output == expected\n        assert isinstance(output, int)", "target": "def int_(cls, ctype_name: str, export_name: Optional[str] = None,\n             doc: Optional[str] = None, required_modules: Tuple[str, ...] = ()):\n        return cls(ctype_name, PrimitiveTypeNode.int_(), export_name, doc, required_modules)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "000901", "source": "def forcePackage(self, package):\n        if package:\n            if package.startswith(\".\"):\n                self.pkg_target += package\n            else:\n                self.pkg_target = package", "target": "def test_tz_constraint_too_high():\n    with pytest.raises(SchemaError, match='OverflowError: Python int too large.*'):\n        SchemaValidator(core_schema.time_schema(tz_constraint=2**64))", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000902", "source": "def process(self, frameworks, data_fetcher):\n        samples_handled = 0\n        conf_mats = [np.zeros((data_fetcher.get_num_classes(), data_fetcher.get_num_classes())) for i in range(len(frameworks))]\n        blobs_l1_diff = [0] * len(frameworks)\n        blobs_l1_diff_count = [0] * len(frameworks)\n        blobs_l_inf_diff = [sys.float_info.min] * len(frameworks)\n        inference_time = [0.0] * len(frameworks)\n        for in_blob_dict, gt in data_fetcher:\n            frameworks_out = []\n            samples_handled += 1\n            for i in range(len(frameworks)):\n                start = time.time()\n                framework_name = frameworks[i].get_name()\n                out = frameworks[i].get_output(in_blob_dict[framework_name])\n                end = time.time()\n                segm = eval_segm_result(out)\n                conf_mats[i] += get_conf_mat(gt, segm[0])\n                frameworks_out.append(out)\n                inference_time[i] += end - start\n                pix_acc, mean_acc, miou = get_metrics(conf_mats[i])\n                name = frameworks[i].get_name()\n                print(samples_handled, 'Pixel accuracy, %s:' % name, 100 * pix_acc, file=self.log)\n                print(samples_handled, 'Mean accuracy, %s:' % name, 100 * mean_acc, file=self.log)\n                print(samples_handled, 'Mean IOU, %s:' % name, 100 * miou, file=self.log)\n                print(\"Inference time, ms \", \\\n                    frameworks[i].get_name(), inference_time[i] / samples_handled * 1000, file=self.log)\n            for i in range(1, len(frameworks)):\n                log_str = frameworks[0].get_name() + \" vs \" + frameworks[i].get_name() + ':'\n                diff = np.abs(frameworks_out[0] - frameworks_out[i])\n                l1_diff = np.sum(diff) / diff.size\n                print(samples_handled, \"L1 difference\", log_str, l1_diff, file=self.log)\n                blobs_l1_diff[i] += l1_diff\n                blobs_l1_diff_count[i] += 1\n                if np.max(diff) > blobs_l_inf_diff[i]:\n                    blobs_l_inf_diff[i] = np.max(diff)\n                print(samples_handled, \"L_INF difference\", log_str, blobs_l_inf_diff[i], file=self.log)\n            self.log.flush()\n        for i in range(1, len(blobs_l1_diff)):\n            log_str = frameworks[0].get_name() + \" vs \" + frameworks[i].get_name() + ':'\n            print('Final l1 diff', log_str, blobs_l1_diff[i] / blobs_l1_diff_count[i], file=self.log)", "target": "def process(self, frameworks, data_fetcher):\n        sorted_imgs_names = sorted(self.img_classes.keys())\n        correct_answers = [0] * len(frameworks)\n        samples_handled = 0\n        blobs_l1_diff = [0] * len(frameworks)\n        blobs_l1_diff_count = [0] * len(frameworks)\n        blobs_l_inf_diff = [sys.float_info.min] * len(frameworks)\n        inference_time = [0.0] * len(frameworks)\n        for x in range(0, len(sorted_imgs_names), self.batch_size):\n            sublist = sorted_imgs_names[x:x + self.batch_size]\n            batch = data_fetcher.get_batch(sublist)\n            samples_handled += len(sublist)\n            fw_accuracy = []\n            fw_time = []\n            frameworks_out = []\n            for i in range(len(frameworks)):\n                start = time.time()\n                out = frameworks[i].get_output(batch)\n                end = time.time()\n                correct_answers[i] += self.get_correct_answers(sublist, out)\n                fw_accuracy.append(100 * correct_answers[i] / float(samples_handled))\n                frameworks_out.append(out)\n                inference_time[i] += end - start\n                fw_time.append(inference_time[i] / samples_handled * 1000)\n                print(samples_handled, 'Accuracy for', frameworks[i].get_name() + ':', fw_accuracy[i], file=self.log)\n                print(\"Inference time, ms \", frameworks[i].get_name(), fw_time[i], file=self.log)\n                self.general_quality_metric.append(fw_accuracy)\n                self.general_inference_time.append(fw_time)\n            for i in range(1, len(frameworks)):\n                log_str = frameworks[0].get_name() + \" vs \" + frameworks[i].get_name() + ':'\n                diff = np.abs(frameworks_out[0] - frameworks_out[i])\n                l1_diff = np.sum(diff) / diff.size\n                print(samples_handled, \"L1 difference\", log_str, l1_diff, file=self.log)\n                blobs_l1_diff[i] += l1_diff\n                blobs_l1_diff_count[i] += 1\n                if np.max(diff) > blobs_l_inf_diff[i]:\n                    blobs_l_inf_diff[i] = np.max(diff)\n                print(samples_handled, \"L_INF difference\", log_str, blobs_l_inf_diff[i], file=self.log)\n            self.log.flush()\n        for i in range(1, len(blobs_l1_diff)):\n            log_str = frameworks[0].get_name() + \" vs \" + frameworks[i].get_name() + ':'\n            print('Final l1 diff', log_str, blobs_l1_diff[i] / blobs_l1_diff_count[i], file=self.log)\n        print(\n            get_final_summary_info(\n                self.general_quality_metric,\n                self.general_inference_time,\n                \"accuracy\"\n            ),\n            file=self.log\n        )", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000903", "source": "def classes(self) -> Dict[str, ClassNode]:\n        return self._children[ASTNodeType.Class]", "target": "def classes(self) -> Dict[str, \"ClassNode\"]:\n        return self._children[ASTNodeType.Class]", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "000904", "source": "def predict(self, X):\n        check_is_fitted(self)\n        return self.classifier_.predict(X)", "target": "def predict(self, X):\n        check_is_fitted(self)\n        predictions = [self.classes_[0]] * len(X)\n        return predictions", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000905", "source": "def test_only_allow_alias(py_and_json) -> None:\n    schema = core_schema.arguments_schema(\n        [\n            core_schema.arguments_parameter(name='a', schema=core_schema.str_schema(), alias='FieldA'),\n        ],\n        validate_by_name=False,\n        validate_by_alias=True,\n    )\n    v = py_and_json(schema)\n    assert v.validate_test(ArgsKwargs((), {'FieldA': 'hello'})) == ((), {'a': 'hello'})\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Missing required argument \\[type=missing_argument,'):\n        assert v.validate_test(ArgsKwargs((), {'a': 'hello'}))", "target": "def test_only_allow_alias(py_and_json) -> None:\n    v = py_and_json(\n        {\n            'type': 'model-fields',\n            'fields': {'field_a': {'validation_alias': 'FieldA', 'type': 'model-field', 'schema': {'type': 'int'}}},\n        },\n        config=CoreConfig(validate_by_name=False, validate_by_alias=True),\n    )\n    assert v.validate_test({'FieldA': '123'}) == ({'field_a': 123}, None, {'field_a'})\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'field_a': '123'})", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000906", "source": "def test_positional_empty(py_and_json: PyAndJson):\n    v = py_and_json({'type': 'tuple', 'items_schema': []})\n    assert v.validate_test([]) == ()\n    assert v.validate_python(()) == ()\n    with pytest.raises(ValidationError, match='type=too_long,'):\n        v.validate_test([1])", "target": "def test_set_fallback(input_value, json_output, expected_type):\n    v = SchemaSerializer(core_schema.set_schema(core_schema.int_schema()))\n    assert v.to_python({1, 2, 3}) == {1, 2, 3}\n    with pytest.warns(\n        UserWarning,\n        match=f'Expected `{expected_type}` - serialized value may not be as expected',\n    ):\n        assert v.to_python(input_value) == input_value\n    with pytest.warns(\n        UserWarning,\n        match=f'Expected `{expected_type}` - serialized value may not be as expected',\n    ):\n        assert v.to_python(input_value, mode='json') == json_output\n    with pytest.warns(\n        UserWarning,\n        match=f'Expected `{expected_type}` - serialized value may not be as expected',\n    ):\n        assert json.loads(v.to_json(input_value)) == json_output", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|scikit-learn/scikit-learn", "data_split": "train", "example_id": "000907", "source": "def get_ocv_arithm_op_table(apply_saturation=False):\n    def saturate(func):\n        def wrapped_func(x, y):\n            dst_dtype = x.dtype\n            if apply_saturation:\n                if np.issubdtype(x.dtype, np.integer):\n                    x = x.astype(np.int64)\n            if not isinstance(y, (float, int)):\n                if len(y) > x.shape[-1]:\n                    y = y[:x.shape[-1]]\n                else:\n                    y = rpad(y, x.shape[-1], pad_value=0)\n            dst = func(x, y)\n            if apply_saturation:\n                min_val, max_val = get_limits(dst_dtype)\n                dst = np.clip(dst, min_val, max_val)\n            return dst.astype(dst_dtype)\n        return wrapped_func\n    @saturate\n    def subtract(x, y):\n        return x - y\n    @saturate\n    def add(x, y):\n        return x + y\n    @saturate\n    def divide(x, y):\n        if not isinstance(y, (int, float)):\n            dst_dtype = np.result_type(x, y)\n            y = np.array(y).astype(dst_dtype)\n            _, max_value = get_limits(dst_dtype)\n            y[y == 0] = max_value\n        dst = 1.0 * x / y\n        if np.issubdtype(x.dtype, np.integer):\n            dst = np.rint(dst)\n        return dst\n    @saturate\n    def multiply(x, y):\n        return x * y\n    @saturate\n    def absdiff(x, y):\n        res = np.abs(x - y)\n        return res\n    return {\n        cv.subtract: subtract,\n        cv.add: add,\n        cv.multiply: multiply,\n        cv.divide: divide,\n        cv.absdiff: absdiff\n    }", "target": "def setup(app):\n    app.connect(\"builder-inited\", setup_link_role)\n    return {\"version\": \"0.1\", \"parallel_read_safe\": True}", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "000908", "source": "def get_module_ns_of(obj: Any) -> dict[str, Any]:\n    module_name = getattr(obj, '__module__', None)\n    if module_name:\n        try:\n            return sys.modules[module_name].__dict__\n        except KeyError:\n            return {}\n    return {}", "target": "def load_hook(self, state_dict, prefix, *args):\n        if prefix + \"wq.weight\" in state_dict:\n            wq = state_dict.pop(prefix + \"wq.weight\")\n            wk = state_dict.pop(prefix + \"wk.weight\")\n            wv = state_dict.pop(prefix + \"wv.weight\")\n            state_dict[prefix + \"wqkv.weight\"] = torch.cat([wq, wk, wv])", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000909", "source": "def handler() -> T | None:\n            if cls.model_rebuild(raise_errors=False, _parent_namespace_depth=5) is not False:\n                return attr_fn(cls)\n            return None", "target": "def handler() -> T | None:\n            if rebuild_dataclass(cls, raise_errors=False, _parent_namespace_depth=5) is not False:\n                return attr_fn(cls)\n            return None", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000910", "source": "def test_function_args():\n    f_info = None\n    def double(value, info):\n        nonlocal f_info\n        f_info = vars(info)\n        return value * 2\n    s = SchemaSerializer(\n        core_schema.any_schema(serialization=core_schema.plain_serializer_function_ser_schema(double, info_arg=True))\n    )\n    assert s.to_python(4) == 8\n    assert f_info == {\n        'mode': 'python',\n        'by_alias': None,\n        'exclude_unset': False,\n        'exclude_defaults': False,\n        'exclude_none': False,\n        'exclude_computed_fields': False,\n        'round_trip': False,\n        'serialize_as_any': False,\n    }\n    assert s.to_python('x') == 'xx'\n    assert s.to_python(4, mode='foobar', by_alias=True) == 8\n    assert f_info == {\n        'mode': 'foobar',\n        'by_alias': True,\n        'exclude_unset': False,\n        'exclude_defaults': False,\n        'exclude_none': False,\n        'exclude_computed_fields': False,\n        'round_trip': False,\n        'serialize_as_any': False,\n    }\n    assert s.to_json(42) == b'84'\n    assert f_info == {\n        'mode': 'json',\n        'by_alias': None,\n        'exclude_unset': False,\n        'exclude_defaults': False,\n        'exclude_none': False,\n        'exclude_computed_fields': False,\n        'round_trip': False,\n        'serialize_as_any': False,\n    }\n    assert s.to_python(7, mode='json', by_alias=False, exclude_unset=True) == 14\n    assert f_info == {\n        'mode': 'json',\n        'by_alias': False,\n        'exclude_unset': True,\n        'exclude_defaults': False,\n        'exclude_none': False,\n        'exclude_computed_fields': False,\n        'round_trip': False,\n        'serialize_as_any': False,\n    }\n    assert s.to_python(1, include={1, 2, 3}, exclude={'foo': {'bar'}}) == 2\n    assert f_info == {\n        'include': {3, 2, 1},\n        'exclude': {'foo': {'bar'}},\n        'mode': 'python',\n        'by_alias': None,\n        'exclude_unset': False,\n        'exclude_defaults': False,\n        'exclude_none': False,\n        'exclude_computed_fields': False,\n        'round_trip': False,\n        'serialize_as_any': False,\n    }\n    assert s.to_python(1, context='context') == 2\n    assert f_info == {\n        'context': 'context',\n        'mode': 'python',\n        'by_alias': None,\n        'exclude_unset': False,\n        'exclude_defaults': False,\n        'exclude_none': False,\n        'exclude_computed_fields': False,\n        'round_trip': False,\n        'serialize_as_any': False,\n    }", "target": "def forbid_inf_nan_check(x: Any) -> Any:\n    if not math.isfinite(x):\n        raise PydanticKnownError('finite_number')\n    return x", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000911", "source": "def test_dict():\n    v = SchemaValidator(\n        core_schema.dict_schema(keys_schema=core_schema.int_schema(), values_schema=core_schema.int_schema())\n    )\n    assert v.validate_json('{\"1\": 2, \"3\": 4}') == {1: 2, 3: 4}\n    assert json.loads('{\"1\": 1, \"1\": 2}') == {'1': 2}\n    assert v.validate_json('{\"1\": 1, \"1\": 2}') == {1: 2}", "target": "def test_dict(py_and_json: PyAndJson):\n    v = py_and_json({'type': 'dict', 'keys_schema': {'type': 'datetime'}, 'values_schema': {'type': 'int'}})\n    assert v.validate_test({'2000-01-01T00:00': 2, '2000-01-02T00:00': 4}) == {\n        datetime(2000, 1, 1): 2,\n        datetime(2000, 1, 2): 4,\n    }", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|scikit-learn/scikit-learn", "data_split": "train", "example_id": "000912", "source": "def add_constant(self, name: str, value: str) -> ConstantNode:\n        return self._add_child(ConstantNode, name, value=value)", "target": "def human_readable_data_quantity(quantity, multiple=1024):\n    if quantity == 0:\n        quantity = +0\n    SUFFIXES = [\"B\"] + [i + {1000: \"B\", 1024: \"iB\"}[multiple] for i in \"KMGTPEZY\"]\n    for suffix in SUFFIXES:\n        if quantity < multiple or suffix == SUFFIXES[-1]:\n            if suffix == SUFFIXES[0]:\n                return \"%d %s\" % (quantity, suffix)\n            else:\n                return \"%.1f %s\" % (quantity, suffix)\n        else:\n            quantity /= multiple", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000913", "source": "def test_with_default():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'field_a': core_schema.model_field(schema=core_schema.str_schema()),\n                'field_b': core_schema.model_field(\n                    schema=core_schema.with_default_schema(schema=core_schema.int_schema(), default=666)\n                ),\n            }\n        )\n    )\n    assert v.validate_python({'field_a': b'abc'}) == ({'field_a': 'abc', 'field_b': 666}, None, {'field_a'})\n    assert v.validate_python({'field_a': b'abc', 'field_b': 1}) == (\n        {'field_a': 'abc', 'field_b': 1},\n        None,\n        {'field_b', 'field_a'},\n    )", "target": "def test_with_default():\n    v = SchemaValidator(\n        core_schema.typed_dict_schema(\n            fields={\n                'field_a': core_schema.typed_dict_field(schema=core_schema.str_schema()),\n                'field_b': core_schema.typed_dict_field(\n                    schema=core_schema.with_default_schema(schema=core_schema.int_schema(), default=666)\n                ),\n            }\n        )\n    )\n    assert v.validate_python({'field_a': b'abc'}) == {'field_a': 'abc', 'field_b': 666}\n    assert v.validate_python({'field_a': b'abc', 'field_b': 1}) == {'field_a': 'abc', 'field_b': 1}", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000914", "source": "def _work(self) -> None:\n        @torch.compile(\n            backend=\"inductor\",\n            fullgraph=True,\n            dynamic=self._dynamic,\n        )\n        def f(a, b):\n            z = torch.mm(a, b)\n            for i in range(200):\n                z = torch.mm(z, b)\n            return z\n        with fresh_cache(), torch._inductor.config.patch(max_autotune=True):\n            f(self.a, self.b)", "target": "def _work(self):\n        @torch.compile(fullgraph=True)\n        def f(a, b):\n            xs = b.tolist()\n            for x in xs:\n                torch._check(x >= 0)\n                torch._check(x <= self.N)\n            return a.split(xs)\n        f(self.input, self.splits)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000915", "source": "def ser_x(self, v: Any, serializer: core_schema.SerializerFunctionWrapHandler, _) -> str:\n            x = serializer(v)\n            assert self.x == 1_000\n            return f'{x:_}'", "target": "def ser_x(data: Model, v: Any) -> str:\n        assert data['x'] == 1_000\n        return f'{v:_}'", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "000916", "source": "def test_duplicate_parameter_name() -> None:\n    with pytest.raises(SchemaError, match=\"Duplicate parameter 'test'\"):\n        SchemaValidator(\n            schema=cs.arguments_v3_schema(\n                [\n                    cs.arguments_v3_parameter(name='test', schema=cs.int_schema()),\n                    cs.arguments_v3_parameter(name='a', schema=cs.int_schema()),\n                    cs.arguments_v3_parameter(name='test', schema=cs.int_schema()),\n                ]\n            )\n        )", "target": "def get_all_ops(self):\n        for key in self.operator_db.keys():\n            try:\n                op = eval(key)\n            except AttributeError:\n                log.warning(\"Evaluating an op name into an OpOverload\", exc_info=True)\n                continue\n            yield op", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000917", "source": "def layernorm_ref(self, x: torch.Tensor, w: torch.Tensor, eps: float = 1e-6):\n        x_f32 = x.float()\n        return F.layer_norm(x_f32, w.shape, w, None, eps).to(x.dtype)", "target": "def layernorm_ref(self, x: torch.Tensor, w: torch.Tensor, eps: float = 1e-6):\n        x_f32 = x.float()\n        return F.layer_norm(x_f32, w.shape, w, None, eps).to(x.dtype)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "000918", "source": "def set_cached_generic_type(\n    parent: type[BaseModel],\n    typevar_values: tuple[Any, ...],\n    type_: type[BaseModel],\n    origin: type[BaseModel] | None = None,\n    args: tuple[Any, ...] | None = None,\n) -> None:\n    _GENERIC_TYPES_CACHE[_early_cache_key(parent, typevar_values)] = type_\n    if len(typevar_values) == 1:\n        _GENERIC_TYPES_CACHE[_early_cache_key(parent, typevar_values[0])] = type_\n    if origin and args:\n        _GENERIC_TYPES_CACHE[_late_cache_key(origin, args, typevar_values)] = type_", "target": "def quack(self, args, kwargs=None) -> Any:\n        from quack.softmax import softmax\n        assert kwargs is None\n        x, dy = args\n        y = softmax(x)\n        return lambda: torch.autograd.grad(y, x, grad_outputs=dy, retain_graph=True)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000919", "source": "def main() -> None:\n    from argparse import ArgumentParser\n    parser = ArgumentParser(\"Build Triton binaries\")\n    parser.add_argument(\"--release\", action=\"store_true\")\n    parser.add_argument(\n        \"--device\", type=str, default=\"cuda\", choices=[\"cuda\", \"rocm\", \"xpu\", \"aarch64\"]\n    )\n    parser.add_argument(\"--py-version\", type=str)\n    parser.add_argument(\"--commit-hash\", type=str)\n    parser.add_argument(\"--with-clang-ldd\", action=\"store_true\")\n    parser.add_argument(\"--triton-version\", type=str, default=None)\n    args = parser.parse_args()\n    triton_version = read_triton_version(args.device)\n    if args.triton_version:\n        triton_version = args.triton_version\n    build_triton(\n        device=args.device,\n        commit_hash=(\n            args.commit_hash if args.commit_hash else read_triton_pin(args.device)\n        ),\n        version=triton_version,\n        py_version=args.py_version,\n        release=args.release,\n        with_clang_ldd=args.with_clang_ldd,\n    )", "target": "def main(args: list[str]) -> None:\n    import generate_binary_build_matrix\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--cuda-stable-version\",\n        action=\"store_true\",\n        help=\"get cuda stable version\",\n    )\n    parser.add_argument(\n        \"--min-python-version\",\n        action=\"store_true\",\n        help=\"get min supported python version\",\n    )\n    options = parser.parse_args(args)\n    if options.cuda_stable_version:\n        return print(generate_binary_build_matrix.CUDA_STABLE)\n    if options.min_python_version:\n        return print(generate_binary_build_matrix.FULL_PYTHON_VERSIONS[0])", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000920", "source": "def test_defs_with_dict():\n    s = SchemaSerializer(\n        core_schema.definitions_schema(\n            schema=core_schema.typed_dict_schema(\n                {\n                    'foo': core_schema.typed_dict_field(\n                        core_schema.dict_schema(\n                            keys_schema=core_schema.definition_reference_schema('key'),\n                            values_schema=core_schema.definition_reference_schema('val'),\n                        )\n                    )\n                }\n            ),\n            definitions=[core_schema.str_schema(ref='key'), core_schema.str_schema(ref='val')],\n        )\n    )\n    assert s.to_json({'foo': {'key': 'val'}}) == b'{\"foo\":{\"key\":\"val\"}}'\n    assert s.to_python({'foo': {'key': 'val'}}) == {'foo': {'key': 'val'}}", "target": "def test_simple():\n    v = SchemaValidator(core_schema.str_schema())\n    assert v.validate_python(b'abc') == 'abc'\n    assert v.isinstance_python(b'abc') is True\n    assert v.validate_python(b'abc', self_instance='foobar') == 'abc'\n    assert v.isinstance_python(b'abc', self_instance='foobar') is True\n    assert v.validate_json('\"abc\"') == 'abc'\n    assert v.validate_json('\"abc\"', self_instance='foobar') == 'abc'", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000921", "source": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "target": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000922", "source": "def get_output(self, input_blob):\n        return super(DnnTfInceptionModel, self).get_output(input_blob)[..., 1:1001]", "target": "def get_output(self, input_blob):\n        if self.need_reshape:\n            self.net.blobs[self.in_blob_name].reshape(*input_blob.shape)\n        return self.net.forward_all(**{self.in_blob_name: input_blob})[self.out_blob_name]", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000923", "source": "def serialize_inner(v: InnerModel, serializer) -> Union[dict[str, str], str]:\n        v.x = v.x + ' modified'\n        return serializer(v)", "target": "def serialize_inner(v: InnerModel) -> str:\n        return v.x + ' modified'", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "000924", "source": "def to_pascal(snake: str) -> str:\n    camel = snake.title()\n    return re.sub('([0-9A-Za-z])_(?=[0-9A-Z])', lambda m: m.group(1), camel)", "target": "def add_enum(self, symbol_name, is_scoped_enum, entries):\n            if symbol_name in self.exported_enums:\n                assert symbol_name.name == \"<unnamed>\", \\\n                    \"Trying to export 2 enums with same symbol \" \\\n                    \"name: {}\".format(symbol_name)\n                enumeration_node = self.exported_enums[symbol_name]\n            else:\n                enumeration_node = EnumerationNode(symbol_name.name,\n                                                   is_scoped_enum)\n                self.exported_enums[symbol_name] = enumeration_node\n            for entry_name, entry_value in entries.items():\n                enumeration_node.add_constant(entry_name, entry_value)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000925", "source": "def description(self):\n        return \"a loop over 100 add node\"", "target": "def description(self) -> str:\n        return \"a mm 100 times in a loop with max auto tune on\"", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "000926", "source": "def test_function_wrap_no_info():\n    def f(input_value, validator):\n        return validator(input_value=input_value) + ' Changed'\n    v = SchemaValidator(core_schema.no_info_wrap_validator_function(f, core_schema.str_schema()))\n    assert v.validate_python('input value') == 'input value Changed'", "target": "def foo(x: int, y: int) -> int:\n            return x + y", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000927", "source": "def required_usage_imports(self) -> Generator[str, None, None]:\n        yield from ()", "target": "def required_usage_imports(self) -> Generator[str, None, None]:\n        yield \"import cv2.typing\"", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "000928", "source": "def make_data(self, params):\n        representation, precompute = params\n        if representation == \"dense\":\n            data = _synth_regression_dataset(n_samples=1000000, n_features=100)\n        else:\n            data = _synth_regression_sparse_dataset(\n                n_samples=50000, n_features=5000, density=0.01\n            )\n        return data", "target": "def make_data(self, params):\n        data = _synth_classification_dataset(\n            n_samples=10000, n_features=100, n_classes=5\n        )\n        return data", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000929", "source": "def test_dict_key(py_and_json: PyAndJson):\n    v = py_and_json(\n        core_schema.dict_schema(\n            core_schema.json_schema(core_schema.tuple_positional_schema([core_schema.int_schema()])),\n            core_schema.int_schema(),\n        )\n    )\n    assert v.validate_test({'[1]': 4}) == {(1,): 4}\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_test({'x': 4})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'json_invalid',\n            'loc': ('x', '[key]'),\n            'msg': 'Invalid JSON: expected value at line 1 column 1',\n            'input': 'x',\n            'ctx': {'error': 'expected value at line 1 column 1'},\n        }\n    ]", "target": "def test_list_any():\n    v = SchemaSerializer(core_schema.list_schema(core_schema.any_schema()))\n    assert v.to_python(['a', 'b', 'c']) == ['a', 'b', 'c']\n    assert v.to_python(['a', 'b', 'c'], mode='json') == ['a', 'b', 'c']\n    assert v.to_json(['a', 'b', 'c']) == b'[\"a\",\"b\",\"c\"]'\n    assert v.to_json(['a', 'b', 'c'], indent=2) == b'[\\n  \"a\",\\n  \"b\",\\n  \"c\"\\n]'", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|pytorch/pytorch", "data_split": "train", "example_id": "000930", "source": "def consolidate_scores(cv_results, scores, metric):\n    if metric == \"MAPE\":\n        scores[metric].append(f\"{value.mean():.2f}  {value.std():.2f}\")\n    else:\n        scores[metric].append(f\"{value.mean():.1f}  {value.std():.1f}\")\n    return scores", "target": "def get_v_for(model: Callable, inp: InputsType, task: str) -> VType:\n    v: VType\n    if task in [\"vjp\"]:\n        out = model(*inp)\n        v = torch.rand_like(out)\n    elif task in [\"jvp\", \"hvp\", \"vhp\"]:\n        if isinstance(inp, tuple):\n            v = tuple(torch.rand_like(i) for i in inp)\n        else:\n            v = torch.rand_like(inp)\n    else:\n        v = None\n    return v", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "000931", "source": "def with_nnc():\n    torch._C._jit_override_can_fuse_on_cpu(True)\n    torch._C._jit_override_can_fuse_on_gpu(True)\n    torch._C._jit_set_texpr_fuser_enabled(True)\n    torch._C._jit_set_nvfuser_enabled(False)\n    torch._C._jit_set_profiling_executor(True)\n    torch._C._jit_set_profiling_mode(True)", "target": "def complete_dataclass(\n    cls: type[Any],\n    config_wrapper: _config.ConfigWrapper,\n    *,\n    raise_errors: bool = True,\n    ns_resolver: NsResolver | None = None,\n    _force_build: bool = False,\n) -> bool:\n    original_init = cls.__init__\n    def __init__(__dataclass_self__: PydanticDataclass, *args: Any, **kwargs: Any) -> None:\n        __tracebackhide__ = True\n        s = __dataclass_self__\n        s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)\n    __init__.__qualname__ = f'{cls.__qualname__}.__init__'\n    cls.__init__ = __init__\n    cls.__pydantic_config__ = config_wrapper.config_dict\n    set_dataclass_fields(cls, config_wrapper=config_wrapper, ns_resolver=ns_resolver)\n    if not _force_build and config_wrapper.defer_build:\n        set_dataclass_mocks(cls)\n        return False\n    if hasattr(cls, '__post_init_post_parse__'):\n        warnings.warn(\n            'Support for `__post_init_post_parse__` has been dropped, the method will not be called',\n            PydanticDeprecatedSince20,\n        )\n    typevars_map = get_standard_typevars_map(cls)\n    gen_schema = GenerateSchema(\n        config_wrapper,\n        ns_resolver=ns_resolver,\n        typevars_map=typevars_map,\n    )\n    cls.__signature__ = LazyClassAttribute(\n        '__signature__',\n        partial(\n            generate_pydantic_signature,\n            init=original_init,\n            fields=cls.__pydantic_fields__,\n            validate_by_name=config_wrapper.validate_by_name,\n            extra=config_wrapper.extra,\n            is_dataclass=True,\n        ),\n    )\n    try:\n        schema = gen_schema.generate_schema(cls)\n    except PydanticUndefinedAnnotation as e:\n        if raise_errors:\n            raise\n        set_dataclass_mocks(cls, f'`{e.name}`')\n        return False\n    core_config = config_wrapper.core_config(title=cls.__name__)\n    try:\n        schema = gen_schema.clean_schema(schema)\n    except InvalidSchemaError:\n        set_dataclass_mocks(cls)\n        return False\n    cls = cast('type[PydanticDataclass]', cls)\n    cls.__pydantic_core_schema__ = schema\n    cls.__pydantic_validator__ = create_schema_validator(\n        schema, cls, cls.__module__, cls.__qualname__, 'dataclass', core_config, config_wrapper.plugin_settings\n    )\n    cls.__pydantic_serializer__ = SchemaSerializer(schema, core_config)\n    cls.__pydantic_complete__ = True\n    return True", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000932", "source": "def set_schema(\n    items_schema: CoreSchema | None = None,\n    *,\n    min_length: int | None = None,\n    max_length: int | None = None,\n    fail_fast: bool | None = None,\n    strict: bool | None = None,\n    ref: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> SetSchema:\n    return _dict_not_none(\n        type='set',\n        items_schema=items_schema,\n        min_length=min_length,\n        max_length=max_length,\n        fail_fast=fail_fast,\n        strict=strict,\n        ref=ref,\n        metadata=metadata,\n        serialization=serialization,\n    )", "target": "def deque_validator(input_value: Any, handler: core_schema.ValidatorFunctionWrapHandler) -> collections.deque[Any]:\n    return collections.deque(handler(input_value), maxlen=getattr(input_value, 'maxlen', None))", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|pydantic/pydantic", "data_split": "train", "example_id": "000933", "source": "def plot_top_words(model, feature_names, n_top_words, title):\n    fig, axes = plt.subplots(2, 5, figsize=(30, 15), sharex=True)\n    axes = axes.flatten()\n    for topic_idx, topic in enumerate(model.components_):\n        top_features_ind = topic.argsort()[-n_top_words:]\n        top_features = feature_names[top_features_ind]\n        weights = topic[top_features_ind]\n        ax = axes[topic_idx]\n        ax.barh(top_features, weights, height=0.7)\n        ax.set_title(f\"Topic {topic_idx + 1}\", fontdict={\"fontsize\": 30})\n        ax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n        for i in \"top right left\".split():\n            ax.spines[i].set_visible(False)\n        fig.suptitle(title, fontsize=40)\n    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n    plt.show()", "target": "def dataclass(\n        _cls: type[_T],\n        *,\n        init: Literal[False] = False,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool | None = None,\n        config: ConfigDict | type[object] | None = None,\n        validate_on_init: bool | None = None,\n    ) -> type[PydanticDataclass]: ...", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000934", "source": "def test_union():\n    v = SchemaValidator(cs.union_schema(choices=[cs.str_schema(), cs.datetime_schema()]))\n    assert v.validate_python('2022-01-02T00:00') == '2022-01-02T00:00'\n    assert v.validate_python(datetime(2022, 1, 2)) == datetime(2022, 1, 2)\n    v = SchemaValidator(cs.union_schema(choices=[cs.datetime_schema(), cs.str_schema()]))\n    assert v.validate_python('2022-01-02T00:00') == '2022-01-02T00:00'\n    assert v.validate_python(datetime(2022, 1, 2)) == datetime(2022, 1, 2)", "target": "def test_union():\n    v = SchemaValidator(core_schema.union_schema(choices=[core_schema.str_schema(), core_schema.time_schema()]))\n    assert v.validate_python('12:01:02') == '12:01:02'\n    assert v.validate_python(time(12, 1, 2)) == time(12, 1, 2)\n    v = SchemaValidator(core_schema.union_schema(choices=[core_schema.time_schema(), core_schema.str_schema()]))\n    assert v.validate_python('12:01:02') == '12:01:02'\n    assert v.validate_python(time(12, 1, 2)) == time(12, 1, 2)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000935", "source": "def test_by_alias_and_name_config_interaction(\n    config_by_alias: bool | None,\n    config_by_name: bool | None,\n    runtime_by_alias: bool | None,\n    runtime_by_name: bool | None,\n) -> None:\n    if config_by_alias is False and config_by_name is False and runtime_by_alias is False and runtime_by_name is False:\n        pytest.skip(\"Can't have both by_alias and by_name as effectively False\")\n    schema = cs.arguments_v3_schema(\n        arguments=[\n            cs.arguments_v3_parameter(name='my_field', schema=cs.int_schema(), alias='my_alias'),\n        ],\n        **({'validate_by_alias': config_by_alias} if config_by_alias is not None else {}),\n        **({'validate_by_name': config_by_name} if config_by_name is not None else {}),\n    )\n    s = SchemaValidator(schema)\n    alias_allowed = next(x for x in (runtime_by_alias, config_by_alias, True) if x is not None)\n    name_allowed = next(x for x in (runtime_by_name, config_by_name, False) if x is not None)\n    if alias_allowed:\n        assert s.validate_python(\n            ArgsKwargs((), {'my_alias': 1}), by_alias=runtime_by_alias, by_name=runtime_by_name\n        ) == (\n            (),\n            {'my_field': 1},\n        )\n    if name_allowed:\n        assert s.validate_python(\n            ArgsKwargs((), {'my_field': 1}), by_alias=runtime_by_alias, by_name=runtime_by_name\n        ) == (\n            (),\n            {'my_field': 1},\n        )", "target": "def test_by_alias_and_name_config_interaction(\n    config_by_alias: Union[bool, None],\n    config_by_name: Union[bool, None],\n    runtime_by_alias: Union[bool, None],\n    runtime_by_name: Union[bool, None],\n) -> None:\n    if config_by_alias is False and config_by_name is False and runtime_by_alias is False and runtime_by_name is False:\n        pytest.skip(\"Can't have both by_alias and by_name as effectively False\")\n    core_config = {\n        **({'validate_by_alias': config_by_alias} if config_by_alias is not None else {}),\n        **({'validate_by_name': config_by_name} if config_by_name is not None else {}),\n    }\n    schema = core_schema.typed_dict_schema(\n        fields={\n            'my_field': core_schema.typed_dict_field(schema=core_schema.int_schema(), validation_alias='my_alias'),\n        },\n        config=core_schema.CoreConfig(**core_config),\n    )\n    s = SchemaValidator(schema)\n    alias_allowed = next(x for x in (runtime_by_alias, config_by_alias, True) if x is not None)\n    name_allowed = next(x for x in (runtime_by_name, config_by_name, False) if x is not None)\n    if alias_allowed:\n        assert s.validate_python({'my_alias': 1}, by_alias=runtime_by_alias, by_name=runtime_by_name) == {'my_field': 1}\n    if name_allowed:\n        assert s.validate_python({'my_field': 1}, by_alias=runtime_by_alias, by_name=runtime_by_name) == {'my_field': 1}", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "000936", "source": "def get_output(self, input_blob):\n        tensor = torch.FloatTensor(input_blob)\n        out = self.net.forward(tensor).numpy()\n        return out", "target": "def f(input_value, validator, info):\n        assert repr(validator) == str(validator)\n        return plain_repr(validator)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "train", "example_id": "000937", "source": "def name(self) -> str:\n        if self.enable_persistent_tma_matmul:\n            return \"triton_persistent_tma\"\n        else:\n            return \"triton\"", "target": "def premul_lstm_cell_no_bias(\n    igates: Tensor, hidden: tuple[Tensor, Tensor], w_hh: Tensor, b_hh: Tensor\n) -> tuple[Tensor, Tensor]:\n    hx, cx = hidden\n    gates = igates + torch.mm(hx, w_hh.t()) + b_hh\n    ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)\n    ingate = torch.sigmoid(ingate)\n    forgetgate = torch.sigmoid(forgetgate)\n    cellgate = torch.tanh(cellgate)\n    outgate = torch.sigmoid(outgate)\n    cy = (forgetgate * cx) + (ingate * cellgate)\n    hy = outgate * torch.tanh(cy)\n    return hy, cy", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000938", "source": "def test_constraints_schema_validation_error(constraint: str) -> None:\n    with pytest.raises(SchemaError, match=f\"'{constraint}' must be coercible to an integer\"):\n        SchemaValidator(cs.int_schema(**{constraint: 'bad_value'}))", "target": "def test_constraints_schema_validation_error(constraint: str) -> None:\n    with pytest.raises(SchemaError, match=f\"'{constraint}' must be coercible to a timedelta instance\"):\n        SchemaValidator(core_schema.timedelta_schema(**{constraint: 'bad_value'}))", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000939", "source": "def train(self, samples, responses):\n        self.model.train(samples, cv.ml.ROW_SAMPLE, responses)", "target": "def train(self, samples, responses):\n        self.model.setType(cv.ml.SVM_C_SVC)\n        self.model.setC(1)\n        self.model.setKernel(cv.ml.SVM_RBF)\n        self.model.setGamma(.1)\n        self.model.train(samples, cv.ml.ROW_SAMPLE, responses.astype(int))", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000940", "source": "def liger(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        from liger_kernel.transformers.cross_entropy import LigerCrossEntropyLoss\n        x, target = args\n        cross_entropy = LigerCrossEntropyLoss(reduction=\"none\")\n        return lambda: cross_entropy(x, target)", "target": "def liger(self, args, kwargs=None) -> Any:\n        from liger_kernel.transformers.softmax import LigerSoftmax\n        assert kwargs is None\n        x, dy = args\n        softmax = LigerSoftmax().to(\"cuda\")\n        y = softmax(x)\n        return lambda: torch.autograd.grad(y, x, grad_outputs=dy, retain_graph=True)", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "000941", "source": "def f(x):\n    x = x.ravel()\n    return np.exp(-(x**2)) + 1.5 * np.exp(-((x - 2) ** 2))", "target": "def f(x):\n    return x * np.sin(x)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "000942", "source": "def as_generator(*items):\n    return (v for v in items)", "target": "def small_broadcast():\n    return (rand(4, 32), rand(32))", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "000943", "source": "def test_int_float():\n    v = SchemaValidator(core_schema.union_schema([core_schema.int_schema(), core_schema.float_schema()]))\n    assert v.validate_python(1) == IsInt(approx=1, delta=0)\n    assert v.validate_json('1') == IsInt(approx=1, delta=0)\n    assert v.validate_python(1.0) == IsFloat(approx=1, delta=0)\n    assert v.validate_json('1.0') == IsFloat(approx=1, delta=0)\n    v = SchemaValidator(core_schema.union_schema([core_schema.float_schema(), core_schema.int_schema()]))\n    assert v.validate_python(1) == IsInt(approx=1, delta=0)\n    assert v.validate_json('1') == IsInt(approx=1, delta=0)\n    assert v.validate_python(1.0) == IsFloat(approx=1, delta=0)\n    assert v.validate_json('1.0') == IsFloat(approx=1, delta=0)", "target": "def tokenize_function(examples):\n        return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000944", "source": "def validate_b(cls, v: str, info: core_schema.ValidationInfo) -> str:\n            assert v == 'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return 'hello world!'", "target": "def validate_b(cls, v: bytes, info: core_schema.ValidationInfo) -> bytes:\n            assert v == b'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return b'hello world!'", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000945", "source": "def outMeta(arr_desc0, arr_desc1, arr_desc2):\n        return cv.empty_array_desc()", "target": "def outMeta(desc, max_corners, quality_lvl,\n                    min_distance, block_sz,\n                    use_harris_detector, k):\n            return cv.empty_array_desc()", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|opencv/opencv", "data_split": "train", "example_id": "000946", "source": "def generate_table(contributors):\n    lines = [\n        \".. raw :: html\\n\",\n        \"    <!-- Generated by generate_authors_table.py -->\",\n        '    <div class=\"sk-authors-container\">',\n        \"    <style>\",\n        \"      img.avatar {border-radius: 10px;}\",\n        \"    </style>\",\n    ]\n    for contributor in contributors:\n        lines.append(\"    <div>\")\n        lines.append(\n            \"    <a href='%s'><img src='%s' class='avatar' /></a> <br />\"\n            % (contributor[\"html_url\"], contributor[\"avatar_url\"])\n        )\n        lines.append(\"    <p>%s</p>\" % (contributor[\"name\"],))\n        lines.append(\"    </div>\")\n    lines.append(\"    </div>\")\n    return \"\\n\".join(lines) + \"\\n\"", "target": "def getTestList(self, white, black):\n        res = [t for t in white or self.tests if self.getAlias(t) not in black]\n        if len(res) == 0:\n            raise Err(\"No tests found\")\n        return set(res)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000947", "source": "def predicate_func(v: Any) -> Any:\n            if not func(v):\n                raise PydanticCustomError(\n                    'predicate_failed',\n                    f'Predicate {predicate_name}failed',\n                )\n            return v", "target": "def _recreate_field_info(\n    field_info: FieldInfo,\n    ns_resolver: NsResolver,\n    typevars_map: Mapping[TypeVar, Any],\n    *,\n    lenient: bool,\n) -> FieldInfo:\n    FieldInfo_ = import_cached_field_info()\n    existing_desc = field_info.description\n    if lenient:\n        ann = _generics.replace_types(field_info._original_annotation, typevars_map)\n        ann, evaluated = _typing_extra.try_eval_type(\n            ann,\n            *ns_resolver.types_namespace,\n        )\n    else:\n        ann = _typing_extra.eval_type(\n            field_info._original_annotation,\n            *ns_resolver.types_namespace,\n        )\n        ann = _generics.replace_types(ann, typevars_map)\n        ann = _typing_extra.eval_type(\n            ann,\n            *ns_resolver.types_namespace,\n        )\n        evaluated = True\n    if (assign := field_info._original_assignment) is PydanticUndefined:\n        new_field = FieldInfo_.from_annotation(ann, _source=AnnotationSource.CLASS)\n    else:\n        new_field = FieldInfo_.from_annotated_attribute(ann, assign, _source=AnnotationSource.CLASS)\n        new_field._original_assignment = assign\n    new_field._original_annotation = ann\n    new_field.description = new_field.description if new_field.description is not None else existing_desc\n    if not evaluated:\n        new_field._complete = False\n    return new_field", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "000948", "source": "def set_frozen(self, fields: list[PydanticModelField], api: SemanticAnalyzerPluginInterface, frozen: bool) -> None:\n        info = self._cls.info\n        for field in fields:\n            sym_node = info.names.get(field.name)\n            if sym_node is not None:\n                var = sym_node.node\n                if isinstance(var, Var):\n                    var.is_property = frozen or field.is_frozen\n                elif isinstance(var, PlaceholderNode) and not self._api.final_iteration:\n                    self._api.defer()\n            else:\n                var = field.to_var(info, api, use_alias=False)\n                var.info = info\n                var.is_property = frozen\n                var._fullname = info.fullname + '.' + var.name\n                info.names[var.name] = SymbolTableNode(MDEF, var)", "target": "def copy(src: Union[str, Path], dst: Union[str, Path]) -> None:\n    src_path = get_path(src, resolve=True)\n    dst_path = get_path(dst, resolve=True)\n    if not src_path.exists():\n        raise FileNotFoundError(f\"Source does not exist: {src_path}\")\n    dst_path.parent.mkdir(parents=True, exist_ok=True)\n    if src_path.is_file():\n        shutil.copy2(src_path, dst_path)\n    elif src_path.is_dir():\n        shutil.copytree(src_path, dst_path, dirs_exist_ok=True)\n    else:\n        raise ValueError(f\"Unsupported path type: {src_path}\")", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000949", "source": "def full_typename(self) -> str:\n        return \"cv2.typing.\" + self.typename", "target": "def full_typename(self) -> str:\n        return self.type_format.format(self.types_separator.join(\n            item.full_typename for item in self\n        ))", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "000950", "source": "def write_base64_json(self, fname):\n        fs = cv.FileStorage(fname, cv.FileStorage_WRITE_BASE64)\n        mats = {'normal_2d_mat': self.get_normal_2d_mat(),\n                'normal_nd_mat': self.get_normal_nd_mat(),\n                'empty_2d_mat': self.get_empty_2d_mat(),\n                'random_mat': self.get_random_mat()}\n        for name, mat in mats.items():\n            fs.write(name, mat)\n        fs.release()\n        data = {}\n        with open(fname) as file:\n            data = json.load(file)\n        for name, mat in mats.items():\n            buffer = b''\n            if mat.size != 0:\n                if hasattr(mat, 'tobytes'):\n                    buffer = mat.tobytes()\n                else:\n                    buffer = mat.tostring()\n            self.assertEqual(buffer, self.decode(data[name]['data']))", "target": "def test_from_attributes_function(input_value, expected):\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={'a': core_schema.model_field(schema=core_schema.any_schema())}, from_attributes=True\n        )\n    )\n    model_dict, model_extra, fields_set = v.validate_python(input_value)\n    assert model_dict == expected\n    assert model_extra is None\n    assert fields_set == {'a'}", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "000951", "source": "def model_training_evaluation(\n    backend, train_dataloader, eval_dataloader, model, optimizer, num_epochs, evaluation\n):\n    model.to(device)\n    model.train()\n    loss_history = []\n    if not backend:\n        opt_training_iter_fn = training_iter_fn\n    else:\n        opt_training_iter_fn = torch._dynamo.optimize(backend)(training_iter_fn)\n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        for i, batch in enumerate(train_dataloader, 0):\n            batch = {k: v.to(device) for k, v in batch.items()}\n            loss = opt_training_iter_fn(batch, model, optimizer)\n            running_loss += loss.item()\n            if i % 100 == 99:\n                loss_history.append(running_loss / 100)\n                running_loss = 0.0\n    if evaluation:\n        metric = load_metric(\"accuracy\")\n        model.eval()\n        if not backend:\n            opt_model = model\n        else:\n            opt_model = torch._dynamo.optimize(backend)(model)\n        for batch in eval_dataloader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            with torch.no_grad():\n                outputs = opt_model(**batch)\n            logits = outputs.logits\n            predictions = torch.argmax(logits, dim=-1)\n            metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n        return loss_history, metric.compute()\n    else:\n        return loss_history, None", "target": "def test_union_frozenset_list(input_value, expected):\n    v = SchemaValidator(cs.union_schema(choices=[cs.frozenset_schema(), cs.list_schema()]))\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            v.validate_python(input_value)\n    else:\n        v.validate_python(input_value)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000952", "source": "def relative_typename(self, root: str) -> Optional[str]:\n            return self.type_node.relative_typename(root)", "target": "def relative_typename(self, module: str) -> str:\n        return self.type_format.format(self.types_separator.join(\n            item.relative_typename(module) for item in self\n        ))", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "000953", "source": "def model_serializer(\n    *, mode: Literal['wrap'], when_used: WhenUsed = 'always', return_type: Any = ...\n) -> Callable[[_ModelWrapSerializerT], _ModelWrapSerializerT]: ...", "target": "def eval_segm_result(net_out):\n    assert type(net_out) is np.ndarray\n    assert len(net_out.shape) == 4\n    channels_dim = 1\n    y_dim = channels_dim + 1\n    x_dim = y_dim + 1\n    res = np.zeros(net_out.shape).astype(int)\n    for i in range(net_out.shape[y_dim]):\n        for j in range(net_out.shape[x_dim]):\n            max_ch = np.argmax(net_out[..., i, j])\n            res[0, max_ch, i, j] = 1\n    return res", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000954", "source": "def outMeta(arr_desc0, arr_desc1, arr_desc2):\n        return cv.empty_array_desc(), cv.empty_array_desc(), \\\n               cv.empty_array_desc(), cv.empty_array_desc()", "target": "def outMeta(mat_desc, scalar_desc, dtype):\n            return mat_desc", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000955", "source": "def typename(self) -> Optional[str]:\n            return getattr(self.type_node, \"full_typename\", None)", "target": "def typename(self) -> str:\n        return self._typename", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000956", "source": "def test_custom_serializer():\n    s = SchemaSerializer(core_schema.any_schema(serialization=core_schema.simple_ser_schema('generator')))\n    assert s.to_python(gen_ok(1, 2), mode='json') == [1, 2]\n    assert s.to_json(gen_ok(1, 2)) == b'[1,2]'", "target": "def test_custom_serializer():\n    s = SchemaSerializer(core_schema.any_schema(serialization=core_schema.simple_ser_schema('json')))\n    assert s.to_python({1: 2}) == {1: 2}\n    assert s.to_python({1: 2}, mode='json') == {'1': 2}\n    assert s.to_python({1: 2}, mode='json', round_trip=True) == '{\"1\":2}'\n    assert s.to_json({1: 2}) == b'{\"1\":2}'\n    assert s.to_json({1: 2}, round_trip=True) == b'\"{\\\\\"1\\\\\":2}\"'", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000957", "source": "def getInfoPlist(self, builddirs):\n        return os.path.join(builddirs[0], \"visionos\", \"Info.plist\")", "target": "def getInfoPlist(self, builddirs):\n        return os.path.join(builddirs[0], \"osx\", \"Info.plist\")", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000958", "source": "def predict(self, samples):\n        _retval, results, _neigh_resp, _dists = self.model.findNearest(samples, self.k)\n        return results.ravel()", "target": "def predict(self, samples):\n        _ret, resp = self.model.predict(samples)\n        return resp.ravel()", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000959", "source": "def constrain(self: _Pipeline[_InT, _NewOutGt], constraint: annotated_types.Gt) -> _Pipeline[_InT, _NewOutGt]: ...", "target": "def constrain(\n        self: _Pipeline[_InT, _NewOutT], constraint: annotated_types.MultipleOf\n    ) -> _Pipeline[_InT, _NewOutT]: ...", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000960", "source": "def cp_torch_cleaning_script(self, params: VllmTestParameters):\n        script = get_path(params.cleaning_script, resolve=True)\n        vllm_script = Path(f\"./{self.work_directory}/use_existing_torch.py\")\n        copy(script, vllm_script)", "target": "def cp_torch_cleaning_script(self, inputs: VllmBuildParameters):\n        script = get_path(inputs.cleaning_script, resolve=True)\n        vllm_script = Path(f\"./{self.work_directory}/use_existing_torch.py\")\n        copy(script, vllm_script)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000961", "source": "def git_revision(dir: Path) -> str:\n    return subprocess.check_output(['git', 'rev-parse', '--short', 'HEAD'], cwd=dir).decode('utf-8').strip()", "target": "def area(self, area: float) -> None:\n            self.side = area**0.5", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000962", "source": "def resolve(self, root: ASTNode):\n        try:\n            self.positive_branch_type.resolve(root)\n            self.negative_branch_type.resolve(root)\n        except TypeResolutionError as e:\n            raise TypeResolutionError(\n                'Failed to resolve alias \"{}\" exposed as \"{}\"'.format(\n                    self.ctype_name, self.typename\n                )\n            ) from e", "target": "def resolve(self, root: ASTNode) -> None:\n        errors = []\n        for item in filter(lambda item: not item.is_resolved, self):\n            try:\n                item.resolve(root)\n            except TypeResolutionError as e:\n                errors.append(str(e))\n        if len(errors) > 0:\n            raise TypeResolutionError(\n                'Failed to resolve one of \"{}\" items. Errors: {}'.format(\n                    self.full_typename, errors\n                )\n            )", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000963", "source": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "target": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000964", "source": "def func(*args: Any) -> Any:\n        calls.append(args)\n        return args[0]", "target": "def func(*args: Any) -> Any:\n        assert len(args) == 2\n        input, handler = args\n        output = handler(input)\n        calls.append((input, output))\n        return output", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000965", "source": "def test_ignored_def():\n    v = SchemaValidator(\n        core_schema.definitions_schema(\n            core_schema.list_schema(core_schema.int_schema()), [core_schema.int_schema(ref='foobar')]\n        )\n    )\n    assert v.validate_python([1, 2, '3']) == [1, 2, 3]\n    r = plain_repr(v)\n    assert r.startswith('SchemaValidator(title=\"list[int]\",')", "target": "def check_in(v: Any) -> bool:\n            return operator.__contains__(values, v)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "000966", "source": "def test_json_bytes_base64_round_trip():\n    data = b'\\xd8\\x07\\xc1Tx$\\x91F%\\xf3\\xf3I\\xca\\xd8@\\x0c\\xee\\xc3\\xab\\xff\\x7f\\xd3\\xcd\\xcd\\xf9\\xc2\\x10\\xe4\\xa1\\xb01e'\n    encoded_std = b'\"2AfBVHgkkUYl8/NJythADO7Dq/9/083N+cIQ5KGwMWU=\"'\n    encoded_url = b'\"2AfBVHgkkUYl8_NJythADO7Dq_9_083N-cIQ5KGwMWU=\"'\n    assert to_json(data, bytes_mode='base64') == encoded_url\n    v = SchemaValidator(core_schema.bytes_schema(), config=CoreConfig(val_json_bytes='base64'))\n    assert v.validate_json(encoded_url) == data\n    assert v.validate_json(encoded_std) == data\n    with pytest.raises(ValidationError) as exc:\n        v.validate_json('\"wrong!\"')\n    [details] = exc.value.errors()\n    assert details['type'] == 'bytes_invalid_encoding'\n    assert to_json({'key': data}, bytes_mode='base64') == b'{\"key\":' + encoded_url + b'}'\n    v = SchemaValidator(\n        core_schema.dict_schema(keys_schema=core_schema.str_schema(), values_schema=core_schema.bytes_schema()),\n        config=CoreConfig(val_json_bytes='base64'),\n    )\n    assert v.validate_json(b'{\"key\":' + encoded_url + b'}') == {'key': data}", "target": "def test_parse_to_size_t_convertible_extra(self):\n        try_to_convert = partial(self._try_to_convert, cv.utils.dumpSizeT)\n        _, max_size_t = get_limits(ctypes.c_size_t)\n        for convertible in (max_size_t,):\n            expected = 'size_t: {0:d}'.format(convertible).lower()\n            actual = try_to_convert(convertible)\n            self.assertEqual(expected, actual,\n                             msg=get_conversion_error_msg(convertible, expected, actual))", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000967", "source": "def test_strict():\n    v = SchemaValidator(\n        {\n            'type': 'model-fields',\n            'fields': {\n                'field_a': {'type': 'model-field', 'schema': {'type': 'str'}},\n                'field_b': {'type': 'model-field', 'schema': {'type': 'int'}},\n            },\n        },\n        CoreConfig(strict=True),\n    )\n    assert v.validate_python({'field_a': 'hello', 'field_b': 12}) == (\n        {'field_a': 'hello', 'field_b': 12},\n        None,\n        {'field_a', 'field_b'},\n    )\n    with pytest.raises(ValidationError) as exc_info:\n        assert v.validate_python({'field_a': 123, 'field_b': '123'})\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'string_type', 'loc': ('field_a',), 'msg': 'Input should be a valid string', 'input': 123},\n        {'type': 'int_type', 'loc': ('field_b',), 'msg': 'Input should be a valid integer', 'input': '123'},\n    ]", "target": "def test_strict():\n    v = SchemaValidator(\n        {\n            'type': 'typed-dict',\n            'fields': {\n                'field_a': {'type': 'typed-dict-field', 'schema': {'type': 'str'}},\n                'field_b': {'type': 'typed-dict-field', 'schema': {'type': 'int'}},\n            },\n            'config': CoreConfig(strict=True),\n        }\n    )\n    assert v.validate_python({'field_a': 'hello', 'field_b': 12}) == {'field_a': 'hello', 'field_b': 12}\n    with pytest.raises(ValidationError) as exc_info:\n        assert v.validate_python({'field_a': 123, 'field_b': '123'})\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'string_type', 'loc': ('field_a',), 'msg': 'Input should be a valid string', 'input': 123},\n        {'type': 'int_type', 'loc': ('field_b',), 'msg': 'Input should be a valid integer', 'input': '123'},\n    ]", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "000968", "source": "def allgather_run(cmd):\n    proc = subprocess.run(shlex.split(cmd), capture_output=True)\n    assert proc.returncode == 0\n    return allgather_object(proc.stdout.decode(\"utf-8\"))", "target": "def _dedent_source_lines(source: list[str]) -> str:\n    dedent_source = textwrap.dedent(''.join(source))\n    if dedent_source.startswith((' ', '\\t')):\n        dedent_source = f'def dedent_workaround():\\n{dedent_source}'\n    return dedent_source", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|opencv/opencv", "data_split": "train", "example_id": "000969", "source": "def write_data(self, fname):\n        fs = cv.FileStorage(fname, cv.FileStorage_WRITE)\n        R = self.R0\n        T = self.T0\n        m = MyData()\n        fs.write('iterationNr', 100)\n        fs.startWriteStruct('strings', cv.FileNode_SEQ)\n        for elem in self.strings_data:\n            fs.write('', elem)\n        fs.endWriteStruct()\n        fs.startWriteStruct('Mapping', cv.FileNode_MAP)\n        fs.write('One', 1)\n        fs.write('Two', 2)\n        fs.endWriteStruct()\n        fs.write('R_MAT', R)\n        fs.write('T_MAT', T)\n        m.write(fs, 'MyData')\n        fs.release()", "target": "def add_constant(self, name: str, value: str) -> ConstantNode:\n        return self._add_child(ConstantNode, name, value=value)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000970", "source": "def test_constraints_schema_validation() -> None:\n    val = SchemaValidator(cs.date_schema(gt='2020-01-01'))\n    with pytest.raises(ValidationError):\n        val.validate_python('2019-01-01')", "target": "def test_constraints_schema_validation() -> None:\n    val = SchemaValidator(cs.int_schema(gt='1'))\n    with pytest.raises(ValidationError):\n        val.validate_python('0')", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000971", "source": "def ser_x(data: Model, v: Any, _) -> str:\n        assert data['x'] == 1_000\n        return f'{v:_}'", "target": "def ser_x(data: Model, v: Any, serializer: core_schema.SerializerFunctionWrapHandler) -> str:\n        assert data['x'] == 1_000\n        x = serializer(v)\n        return f'{x:_}'", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000972", "source": "def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = F.max_pool2d(x, 2)\n        x = torch.flatten(x, 1)\n        output = self.fc1(x)\n        return output", "target": "def forward(self, x):\n        for i, l in enumerate(self.linears):\n            x = self.linears[i // 2](x) + l(x)\n        return x", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000973", "source": "def time_schema(\n    *,\n    strict: bool | None = None,\n    le: time | None = None,\n    ge: time | None = None,\n    lt: time | None = None,\n    gt: time | None = None,\n    tz_constraint: Literal['aware', 'naive'] | int | None = None,\n    microseconds_precision: Literal['truncate', 'error'] = 'truncate',\n    ref: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> TimeSchema:\n    return _dict_not_none(\n        type='time',\n        strict=strict,\n        le=le,\n        ge=ge,\n        lt=lt,\n        gt=gt,\n        tz_constraint=tz_constraint,\n        microseconds_precision=microseconds_precision,\n        ref=ref,\n        metadata=metadata,\n        serialization=serialization,\n    )", "target": "def test_decimal():\n    v = SchemaSerializer(core_schema.decimal_schema())\n    assert v.to_python(Decimal('123.456')) == Decimal('123.456')\n    assert v.to_python(Decimal('123.456'), mode='json') == '123.456'\n    assert v.to_json(Decimal('123.456')) == b'\"123.456\"'\n    assert v.to_python(Decimal('123456789123456789123456789.123456789123456789123456789')) == Decimal(\n        '123456789123456789123456789.123456789123456789123456789'\n    )\n    assert (\n        v.to_json(Decimal('123456789123456789123456789.123456789123456789123456789'))\n        == b'\"123456789123456789123456789.123456789123456789123456789\"'\n    )\n    with pytest.warns(\n        UserWarning,\n        match=r'Expected `decimal` - serialized value may not be as expected \\[input_value=123, input_type=int\\]',\n    ):\n        assert v.to_python(123, mode='json') == 123\n    with pytest.warns(\n        UserWarning,\n        match=r'Expected `decimal` - serialized value may not be as expected \\[input_value=123, input_type=int\\]',\n    ):\n        assert v.to_json(123) == b'123'", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "train", "example_id": "000974", "source": "def _inner(fn):\n        if \"pre_grad_fusion_options\" in optimus_inductor_config:\n            torch._inductor.config.pre_grad_fusion_options = optimus_inductor_config[\n                \"pre_grad_fusion_options\"\n            ]\n        if \"post_grad_fusion_options\" in optimus_inductor_config:\n            torch._inductor.config.post_grad_fusion_options = optimus_inductor_config[\n                \"post_grad_fusion_options\"\n            ]\n        return torch.compile(\n            fn, backend=\"inductor\", fullgraph=nopython, mode=inductor_compile_mode\n        )", "target": "def test_overwrites_and_restores_existing_var(self):\n        var = \"TEST_TMP_ENV_OVERWRITE\"\n        os.environ[var] = \"orig\"\n        with temp_environ({var: \"override\"}):\n            self.assertEqual(os.environ[var], \"override\")\n        self.assertEqual(os.environ[var], \"orig\")", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000975", "source": "def definition_reference_schema(\n    schema_ref: str,\n    ref: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> DefinitionReferenceSchema:\n    return _dict_not_none(\n        type='definition-ref', schema_ref=schema_ref, ref=ref, metadata=metadata, serialization=serialization\n    )", "target": "def test_on_error_bad_default(self):\n        with pytest.raises(SchemaError, match=\"'on_error = default' requires a `default` or `default_factory`\"):\n            SchemaValidator(\n                schema=core_schema.typed_dict_schema(\n                    fields={\n                        'x': core_schema.typed_dict_field(\n                            schema=core_schema.with_default_schema(schema=core_schema.str_schema(), on_error='default')\n                        )\n                    }\n                )\n            )", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "000976", "source": "def contains_tensor(elems):\n    for elem in pytree.tree_leaves(elems):\n        if isinstance(elem, torch.Tensor):\n            return True\n    return False", "target": "def test_union_ref_strictness():\n    v = SchemaValidator(\n        core_schema.definitions_schema(\n            core_schema.typed_dict_schema(\n                {\n                    'a': core_schema.typed_dict_field(core_schema.definition_reference_schema('int-type')),\n                    'b': core_schema.typed_dict_field(\n                        core_schema.union_schema(\n                            [core_schema.definition_reference_schema('int-type'), core_schema.str_schema()]\n                        )\n                    ),\n                }\n            ),\n            [core_schema.int_schema(ref='int-type')],\n        )\n    )\n    assert v.validate_python({'a': 1, 'b': '2'}) == {'a': 1, 'b': '2'}\n    assert v.validate_python({'a': 1, 'b': 2}) == {'a': 1, 'b': 2}\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python({'a': 1, 'b': []})\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'int_type', 'loc': ('b', 'int'), 'msg': 'Input should be a valid integer', 'input': []},\n        {'type': 'string_type', 'loc': ('b', 'str'), 'msg': 'Input should be a valid string', 'input': []},\n    ]", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000977", "source": "def validate_inner(data, validator) -> InnerModel:\n        data['x'] = data['x'] + ' modified'\n        return validator(data)", "target": "def validate_inner(data) -> InnerModel:\n        data['x'] = data['x'] + ' modified'\n        return InnerModel(**data)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000978", "source": "def eager(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        (x,) = args\n        return lambda: F.softmax(x, dim=-1)", "target": "def eager(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, w = args\n        return lambda: self.rms_norm_ref(x, w)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000979", "source": "def test_include_dict(schema_func, seq_f):\n    v = SchemaSerializer(\n        schema_func(core_schema.any_schema(), serialization=core_schema.filter_seq_schema(include={1, 3, 5}))\n    )\n    assert v.to_python(seq_f(0, 1, 2, 3, 4)) == seq_f(1, 3)\n    assert v.to_python(seq_f('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h')) == seq_f('b', 'd', 'f')\n    assert v.to_python(seq_f(0, 1, 2, 3, 4), include={2: None}) == seq_f(1, 2, 3)\n    assert v.to_python(seq_f(0, 1, 2, 3, 4), include={2: {1, 2}}) == seq_f(1, 2, 3)\n    assert v.to_python(seq_f(0, 1, 2, 3, 4), include={2}) == seq_f(1, 2, 3)", "target": "def test_with_default():\n    v = SchemaValidator(\n        core_schema.typed_dict_schema(\n            fields={\n                'field_a': core_schema.typed_dict_field(schema=core_schema.str_schema()),\n                'field_b': core_schema.typed_dict_field(\n                    schema=core_schema.with_default_schema(schema=core_schema.int_schema(), default=666)\n                ),\n            }\n        )\n    )\n    assert v.validate_python({'field_a': b'abc'}) == {'field_a': 'abc', 'field_b': 666}\n    assert v.validate_python({'field_a': b'abc', 'field_b': 1}) == {'field_a': 'abc', 'field_b': 1}", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "000980", "source": "def name(self) -> str:\n        prefix = f\"{self.category()}\"\n        return prefix", "target": "def name(self):\n        if self.use_loop:\n            return f\"{self.category()}_loop\"\n        return self.category()", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000981", "source": "def required_usage_imports(self) -> Generator[str, None, None]:\n        yield \"import numpy\"\n        yield \"import typing as _typing\"", "target": "def required_usage_imports(self) -> Generator[str, None, None]:\n        yield \"import typing as _typing\"\n        yield from super().required_usage_imports", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "000982", "source": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "target": "def test_bytes():\n    s = SchemaValidator(core_schema.bytes_schema())\n    assert s.validate_json('\"foobar\"') == b'foobar'\n    with pytest.raises(ValidationError, match=r'Input should be a valid bytes \\[type=bytes_type,'):\n        s.validate_json('false')\n    with pytest.raises(ValidationError, match=r'Input should be a valid bytes \\[type=bytes_type,'):\n        s.validate_json('123')", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000983", "source": "def typename(self) -> str:\n        if self._use_numpy_generics:\n            dtype = self.dtype if self.dtype is not None else \"numpy.generic\"\n            return f\"numpy.ndarray[_typing.Any, numpy.dtype[{dtype}]]\"\n        return \"numpy.ndarray\"", "target": "def typename(self) -> str:\n        if self._ast_node is None:\n            return self._typename\n        typename = self._ast_node.export_name\n        if self._ast_node.node_type is not ASTNodeType.Enumeration:\n            return typename\n        parent = self._ast_node.parent\n        while parent.node_type is ASTNodeType.Class:\n            typename = parent.export_name + \"_\" + typename\n            parent = parent.parent\n        return typename", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "000984", "source": "def test_correct_function_signature() -> None:\n    def my_validator(value: Any, info: Any) -> str:\n        return str(value)\n    v = SchemaValidator(core_schema.with_info_plain_validator_function(my_validator))\n    assert v.validate_python(1) == '1'", "target": "def forward(\n        self, input: Tensor, state: tuple[Tensor, Tensor]\n    ) -> tuple[Tensor, tuple[Tensor, Tensor]]:\n        inputs = reverse(input.unbind(0))\n        outputs = jit.annotate(list[Tensor], [])\n        for i in range(len(inputs)):\n            out, state = self.cell(inputs[i], state)\n            outputs += [out]\n        return torch.stack(reverse(outputs)), state", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "000985", "source": "def test_function_wrap():\n    def f(input_value, handler, _info):\n        assert isinstance(input_value, dict)\n        v = handler(input_value)\n        assert isinstance(v, MyModel)\n        v.field_a += ' Changed'\n        return v\n    v = SchemaValidator(\n        {\n            'type': 'function-wrap',\n            'function': {'type': 'with-info', 'function': f},\n            'schema': core_schema.model_schema(\n                cls=MyModel,\n                schema=core_schema.model_fields_schema(\n                    fields={\n                        'field_a': core_schema.model_field(schema=core_schema.str_schema()),\n                        'field_b': core_schema.model_field(schema=core_schema.int_schema()),\n                    }\n                ),\n            ),\n        }\n    )\n    m = v.validate_python({'field_a': b'321', 'field_b': '12'})\n    assert isinstance(m, MyModel)\n    assert m.field_a == '321 Changed'\n    assert m.field_b == 12\n    m2 = MyModel()\n    v.validate_python({'field_a': b'321', 'field_b': '12'}, self_instance=m2)\n    assert m2.__dict__ == {'field_a': '321 Changed', 'field_b': 12}\n    assert m2.__pydantic_fields_set__ == {'field_a', 'field_b'}", "target": "def load_model(\n        self,\n        device,\n        model_name,\n        batch_size=None,\n        part=None,\n        extra_args=None,\n    ):\n        if self.args.enable_activation_checkpointing:\n            raise NotImplementedError(\n                \"Activation checkpointing not implemented for Torchbench models\"\n            )\n        is_training = self.args.training\n        use_eval_mode = self.args.use_eval_mode\n        candidates = [\n            f\"torchbenchmark.models.{model_name}\",\n            f\"torchbenchmark.canary_models.{model_name}\",\n            f\"torchbenchmark.models.fb.{model_name}\",\n        ]\n        for c in candidates:\n            try:\n                module = importlib.import_module(c)\n                break\n            except ModuleNotFoundError as e:\n                if e.name != c:\n                    raise\n        else:\n            raise ImportError(f\"could not import any of {candidates}\")\n        benchmark_cls = getattr(module, \"Model\", None)\n        if benchmark_cls is None:\n            raise NotImplementedError(f\"{model_name}.Model is None\")\n        if not hasattr(benchmark_cls, \"name\"):\n            benchmark_cls.name = model_name\n        cant_change_batch_size = (\n            not getattr(benchmark_cls, \"ALLOW_CUSTOMIZE_BSIZE\", True)\n            or model_name in self._config[\"dont_change_batch_size\"]\n        )\n        if cant_change_batch_size:\n            batch_size = None\n        if (\n            batch_size is None\n            and is_training\n            and model_name in self._batch_size[\"training\"]\n        ):\n            batch_size = self._batch_size[\"training\"][model_name]\n        elif (\n            batch_size is None\n            and not is_training\n            and model_name in self._batch_size[\"inference\"]\n        ):\n            batch_size = self._batch_size[\"inference\"][model_name]\n        if self.args.accuracy and model_name in self._accuracy[\"max_batch_size\"]:\n            batch_size = min(batch_size, self._accuracy[\"max_batch_size\"][model_name])\n        torch.backends.__allow_nonbracketed_mutation_flag = True\n        if extra_args is None:\n            extra_args = []\n        if part:\n            extra_args += [\"--part\", part]\n        if model_name == \"sam_fast\":\n            self.args.amp = True\n            self.setup_amp()\n        if model_name == \"vision_maskrcnn\" and is_training:\n            model_kwargs = {\"box_detections_per_img\": 4}\n            benchmark = benchmark_cls(\n                test=\"train\",\n                device=device,\n                batch_size=batch_size,\n                extra_args=extra_args,\n                model_kwargs=model_kwargs,\n            )\n            use_eval_mode = True\n        elif is_training:\n            benchmark = benchmark_cls(\n                test=\"train\",\n                device=device,\n                batch_size=batch_size,\n                extra_args=extra_args,\n            )\n        else:\n            benchmark = benchmark_cls(\n                test=\"eval\",\n                device=device,\n                batch_size=batch_size,\n                extra_args=extra_args,\n            )\n        model, example_inputs = benchmark.get_module()\n        if model_name in [\n            \"basic_gnn_edgecnn\",\n            \"basic_gnn_gcn\",\n            \"basic_gnn_sage\",\n            \"basic_gnn_gin\",\n        ]:\n            _reassign_parameters(model)\n        if is_training and (\n            not use_eval_mode or model_name in self._config[\"only_training\"]\n        ):\n            model.train()\n        else:\n            model.eval()\n        gc.collect()\n        batch_size = benchmark.batch_size\n        if model_name == \"torchrec_dlrm\":\n            batch_namedtuple = namedtuple(\n                \"Batch\", \"dense_features sparse_features labels\"\n            )\n            example_inputs = tuple(\n                batch_namedtuple(\n                    dense_features=batch.dense_features,\n                    sparse_features=batch.sparse_features,\n                    labels=batch.labels,\n                )\n                for batch in example_inputs\n            )\n        if model_name == \"yolov3\":\n            example_inputs = (torch.rand(batch_size, 3, 384, 512).to(device),)\n        if model_name == \"maml_omniglot\":\n            batch_size = 5\n            assert example_inputs[0].shape[0] == batch_size\n        if model_name == \"vision_maskrcnn\":\n            batch_size = 1\n        if self.args.trace_on_xla:\n            import torch_xla\n        if (\n            model_name.startswith(\"hf\")\n            and hasattr(model, \"config\")\n            and hasattr(model.config, \"use_cache\")\n        ):\n            model.config.use_cache = False\n        self.validate_model(model, example_inputs)\n        return device, benchmark.name, model, example_inputs, batch_size", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000986", "source": "def test_union():\n    v = SchemaValidator(cs.union_schema(choices=[cs.str_schema(), cs.date_schema()]))\n    assert v.validate_python('2022-01-02') == '2022-01-02'\n    assert v.validate_python(date(2022, 1, 2)) == date(2022, 1, 2)\n    v = SchemaValidator(cs.union_schema(choices=[cs.date_schema(), cs.str_schema()]))\n    assert v.validate_python('2022-01-02') == '2022-01-02'\n    assert v.validate_python(date(2022, 1, 2)) == date(2022, 1, 2)", "target": "def test_union():\n    v = SchemaValidator(core_schema.union_schema(choices=[core_schema.str_schema(), core_schema.time_schema()]))\n    assert v.validate_python('12:01:02') == '12:01:02'\n    assert v.validate_python(time(12, 1, 2)) == time(12, 1, 2)\n    v = SchemaValidator(core_schema.union_schema(choices=[core_schema.time_schema(), core_schema.str_schema()]))\n    assert v.validate_python('12:01:02') == '12:01:02'\n    assert v.validate_python(time(12, 1, 2)) == time(12, 1, 2)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "000987", "source": "def test_ignored_def():\n    s = SchemaSerializer(\n        core_schema.definitions_schema(\n            core_schema.list_schema(core_schema.int_schema()),\n            [core_schema.int_schema(ref='foobar', serialization=core_schema.to_string_ser_schema(when_used='always'))],\n        )\n    )\n    assert s.to_python([1, 2, 3]) == [1, 2, 3]", "target": "def test_cuda_upload_download(self):\n        npMat = (np.random.random((128, 128, 3)) * 255).astype(np.uint8)\n        cuMat = cv.cuda_GpuMat()\n        cuMat.upload(npMat)\n        self.assertTrue(np.allclose(cuMat.download(), npMat))", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|opencv/opencv", "data_split": "train", "example_id": "000988", "source": "def log_to_file(self, output_filename, *, skip_non_compute_operators=True):\n        sorted_operators = sorted(self.func_db.keys())\n        with open(output_filename, \"w\") as f:\n            for operator in sorted_operators:\n                if skip_non_compute_operators and non_compute_operator(eval(operator)):\n                    continue\n                f.write(f\"Operator: {operator}\\n\")\n                operator_inputs = self.func_db[operator]\n                for inps, count in operator_inputs.items():\n                    f.write(f\"cnt: {count}, \")\n                    for dtype_abbr in dtype_abbrs.values():\n                        inps = inps.replace(\"'\" + dtype_abbr + \"'\", dtype_abbr)\n                    f.write(inps)\n                    f.write(\"\\n\")", "target": "def types_separator(self) -> str:\n        return \", \"", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000989", "source": "def test_extract_used_refs_ignores_metadata():\n    v = SchemaValidator(core_schema.any_schema(metadata={'type': 'definition-ref'}))\n    assert v.validate_python([1, 2, 3]) == [1, 2, 3]\n    assert plain_repr(v).endswith('definitions=[],cache_strings=True)')", "target": "def test_from_attributes_by_name():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={'a': core_schema.model_field(schema=core_schema.int_schema(), validation_alias='a_alias')},\n            from_attributes=True,\n        ),\n        config=CoreConfig(validate_by_name=True),\n    )\n    assert v.validate_python(Cls(a_alias=1)) == ({'a': 1}, None, {'a'})\n    assert v.validate_python(Cls(a=1)) == ({'a': 1}, None, {'a'})", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000990", "source": "def test_list_py_or_json(py_and_json: PyAndJson, input_value, expected):\n    v = py_and_json({'type': 'list', 'items_schema': {'type': 'int'}})\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=expected.message):\n            v.validate_test(input_value)\n    else:\n        assert v.validate_test(input_value) == expected", "target": "def test_var_args_validation_error(py_and_json: PyAndJson, input_value, err_loc) -> None:\n    v = py_and_json(\n        cs.arguments_v3_schema(\n            [\n                cs.arguments_v3_parameter(name='args', schema=cs.int_schema(), mode='var_args'),\n            ]\n        )\n    )\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_test(input_value)\n    error = exc_info.value.errors()[0]\n    assert error['type'] == 'int_parsing'\n    assert error['loc'] == err_loc", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "000991", "source": "def test_uuid_deepcopy():\n    output = SchemaValidator(core_schema.uuid_schema()).validate_python('a6cc5730-2261-11ee-9c43-2eb5a363657c')\n    c = copy.deepcopy(output)\n    assert repr(output) == \"UUID('a6cc5730-2261-11ee-9c43-2eb5a363657c')\"\n    assert c == output\n    assert isinstance(output, UUID)", "target": "def pix_to_c(pix):\n        return pix[0] * 256 * 256 + pix[1] * 256 + pix[2]", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000992", "source": "def prepare_config(config: ConfigDict | dict[str, Any] | type[Any] | None) -> ConfigDict:\n    if config is None:\n        return ConfigDict()\n    if not isinstance(config, dict):\n        warnings.warn(DEPRECATION_MESSAGE, PydanticDeprecatedSince20, stacklevel=4)\n        config = {k: getattr(config, k) for k in dir(config) if not k.startswith('__')}\n    config_dict = cast(ConfigDict, config)\n    check_deprecated(config_dict)\n    return config_dict", "target": "def inlined_schema() -> cs.CoreSchema:\n    level = N\n    schema: cs.CoreSchema = {\n        'type': 'model',\n        'cls': MyModel,\n        'schema': {\n            'type': 'model-fields',\n            'fields': {str(c): {'type': 'model-field', 'schema': {'type': 'int'}} for c in range(N)},\n        },\n        'ref': f'model_{N}',\n    }\n    for level in reversed(range(N)):\n        schema = {\n            'type': 'model',\n            'cls': MyModel,\n            'schema': {\n                'type': 'model-fields',\n                'fields': {str(c): {'type': 'model-field', 'schema': schema} for c in range(N)},\n            },\n            'ref': f'model_{level}',\n        }\n    return schema", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "000993", "source": "def wrap_serializer_function_ser_schema(\n    function: WrapSerializerFunction,\n    *,\n    is_field_serializer: bool | None = None,\n    info_arg: bool | None = None,\n    schema: CoreSchema | None = None,\n    return_schema: CoreSchema | None = None,\n    when_used: WhenUsed = 'always',\n) -> WrapSerializerFunctionSerSchema:\n    if when_used == 'always':\n        when_used = None\n    return _dict_not_none(\n        type='function-wrap',\n        function=function,\n        is_field_serializer=is_field_serializer,\n        info_arg=info_arg,\n        schema=schema,\n        return_schema=return_schema,\n        when_used=when_used,\n    )", "target": "def inductor_aten_mm(a, b):\n    return torch.mm(a, b)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000994", "source": "def test_repr(any_serializer):\n    assert plain_repr(any_serializer) == 'SchemaSerializer(serializer=Any(AnySerializer),definitions=[])'", "target": "def f(input_value: Any, info: core_schema.ValidationInfo) -> Any:\n        return f'{info.field_name}: {input_value}'", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000995", "source": "def relative_typename(self, root: str) -> Optional[str]:\n            if self.type_node is not None:\n                return self.type_node.relative_typename(root)\n            return None", "target": "def relative_typename(self, module: str) -> str:\n        return self.full_typename", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "000996", "source": "def f(input_value, _info):\n        return input_value + ' Changed'", "target": "def _apply_step(step: _Step, s: cs.CoreSchema | None, handler: GetCoreSchemaHandler, source_type: Any) -> cs.CoreSchema:\n    if isinstance(step, _ValidateAs):\n        s = _apply_parse(s, step.tp, step.strict, handler, source_type)\n    elif isinstance(step, _ValidateAsDefer):\n        s = _apply_parse(s, step.tp, False, handler, source_type)\n    elif isinstance(step, _Transform):\n        s = _apply_transform(s, step.func, handler)\n    elif isinstance(step, _Constraint):\n        s = _apply_constraint(s, step.constraint)\n    elif isinstance(step, _PipelineOr):\n        s = cs.union_schema([handler(step.left), handler(step.right)])\n    else:\n        assert isinstance(step, _PipelineAnd)\n        s = cs.chain_schema([handler(step.left), handler(step.right)])\n    return s", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "000997", "source": "def area(self) -> int:\n            return self.width * self.height", "target": "def area(self) -> None:\n            self.side = 0.0", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "000998", "source": "def test_smart_union_validator_function():\n    inner_schema = core_schema.union_schema([core_schema.int_schema(), core_schema.float_schema()])\n    validator = SchemaValidator(inner_schema)\n    assert repr(validator.validate_python(1)) == '1'\n    assert repr(validator.validate_python(1.0)) == '1.0'\n    schema = core_schema.union_schema(\n        [core_schema.no_info_after_validator_function(lambda v: v * 2, inner_schema), core_schema.str_schema()]\n    )\n    validator = SchemaValidator(schema)\n    assert repr(validator.validate_python(1)) == '2'\n    assert repr(validator.validate_python(1.0)) == '2.0'\n    assert validator.validate_python('1') == '1'\n    schema = core_schema.union_schema(\n        [\n            core_schema.no_info_wrap_validator_function(lambda v, handler: handler(v) * 2, inner_schema),\n            core_schema.str_schema(),\n        ]\n    )\n    validator = SchemaValidator(schema)\n    assert repr(validator.validate_python(1)) == '2'\n    assert repr(validator.validate_python(1.0)) == '2.0'\n    assert validator.validate_python('1') == '1'", "target": "def test_parse_to_rect_not_convertible(self):\n        for not_convertible in (np.empty(shape=(4, 1)), (), [], np.array([]), (12, ),\n                                [3, 4, 5, 10, 123], {1: 2, 3:4, 5:10, 6:30},\n                                '1234', np.array([1, 2, 3, 4], dtype=np.float32),\n                                np.array([[1, 2], [3, 4], [5, 6], [6, 8]]), (1, 2, 5, 1.5)):\n            with self.assertRaises((TypeError), msg=get_no_exception_msg(not_convertible)):\n                _ = cv.utils.dumpRect(not_convertible)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "000999", "source": "def execute(cmd, cwd = None, output = None):\n    if not output:\n        print(\"Executing: %s in %s\" % (cmd, cwd), file=sys.stderr)\n        print('Executing: ' + ' '.join(cmd))\n        retcode = check_call(cmd, cwd = cwd)\n        if retcode != 0:\n            raise Exception(\"Child returned:\", retcode)\n    else:\n        with open(output, \"a\") as f:\n            f.flush()\n            p = Popen(cmd, cwd = cwd, stdout = f)\n            os.waitpid(p.pid, 0)", "target": "def execute(cmd, cwd = None):\n    print(\"Executing: %s in %s\" % (cmd, cwd), file=sys.stderr)\n    print('Executing: ' + ' '.join(cmd))\n    retcode = check_call(cmd, cwd = cwd)\n    if retcode != 0:\n        raise Exception(\"Child returned:\", retcode)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|opencv/opencv", "data_split": "train", "example_id": "001000", "source": "def main(args):\n    options = parser.parse_args(args)\n    build = confu.Build.from_options(options)\n    build.export_cpath(\"include\", [\"q8gemm.h\"])\n    with build.options(\n        source_dir=\"src\",\n        deps=[\n            build.deps.cpuinfo,\n            build.deps.clog,\n            build.deps.psimd,\n            build.deps.fxdiv,\n            build.deps.pthreadpool,\n            build.deps.FP16,\n        ],\n        extra_include_dirs=\"src\",\n    ):\n        requantization_objects = [\n            build.cc(\"requantization/precise-scalar.c\"),\n            build.cc(\"requantization/fp32-scalar.c\"),\n            build.cc(\"requantization/q31-scalar.c\"),\n            build.cc(\"requantization/gemmlowp-scalar.c\"),\n        ]\n        with build.options(isa=arm.neon if build.target.is_arm else None):\n            requantization_objects += [\n                build.cc(\"requantization/precise-psimd.c\"),\n                build.cc(\"requantization/fp32-psimd.c\"),\n            ]\n        if build.target.is_x86 or build.target.is_x86_64:\n            with build.options(isa=x86.sse2):\n                requantization_objects += [\n                    build.cc(\"requantization/precise-sse2.c\"),\n                    build.cc(\"requantization/fp32-sse2.c\"),\n                    build.cc(\"requantization/q31-sse2.c\"),\n                    build.cc(\"requantization/gemmlowp-sse2.c\"),\n                ]\n            with build.options(isa=x86.ssse3):\n                requantization_objects += [\n                    build.cc(\"requantization/precise-ssse3.c\"),\n                    build.cc(\"requantization/q31-ssse3.c\"),\n                    build.cc(\"requantization/gemmlowp-ssse3.c\"),\n                ]\n            with build.options(isa=x86.sse4_1):\n                requantization_objects += [\n                    build.cc(\"requantization/precise-sse4.c\"),\n                    build.cc(\"requantization/q31-sse4.c\"),\n                    build.cc(\"requantization/gemmlowp-sse4.c\"),\n                ]\n        if build.target.is_arm or build.target.is_arm64:\n            with build.options(isa=arm.neon if build.target.is_arm else None):\n                requantization_objects += [\n                    build.cc(\"requantization/precise-neon.c\"),\n                    build.cc(\"requantization/fp32-neon.c\"),\n                    build.cc(\"requantization/q31-neon.c\"),\n                    build.cc(\"requantization/gemmlowp-neon.c\"),\n                ]\n        qnnpytorch_pack_objects = [\n            build.cc(\"init.c\"),\n            build.cc(\"operator-delete.c\"),\n            build.cc(\"operator-run.c\"),\n            build.cc(\"add.c\"),\n            build.cc(\"average-pooling.c\"),\n            build.cc(\"channel-shuffle.c\"),\n            build.cc(\"clamp.c\"),\n            build.cc(\"convolution.c\"),\n            build.cc(\"indirection.c\"),\n            build.cc(\"deconvolution.c\"),\n            build.cc(\"fully-connected.c\"),\n            build.cc(\"global-average-pooling.c\"),\n            build.cc(\"hardsigmoid.c\"),\n            build.cc(\"hardswish.c\"),\n            build.cc(\"leaky-relu.c\"),\n            build.cc(\"max-pooling.c\"),\n            build.cc(\"sigmoid.c\"),\n            build.cc(\"softargmax.c\"),\n            build.cc(\"tanh.c\"),\n            build.cc(\"u8lut32norm/scalar.c\"),\n            build.cc(\"x8lut/scalar.c\"),\n        ]\n        with build.options(isa=arm.neon if build.target.is_arm else None):\n            qnnpytorch_pack_objects += [\n                build.cc(\"sconv/6x8-psimd.c\"),\n                build.cc(\"sdwconv/up4x9-psimd.c\"),\n                build.cc(\"sgemm/6x8-psimd.c\"),\n            ]\n        with build.options(isa=arm.neon if build.target.is_arm else None):\n            if build.target.is_arm or build.target.is_arm64:\n                qnnpytorch_pack_objects += [\n                    build.cc(\"q8avgpool/mp8x9p8q-neon.c\"),\n                    build.cc(\"q8avgpool/up8x9-neon.c\"),\n                    build.cc(\"q8avgpool/up8xm-neon.c\"),\n                    build.cc(\"q8conv/4x8-neon.c\"),\n                    build.cc(\"q8conv/8x8-neon.c\"),\n                    build.cc(\"q8dwconv/mp8x25-neon.c\"),\n                    build.cc(\"q8dwconv/mp8x27-neon.c\"),\n                    build.cc(\"q8dwconv/up8x9-neon.c\"),\n                    build.cc(\"q8gavgpool/mp8x7p7q-neon.c\"),\n                    build.cc(\"q8gavgpool/up8x7-neon.c\"),\n                    build.cc(\"q8gavgpool/up8xm-neon.c\"),\n                    build.cc(\"q8gemm/4x-sumrows-neon.c\"),\n                    build.cc(\"q8gemm/4x8-neon.c\"),\n                    build.cc(\"q8gemm/4x8c2-xzp-neon.c\"),\n                    build.cc(\"q8gemm/6x4-neon.c\"),\n                    build.cc(\"q8gemm/8x8-neon.c\"),\n                    build.cc(\"q8vadd/neon.c\"),\n                    build.cc(\"sgemm/5x8-neon.c\"),\n                    build.cc(\"sgemm/6x8-neon.c\"),\n                    build.cc(\"u8clamp/neon.c\"),\n                    build.cc(\"u8maxpool/16x9p8q-neon.c\"),\n                    build.cc(\"u8maxpool/sub16-neon.c\"),\n                    build.cc(\"u8rmax/neon.c\"),\n                    build.cc(\"x8zip/x2-neon.c\"),\n                    build.cc(\"x8zip/x3-neon.c\"),\n                    build.cc(\"x8zip/x4-neon.c\"),\n                    build.cc(\"x8zip/xm-neon.c\"),\n                ]\n            if build.target.is_arm:\n                qnnpytorch_pack_objects += [\n                    build.cc(\"hgemm/8x8-aarch32-neonfp16arith.S\"),\n                    build.cc(\"q8conv/4x8-aarch32-neon.S\"),\n                    build.cc(\"q8dwconv/up8x9-aarch32-neon.S\"),\n                    build.cc(\"q8gemm/4x8-aarch32-neon.S\"),\n                    build.cc(\"q8gemm/4x8c2-xzp-aarch32-neon.S\"),\n                ]\n            if build.target.is_arm64:\n                qnnpytorch_pack_objects += [\n                    build.cc(\"q8gemm/8x8-aarch64-neon.S\"),\n                    build.cc(\"q8conv/8x8-aarch64-neon.S\"),\n                ]\n            if build.target.is_x86 or build.target.is_x86_64:\n                with build.options(isa=x86.sse2):\n                    qnnpytorch_pack_objects += [\n                        build.cc(\"q8avgpool/mp8x9p8q-sse2.c\"),\n                        build.cc(\"q8avgpool/up8x9-sse2.c\"),\n                        build.cc(\"q8avgpool/up8xm-sse2.c\"),\n                        build.cc(\"q8conv/4x4c2-sse2.c\"),\n                        build.cc(\"q8dwconv/mp8x25-sse2.c\"),\n                        build.cc(\"q8dwconv/mp8x27-sse2.c\"),\n                        build.cc(\"q8dwconv/up8x9-sse2.c\"),\n                        build.cc(\"q8gavgpool/mp8x7p7q-sse2.c\"),\n                        build.cc(\"q8gavgpool/up8x7-sse2.c\"),\n                        build.cc(\"q8gavgpool/up8xm-sse2.c\"),\n                        build.cc(\"q8gemm/2x4c8-sse2.c\"),\n                        build.cc(\"q8gemm/4x4c2-sse2.c\"),\n                        build.cc(\"q8vadd/sse2.c\"),\n                        build.cc(\"u8clamp/sse2.c\"),\n                        build.cc(\"u8maxpool/16x9p8q-sse2.c\"),\n                        build.cc(\"u8maxpool/sub16-sse2.c\"),\n                        build.cc(\"u8rmax/sse2.c\"),\n                        build.cc(\"x8zip/x2-sse2.c\"),\n                        build.cc(\"x8zip/x3-sse2.c\"),\n                        build.cc(\"x8zip/x4-sse2.c\"),\n                        build.cc(\"x8zip/xm-sse2.c\"),\n                    ]\n            build.static_library(\"qnnpack\", qnnpytorch_pack_objects)\n    with build.options(\n        source_dir=\"test\",\n        deps={\n            (\n                build,\n                build.deps.cpuinfo,\n                build.deps.clog,\n                build.deps.pthreadpool,\n                build.deps.FP16,\n                build.deps.googletest,\n            ): any,\n            \"log\": build.target.is_android,\n        },\n        extra_include_dirs=[\"src\", \"test\"],\n    ):\n        build.unittest(\"hgemm-test\", build.cxx(\"hgemm.cc\"))\n        build.unittest(\"q8avgpool-test\", build.cxx(\"q8avgpool.cc\"))\n        build.unittest(\"q8conv-test\", build.cxx(\"q8conv.cc\"))\n        build.unittest(\"q8dwconv-test\", build.cxx(\"q8dwconv.cc\"))\n        build.unittest(\"q8gavgpool-test\", build.cxx(\"q8gavgpool.cc\"))\n        build.unittest(\"q8gemm-test\", build.cxx(\"q8gemm.cc\"))\n        build.unittest(\"q8vadd-test\", build.cxx(\"q8vadd.cc\"))\n        build.unittest(\"sconv-test\", build.cxx(\"sconv.cc\"))\n        build.unittest(\"sgemm-test\", build.cxx(\"sgemm.cc\"))\n        build.unittest(\"u8clamp-test\", build.cxx(\"u8clamp.cc\"))\n        build.unittest(\"u8lut32norm-test\", build.cxx(\"u8lut32norm.cc\"))\n        build.unittest(\"u8maxpool-test\", build.cxx(\"u8maxpool.cc\"))\n        build.unittest(\"u8rmax-test\", build.cxx(\"u8rmax.cc\"))\n        build.unittest(\"x8lut-test\", build.cxx(\"x8lut.cc\"))\n        build.unittest(\"x8zip-test\", build.cxx(\"x8zip.cc\"))\n        build.unittest(\"add-test\", build.cxx(\"add.cc\"))\n        build.unittest(\"average-pooling-test\", build.cxx(\"average-pooling.cc\"))\n        build.unittest(\"channel-shuffle-test\", build.cxx(\"channel-shuffle.cc\"))\n        build.unittest(\"clamp-test\", build.cxx(\"clamp.cc\"))\n        build.unittest(\"convolution-test\", build.cxx(\"convolution.cc\"))\n        build.unittest(\"deconvolution-test\", build.cxx(\"deconvolution.cc\"))\n        build.unittest(\"fully-connected-test\", build.cxx(\"fully-connected.cc\"))\n        build.unittest(\n            \"global-average-pooling-test\", build.cxx(\"global-average-pooling.cc\")\n        )\n        build.unittest(\"leaky-relu-test\", build.cxx(\"leaky-relu.cc\"))\n        build.unittest(\"max-pooling-test\", build.cxx(\"max-pooling.cc\"))\n        build.unittest(\"sigmoid-test\", build.cxx(\"sigmoid.cc\"))\n        build.unittest(\"softargmax-test\", build.cxx(\"softargmax.cc\"))\n        build.unittest(\"tanh-test\", build.cxx(\"tanh.cc\"))\n        build.unittest(\"hardsigmoid-test\", build.cxx(\"hardsigmoid.cc\"))\n        build.unittest(\"hardswish-test\", build.cxx(\"hardswish.cc\"))\n        build.unittest(\n            \"requantization-test\",\n            [build.cxx(\"requantization.cc\")] + requantization_objects,\n        )\n    benchmark_isa = None\n    if build.target.is_arm:\n        benchmark_isa = arm.neon\n    elif build.target.is_x86:\n        benchmark_isa = x86.sse4_1\n    with build.options(\n        source_dir=\"bench\",\n        deps={\n            (\n                build,\n                build.deps.cpuinfo,\n                build.deps.clog,\n                build.deps.pthreadpool,\n                build.deps.FP16,\n                build.deps.googlebenchmark,\n            ): any,\n            \"log\": build.target.is_android,\n        },\n        isa=benchmark_isa,\n        extra_include_dirs=\"src\",\n    ):\n        build.benchmark(\"add-bench\", build.cxx(\"add.cc\"))\n        build.benchmark(\"average-pooling-bench\", build.cxx(\"average-pooling.cc\"))\n        build.benchmark(\"channel-shuffle-bench\", build.cxx(\"channel-shuffle.cc\"))\n        build.benchmark(\"convolution-bench\", build.cxx(\"convolution.cc\"))\n        build.benchmark(\n            \"global-average-pooling-bench\", build.cxx(\"global-average-pooling.cc\")\n        )\n        build.benchmark(\"max-pooling-bench\", build.cxx(\"max-pooling.cc\"))\n        build.benchmark(\"sigmoid-bench\", build.cxx(\"sigmoid.cc\"))\n        build.benchmark(\"softargmax-bench\", build.cxx(\"softargmax.cc\"))\n        build.benchmark(\"tanh-bench\", build.cxx(\"tanh.cc\"))\n        build.benchmark(\"hardsigmoid-bench\", build.cxx(\"hardsigmoid.cc\"))\n        build.benchmark(\"hardswish-bench\", build.cxx(\"hardswish.cc\"))\n        build.benchmark(\"q8gemm-bench\", build.cxx(\"q8gemm.cc\"))\n        build.benchmark(\"hgemm-bench\", build.cxx(\"hgemm.cc\"))\n        build.benchmark(\"sgemm-bench\", build.cxx(\"sgemm.cc\"))\n        build.benchmark(\n            \"requantization-bench\",\n            [build.cxx(\"requantization.cc\")] + requantization_objects,\n        )\n    return build", "target": "def get_not_tested(self):\n        mset = self.dict2set(self.mdict)\n        tset = self.dict2set(self.tdict)\n        nottested = mset - tset\n        out = set()\n        for name in nottested:\n            out.add(name + \"   \" + self.mwhere[name])\n        return out", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001001", "source": "def create_generic_submodel(\n    model_name: str, origin: type[BaseModel], args: tuple[Any, ...], params: tuple[Any, ...]\n) -> type[BaseModel]:\n    namespace: dict[str, Any] = {'__module__': origin.__module__}\n    bases = (origin,)\n    meta, ns, kwds = prepare_class(model_name, bases)\n    namespace.update(ns)\n    created_model = meta(\n        model_name,\n        bases,\n        namespace,\n        __pydantic_generic_metadata__={\n            'origin': origin,\n            'args': args,\n            'parameters': params,\n        },\n        __pydantic_reset_parent_namespace__=False,\n        **kwds,\n    )\n    model_module, called_globally = _get_caller_frame_info(depth=3)\n    if called_globally:\n        object_by_reference = None\n        reference_name = model_name\n        reference_module_globals = sys.modules[created_model.__module__].__dict__\n        while object_by_reference is not created_model:\n            object_by_reference = reference_module_globals.setdefault(reference_name, created_model)\n            reference_name += '_'\n    return created_model", "target": "def test_any_with_timedelta_serializer():\n    s = SchemaSerializer(core_schema.any_schema(serialization={'type': 'timedelta'}))\n    assert s.to_python(timedelta(hours=2)) == timedelta(hours=2)\n    assert s.to_python(timedelta(hours=2), mode='json') == 'PT2H'\n    assert s.to_json(timedelta(hours=2)) == b'\"PT2H\"'\n    with pytest.warns(UserWarning) as warning_info:\n        assert s.to_python(b'bang', mode='json') == 'bang'\n    assert (\n        \"Expected `timedelta` - serialized value may not be as expected [input_value=b'bang', input_type=bytes]\"\n        in warning_info.list[0].message.args[0]\n    )", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001002", "source": "def name(self):\n        prefix = f\"{self.category()}_{self.backend()}\"\n        if self.is_dynamic():\n            prefix += \"_dynamic\"\n        if self.device() == \"cuda\":\n            prefix += \"_gpu\"\n        return prefix", "target": "def name(self) -> str:\n        prefix = f\"{self.category()}_{self.backend()}\"\n        if self.is_dynamic():\n            prefix += \"_dynamic\"\n        if self.device() == \"cuda\":\n            prefix += \"_gpu\"\n        return prefix", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "001003", "source": "def main(output_file=DEFAULT_OUTPUT_FILE, only_model=None):\n    results = []\n    if not only_model:\n        experiments = all_experiments.values()\n    else:\n        if only_model not in all_experiments:\n            print(\n                f\"Unknown model: {only_model}, all available models: {all_experiments.keys()}\"\n            )\n        experiments = [all_experiments[only_model]]\n    for func in experiments:\n        try:\n            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        except AssertionError:\n            device = \"cpu\"\n        torch.compiler.cudagraph_mark_step_begin()\n        lst = func(device)\n        for x in lst:\n            results.append(dataclasses.astuple(x))\n    headers = [field.name for field in dataclasses.fields(Experiment)]\n    for row in results:\n        output_csv(output_file, headers, row)\n        output_json(output_file, headers, row)", "target": "def test_only_allow_alias(py_and_json) -> None:\n    v = py_and_json(\n        {\n            'type': 'typed-dict',\n            'fields': {\n                'field_a': {'validation_alias': 'FieldA', 'type': 'typed-dict-field', 'schema': {'type': 'int'}}\n            },\n            'config': {'validate_by_name': False, 'validate_by_alias': True},\n        }\n    )\n    assert v.validate_test({'FieldA': '123'}) == {'field_a': 123}\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'field_a': '123'})", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "001004", "source": "def get_shapes(self) -> tuple[tuple[int, ...], ...]:\n        return (\n            (32768, 256),\n            (32768, 512),\n            (32768, 1024),\n            (32768, 2048),\n            (32768, 4096),\n            (32768, 8192),\n            (32768, 16384),\n            (32768, 32768),\n            (32768, 65536),\n            (16384, 131072),\n            (8192, 262144),\n        )", "target": "def test_dataclass_json():\n    schema = core_schema.dataclass_schema(\n        FooDataclass,\n        core_schema.dataclass_args_schema(\n            'FooDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    assert v.validate_json('{\"a\": \"hello\", \"b\": true}') == FooDataclass(a='hello', b=True)\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_json('[\"a\", \"b\"]')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'class_name': 'FooDataclass'},\n            'input': ['a', 'b'],\n            'loc': (),\n            'msg': 'Input should be an object',\n            'type': 'dataclass_type',\n        }\n    ]", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "001005", "source": "def test_dataclass_fields_read_env_at_instantiation(self):\n        @dataclass\n        class Cfg:\n            flag: bool = m.env_bool_field(\"FLAG\", default=False)\n            out: Path = m.env_path_field(\"OUT\", default=\"ab\", resolve=True)\n            name: str = m.env_str_field(\"NAME\", default=\"anon\")\n        with patch.dict(\n            os.environ, {\"FLAG\": \"true\", \"OUT\": \"outdir\", \"NAME\": \"alice\"}, clear=True\n        ):\n            cfg1 = Cfg()\n            self.assertTrue(cfg1.flag)\n            self.assertIsInstance(cfg1.out, Path)\n            self.assertTrue(cfg1.out.is_absolute())\n            self.assertEqual(cfg1.name, \"alice\")\n            cfg1.name = \"bob\"\n            self.assertEqual(cfg1.name, \"bob\")\n        with patch.dict(os.environ, {\"FLAG\": \"false\", \"NAME\": \"\"}, clear=True):\n            cfg2 = Cfg()\n            self.assertFalse(cfg2.flag)\n            self.assertTrue(\"ab\" in str(cfg2.out))\n            self.assertIsInstance(cfg2.out, Path)\n            self.assertTrue(cfg2.out.is_absolute())\n            self.assertEqual(cfg2.name, \"anon\")", "target": "def test_alias_allow_pop(py_and_json: PyAndJson):\n    v = py_and_json(\n        {\n            'type': 'model-fields',\n            'fields': {'field_a': {'validation_alias': 'FieldA', 'type': 'model-field', 'schema': {'type': 'int'}}},\n        },\n        config=CoreConfig(validate_by_name=True),\n    )\n    assert v.validate_test({'FieldA': '123'}) == ({'field_a': 123}, None, {'field_a'})\n    assert v.validate_test({'field_a': '123'}) == ({'field_a': 123}, None, {'field_a'})\n    assert v.validate_test({'FieldA': '1', 'field_a': '2'}) == ({'field_a': 1}, None, {'field_a'})\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'foobar': '123'})", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "train", "example_id": "001006", "source": "def sample(logits, temperature: float = 1.0, top_k: Optional[int] = None):\n    probs = logits_to_probs(logits[0, -1], temperature, top_k)\n    idx_next = multinomial_sample_one_no_sync(probs)\n    return idx_next, probs", "target": "def timed(fn):\n    start = torch.cuda.Event(enable_timing=True)\n    end = torch.cuda.Event(enable_timing=True)\n    start.record()\n    result = fn()\n    end.record()\n    torch.cuda.synchronize()\n    return result, start.elapsed_time(end) / 1000", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "001007", "source": "def skip(self, params):\n        representation, solver = params\n        if representation == \"sparse\" and solver == \"svd\":\n            return True\n        return False", "target": "def skip(self, params):\n        representation, precompute = params\n        if representation == \"sparse\" and precompute is False:\n            return True\n        return False", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "001008", "source": "def modeldef(request, net_name, executor, fuser):\n    set_fuser(fuser, executor)\n    name, rnn_creator, context = get_nn_runners(net_name)[0]\n    creator_args = {\n        \"seqLength\": 100,\n        \"numLayers\": 1,\n        \"inputSize\": 512,\n        \"hiddenSize\": 512,\n        \"miniBatch\": 64,\n        \"device\": \"cuda\",\n        \"seed\": None,\n    }\n    return rnn_creator(**creator_args)", "target": "def _safe_repr(v: Any) -> int | float | str:\n    if isinstance(v, (int, float, str)):\n        return v\n    return repr(v)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pytorch/pytorch", "data_split": "train", "example_id": "001009", "source": "def required_usage_imports(self) -> Generator[str, None, None]:\n        yield from ()", "target": "def test_cuda_gds_errors_captured() -> None:\n    major_version = int(torch.version.cuda.split(\".\")[0])\n    minor_version = int(torch.version.cuda.split(\".\")[1])\n    if target_os == \"windows\":\n        print(f\"{target_os} is not supported for GDS smoke test\")\n        return\n    if major_version < 12 or (major_version == 12 and minor_version < 6):\n        print(\"CUDA version is not supported for GDS smoke test\")\n        return\n    cuda_exception_missed = True\n    try:\n        print(\"Testing test_cuda_gds_errors_captured\")\n        with NamedTemporaryFile() as f:\n            torch.cuda.gds.GdsFile(f.name, os.O_CREAT | os.O_RDWR)\n    except RuntimeError as e:\n        expected_error = \"cuFileHandleRegister failed\"\n        if re.search(expected_error, f\"{e}\"):\n            print(f\"Caught CUDA exception with success: {e}\")\n            cuda_exception_missed = False\n        else:\n            raise e\n    if cuda_exception_missed:\n        raise RuntimeError(\n            \"Expected cuFileHandleRegister failed RuntimeError but have not received!\"\n        )", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001010", "source": "def f(value, serializer, _info):\n        return f'result={serializer(value)}'", "target": "def f(value, handler, _info):\n        return handler(value)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001011", "source": "def liger(self, args, kwargs) -> Any:\n        from liger_kernel.transformers.rms_norm import LigerRMSNorm\n        x, w = args\n        M, N = x.shape\n        liger_rmsnorm = LigerRMSNorm(hidden_size=N, eps=1e-6).cuda()\n        liger_rmsnorm.weight.data.copy_(w)\n        return lambda: liger_rmsnorm(x)", "target": "def liger(self, args, kwargs) -> Any:\n        from liger_kernel.ops.layer_norm import layer_norm_backward\n        x, w, dy = args\n        eps = 1e-6\n        mean, rstd = self.compute_mean_rstd(x, eps)\n        M, N = x.shape\n        return lambda: layer_norm_backward(dy, x, w, None, mean, rstd)[0:2]", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "001012", "source": "def print_error(text):\n    print(\"=\"*60, file=sys.stderr)\n    print(\"ERROR: %s\" % text, file=sys.stderr)\n    print(\"=\"*60, file=sys.stderr)", "target": "def test_on_config():\n    v = SchemaValidator(cs.str_schema(), config=CoreConfig(str_max_length=5))\n    assert 'max_length:Some(5)' in plain_repr(v)\n    assert v.isinstance_python('test') is True\n    assert v.isinstance_python('test long') is False", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|opencv/opencv", "data_split": "train", "example_id": "001013", "source": "def type_format(self) -> str:\n        if TypeNode.compatible_to_runtime_usage:\n            return \"_typing.Union[{}]\"\n        return \"{}\"", "target": "def run(self):\n        try:\n            self._run()\n        except Exception as e:\n            print(\"=\"*60, file=sys.stderr)\n            print(\"ERROR: %s\" % e, file=sys.stderr)\n            print(\"=\"*60, file=sys.stderr)\n            traceback.print_exc(file=sys.stderr)\n            sys.exit(1)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "001014", "source": "def test_from_attributes_path_error():\n    class PropertyError:\n        @property\n        def foo(self):\n            raise RuntimeError('intentional error')\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'my_field': core_schema.model_field(\n                    validation_alias=[['foo', 'bar', 'bat'], ['foo', 3], ['spam']], schema=core_schema.int_schema()\n                )\n            },\n            from_attributes=True,\n        )\n    )\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python(PropertyError())\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'get_attribute_error',\n            'loc': ('my_field',),\n            'msg': 'Error extracting attribute: RuntimeError: intentional error',\n            'input': HasRepr(IsStr(regex='.+PropertyError object at.+')),\n            'ctx': {'error': 'RuntimeError: intentional error'},\n        }\n    ]", "target": "def benchmark(self):\n        for M, N in self.get_shapes():\n            print(f\"\\n Tensor dimensions: [{M}, {N}]\")\n            torch_dtype = cutlass_torch.dtype(cutlass.BFloat16)\n            x = 0.1 * torch.randn(M, N, device=\"cuda\", dtype=torch_dtype)\n            target = torch.randint(0, N, (M,), device=\"cuda\", dtype=torch.int64)\n            self.benchmark_single_shape((x, target), setting=f\"shape: [{M}, {N}]\")", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001015", "source": "def test_constraints_schema_validation_error(constraint: str) -> None:\n    with pytest.raises(SchemaError, match=f\"'{constraint}' must be coercible to a timedelta instance\"):\n        SchemaValidator(core_schema.timedelta_schema(**{constraint: 'bad_value'}))", "target": "def test_custom_error_invalid():\n    msg = \"custom_error_message should not be provided if 'custom_error_type' matches a known error\"\n    with pytest.raises(SchemaError, match=msg):\n        SchemaValidator(\n            schema=core_schema.custom_error_schema(\n                core_schema.int_schema(), 'recursion_loop', custom_error_message='xxx'\n            )\n        )", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001016", "source": "def test_core_dict_filter(self, core_validator: SchemaValidator, core_serializer: SchemaSerializer, benchmark):\n        m = core_validator.validate_python(self.data)\n        exclude = {'age': ..., 'fields': {41, 42}}\n        @benchmark\n        def _():\n            core_serializer.to_python(m, exclude=exclude)", "target": "def test_branch_nullable():\n    s = SchemaSerializer(\n        core_schema.definitions_schema(\n            core_schema.definition_reference_schema('Branch'),\n            [\n                core_schema.typed_dict_schema(\n                    {\n                        'name': core_schema.typed_dict_field(core_schema.str_schema()),\n                        'sub_branch': core_schema.typed_dict_field(\n                            core_schema.nullable_schema(core_schema.definition_reference_schema('Branch'))\n                        ),\n                    },\n                    ref='Branch',\n                )\n            ],\n        )\n    )\n    assert s.to_python({'name': 'root', 'sub_branch': {'name': 'branch', 'sub_branch': None}}) == {\n        'name': 'root',\n        'sub_branch': {'name': 'branch', 'sub_branch': None},\n    }\n    assert s.to_python({'name': 'root', 'sub_branch': {'name': 'branch', 'sub_branch': None}}, exclude_none=True) == {\n        'name': 'root',\n        'sub_branch': {'name': 'branch'},\n    }", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pytorch/pytorch", "data_split": "train", "example_id": "001017", "source": "def relative_typename(self, module: str) -> str:\n        return self.full_typename", "target": "def _merge_lastfailed_files(source_pytest_cache: Path, dest_pytest_cache: Path) -> None:\n    source_lastfailed_file = source_pytest_cache / LASTFAILED_FILE_PATH\n    dest_lastfailed_file = dest_pytest_cache / LASTFAILED_FILE_PATH\n    if not source_lastfailed_file.exists():\n        return\n    if not dest_lastfailed_file.exists():\n        copy_file(source_lastfailed_file, dest_lastfailed_file)\n        return\n    from_lastfailed = load_json_file(source_lastfailed_file)\n    to_lastfailed = load_json_file(dest_lastfailed_file)\n    merged_content = _merged_lastfailed_content(from_lastfailed, to_lastfailed)\n    write_json_file(dest_lastfailed_file, merged_content)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001018", "source": "def get_shapes(self) -> tuple[tuple[int, ...], ...]:\n        return (\n            (32768, 256),\n            (32768, 512),\n            (32768, 1024),\n            (32768, 2048),\n            (32768, 4096),\n            (32768, 8192),\n            (32768, 16384),\n            (32768, 32768),\n            (32768, 65536),\n            (16384, 131072),\n            (8192, 262144),\n        )", "target": "def get_shapes(self) -> tuple[tuple[int, ...], ...]:\n        return (\n            (32768, 256),\n            (32768, 512),\n            (32768, 1024),\n            (32768, 2048),\n            (32768, 4096),\n            (32768, 8192),\n            (32768, 16384),\n            (32768, 32768),\n            (32768, 65536),\n            (16384, 131072),\n            (8192, 262144),\n        )", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001019", "source": "def get_shapes(self) -> tuple[tuple[int, ...], ...]:\n        return (\n            (32768, 256),\n            (32768, 512),\n            (32768, 1024),\n            (32768, 2048),\n            (32768, 4096),\n            (32768, 8192),\n            (32768, 16384),\n            (32768, 32768),\n            (32768, 65536),\n            (16384, 131072),\n            (8192, 262144),\n        )", "target": "def get_shapes(self) -> tuple[tuple[int, ...], ...]:\n        return (\n            (32768, 256),\n            (32768, 512),\n            (32768, 1024),\n            (32768, 2048),\n            (32768, 4096),\n            (32768, 8192),\n            (32768, 16384),\n        ) + extra_shapes_for_norm", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001020", "source": "def typename(self) -> str:\n            return self.type_node.full_typename", "target": "def typename(self) -> str:\n        return self._typename", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pytorch/pytorch", "data_split": "train", "example_id": "001021", "source": "def split2d(img, cell_size, flatten=True):\n    h, w = img.shape[:2]\n    sx, sy = cell_size\n    cells = [np.hsplit(row, w//sx) for row in np.vsplit(img, h//sy)]\n    cells = np.array(cells)\n    if flatten:\n        cells = cells.reshape(-1, sy, sx)\n    return cells", "target": "def gh_post_commit_comment(\n    org: str, repo: str, sha: str, comment: str, dry_run: bool = False\n) -> list[dict[str, Any]]:\n    return _gh_post_comment(\n        f\"{GITHUB_API_URL}/repos/{org}/{repo}/commits/{sha}/comments\",\n        comment,\n        dry_run,\n    )", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|pydantic/pydantic", "data_split": "train", "example_id": "001022", "source": "def track_train_score(self, *args):\n        if hasattr(self.estimator, \"predict\"):\n            y_pred = self.estimator.predict(self.X)\n        else:\n            y_pred = None\n        return float(self.train_scorer(self.y, y_pred))", "target": "def test_empty_positional_tuple(fail_fast):\n    v = SchemaValidator(core_schema.tuple_schema(items_schema=[], fail_fast=fail_fast))\n    assert v.validate_python(()) == ()\n    assert v.validate_python([]) == ()\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python((1,))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'too_long',\n            'loc': (),\n            'msg': 'Tuple should have at most 0 items after validation, not 1',\n            'input': (1,),\n            'ctx': {'field_type': 'Tuple', 'max_length': 0, 'actual_length': 1},\n        }\n    ]", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001023", "source": "def test_discriminator_function(py_and_json: PyAndJson, input_value, expected):\n    def discriminator_function(obj):\n        if isinstance(obj, str):\n            return 'str'\n        elif isinstance(obj, int):\n            return 'int'\n        elif obj is None:\n            return None\n        else:\n            return 'other'\n    v = py_and_json(\n        {\n            'type': 'tagged-union',\n            'discriminator': discriminator_function,\n            'choices': {'str': {'type': 'literal', 'expected': ['foo', 'bar']}, 'int': {'type': 'int'}},\n        }\n    )\n    assert 'discriminator: Function' in repr(v.validator)\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=expected.message) as exc_info:\n            v.validate_python(input_value)\n        assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        assert v.validate_test(input_value) == expected", "target": "def test_computed_field_exclude_none_different_order():\n    @dataclasses.dataclass\n    class Model:\n        width: int\n        height: int\n        @property\n        def volume(self) -> None:\n            return None\n        @property\n        def area(self) -> int:\n            return self.width * self.height\n    s = SchemaSerializer(\n        core_schema.model_schema(\n            Model,\n            core_schema.model_fields_schema(\n                {\n                    'width': core_schema.model_field(core_schema.int_schema()),\n                    'height': core_schema.model_field(core_schema.int_schema()),\n                },\n                computed_fields=[\n                    core_schema.computed_field('volume', core_schema.int_schema()),\n                    core_schema.computed_field('area', core_schema.int_schema(), alias='Area'),\n                ],\n            ),\n        )\n    )\n    assert s.to_python(Model(3, 4), by_alias=True, exclude_none=False) == {\n        'width': 3,\n        'height': 4,\n        'Area': 12,\n        'volume': None,\n    }\n    assert s.to_python(Model(3, 4), by_alias=True, exclude_none=True) == {'width': 3, 'height': 4, 'Area': 12}\n    assert s.to_python(Model(3, 4), by_alias=True, mode='json', exclude_none=False) == {\n        'width': 3,\n        'height': 4,\n        'Area': 12,\n        'volume': None,\n    }\n    assert s.to_python(Model(3, 4), mode='json', by_alias=True, exclude_none=True) == {\n        'width': 3,\n        'height': 4,\n        'Area': 12,\n    }\n    assert (\n        s.to_json(Model(3, 4), exclude_none=False, by_alias=True) == b'{\"width\":3,\"height\":4,\"volume\":null,\"Area\":12}'\n    )\n    assert s.to_json(Model(3, 4), exclude_none=True, by_alias=True) == b'{\"width\":3,\"height\":4,\"Area\":12}'", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001024", "source": "def benchmark(self):\n        for M, N in self.get_shapes():\n            print(f\"\\n Tensor dimensions: [{M}, {N}]\")\n            torch_dtype = cutlass_torch.dtype(cutlass.BFloat16)\n            x = 0.1 * torch.randn(M, N, device=\"cuda\", dtype=torch_dtype)\n            target = torch.randint(0, N, (M,), device=\"cuda\", dtype=torch.int64)\n            self.benchmark_single_shape((x, target), setting=f\"shape: [{M}, {N}]\")", "target": "def benchmark(self):\n        for M, N in self.get_shapes():\n            print(f\"Tensor dimensions: [{M}, {N}]\")\n            torch_dtype = cutlass_torch.dtype(cutlass.BFloat16)\n            x = torch.randn(M, N, device=\"cuda\", dtype=torch_dtype, requires_grad=True)\n            w = torch.randn(N, device=\"cuda\", dtype=torch.float32, requires_grad=True)\n            dy = torch.randn(M, N, device=\"cuda\", dtype=torch_dtype)\n            self.benchmark_single_shape((x, w, dy), setting=f\"shape: [{M}, {N}]\")", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001025", "source": "def test_dict(py_and_json: PyAndJson):\n    v = py_and_json({'type': 'dict', 'keys_schema': {'type': 'date'}, 'values_schema': {'type': 'int'}})\n    assert v.validate_test({'2000-01-01': 2, '2000-01-02': 4}) == {date(2000, 1, 1): 2, date(2000, 1, 2): 4}", "target": "def test_dict(py_and_json: PyAndJson):\n    v = py_and_json({'type': 'dict', 'keys_schema': {'type': 'datetime'}, 'values_schema': {'type': 'int'}})\n    assert v.validate_test({'2000-01-01T00:00': 2, '2000-01-02T00:00': 4}) == {\n        datetime(2000, 1, 1): 2,\n        datetime(2000, 1, 2): 4,\n    }", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001026", "source": "def test_by_alias_and_name_config_interaction(config, runtime, expected) -> None:\n    @dataclasses.dataclass\n    class Foo:\n        my_field: str\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(\n                    name='my_field', schema=core_schema.str_schema(), serialization_alias='my_alias'\n                ),\n            ],\n        ),\n        ['my_field'],\n        config=core_schema.CoreConfig(serialize_by_alias=config or False),\n    )\n    s = SchemaSerializer(schema)\n    assert s.to_python(Foo(my_field='hello'), by_alias=runtime) == expected", "target": "def test_by_alias_and_name_config_interaction(config, runtime, expected) -> None:\n    class Model:\n        def __init__(self, my_field: int) -> None:\n            self.my_field = my_field\n    schema = core_schema.model_schema(\n        Model,\n        core_schema.model_fields_schema(\n            {\n                'my_field': core_schema.model_field(core_schema.int_schema(), serialization_alias='my_alias'),\n            }\n        ),\n        config=core_schema.CoreConfig(serialize_by_alias=config or False),\n    )\n    s = SchemaSerializer(schema)\n    assert s.to_python(Model(1), by_alias=runtime) == expected", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001027", "source": "def test_is_instance_invalid(input_cls):\n    with pytest.raises(SchemaError, match=\"SchemaError: 'cls' must be valid as the first argument to 'isinstance'\"):\n        SchemaValidator(cs.is_instance_schema(cls=input_cls))", "target": "def field_type_str(self) -> str:\n        return f'{self.field_type.__name__}' if hasattr(self.field_type, '__name__') else f'{self.field_type}'", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "001028", "source": "def test_bytes_invalid_all():\n    s = SchemaSerializer(core_schema.bytes_schema())\n    assert s.to_python(b'\\x81') == b'\\x81'\n    msg = 'Error serializing to JSON: invalid utf-8 sequence of 1 bytes from index 0'\n    with pytest.raises(PydanticSerializationError, match=msg):\n        s.to_json(b'\\x81')", "target": "def get_field(csv, model_name: str, field: str):\n    try:\n        return csv.loc[csv[\"name\"] == model_name][field].item()\n    except Exception:\n        return None", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001029", "source": "def nullable_schema(\n    schema: CoreSchema,\n    *,\n    strict: bool | None = None,\n    ref: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> NullableSchema:\n    return _dict_not_none(\n        type='nullable', schema=schema, strict=strict, ref=ref, metadata=metadata, serialization=serialization\n    )", "target": "def double_or_bust(input_value):\n    if input_value == 1:\n        raise RuntimeError('bust')\n    return input_value * 2", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001030", "source": "def compiled(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, target = args\n        torch._dynamo.mark_dynamic(x, 0)\n        torch._dynamo.mark_dynamic(target, 0)\n        compiled_cross_entropy = torch.compile(\n            lambda x, target: F.cross_entropy(x, target, reduction=\"none\"),\n            mode=self.compile_mode,\n            fullgraph=True,\n        )\n        return lambda: compiled_cross_entropy(x, target)", "target": "def compiled(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, w = args\n        torch._dynamo.mark_dynamic(x, 0)\n        compiled_rms_norm = torch.compile(\n            self.rms_norm_ref, mode=self.compile_mode, fullgraph=True\n        )\n        return lambda: compiled_rms_norm(x, w)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001031", "source": "def f(value, serializer, _info):\n        return f'result={serializer(len(value))} repr={serializer!r}'", "target": "def f(value, serializer, _info):\n        return f'result={serializer(len(value))} repr={serializer!r}'", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001032", "source": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "target": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "001033", "source": "def test_detect_and_decode(self):\n        img = cv.imread(os.path.join(self.extraTestDataPath, 'cv/qrcode/link_ocv.jpg'))\n        self.assertFalse(img is None)\n        detector = cv.QRCodeDetector()\n        retval, points, straight_qrcode = detector.detectAndDecode(img)\n        self.assertEqual(retval, \"https://opencv.org/\")\n        self.assertEqual(points.shape, (1, 4, 2))", "target": "def test_alias_extra_forbid(py_and_json: PyAndJson):\n    v = py_and_json(\n        {\n            'type': 'typed-dict',\n            'extra_behavior': 'forbid',\n            'fields': {\n                'field_a': {'type': 'typed-dict-field', 'validation_alias': 'FieldA', 'schema': {'type': 'int'}}\n            },\n        }\n    )\n    assert v.validate_test({'FieldA': 1}) == {'field_a': 1}", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001034", "source": "def generate_schema(self, source_type: Any, /) -> core_schema.CoreSchema:\n        raise NotImplementedError", "target": "def test_on_error_raise_explicit(self, py_and_json: PyAndJson):\n        v = py_and_json(\n            {\n                'type': 'typed-dict',\n                'fields': {\n                    'x': {\n                        'type': 'typed-dict-field',\n                        'schema': {'type': 'default', 'schema': {'type': 'str'}, 'on_error': 'raise'},\n                    }\n                },\n            }\n        )\n        assert v.validate_test({'x': 'foo'}) == {'x': 'foo'}\n        with pytest.raises(ValidationError) as exc_info:\n            v.validate_test({'x': ['foo']})\n        assert exc_info.value.errors(include_url=False) == [\n            {'input': ['foo'], 'type': 'string_type', 'loc': ('x',), 'msg': 'Input should be a valid string'}\n        ]", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001035", "source": "def test_any_dataclass():\n    @dataclasses.dataclass\n    class Foo:\n        a: str\n        b: bytes\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo', [core_schema.dataclass_field(name='a', schema=core_schema.str_schema())]\n        ),\n        ['a'],\n    )\n    Foo.__pydantic_serializer__ = SchemaSerializer(schema)\n    s = SchemaSerializer(core_schema.any_schema())\n    assert s.to_python(Foo(a='hello', b=b'more')) == IsStrictDict(a='hello')\n    assert s.to_python(Foo(a='hello', b=b'more'), mode='json') == IsStrictDict(a='hello')\n    j = s.to_json(Foo(a='hello', b=b'more'))\n    if on_pypy:\n        assert json.loads(j) == {'a': 'hello'}\n    else:\n        assert j == b'{\"a\":\"hello\"}'\n    assert s.to_python(Foo(a='hello', b=b'more'), exclude={'a'}) == IsStrictDict()", "target": "def test_double_nested():\n    v = SchemaValidator(\n        core_schema.typed_dict_schema(\n            {\n                'a': core_schema.typed_dict_field(core_schema.int_schema(gt=10)),\n                'b': core_schema.typed_dict_field(\n                    core_schema.list_schema(\n                        core_schema.dict_schema(core_schema.str_schema(), core_schema.int_schema(ge=10))\n                    )\n                ),\n            },\n            total=False,\n        )\n    )\n    assert v.validate_python({'a': 11, 'b': [{'a': 10, 'b': 20}, {'a': 30, 'b': 40}]}) == snapshot(\n        {'a': 11, 'b': [{'a': 10, 'b': 20}, {'a': 30, 'b': 40}]}\n    )\n    assert v.validate_python({'a': 11, 'b': [{'a': 10, 'b': 20}, {'a': 30, 'b': 4}]}, allow_partial=True) == snapshot(\n        {'a': 11, 'b': [{'a': 10, 'b': 20}, {'a': 30}]}\n    )\n    assert v.validate_python({'a': 11, 'b': [{'a': 10, 'b': 20}, {'a': 30, 123: 4}]}, allow_partial=True) == snapshot(\n        {'a': 11, 'b': [{'a': 10, 'b': 20}]}\n    )\n    assert v.validate_python({'a': 11, 'b': [{'a': 10, 'b': 2}, {'a': 30}]}, allow_partial=True) == snapshot({'a': 11})\n    with pytest.raises(ValidationError, match=r'b\\.0\\.b\\s+Input should be greater than or equal to 10'):\n        v.validate_python({'b': [{'a': 10, 'b': 2}, {'a': 30}], 'a': 11}, allow_partial=True)\n    with pytest.raises(ValidationError, match=r'b\\.1\\.a\\s+Input should be greater than or equal to 10'):\n        v.validate_python({'b': [{'a': 10, 'b': 20}, {'a': 3}], 'a': 11}, allow_partial=True)\n    assert v.validate_python({'a': 11, 'b': [{'a': 1, 'b': 20}, {'a': 3, 'b': 40}]}, allow_partial=True) == snapshot(\n        {'a': 11}\n    )\n    json = b'{\"a\": 11, \"b\": [{\"a\": 10, \"b\": 20}, {\"a\": 30, \"b\": 40}]}'\n    assert v.validate_json(json, allow_partial=True) == snapshot(\n        {'a': 11, 'b': [{'a': 10, 'b': 20}, {'a': 30, 'b': 40}]}\n    )\n    for i in range(1, len(json)):\n        value = v.validate_json(json[:i], allow_partial=True)\n        assert isinstance(value, dict)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001036", "source": "def test_by_alias_and_name_config_interaction(\n    config_by_alias: Union[bool, None],\n    config_by_name: Union[bool, None],\n    runtime_by_alias: Union[bool, None],\n    runtime_by_name: Union[bool, None],\n) -> None:\n    if config_by_alias is False and config_by_name is False and runtime_by_alias is False and runtime_by_name is False:\n        pytest.skip(\"Can't have both by_alias and by_name as effectively False\")\n    class Model:\n        def __init__(self, my_field: int) -> None:\n            self.my_field = my_field\n    core_config = {\n        **({'validate_by_alias': config_by_alias} if config_by_alias is not None else {}),\n        **({'validate_by_name': config_by_name} if config_by_name is not None else {}),\n    }\n    schema = core_schema.model_schema(\n        Model,\n        core_schema.model_fields_schema(\n            {\n                'my_field': core_schema.model_field(core_schema.int_schema(), validation_alias='my_alias'),\n            }\n        ),\n        config=core_schema.CoreConfig(**core_config),\n    )\n    s = SchemaValidator(schema)\n    alias_allowed = next(x for x in (runtime_by_alias, config_by_alias, True) if x is not None)\n    name_allowed = next(x for x in (runtime_by_name, config_by_name, False) if x is not None)\n    if alias_allowed:\n        assert s.validate_python({'my_alias': 1}, by_alias=runtime_by_alias, by_name=runtime_by_name).my_field == 1\n    if name_allowed:\n        assert s.validate_python({'my_field': 1}, by_alias=runtime_by_alias, by_name=runtime_by_name).my_field == 1", "target": "def test_by_alias_and_name_config_interaction(\n    config_by_alias: Union[bool, None],\n    config_by_name: Union[bool, None],\n    runtime_by_alias: Union[bool, None],\n    runtime_by_name: Union[bool, None],\n) -> None:\n    if config_by_alias is False and config_by_name is False and runtime_by_alias is False and runtime_by_name is False:\n        pytest.skip(\"Can't have both by_alias and by_name as effectively False\")\n    core_config = {\n        **({'validate_by_alias': config_by_alias} if config_by_alias is not None else {}),\n        **({'validate_by_name': config_by_name} if config_by_name is not None else {}),\n    }\n    schema = core_schema.typed_dict_schema(\n        fields={\n            'my_field': core_schema.typed_dict_field(schema=core_schema.int_schema(), validation_alias='my_alias'),\n        },\n        config=core_schema.CoreConfig(**core_config),\n    )\n    s = SchemaValidator(schema)\n    alias_allowed = next(x for x in (runtime_by_alias, config_by_alias, True) if x is not None)\n    name_allowed = next(x for x in (runtime_by_name, config_by_name, False) if x is not None)\n    if alias_allowed:\n        assert s.validate_python({'my_alias': 1}, by_alias=runtime_by_alias, by_name=runtime_by_name) == {'my_field': 1}\n    if name_allowed:\n        assert s.validate_python({'my_field': 1}, by_alias=runtime_by_alias, by_name=runtime_by_name) == {'my_field': 1}", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001037", "source": "def type_format(self) -> str:\n        if TypeNode.compatible_to_runtime_usage:\n            return \"_typing.Dict[{}]\"\n        return \"dict[{}]\"", "target": "def type_format(self) -> str:\n        return \"_typing.Type[{}]\"", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "001038", "source": "def _prepare(self):\n        if self._backward:\n            self.forward_val = self._add1(self.a).sum()", "target": "def test_dataclass_json():\n    schema = core_schema.dataclass_schema(\n        FooDataclass,\n        core_schema.dataclass_args_schema(\n            'FooDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    assert v.validate_json('{\"a\": \"hello\", \"b\": true}') == FooDataclass(a='hello', b=True)\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_json('[\"a\", \"b\"]')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'class_name': 'FooDataclass'},\n            'input': ['a', 'b'],\n            'loc': (),\n            'msg': 'Input should be an object',\n            'type': 'dataclass_type',\n        }\n    ]", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001039", "source": "def add_constant(self, name: str, value: str) -> ConstantNode:\n        return self._add_child(ConstantNode, name, value=value)", "target": "def add_constant(self, name: str, value: str) -> ConstantNode:\n        return self._add_child(ConstantNode, name, value=value)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001040", "source": "def test_fields_required_by_default():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'x': core_schema.model_field(schema=core_schema.str_schema()),\n                'y': core_schema.model_field(schema=core_schema.str_schema()),\n            }\n        )\n    )\n    assert v.validate_python({'x': 'pika', 'y': 'chu'}) == ({'x': 'pika', 'y': 'chu'}, None, {'x', 'y'})\n    with pytest.raises(ValidationError) as exc_info:\n        assert v.validate_python({'x': 'pika'})\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'missing', 'loc': ('y',), 'msg': 'Field required', 'input': {'x': 'pika'}}\n    ]", "target": "def test_fields_required_by_default():\n    v = SchemaValidator(\n        core_schema.typed_dict_schema(\n            fields={\n                'x': core_schema.typed_dict_field(schema=core_schema.str_schema()),\n                'y': core_schema.typed_dict_field(schema=core_schema.str_schema()),\n            }\n        )\n    )\n    assert v.validate_python({'x': 'pika', 'y': 'chu'}) == {'x': 'pika', 'y': 'chu'}\n    with pytest.raises(ValidationError) as exc_info:\n        assert v.validate_python({'x': 'pika'})\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'missing', 'loc': ('y',), 'msg': 'Field required', 'input': {'x': 'pika'}}\n    ]", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001041", "source": "def gen():\n    yield 1\n    yield 2\n    yield 3", "target": "def gen(error: bool):\n        yield 1\n        yield 2\n        if error:\n            raise RuntimeError('my error')\n        yield 3", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "001042", "source": "def test_too_long(pydantic_version):\n    v = SchemaValidator(cs.int_schema())\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python('1' * 4301)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing_size',\n            'loc': (),\n            'msg': 'Unable to parse input string as an integer, exceeded maximum size',\n            'input': '1' * 4301,\n        }\n    ]\n    assert repr(exc_info.value) == (\n        '1 validation error for int\\n'\n        '  Unable to parse input string as an integer, exceeded maximum size '\n        \"[type=int_parsing_size, input_value='111111111111111111111111...11111111111111111111111', input_type=str]\"\n        + (\n            f'\\n    For further information visit https://errors.pydantic.dev/{pydantic_version}/v/int_parsing_size'\n            if os.environ.get('PYDANTIC_ERRORS_INCLUDE_URL', '1') != 'false'\n            else ''\n        )\n    )", "target": "def forward(self, x, y):\n        return (x + y,)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001043", "source": "def test_slots() -> None:\n    @dataclasses.dataclass(slots=True)\n    class Model:\n        x: int\n    schema = core_schema.dataclass_schema(\n        Model,\n        core_schema.dataclass_args_schema(\n            'Model', [core_schema.dataclass_field(name='x', schema=core_schema.int_schema())]\n        ),\n        ['x'],\n        slots=True,\n    )\n    val = SchemaValidator(schema)\n    m: Model\n    m = val.validate_python({'x': 123})\n    assert m == Model(x=123)\n    with pytest.raises(ValidationError):\n        val.validate_python({'x': 'abc'})\n    val.validate_assignment(m, 'x', 456)\n    assert m.x == 456\n    with pytest.raises(ValidationError):\n        val.validate_assignment(m, 'x', 'abc')", "target": "def val_function(x, *args: Any):\n    return x", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "001044", "source": "def takes_validated_data_argument(\n    default_factory: Callable[[], Any] | Callable[[dict[str, Any]], Any],\n) -> TypeIs[Callable[[dict[str, Any]], Any]]:\n    try:\n        sig = signature(default_factory)\n    except (ValueError, TypeError):\n        return False\n    parameters = list(sig.parameters.values())\n    return len(parameters) == 1 and can_be_positional(parameters[0]) and parameters[0].default is Parameter.empty", "target": "def test_compute_diff(self) -> None:\n        diff = self.repo.diff(\"HEAD\")\n        sha = _shasum(diff)\n        self.assertEqual(len(sha), 64)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001045", "source": "def build_wrapper(func: Callable[P, R], event_handlers: list[BaseValidateHandlerProtocol]) -> Callable[P, R]:\n    if not event_handlers:\n        return func\n    else:\n        on_enters = tuple(h.on_enter for h in event_handlers if filter_handlers(h, 'on_enter'))\n        on_successes = tuple(h.on_success for h in event_handlers if filter_handlers(h, 'on_success'))\n        on_errors = tuple(h.on_error for h in event_handlers if filter_handlers(h, 'on_error'))\n        on_exceptions = tuple(h.on_exception for h in event_handlers if filter_handlers(h, 'on_exception'))\n        @functools.wraps(func)\n        def wrapper(*args: P.args, **kwargs: P.kwargs) -> R:\n            for on_enter_handler in on_enters:\n                on_enter_handler(*args, **kwargs)\n            try:\n                result = func(*args, **kwargs)\n            except ValidationError as error:\n                for on_error_handler in on_errors:\n                    on_error_handler(error)\n                raise\n            except Exception as exception:\n                for on_exception_handler in on_exceptions:\n                    on_exception_handler(exception)\n                raise\n            else:\n                for on_success_handler in on_successes:\n                    on_success_handler(result)\n                return result\n        return wrapper", "target": "def test_include_exclude_schema():\n    s = SchemaSerializer(\n        core_schema.typed_dict_schema(\n            {\n                '0': core_schema.typed_dict_field(core_schema.int_schema(), serialization_exclude=True),\n                '1': core_schema.typed_dict_field(core_schema.int_schema()),\n                '2': core_schema.typed_dict_field(\n                    core_schema.int_schema(), serialization_exclude=True, serialization_exclude_if=lambda x: x < 0\n                ),\n                '3': core_schema.typed_dict_field(\n                    core_schema.int_schema(), serialization_exclude=False, serialization_exclude_if=lambda x: x < 0\n                ),\n            }\n        )\n    )\n    value = {'0': 0, '1': 1, '2': 2, '3': 3}\n    assert s.to_python(value) == {'1': 1, '3': 3}\n    assert s.to_python(value, mode='json') == {'1': 1, '3': 3}\n    assert json.loads(s.to_json(value)) == {'1': 1, '3': 3}\n    value = {'0': 0, '1': 1, '2': 2, '3': -3}\n    assert s.to_python(value) == {'1': 1}\n    assert s.to_python(value, mode='json') == {'1': 1}\n    assert json.loads(s.to_json(value)) == {'1': 1}", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|opencv/opencv", "data_split": "train", "example_id": "001046", "source": "def get_shapes(self) -> tuple[tuple[int, ...], ...]:\n        return (\n            (32768, 256),\n            (32768, 512),\n            (32768, 1024),\n            (32768, 2048),\n            (32768, 4096),\n            (32768, 8192),\n            (32768, 16384),\n        ) + extra_shapes_for_norm", "target": "def test_imencode(self):\n        a = np.zeros((480, 640), dtype=np.uint8)\n        flag, ajpg = cv.imencode(\"img_q90.jpg\", a, [cv.IMWRITE_JPEG_QUALITY, 90])\n        self.assertEqual(flag, True)\n        self.assertEqual(ajpg.dtype, np.uint8)\n        self.assertTrue(isinstance(ajpg, np.ndarray), \"imencode returned buffer of wrong type: {}\".format(type(ajpg)))\n        self.assertEqual(len(ajpg.shape), 1, \"imencode returned buffer with wrong shape: {}\".format(ajpg.shape))\n        self.assertGreaterEqual(len(ajpg), 1, \"imencode length of the returned buffer should be at least 1\")\n        self.assertLessEqual(\n            len(ajpg), a.size,\n            \"imencode length of the returned buffer shouldn't exceed number of elements in original image\"\n        )", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001047", "source": "def test_total_time(shapes, types):\n    print(\n        \"shape; type; torch scatter_add; inductor scatter_add; torch scatter_add (worst case); inductor scatter_add (worst case)\"\n    )\n    for shape, dtype in itertools.product(shapes, types):\n        print(shape, dtype, sep=\"; \", end=\"; \")\n        torch.manual_seed(1)\n        if dtype.is_floating_point:\n            src = torch.randn(shape, device=\"cpu\", dtype=dtype)\n            dst = torch.randn(shape, device=\"cpu\", dtype=dtype)\n        else:\n            src = torch.randint(0, shape[1], shape, device=\"cpu\", dtype=dtype)\n            dst = torch.randint(0, shape[1], shape, device=\"cpu\", dtype=dtype)\n        index = torch.randint(0, shape[1], shape, device=\"cpu\", dtype=torch.int64)\n        worst_index = torch.tensor([[0] * shape[1]], device=\"cpu\", dtype=torch.int64)\n        torch_result = torch_scatter_add(dst, src, index)\n        inductor_result = inductor_scatter_add(dst, src, index)\n        torch.testing.assert_close(torch_result, inductor_result)\n        torch_ms = (\n            time_with_torch_timer(torch_scatter_add, (dst, src, index)).mean * 1000\n        )\n        inductor_ms = (\n            time_with_torch_timer(inductor_scatter_add, (dst, src, index)).mean * 1000\n        )\n        torch_worst_ms = (\n            time_with_torch_timer(torch_scatter_add, (dst, src, worst_index)).mean\n            * 1000\n        )\n        inductor_worst_ms = (\n            time_with_torch_timer(inductor_scatter_add, (dst, src, worst_index)).mean\n            * 1000\n        )\n        print(torch_ms, inductor_ms, torch_worst_ms, inductor_worst_ms, sep=\"; \")\n        torch._dynamo.reset()", "target": "def test_total_time(shapes):\n    print(\"shape; torch mm; triton mm; inductor aten mm; inductor triton mm\")\n    for i in range(len(shapes)):\n        a_shape, b_shape = shapes[i]\n        print(a_shape, \"x\", b_shape, end=\"; \")\n        a = torch.randn(a_shape, device=\"cuda\", dtype=torch.float16)\n        b = torch.randn(b_shape, device=\"cuda\", dtype=a.dtype)\n        config.triton.mm = \"aten\"\n        inductor_aten_mm(a, b)\n        config.triton.mm = \"triton\"\n        inductor_triton_mm(a, b)\n        torch_ms = time_with_torch_timer(torch_mm, (a, b)).mean * 1000\n        triton_ms = time_with_torch_timer(triton_mm, (a, b)).mean * 1000\n        config.triton.mm = \"aten\"\n        ind_aten_ms = time_with_torch_timer(inductor_aten_mm, (a, b)).mean * 1000\n        config.triton.mm = \"triton\"\n        ind_triton_ms = time_with_torch_timer(inductor_triton_mm, (a, b)).mean * 1000\n        print(torch_ms, triton_ms, ind_aten_ms, ind_triton_ms, sep=\"; \")\n        torch._dynamo.reset()", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001048", "source": "def test_schema_validator_wrong() -> None:\n    try:\n        SchemaValidator({'type': 'bad'})\n    except SchemaError:\n        pass\n    else:\n        raise AssertionError('SchemaValidator did not raise SchemaError')", "target": "def test_positional_or_keyword_validation_error(py_and_json: PyAndJson, input_value, err_loc) -> None:\n    v = py_and_json(\n        cs.arguments_v3_schema(\n            [\n                cs.arguments_v3_parameter(name='a', schema=cs.int_schema(), mode='positional_or_keyword'),\n            ]\n        )\n    )\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_test(input_value)\n    error = exc_info.value.errors()[0]\n    assert error['type'] == 'int_parsing'\n    assert error['loc'] == err_loc", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001049", "source": "def test_set_ints_both(py_and_json: PyAndJson, input_value, expected):\n    v = py_and_json({'type': 'set', 'items_schema': {'type': 'int'}})\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            v.validate_test(input_value)\n    else:\n        assert v.validate_test(input_value) == expected", "target": "def fn():\n        class Model:\n            a: int\n            @classmethod\n            def _validator(cls, v, info):\n                return v\n            @classmethod\n            def _wrap_validator(cls, v, validator, info):\n                return validator(v)\n        field_schema = core_schema.int_schema()\n        if validator == 'field':\n            field_schema = core_schema.with_info_before_validator_function(Model._validator, field_schema)\n            field_schema = core_schema.with_info_wrap_validator_function(Model._wrap_validator, field_schema)\n            field_schema = core_schema.with_info_after_validator_function(Model._validator, field_schema)\n        model_schema = core_schema.model_schema(\n            Model, core_schema.model_fields_schema({'a': core_schema.model_field(field_schema)})\n        )\n        if validator == 'model':\n            model_schema = core_schema.with_info_before_validator_function(Model._validator, model_schema)\n            model_schema = core_schema.with_info_wrap_validator_function(Model._wrap_validator, model_schema)\n            model_schema = core_schema.with_info_after_validator_function(Model._validator, model_schema)\n        Model.__pydantic_validator__ = SchemaValidator(model_schema)\n        return Model", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "001050", "source": "def test_literal_py_and_json(py_and_json: PyAndJson, kwarg_expected, input_value, expected):\n    v = py_and_json({'type': 'literal', 'expected': kwarg_expected})\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)) as exc_info:\n            v.validate_test(input_value)\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        assert v.validate_test(input_value) == expected", "target": "def run(img, sc, dtype):\n            return img + np.array(sc, dtype=np.uint8)[:-1]", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "001051", "source": "def test_parse_to_int_convertible(self):\n        try_to_convert = partial(self._try_to_convert, cv.utils.dumpInt)\n        min_int, max_int = get_limits(ctypes.c_int)\n        for convertible in (-10, -1, 2, int(43.2), np.uint8(15), np.int8(33), np.int16(-13),\n                            np.int32(4), np.int64(345), (23), min_int, max_int, np.int_(33)):\n            expected = 'int: {0:d}'.format(convertible)\n            actual = try_to_convert(convertible)\n            self.assertEqual(expected, actual,\n                             msg=get_conversion_error_msg(convertible, expected, actual))", "target": "def test_validate_assignment_with_context():\n    def f1(input_value, info):\n        info.context['f1'] = input_value\n        return input_value + f'| context: {info.context}'\n    def f2(input_value, info):\n        info.context['f2'] = input_value\n        return input_value + f'| context: {info.context}'\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            {\n                'f1': core_schema.model_field(core_schema.with_info_plain_validator_function(f1)),\n                'f2': core_schema.model_field(core_schema.with_info_plain_validator_function(f2)),\n            }\n        )\n    )\n    m1, model_extra, fields_set = v.validate_python({'f1': '1', 'f2': '2'}, strict=None, context={'x': 'y'})\n    assert m1 == {'f1': \"1| context: {'x': 'y', 'f1': '1'}\", 'f2': \"2| context: {'x': 'y', 'f1': '1', 'f2': '2'}\"}\n    assert model_extra is None\n    assert fields_set == {'f1', 'f2'}\n    m2, model_extra, fields_set = v.validate_assignment(m1, 'f1', '3', context={'x': 'y'})\n    assert m2 == {'f1': \"3| context: {'x': 'y', 'f1': '3'}\", 'f2': \"2| context: {'x': 'y', 'f1': '1', 'f2': '2'}\"}\n    assert model_extra is None\n    assert fields_set == {'f1'}", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001052", "source": "def call_schema(\n    arguments: CoreSchema,\n    function: Callable[..., Any],\n    *,\n    function_name: str | None = None,\n    return_schema: CoreSchema | None = None,\n    ref: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> CallSchema:\n    return _dict_not_none(\n        type='call',\n        arguments_schema=arguments,\n        function=function,\n        function_name=function_name,\n        return_schema=return_schema,\n        ref=ref,\n        metadata=metadata,\n        serialization=serialization,\n    )", "target": "def test_on_error_bad_omit(self):\n        with pytest.raises(SchemaError, match=\"Field 'x': 'on_error = omit' cannot be set for required fields\"):\n            SchemaValidator(\n                schema=core_schema.typed_dict_schema(\n                    fields={\n                        'x': core_schema.typed_dict_field(\n                            schema=core_schema.with_default_schema(schema=core_schema.str_schema(), on_error='omit')\n                        )\n                    }\n                )\n            )", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001053", "source": "def _validator(cls, v, info):\n                return v", "target": "def _validator(cls, v, info):\n                return v", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001054", "source": "def test_only_allow_alias(py_and_json) -> None:\n    schema = core_schema.arguments_schema(\n        [\n            core_schema.arguments_parameter(name='a', schema=core_schema.str_schema(), alias='FieldA'),\n        ],\n        validate_by_name=False,\n        validate_by_alias=True,\n    )\n    v = py_and_json(schema)\n    assert v.validate_test(ArgsKwargs((), {'FieldA': 'hello'})) == ((), {'a': 'hello'})\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Missing required argument \\[type=missing_argument,'):\n        assert v.validate_test(ArgsKwargs((), {'a': 'hello'}))", "target": "def test_only_allow_alias(py_and_json) -> None:\n    v = py_and_json(\n        {\n            'type': 'typed-dict',\n            'fields': {\n                'field_a': {'validation_alias': 'FieldA', 'type': 'typed-dict-field', 'schema': {'type': 'int'}}\n            },\n            'config': {'validate_by_name': False, 'validate_by_alias': True},\n        }\n    )\n    assert v.validate_test({'FieldA': '123'}) == {'field_a': 123}\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'field_a': '123'})", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001055", "source": "def typename(self) -> str:\n            return self.type_node.full_typename", "target": "def typename(self) -> str:\n        return self.alias_export_name", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "001056", "source": "def forward(self, x):\n        t, n = x.size(0), x.size(1)\n        x = x.view(t * n, -1)\n        x = self.module(x)\n        x = x.view(t, n, -1)\n        return x", "target": "def test_function_validation_info_mode():\n    calls: list[str] = []\n    def f(v: Any, info: core_schema.ValidationInfo) -> Any:\n        calls.append(info.mode)\n        return v\n    v = SchemaValidator(core_schema.with_info_before_validator_function(f, core_schema.int_schema()))\n    assert v.validate_python(1) == 1\n    assert calls == ['python']\n    calls.clear()\n    assert v.validate_json('1') == 1\n    assert calls == ['json']\n    calls.clear()\n    v = SchemaValidator(core_schema.with_info_after_validator_function(f, core_schema.int_schema()))\n    assert v.validate_python(1) == 1\n    assert calls == ['python']\n    calls.clear()\n    assert v.validate_json('1') == 1\n    assert calls == ['json']\n    calls.clear()\n    def f_w(v: Any, handler: core_schema.ValidatorFunctionWrapHandler, info: core_schema.ValidationInfo) -> Any:\n        calls.append(info.mode)\n        return handler(v)\n    v = SchemaValidator(core_schema.with_info_wrap_validator_function(f_w, core_schema.int_schema()))\n    assert v.validate_python(1) == 1\n    assert calls == ['python']\n    calls.clear()\n    assert v.validate_json('1') == 1\n    assert calls == ['json']\n    calls.clear()", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|pytorch/pytorch", "data_split": "train", "example_id": "001057", "source": "def title(y_pred, y_test, target_names, i):\n    pred_name = target_names[y_pred[i]].rsplit(\" \", 1)[-1]\n    true_name = target_names[y_test[i]].rsplit(\" \", 1)[-1]\n    return \"predicted: %s\\ntrue:      %s\" % (pred_name, true_name)", "target": "def check_and_replace(inp: str, src: str, dst: str) -> str:\n    if src not in inp:\n        raise RuntimeError(f\"Can't find ${src} in the input\")\n    return inp.replace(src, dst)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001058", "source": "def test_exclude_none():\n    s = SchemaSerializer(\n        core_schema.model_schema(\n            BasicModel,\n            core_schema.model_fields_schema(\n                {\n                    'foo': core_schema.model_field(core_schema.nullable_schema(core_schema.int_schema())),\n                    'bar': core_schema.model_field(core_schema.bytes_schema()),\n                },\n                extra_behavior='ignore',\n            ),\n        )\n    )\n    assert s.to_python(BasicModel(foo=1, bar=b'more')) == {'foo': 1, 'bar': b'more'}\n    assert s.to_python(BasicModel(foo=None, bar=b'more')) == {'foo': None, 'bar': b'more'}\n    assert s.to_python(BasicModel(foo=None, bar=b'more'), exclude_none=True) == {'bar': b'more'}\n    assert s.to_python(BasicModel(foo=None, bar=b'more'), mode='json') == {'foo': None, 'bar': 'more'}\n    assert s.to_python(BasicModel(foo=None, bar=b'more'), mode='json', exclude_none=True) == {'bar': 'more'}\n    assert s.to_json(BasicModel(foo=1, bar=b'more')) == b'{\"foo\":1,\"bar\":\"more\"}'\n    assert s.to_json(BasicModel(foo=None, bar=b'more')) == b'{\"foo\":null,\"bar\":\"more\"}'\n    assert s.to_json(BasicModel(foo=None, bar=b'more'), exclude_none=True) == b'{\"bar\":\"more\"}'", "target": "def test_exclude_none():\n    v = SchemaSerializer(\n        core_schema.typed_dict_schema(\n            {\n                'foo': core_schema.typed_dict_field(core_schema.nullable_schema(core_schema.int_schema())),\n                'bar': core_schema.typed_dict_field(core_schema.bytes_schema()),\n            },\n            extra_behavior='allow',\n        )\n    )\n    assert v.to_python({'foo': 1, 'bar': b'more', 'c': 3}) == {'foo': 1, 'bar': b'more', 'c': 3}\n    assert v.to_python({'foo': None, 'bar': b'more', 'c': None}) == {'foo': None, 'bar': b'more', 'c': None}\n    assert v.to_python({'foo': None, 'bar': b'more', 'c': None}, exclude_none=True) == {'bar': b'more'}\n    assert v.to_python({'foo': None, 'bar': b'more', 'c': None}, mode='json') == {'foo': None, 'bar': 'more', 'c': None}\n    assert v.to_python({'foo': None, 'bar': b'more', 'c': None}, mode='json', exclude_none=True) == {'bar': 'more'}\n    assert v.to_json({'foo': 1, 'bar': b'more', 'c': None}) == b'{\"foo\":1,\"bar\":\"more\",\"c\":null}'\n    assert v.to_json({'foo': None, 'bar': b'more'}) == b'{\"foo\":null,\"bar\":\"more\"}'\n    assert v.to_json({'foo': None, 'bar': b'more', 'c': None}, exclude_none=True) == b'{\"bar\":\"more\"}'", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "001059", "source": "def test_json_bytes_hex_round_trip():\n    data = b'hello'\n    encoded = b'\"68656c6c6f\"'\n    assert to_json(data, bytes_mode='hex') == encoded\n    v = SchemaValidator(core_schema.bytes_schema(), config=CoreConfig(val_json_bytes='hex'))\n    assert v.validate_json(encoded) == data\n    assert to_json({'key': data}, bytes_mode='hex') == b'{\"key\":\"68656c6c6f\"}'\n    v = SchemaValidator(\n        core_schema.dict_schema(keys_schema=core_schema.str_schema(), values_schema=core_schema.bytes_schema()),\n        config=CoreConfig(val_json_bytes='hex'),\n    )\n    assert v.validate_json('{\"key\":\"68656c6c6f\"}') == {'key': data}", "target": "def forward(\n        self,\n        query: torch.Tensor,\n        key: torch.Tensor,\n        value: torch.Tensor,\n        attn_mask: Optional[torch.Tensor] = None,\n        bias_k: Optional[torch.Tensor] = None,\n        bias_v: Optional[torch.Tensor] = None,\n    ) -> tuple[torch.Tensor, torch.Tensor]:\n        r\n        tgt_len, src_len, bsz, embed_dim = (\n            query.size(-3),\n            key.size(-3),\n            query.size(-2),\n            query.size(-1),\n        )\n        q, k, v = self.in_proj_container(query, key, value)\n        assert q.size(-1) % self.nhead == 0, (\n            \"query's embed_dim must be divisible by the number of heads\"\n        )\n        head_dim = q.size(-1) // self.nhead\n        q = q.reshape(tgt_len, bsz * self.nhead, head_dim)\n        assert k.size(-1) % self.nhead == 0, (\n            \"key's embed_dim must be divisible by the number of heads\"\n        )\n        head_dim = k.size(-1) // self.nhead\n        k = k.reshape(src_len, bsz * self.nhead, head_dim)\n        assert v.size(-1) % self.nhead == 0, (\n            \"value's embed_dim must be divisible by the number of heads\"\n        )\n        head_dim = v.size(-1) // self.nhead\n        v = v.reshape(src_len, bsz * self.nhead, head_dim)\n        attn_output, attn_output_weights = self.attention_layer(\n            q, k, v, attn_mask=attn_mask, bias_k=bias_k, bias_v=bias_v\n        )\n        attn_output = attn_output.reshape(tgt_len, bsz, embed_dim)\n        attn_output = self.out_proj(attn_output)\n        return attn_output, attn_output_weights", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001060", "source": "def get_shapes(self) -> tuple[tuple[int, ...], ...]:\n        return (\n            (32768, 256),\n            (32768, 512),\n            (32768, 1024),\n            (32768, 2048),\n            (32768, 4096),\n            (32768, 8192),\n            (32768, 16384),\n            (32768, 32768),\n            (32768, 65536),\n            (16384, 131072),\n            (8192, 262144),\n        )", "target": "def get_shapes(self) -> tuple[tuple[int, ...], ...]:\n        return (\n            (32768, 256),\n            (32768, 512),\n            (32768, 1024),\n            (32768, 2048),\n            (32768, 4096),\n            (32768, 8192),\n            (32768, 16384),\n            (32768, 32768),\n            (32768, 65536),\n            (16384, 131072),\n            (8192, 262144),\n        ) + extra_shapes_for_norm", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001061", "source": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "target": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "001062", "source": "def test_custom_error_type(py_and_json: PyAndJson):\n    v = py_and_json(core_schema.custom_error_schema(core_schema.int_schema(), 'recursion_loop'))\n    assert v.validate_test(1) == 1\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_test('X')\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'recursion_loop', 'loc': (), 'msg': 'Recursion error - cyclic reference detected', 'input': 'X'}\n    ]", "target": "def get_list_of_3rdparty_libs(sdk_dir, abis):\n    libs = []\n    for abi in abis:\n        files = os.listdir(path.join(sdk_dir, \"sdk/native/3rdparty/libs/\" + abi))\n        cur_libs = [f[3:-2] for f in files if f[:3] == \"lib\" and f[-2:] == \".a\"]\n        for lib in cur_libs:\n            if lib not in libs:\n                libs.append(lib)\n    return libs", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001063", "source": "def from_name(cls, name: str):\n        if name in transformer_configs:\n            return cls(**transformer_configs[name])\n        config = [\n            config\n            for config in transformer_configs\n            if config in str(name).upper() or config in str(name)\n        ]\n        assert len(config) == 1, name\n        return cls(**transformer_configs[config[0]])", "target": "def from_name(cls, name: str):\n        if name in transformer_configs:\n            return cls(**transformer_configs[name])\n        config = [\n            config\n            for config in transformer_configs\n            if config in str(name).upper() or config in str(name)\n        ]\n        if len(config) > 1:\n            config.sort(key=len, reverse=True)\n            assert len(config[0]) != len(config[1]), (\n                name\n            )\n        return cls(**transformer_configs[config[0]])", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001064", "source": "def forward(self, inputs):\n        output = self.pool(F.relu(self.conv(inputs)))\n        output = output.view(1)\n        return output", "target": "def forward(self, x):\n        for _ in range(self._n):\n            x = fn9(x)\n        return x", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|opencv/opencv", "data_split": "train", "example_id": "001065", "source": "def bhtsne(X):\n            n_iter = -1\n            return (\n                run_bh_tsne(\n                    X,\n                    use_pca=False,\n                    perplexity=args.perplexity,\n                    verbose=args.verbose > 0,\n                ),\n                n_iter,\n            )", "target": "def getOS(self):\n        return getPlatformVersion() or self.cache.getOS()", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001066", "source": "def test_by_alias_and_name_config_interaction(config, runtime, expected) -> None:\n    @dataclasses.dataclass\n    class Foo:\n        my_field: str\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(\n                    name='my_field', schema=core_schema.str_schema(), serialization_alias='my_alias'\n                ),\n            ],\n        ),\n        ['my_field'],\n        config=core_schema.CoreConfig(serialize_by_alias=config or False),\n    )\n    s = SchemaSerializer(schema)\n    assert s.to_python(Foo(my_field='hello'), by_alias=runtime) == expected", "target": "def test_by_alias_and_name_config_interaction(\n    config_by_alias: Union[bool, None],\n    config_by_name: Union[bool, None],\n    runtime_by_alias: Union[bool, None],\n    runtime_by_name: Union[bool, None],\n) -> None:\n    if config_by_alias is False and config_by_name is False and runtime_by_alias is False and runtime_by_name is False:\n        pytest.skip(\"Can't have both by_alias and by_name as effectively False\")\n    core_config = {\n        **({'validate_by_alias': config_by_alias} if config_by_alias is not None else {}),\n        **({'validate_by_name': config_by_name} if config_by_name is not None else {}),\n    }\n    @dataclasses.dataclass\n    class MyDataclass:\n        my_field: int\n    schema = core_schema.dataclass_schema(\n        MyDataclass,\n        core_schema.dataclass_args_schema(\n            'MyDataclass',\n            [\n                core_schema.dataclass_field(\n                    name='my_field', schema=core_schema.int_schema(), validation_alias='my_alias'\n                ),\n            ],\n        ),\n        ['my_field'],\n        config=core_schema.CoreConfig(**core_config),\n    )\n    s = SchemaValidator(schema)\n    alias_allowed = next(x for x in (runtime_by_alias, config_by_alias, True) if x is not None)\n    name_allowed = next(x for x in (runtime_by_name, config_by_name, False) if x is not None)\n    if alias_allowed:\n        assert dataclasses.asdict(\n            s.validate_python({'my_alias': 1}, by_alias=runtime_by_alias, by_name=runtime_by_name)\n        ) == {'my_field': 1}\n    if name_allowed:\n        assert dataclasses.asdict(\n            s.validate_python({'my_field': 1}, by_alias=runtime_by_alias, by_name=runtime_by_name)\n        ) == {'my_field': 1}", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001067", "source": "def area(self) -> int:\n            return self.width * self.height", "target": "def area(self) -> None:\n            self.side = 0.0", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001068", "source": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "target": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001069", "source": "def test_leak_typed_dict():\n    def fn():\n        def validate(v, info):\n            return v\n        schema = core_schema.with_info_plain_validator_function(validate)\n        schema = core_schema.typed_dict_schema(\n            {'f': core_schema.typed_dict_field(schema)}, extra_behavior='allow', extras_schema=schema\n        )\n        validate.__pydantic_validator__ = SchemaValidator(schema)\n        return validate\n    cycle = fn()\n    ref = weakref.ref(cycle)\n    assert ref() is not None\n    del cycle\n    assert_gc(lambda: ref() is None)", "target": "def test_simple_tagged_union(py_and_json: PyAndJson, input_value, expected):\n    v = py_and_json(\n        {\n            'type': 'tagged-union',\n            'discriminator': 'foo',\n            'from_attributes': False,\n            'choices': {\n                'apple': {\n                    'type': 'typed-dict',\n                    'fields': {\n                        'foo': {'type': 'typed-dict-field', 'schema': {'type': 'str'}},\n                        'bar': {'type': 'typed-dict-field', 'schema': {'type': 'int'}},\n                    },\n                },\n                'banana': {\n                    'type': 'typed-dict',\n                    'fields': {\n                        'foo': {'type': 'typed-dict-field', 'schema': {'type': 'str'}},\n                        'spam': {\n                            'type': 'typed-dict-field',\n                            'schema': {'type': 'list', 'items_schema': {'type': 'int'}},\n                        },\n                    },\n                },\n            },\n        }\n    )\n    assert 'discriminator: LookupKey' in repr(v.validator)\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=expected.message) as exc_info:\n            v.validate_test(input_value)\n        assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        assert v.validate_test(input_value) == expected", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001070", "source": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "target": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|pytorch/pytorch", "data_split": "train", "example_id": "001071", "source": "def cv_estimate(n_splits=None):\n    cv = KFold(n_splits=n_splits)\n    cv_clf = ensemble.GradientBoostingClassifier(**params)\n    val_scores = np.zeros((n_estimators,), dtype=np.float64)\n    for train, test in cv.split(X_train, y_train):\n        cv_clf.fit(X_train[train], y_train[train])\n        val_scores += heldout_score(cv_clf, X_train[test], y_train[test])\n    val_scores /= n_splits\n    return val_scores", "target": "def _merged_lastfailed_content(\n    from_lastfailed: dict[str, bool], to_lastfailed: dict[str, bool]\n) -> dict[str, bool]:\n    for key in from_lastfailed:\n        if key not in to_lastfailed:\n            to_lastfailed[key] = from_lastfailed[key]\n    if len(to_lastfailed) > 1:\n        if \"\" in to_lastfailed:\n            del to_lastfailed[\"\"]\n    return to_lastfailed", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001072", "source": "def f(*args):\n            outs = [torch.add(x, x) for x in args]\n            return outs", "target": "def f(inp, *weights):\n            x = inp\n            for w in weights:\n                x = torch.matmul(w, x).sin().sin()\n            return x", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001073", "source": "def area(self, area: float) -> None:\n            self.side = area**0.5", "target": "def area(self) -> None:\n            self.side = 0.0", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001074", "source": "def test_complex_json(value, expected):\n    v = SchemaSerializer(core_schema.complex_schema())\n    c = v.to_python(value)\n    c_json = v.to_python(value, mode='json')\n    json_str = v.to_json(value).decode()\n    assert c_json == expected\n    assert json_str == f'\"{expected}\"'\n    if math.isnan(value.imag):\n        assert math.isnan(c.imag)\n    else:\n        assert c.imag == value.imag\n    if math.isnan(value.real):\n        assert math.isnan(c.real)\n    else:\n        assert c.imag == value.imag", "target": "def test_uuid_deepcopy():\n    output = SchemaValidator(core_schema.uuid_schema()).validate_python('a6cc5730-2261-11ee-9c43-2eb5a363657c')\n    c = copy.deepcopy(output)\n    assert repr(output) == \"UUID('a6cc5730-2261-11ee-9c43-2eb5a363657c')\"\n    assert c == output\n    assert isinstance(output, UUID)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "001075", "source": "def test_positional_only_error_required(py_and_json: PyAndJson, input_value) -> None:\n    v = py_and_json(\n        cs.arguments_v3_schema(\n            [\n                cs.arguments_v3_parameter(name='a', schema=cs.int_schema(), mode='positional_or_keyword'),\n            ]\n        )\n    )\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_test(input_value)\n    error = exc_info.value.errors()[0]\n    assert error['type'] == 'missing_argument'\n    assert error['loc'] == ('a',)", "target": "def node_type(self) -> ASTNodeType:\n        return ASTNodeType.Constant", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|scikit-learn/scikit-learn", "data_split": "train", "example_id": "001076", "source": "def test_dict_any_any():\n    v = SchemaSerializer(core_schema.dict_schema())\n    assert v.to_python({'a': 1, b'b': 2, 33: 3}) == {'a': 1, b'b': 2, 33: 3}\n    assert v.to_python({'a': 1, b'b': 2, 33: 3, True: 4}, mode='json') == {'a': 1, 'b': 2, '33': 3, 'true': 4}\n    assert v.to_json({'a': 1, b'b': 2, 33: 3, True: 4}) == b'{\"a\":1,\"b\":2,\"33\":3,\"true\":4}'\n    assert v.to_python({(1, 2): 3}) == {(1, 2): 3}\n    assert v.to_python({(1, 2): 3}, mode='json') == {'1,2': 3}\n    assert v.to_json({(1, 2): 3}) == b'{\"1,2\":3}'", "target": "def plot(X, labels, probabilities=None, parameters=None, ground_truth=False, ax=None):\n    if ax is None:\n        _, ax = plt.subplots(figsize=(10, 4))\n    labels = labels if labels is not None else np.ones(X.shape[0])\n    probabilities = probabilities if probabilities is not None else np.ones(X.shape[0])\n    unique_labels = set(labels)\n    colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\n    proba_map = {idx: probabilities[idx] for idx in range(len(labels))}\n    for k, col in zip(unique_labels, colors):\n        if k == -1:\n            col = [0, 0, 0, 1]\n        class_index = (labels == k).nonzero()[0]\n        for ci in class_index:\n            ax.plot(\n                X[ci, 0],\n                X[ci, 1],\n                \"x\" if k == -1 else \"o\",\n                markerfacecolor=tuple(col),\n                markeredgecolor=\"k\",\n                markersize=4 if k == -1 else 1 + 5 * proba_map[ci],\n            )\n    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n    preamble = \"True\" if ground_truth else \"Estimated\"\n    title = f\"{preamble} number of clusters: {n_clusters_}\"\n    if parameters is not None:\n        parameters_str = \", \".join(f\"{k}={v}\" for k, v in parameters.items())\n        title += f\" | {parameters_str}\"\n    ax.set_title(title)\n    plt.tight_layout()", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001077", "source": "def foo(x: torch.Tensor) -> torch.Tensor:\n        return torch.sin(x) + torch.cos(x)", "target": "def foo(x: int, y: int) -> int:\n            return x + y", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "001078", "source": "def _get_assignment_statements_from_if_statement(self, stmt: IfStmt) -> Iterator[AssignmentStmt]:\n        for body in stmt.body:\n            if not body.is_unreachable:\n                yield from self._get_assignment_statements_from_block(body)\n        if stmt.else_body is not None and not stmt.else_body.is_unreachable:\n            yield from self._get_assignment_statements_from_block(stmt.else_body)", "target": "def test_parse_to_bool_convertible_extra(self):\n        try_to_convert = partial(self._try_to_convert, cv.utils.dumpBool)\n        _, max_size_t = get_limits(ctypes.c_size_t)\n        for convertible_true in (-1, max_size_t):\n            actual = try_to_convert(convertible_true)\n            self.assertEqual('bool: true', actual,\n                             msg=get_conversion_error_msg(convertible_true, 'bool: true', actual))", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001079", "source": "def forward(self, inputs):\n        output = self.pool(F.relu(self.conv(inputs)))\n        output = output.view(1)\n        return output", "target": "def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = F.max_pool2d(x, 2)\n        x = torch.flatten(x, 1)\n        output = self.fc1(x)\n        return output", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "train", "example_id": "001080", "source": "def gh_owner_and_name(self) -> tuple[str, str]:\n        url = os.getenv(\"GIT_REMOTE_URL\", None)\n        if url is None:\n            url = self.remote_url()\n        rc = RE_GITHUB_URL_MATCH.match(url)\n        if rc is None:\n            raise RuntimeError(f\"Unexpected url format {url}\")\n        return cast(tuple[str, str], rc.groups())", "target": "def _prepare_once(self) -> None:\n        self.a = torch.ones(10, 10, device=self.device())\n        self.b = torch.torch.ones(10, 10, device=self.device())", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|opencv/opencv", "data_split": "train", "example_id": "001081", "source": "def _run_torchbench_model(\n    cmd_args: argparse.Namespace,\n    results: list[RunResult],\n    model: str,\n) -> None:\n    cur_file = os.path.abspath(__file__)\n    torchbench_file = os.path.join(\n        os.path.dirname(cur_file), BENCHMARK_FILE[cmd_args.benchmark]\n    )\n    assert os.path.exists(torchbench_file), (\n        f\"Torchbench does not exist at {torchbench_file}\"\n    )\n    dynamic = cmd_args.dynamic\n    dynamic_args = [\"--dynamic-shapes\", \"--dynamic-batch-only\"] if dynamic else []\n    args = (\n        [\n            sys.executable,\n            torchbench_file,\n            f\"--only={model}\",\n            \"--repeat=1\",\n            \"--performance\",\n            \"--backend=inductor\",\n            f\"--device={cmd_args.device}\",\n        ]\n        + MODE_ARGS_DICT[cmd_args.mode]\n        + dynamic_args\n    )\n    logger.info(f\"Command: {args}\")\n    try:\n        cold_compile_t, warm_compile_t = _run_torchbench_from_args(\n            cmd_args, model, args\n        )\n        speedup_pct = (1 - (sum(warm_compile_t) / sum(cold_compile_t))) * 100\n        results.append(\n            RunResult(\n                model=model,\n                mode=cmd_args.mode,\n                benchmark=cmd_args.benchmark,\n                dynamic=dynamic,\n                device=cmd_args.device,\n                cold_compile_s=cold_compile_t,\n                warm_compile_s=warm_compile_t,\n                speedup_pct=speedup_pct,\n            )\n        )\n    except Exception:\n        logger.info(\"fail\", exc_info=True)\n        return None", "target": "def type_format(self) -> str:\n        return \"\"", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001082", "source": "def benchmark(self):\n        for M, N in self.get_shapes():\n            print(f\"Tensor dimensions: [{M}, {N}]\")\n            torch_dtype = cutlass_torch.dtype(cutlass.BFloat16)\n            x = torch.randn(M, N, device=\"cuda\", dtype=torch_dtype)\n            w = torch.randn(N, device=\"cuda\", dtype=torch.float32)\n            self.benchmark_single_shape((x, w), setting=f\"shape: [{M}, {N}]\")", "target": "def benchmark(self):\n        for M, N in self.get_shapes():\n            print(f\"Tensor dimensions: [{M}, {N}]\")\n            torch_dtype = cutlass_torch.dtype(cutlass.BFloat16)\n            x = torch.randn(M, N, device=\"cuda\", dtype=torch_dtype, requires_grad=True)\n            w = torch.randn(N, device=\"cuda\", dtype=torch.float32, requires_grad=True)\n            dy = torch.randn(M, N, device=\"cuda\", dtype=torch_dtype)\n            self.benchmark_single_shape((x, w, dy), setting=f\"shape: [{M}, {N}]\")", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|scikit-learn/scikit-learn", "data_split": "train", "example_id": "001083", "source": "def test_float_repr():\n    v = SchemaValidator(cs.float_schema())\n    assert (\n        plain_repr(v)\n        == 'SchemaValidator(title=\"float\",validator=Float(FloatValidator{strict:false,allow_inf_nan:true}),definitions=[],cache_strings=True)'\n    )\n    v = SchemaValidator(cs.float_schema(strict=True))\n    assert (\n        plain_repr(v)\n        == 'SchemaValidator(title=\"float\",validator=Float(FloatValidator{strict:true,allow_inf_nan:true}),definitions=[],cache_strings=True)'\n    )\n    v = SchemaValidator(cs.float_schema(multiple_of=7))\n    assert plain_repr(v).startswith('SchemaValidator(title=\"constrained-float\",validator=ConstrainedFloat(')", "target": "def get_git_grep_info():\n    git_grep_filenames = subprocess.check_output(\n        [\"git\", \"grep\", \"-lP\", \"cython.*parallel|_openmp_helpers\"], text=True\n    ).splitlines()\n    git_grep_filenames = [f for f in git_grep_filenames if \".pyx\" in f]\n    return [get_canonical_name_git_grep(each) for each in git_grep_filenames]", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001084", "source": "def name(self):\n        prefix = f\"{self.category()}_{self.backend()}\"\n        if self.is_dynamic():\n            prefix += \"_dynamic\"\n        if self.device() == \"cuda\":\n            prefix += \"_gpu\"\n        return prefix", "target": "def name(self):\n        prefix = f\"{self.category()}_{self.backend()}\"\n        if self._requires_grad:\n            prefix += \"_requires_grad\"\n        if self._inference_mode:\n            prefix += \"_inference_mode\"\n        if self._backward:\n            prefix += \"_backward\"\n        if self.is_dynamic():\n            prefix += \"_dynamic\"\n        return prefix", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001085", "source": "def test_dict_py():\n    v = SchemaValidator(cs.dict_schema(keys_schema=cs.date_schema(), values_schema=cs.int_schema()))\n    assert v.validate_python({date(2000, 1, 1): 2, date(2000, 1, 2): 4}) == {date(2000, 1, 1): 2, date(2000, 1, 2): 4}", "target": "def test_dict_py():\n    v = SchemaValidator(\n        core_schema.dict_schema(keys_schema=core_schema.time_schema(), values_schema=core_schema.int_schema())\n    )\n    assert v.validate_python({time(12, 1, 1): 2, time(12, 1, 2): 4}) == {time(12, 1, 1): 2, time(12, 1, 2): 4}", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001086", "source": "def test_function_plain():\n    s = SchemaSerializer(core_schema.with_info_plain_validator_function(lambda v, info: v + 1))\n    assert plain_repr(s) == 'SchemaSerializer(serializer=Any(AnySerializer),definitions=[])'", "target": "def test_function_plain():\n    def f(input_value, _info):\n        return input_value * 2\n    v = SchemaValidator(core_schema.with_info_plain_validator_function(f))\n    assert v.validate_python(1) == 2\n    assert v.validate_python('x') == 'xx'", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|pytorch/pytorch", "data_split": "train", "example_id": "001087", "source": "def decision_function(self, X):\n        check_is_fitted(self)\n        return self.classifier_.decision_function(X)", "target": "def forward(self, inputs):\n        output = self.pool(F.relu(self.conv(inputs)))\n        output = output.view(1)\n        return output", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|opencv/opencv", "data_split": "train", "example_id": "001088", "source": "def update(self, input_pos, k_val, v_val):\n        assert input_pos.shape[0] == k_val.shape[2]\n        k_out = self.k_cache\n        v_out = self.v_cache\n        k_out[:, :, input_pos] = k_val\n        v_out[:, :, input_pos] = v_val\n        return k_out, v_out", "target": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "001089", "source": "def _work(self):\n        @torch.compile(backend=self.backend(), fullgraph=True)\n        def f(inp, *weights):\n            x = inp\n            for w in weights:\n                x = torch.matmul(w, x).sin().sin()\n            return x\n        f(self.inp, *self.weights)", "target": "def val2(value, handler, info):\n        nonlocal field_names\n        field_names.append(('val2', info.field_name))\n        return handler(value)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|scikit-learn/scikit-learn", "data_split": "train", "example_id": "001090", "source": "def clear(self):\n        self.mdict = {}\n        self.tdict = {}\n        self.mwhere = {}\n        self.twhere = {}\n        self.empty_stubs_cnt = 0\n        self.r1 = re.compile(\"\\s*public\\s+(?:static\\s+)?(\\w+)\\(([^)]*)\\)\")\n        self.r2 = re.compile(\"\\s*(?:(?:public|static|final)\\s+){1,3}\\S+\\s+(\\w+)\\(([^)]*)\\)\")\n        self.r3 = re.compile('\\s*fail\\(\"Not yet implemented\"\\);')", "target": "def construct_grids(batch):\n    xmin = batch.x_left_lower_corner + batch.grid_size\n    xmax = xmin + (batch.Nx * batch.grid_size)\n    ymin = batch.y_left_lower_corner + batch.grid_size\n    ymax = ymin + (batch.Ny * batch.grid_size)\n    xgrid = np.arange(xmin, xmax, batch.grid_size)\n    ygrid = np.arange(ymin, ymax, batch.grid_size)\n    return (xgrid, ygrid)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001091", "source": "def _work(self):\n        @torch.compile(backend=self.backend(), fullgraph=True)\n        def f(inp, *weights):\n            x = inp\n            for w in weights:\n                x = torch.matmul(w, x).sin().sin()\n            return x\n        f(self.inp, *self.weights)", "target": "def _work(self):\n        @torch.compile(fullgraph=True)\n        def f(a):\n            xs = a.tolist()\n            y = 0\n            if self.use_loop:\n                for i in xs:\n                    y += i\n            else:\n                y = sum(xs)\n            return torch.tensor(y)\n        f(self.splits)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001092", "source": "def torch_mm(a, b):\n    return torch.mm(a, b)", "target": "def torch_mm(a, b):\n    return torch.mm(a, b)", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001093", "source": "def typename(self) -> str:\n        return \"\"", "target": "def typename(self) -> str:\n        return self._typename", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "001094", "source": "def intEq0None(self, input: int) -> Optional[int]:\n        if input == 0:\n            return None\n        return input", "target": "def update_not_none(mapping: dict[Any, Any], **update: Any) -> None:\n    mapping.update({k: v for k, v in update.items() if v is not None})", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001095", "source": "def getCMakeArgs(self):\n        args = [\n            \"cmake\",\n            \"-GXcode\",\n            \"-DFRAMEWORK_DIR=%s\" % self.framework_dir,\n            \"-DFRAMEWORK_NAME=%s\" % self.framework_name,\n        ]\n        return args", "target": "def getCMakeArgs(self):\n        args = TestRunner.getCMakeArgs(self)\n        args = args + [\n            '-DMACOSX_DEPLOYMENT_TARGET=%s' % os.environ['MACOSX_DEPLOYMENT_TARGET']\n        ]\n        return args", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001096", "source": "def required_usage_imports(self) -> Generator[str, None, None]:\n        if TypeNode.compatible_to_runtime_usage:\n            yield \"import typing as _typing\"\n        yield from super().required_usage_imports", "target": "def required_usage_imports(self) -> Generator[str, None, None]:\n        yield \"import typing as _typing\"\n        yield from super().required_usage_imports", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|scikit-learn/scikit-learn", "data_split": "train", "example_id": "001097", "source": "def test_model_b_preferred(self, schema_validator: SchemaValidator):\n        m = schema_validator.validate_python({'a': 1, 'b': 'hello', 'c': 2.0})\n        assert isinstance(m, self.ModelB)\n        assert m.a == 1\n        assert m.b == 'hello'\n        assert m.c == 2.0", "target": "def fit(est, data_train, target_train, libname, **fit_params):\n    print(f\"Fitting a {libname} model...\")\n    tic = time()\n    est.fit(data_train, target_train, **fit_params)\n    toc = time()\n    print(f\"fitted in {toc - tic:.3f}s\")", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "001098", "source": "def input_data_valid(levels: int = N) -> Any:\n    data = {str(c): 1 for c in range(N)}\n    for _ in range(levels):\n        data = {str(c): data for c in range(N)}\n    return data", "target": "def arch_type(arch_version: str) -> str:\n    if arch_version in CUDA_ARCHES:\n        return \"cuda\"\n    elif arch_version in ROCM_ARCHES:\n        return \"rocm\"\n    elif arch_version in XPU_ARCHES:\n        return \"xpu\"\n    elif arch_version in CPU_AARCH64_ARCH:\n        return \"cpu-aarch64\"\n    elif arch_version in CPU_S390X_ARCH:\n        return \"cpu-s390x\"\n    elif arch_version in CUDA_AARCH64_ARCHES:\n        return \"cuda-aarch64\"\n    else:\n        return \"cpu\"", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "001099", "source": "def run_llama2_7b_bf16(device: str = \"cuda\"):\n    model = GPTModelConfig(\n        \"Llama-2-7b-chat-hf\",\n        LLaMA,\n        \"bfloat16\",\n        LLaMAWeightOnlyInt8QuantHandler,\n        94,\n        1253,\n        133,\n    )\n    token_per_sec, memory_bandwidth, compilation_time = run_experiment(\n        model, device=device\n    )\n    return [\n        Experiment(\n            model.name,\n            \"token_per_sec\",\n            model.token_per_sec,\n            f\"{token_per_sec:.02f}\",\n            model.mode,\n            device,\n            get_arch_name(),\n            True,\n        ),\n        Experiment(\n            model.name,\n            \"memory_bandwidth(GB/s)\",\n            model.memory_bandwidth,\n            f\"{memory_bandwidth:.02f}\",\n            model.mode,\n            device,\n            get_arch_name(),\n            True,\n        ),\n        Experiment(\n            model.name,\n            \"compilation_time(s)\",\n            model.compilation_time,\n            f\"{compilation_time:.02f}\",\n            model.mode,\n            device,\n            get_arch_name(),\n            True,\n        ),\n    ]", "target": "def type_var_default_factory() -> None:\n                raise RuntimeError(\n                    'Generic defaultdict cannot be used without a concrete value type or an'\n                    ' explicit default factory, ' + instructions\n                )", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001100", "source": "def ser_root(self, v: Any, serializer: core_schema.SerializerFunctionWrapHandler, _) -> str:\n            root = serializer(v)\n            assert self.root == 1_000\n            return f'{root:_}'", "target": "def ser_root(self, v: Any, serializer: core_schema.SerializerFunctionWrapHandler, _) -> str:\n            assert self.root == 1_000\n            root = serializer(v)\n            return f'{root:_}'", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "001101", "source": "def test_typed_dict_validator_reuse() -> None:\n    def f(input_value: Any, info: core_schema.ValidationInfo) -> Any:\n        return f'{info.field_name}: {input_value}'\n    with pytest.warns(\n        DeprecationWarning, match='`field_name` argument on `with_info_plain_validator_function` is deprecated'\n    ):\n        validator = core_schema.with_info_plain_validator_function(f, field_name='x')\n    v = SchemaValidator(\n        core_schema.typed_dict_schema(\n            {\n                'x': core_schema.model_field(validator),\n                'y': core_schema.model_field(validator),\n            }\n        )\n    )\n    data = v.validate_python({'x': 'foo', 'y': 'bar'})\n    assert data['x'] == 'x: foo'\n    assert data['y'] == 'y: bar'", "target": "def detectSerial(self):\n        adb_res = self.run([\"devices\"], silent=True)\n        connected_devices = re.findall(r\"^[^\\n]+[ \\t]+device\\r?$\", adb_res, re.MULTILINE)\n        if not connected_devices:\n            raise Err(\"Can not find Android device\")\n        elif len(connected_devices) != 1:\n            raise Err(\"Too many (%s) devices are connected. Please specify single device using --serial option:\\n\\n%s\", len(connected_devices), adb_res)\n        else:\n            return connected_devices[0].split(\"\\t\")[0]", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|opencv/opencv", "data_split": "train", "example_id": "001102", "source": "def main() -> None:\n    if \"install_root\" in os.environ:\n        install_root = Path(os.getenv(\"install_root\"))\n    else:\n        if os.getenv(\"PACKAGE_TYPE\") == \"libtorch\":\n            install_root = Path(os.getcwd())\n        else:\n            install_root = Path(distutils.sysconfig.get_python_lib()) / \"torch\"\n    libtorch_cpu_path = str(install_root / \"lib\" / \"libtorch_cpu.so\")\n    check_lib_symbols_for_abi_correctness(libtorch_cpu_path)\n    check_lib_statically_linked_libstdc_cxx_abi_symbols(libtorch_cpu_path)", "target": "def getTestCommand(self):\n        testcmd = [\n            \"xcodebuild\",\n            \"test\",\n            \"-project\", \"OpenCVTest.xcodeproj\",\n            \"-scheme\", \"OpenCVTestTests\",\n            \"-destination\", \"platform=%s\" % self.platform\n        ]\n        return testcmd", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "001103", "source": "def forward(\n        nu_value: Tensor, sigma_unconstrained_value: Tensor, beta_value: Tensor\n    ) -> Tensor:\n        sigma_constrained_value = sigma_unconstrained_value.exp()\n        mu = X.mm(beta_value)\n        nu_score = dist.StudentT(nu_value, mu, sigma_constrained_value).log_prob(\n            Y\n        ).sum() + nu.log_prob(nu_value)\n        sigma_score = (\n            dist.StudentT(nu_value, mu, sigma_constrained_value).log_prob(Y).sum()\n            + sigma.log_prob(sigma_constrained_value)\n            + sigma_unconstrained_value\n        )\n        beta_score = dist.StudentT(nu_value, mu, sigma_constrained_value).log_prob(\n            Y\n        ).sum() + beta.log_prob(beta_value)\n        return nu_score.sum() + sigma_score.sum() + beta_score.sum()", "target": "def import_execute(request, tmp_work_path: Path):\n    def _import_execute(source: str, *, custom_module_name: str | None = None):\n        module_name = custom_module_name or request.node.name\n        module_path = tmp_work_path / f'{module_name}.py'\n        module_path.write_text(source)\n        spec = importlib.util.spec_from_file_location('__main__', str(module_path))\n        module = importlib.util.module_from_spec(spec)\n        try:\n            spec.loader.exec_module(module)\n        except KeyboardInterrupt:\n            print('KeyboardInterrupt')\n        else:\n            return module\n    return _import_execute", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|opencv/opencv", "data_split": "train", "example_id": "001104", "source": "def unroll_samples(self, samples):\n        sample_n, var_n = samples.shape\n        new_samples = np.zeros((sample_n * self.class_n, var_n+1), np.float32)\n        new_samples[:,:-1] = np.repeat(samples, self.class_n, axis=0)\n        new_samples[:,-1] = np.tile(np.arange(self.class_n), sample_n)\n        return new_samples", "target": "def callable_(cls, ctype_name: str,\n                  arg_types: Union[TypeNode, Sequence[TypeNode]],\n                  ret_type: TypeNode = NoneTypeNode(\"void\"),\n                  export_name: Optional[str] = None,\n                  doc: Optional[str] = None,\n                  required_modules: Tuple[str, ...] = ()):\n        return cls(ctype_name,\n                   CallableTypeNode(ctype_name, arg_types, ret_type),\n                   export_name, doc, required_modules)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001105", "source": "def area(self) -> int:\n            return self.width * self.height", "target": "def area(self) -> int:\n            return self.width * self.height", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "001106", "source": "def test_python_json_list_none(benchmark):\n    serializer = SchemaSerializer({'type': 'list', 'items_schema': {'type': 'none'}})\n    assert serializer.to_python([None, None, None], mode='json') == [None, None, None]\n    items = [None for v in range(1000)]\n    @benchmark\n    def t():\n        serializer.to_python(items, mode='json')", "target": "def common_args(p: argparse.ArgumentParser) -> None:\n    p.add_argument(\"--verbose\", action=\"store_true\", help=\"verbose flag\")", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "001107", "source": "def _create_marker_bits(markerSize_bits, byteList):\n        marker = np.zeros((markerSize_bits+2, markerSize_bits+2))\n        bits = marker[1:markerSize_bits+1, 1:markerSize_bits+1]\n        for i in range(markerSize_bits):\n            for j in range(markerSize_bits):\n                bits[i][j] = int(byteList[i*markerSize_bits+j])\n        return marker", "target": "def test_tagged_union_with_aliases() -> None:\n    @dataclasses.dataclass\n    class ModelA:\n        field: int\n        tag: Literal['a'] = 'a'\n    @dataclasses.dataclass\n    class ModelB:\n        field: int\n        tag: Literal['b'] = 'b'\n    s = SchemaSerializer(\n        core_schema.tagged_union_schema(\n            choices={\n                'a': core_schema.dataclass_schema(\n                    ModelA,\n                    core_schema.dataclass_args_schema(\n                        'ModelA',\n                        [\n                            core_schema.dataclass_field(name='field', schema=core_schema.int_schema()),\n                            core_schema.dataclass_field(\n                                name='tag',\n                                schema=core_schema.literal_schema(['a']),\n                                validation_alias='TAG',\n                                serialization_alias='TAG',\n                            ),\n                        ],\n                    ),\n                    ['field', 'tag'],\n                ),\n                'b': core_schema.dataclass_schema(\n                    ModelB,\n                    core_schema.dataclass_args_schema(\n                        'ModelB',\n                        [\n                            core_schema.dataclass_field(name='field', schema=core_schema.int_schema()),\n                            core_schema.dataclass_field(\n                                name='tag',\n                                schema=core_schema.literal_schema(['b']),\n                                validation_alias='TAG',\n                                serialization_alias='TAG',\n                            ),\n                        ],\n                    ),\n                    ['field', 'tag'],\n                ),\n            },\n            discriminator=[['tag'], ['TAG']],\n        )\n    )\n    assert 'TaggedUnionSerializer' in repr(s)\n    model_a = ModelA(field=1)\n    model_b = ModelB(field=1)\n    assert s.to_python(model_a, by_alias=True) == {'field': 1, 'TAG': 'a'}\n    assert s.to_python(model_b, by_alias=True) == {'field': 1, 'TAG': 'b'}", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001108", "source": "def area(self) -> bytes:\n            a = self.width * self.height\n            return b'%d' % a", "target": "def area(self) -> int:\n            return self.width * self.height", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001109", "source": "def ser_x(self, v: Any, _) -> str:\n            assert self.x == 1_000\n            return f'{v:_}'", "target": "def ser_x(\n        data: Model,\n        v: Any,\n        serializer: core_schema.SerializerFunctionWrapHandler,\n        info: core_schema.FieldSerializationInfo,\n    ) -> str:\n        assert data['x'] == 1_000\n        x = serializer(v)\n        return f'{x:_}-{info.field_name}'", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001110", "source": "def ser_x(\n        data: Model,\n        v: Any,\n        serializer: core_schema.SerializerFunctionWrapHandler,\n        info: core_schema.FieldSerializationInfo,\n    ) -> str:\n        assert data['x'] == 1_000\n        x = serializer(v)\n        return f'{x:_}-{info.field_name}'", "target": "def ser_x(data: Model, v: Any, serializer: core_schema.SerializerFunctionWrapHandler) -> str:\n        assert data['x'] == 1_000\n        x = serializer(v)\n        return f'{x:_}'", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001111", "source": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "target": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001112", "source": "def execute(cmd, cwd = None):\n    print(\"Executing: %s in %s\" % (cmd, cwd), file=sys.stderr)\n    print('Executing: ' + ' '.join(cmd))\n    retcode = check_call(cmd, cwd = cwd)\n    if retcode != 0:\n        raise Exception(\"Child returned:\", retcode)", "target": "def execute(cmd, cwd=None, shell=False):\n    try:\n        log.debug(\"Executing: %s\" % cmd)\n        log.info('Executing: ' + ' '.join(cmd))\n        if cwd:\n            log.info(\"    in: %s\" % cwd)\n        retcode = subprocess.call(cmd, shell=shell, cwd=str(cwd) if cwd else None)\n        if retcode < 0:\n            raise Fail(\"Child was terminated by signal: %s\" % -retcode)\n        elif retcode > 0:\n            raise Fail(\"Child returned: %s\" % retcode)\n    except OSError as e:\n        raise Fail(\"Execution failed: %d / %s\" % (e.errno, e.strerror))", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001113", "source": "def outMeta(arr_desc0, arr_desc1, arr_desc2):\n        return cv.empty_array_desc()", "target": "def outMeta(mat_desc, scalar_desc, dtype):\n            return mat_desc", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001114", "source": "def dynamic_rnn(\n        input: Tensor,\n        hidden: tuple[Tensor, Tensor],\n        w_ih: Tensor,\n        w_hh: Tensor,\n        b_ih: Tensor,\n        b_hh: Tensor,\n    ) -> tuple[Tensor, tuple[Tensor, Tensor]]:\n        hx, cx = hidden\n        outputs = []\n        inputs = input.unbind(0)\n        hy, cy = hx[0], cx[0]\n        for seq_idx in range(len(inputs)):\n            hy, cy = cell(inputs[seq_idx], (hy, cy), w_ih, w_hh, b_ih, b_hh)\n            outputs += [hy]\n        return torch.stack(outputs), (hy.unsqueeze(0), cy.unsqueeze(0))", "target": "def dynamic_rnn(\n        input: Tensor,\n        hidden: tuple[Tensor, Tensor],\n        w_ih: Tensor,\n        w_hh: Tensor,\n        b_ih: Tensor,\n        b_hh: Tensor,\n    ) -> tuple[Tensor, tuple[Tensor, Tensor]]:\n        hx, cx = hidden\n        outputs = []\n        inputs = torch.matmul(input, w_ih.t()).unbind(0)\n        hy, cy = hx[0], cx[0]\n        for seq_idx in range(len(inputs)):\n            hy, cy = premul_cell(inputs[seq_idx], (hy, cy), w_hh, b_ih, b_hh)\n            outputs += [hy]\n        return torch.stack(outputs), (hy.unsqueeze(0), cy.unsqueeze(0))", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001115", "source": "def outMeta(arr_desc0, arr_desc1):\n        return cv.empty_array_desc(), cv.empty_array_desc()", "target": "def outMeta(mat_desc, scalar_desc, dtype):\n            return mat_desc", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|pydantic/pydantic", "data_split": "train", "example_id": "001116", "source": "def generate_missing_values(X, missing_fraction):\n    total_cells = X.shape[0] * X.shape[1]\n    num_missing_cells = int(total_cells * missing_fraction)\n    row_indices = rng.choice(X.shape[0], num_missing_cells, replace=True)\n    col_indices = rng.choice(X.shape[1], num_missing_cells, replace=True)\n    X_missing = X.copy()\n    X_missing.iloc[row_indices, col_indices] = np.nan\n    return X_missing", "target": "def test_empty_model():\n    v = SchemaValidator(core_schema.model_fields_schema(fields={}))\n    assert v.validate_python({}) == ({}, None, set())\n    with pytest.raises(\n        ValidationError, match=re.escape('Input should be a valid dictionary or instance of Model [type=model_type,')\n    ):\n        v.validate_python('x')", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001117", "source": "def generate_repeats():\n    for i in 1, 2, 3:\n        yield i\n        yield i", "target": "def generate_repeats():\n    for i in 1, 2, 3:\n        yield i\n        yield i", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001118", "source": "def test_chain():\n    s = SchemaSerializer(core_schema.chain_schema([core_schema.str_schema(), core_schema.int_schema()]))\n    assert plain_repr(s) == 'SchemaSerializer(serializer=Int(IntSerializer),definitions=[])'\n    assert s.to_python(1) == 1\n    assert s.to_json(1) == b'1'", "target": "def ser_x(self, v: Any, _) -> str:\n            assert self.x == 1_000 == v\n            return self.x_formatted", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001119", "source": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "target": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001120", "source": "def constrain(self: _Pipeline[_InT, _NewOutGt], constraint: annotated_types.Gt) -> _Pipeline[_InT, _NewOutGt]: ...", "target": "def constrain(self: _Pipeline[_InT, _OutT], constraint: _Eq) -> _Pipeline[_InT, _OutT]: ...", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001121", "source": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "target": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "001122", "source": "def is_field_frozen(stmt: AssignmentStmt) -> bool:\n        expr = stmt.rvalue\n        if isinstance(expr, TempNode):\n            return False\n        if not (\n            isinstance(expr, CallExpr) and isinstance(expr.callee, RefExpr) and expr.callee.fullname == FIELD_FULLNAME\n        ):\n            return False\n        for i, arg_name in enumerate(expr.arg_names):\n            if arg_name == 'frozen':\n                arg = expr.args[i]\n                return isinstance(arg, NameExpr) and arg.fullname == 'builtins.True'\n        return False", "target": "def forward(\n        self, input: Tensor, states: list[tuple[Tensor, Tensor]]\n    ) -> tuple[Tensor, list[tuple[Tensor, Tensor]]]:\n        output_states = jit.annotate(list[tuple[Tensor, Tensor]], [])\n        output = input\n        i = 0\n        for rnn_layer in self.layers:\n            state = states[i]\n            output, out_state = rnn_layer(output, state)\n            if i < self.num_layers - 1:\n                output = self.dropout_layer(output)\n            output_states += [out_state]\n            i += 1\n        return output, output_states", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001123", "source": "def outMeta(arr_desc0, arr_desc1, arr_desc2):\n        return cv.empty_array_desc(), cv.empty_array_desc(), \\\n               cv.empty_array_desc(), cv.empty_array_desc()", "target": "def outMeta(arr_desc0, arr_desc1):\n        return cv.empty_array_desc(), cv.empty_array_desc()", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001124", "source": "def type_format(self) -> str:\n        return \"_typing.Sequence[{}]\"", "target": "def type_format(self) -> str:\n        if TypeNode.compatible_to_runtime_usage:\n            return \"_typing.Tuple[{}]\"\n        return \"tuple[{}]\"", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "001125", "source": "def validate_arguments(func: 'AnyCallableT') -> 'AnyCallableT': ...", "target": "def load_tests(loader, tests, pattern):\n    cwd = os.getcwd()\n    config_file = 'opencv_apps_python_tests.cfg'\n    locations = [cwd, basedir]\n    if os.path.exists(config_file):\n        with open(config_file, 'r') as f:\n            locations += [str(s).strip() for s in f.readlines()]\n    else:\n        print('WARNING: OpenCV tests config file ({}) is missing, running subset of tests'.format(config_file))\n    tests_pattern = os.environ.get('OPENCV_APPS_TEST_FILTER', 'test_*') + '.py'\n    if tests_pattern != 'test_*.py':\n        print('Tests filter: {}'.format(tests_pattern))\n    processed = set()\n    for l in locations:\n        if not os.path.isabs(l):\n            l = os.path.normpath(os.path.join(cwd, l))\n        if l in processed:\n            continue\n        processed.add(l)\n        print('Discovering python tests from: {}'.format(l))\n        sys_path_modify = l not in sys.path\n        if sys_path_modify:\n            sys.path.append(l)\n        discovered_tests = loader.discover(l, pattern=tests_pattern, top_level_dir=l)\n        print('    found {} tests'.format(discovered_tests.countTestCases()))\n        tests.addTests(loader.discover(l, pattern=tests_pattern))\n        if sys_path_modify:\n            sys.path.remove(l)\n    return tests", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "train", "example_id": "001126", "source": "def decode_one_token(\n    model: torch.nn.Module, x: torch.Tensor, input_pos: torch.Tensor, **sampling_kwargs\n) -> tuple[torch.Tensor, torch.Tensor]:\n    assert input_pos.shape[-1] == 1\n    logits = model(x, input_pos)\n    return sample(logits, **sampling_kwargs)", "target": "def print_compile(gm, ex):\n            print(\n                f\"print_compile:\\n{str(gm.graph)}\\n-----------------------------------------\"\n            )\n            return gm", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001127", "source": "def benchmark(self):\n        for M, N in self.get_shapes():\n            print(f\"Tensor dimensions: [{M}, {N}]\")\n            torch_dtype = cutlass_torch.dtype(cutlass.BFloat16)\n            x = torch.randn(M, N, device=\"cuda\", dtype=torch_dtype, requires_grad=True)\n            w = torch.randn(N, device=\"cuda\", dtype=torch.float32, requires_grad=True)\n            dy = torch.randn(M, N, device=\"cuda\", dtype=torch_dtype)\n            self.benchmark_single_shape((x, w, dy), setting=f\"shape: [{M}, {N}]\")", "target": "def benchmark(self):\n        for M, N in self.get_shapes():\n            print(f\"Tensor dimensions: [{M}, {N}]\")\n            torch_dtype = cutlass_torch.dtype(cutlass.BFloat16)\n            x = torch.randn(M, N, device=\"cuda\", dtype=torch_dtype, requires_grad=True)\n            w = torch.randn(N, device=\"cuda\", dtype=torch.float32, requires_grad=True)\n            dy = torch.randn(M, N, device=\"cuda\", dtype=torch_dtype)\n            self.benchmark_single_shape((x, w, dy), setting=f\"shape: [{M}, {N}]\")", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001128", "source": "def test_only_allow_alias(py_and_json) -> None:\n    v = py_and_json(\n        cs.arguments_v3_schema(\n            [\n                cs.arguments_v3_parameter(\n                    name='a', schema=cs.str_schema(), alias='FieldA', mode='positional_or_keyword'\n                ),\n            ],\n            validate_by_name=False,\n            validate_by_alias=True,\n        )\n    )\n    assert v.validate_test(ArgsKwargs((), {'FieldA': 'hello'})) == ((), {'a': 'hello'})\n    assert v.validate_test({'FieldA': 'hello'}) == (('hello',), {})\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Missing required argument \\[type=missing_argument,'):\n        assert v.validate_test(ArgsKwargs((), {'a': 'hello'}))\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Missing required argument \\[type=missing_argument,'):\n        assert v.validate_test({'a': 'hello'})", "target": "def test_only_allow_alias(py_and_json) -> None:\n    v = py_and_json(\n        {\n            'type': 'typed-dict',\n            'fields': {\n                'field_a': {'validation_alias': 'FieldA', 'type': 'typed-dict-field', 'schema': {'type': 'int'}}\n            },\n            'config': {'validate_by_name': False, 'validate_by_alias': True},\n        }\n    )\n    assert v.validate_test({'FieldA': '123'}) == {'field_a': 123}\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'field_a': '123'})", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "001129", "source": "def test_is_str_path_exist(self):\n        p = self.tmp_path / \"x.txt\"\n        p.write_text(\"1\")\n        self.assertTrue(is_path_exist(str(p)))\n        self.assertTrue(is_path_exist(p))\n        self.assertFalse(is_path_exist(str(self.tmp_path / \"missing\")))\n        self.assertFalse(is_path_exist(self.tmp_path / \"missing\"))\n        self.assertFalse(is_path_exist(\"\"))", "target": "def test_alias_error_loc_alias(py_and_json: PyAndJson):\n    v = py_and_json(\n        {\n            'type': 'typed-dict',\n            'fields': {\n                'field_a': {\n                    'type': 'typed-dict-field',\n                    'schema': {'type': 'int'},\n                    'validation_alias': [['foo', 'x'], ['bar', 1, -1]],\n                }\n            },\n        },\n        {'loc_by_alias': True},\n    )\n    assert v.validate_test({'foo': {'x': 42}}) == {'field_a': 42}\n    assert v.validate_python({'bar': ['x', {-1: 42}]}) == {'field_a': 42}\n    assert v.validate_test({'bar': ['x', [1, 2, 42]]}) == {'field_a': 42}\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_test({'foo': {'x': 'not_int'}})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('foo', 'x'),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'not_int',\n        }\n    ]\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_test({'bar': ['x', [1, 2, 'not_int']]})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('bar', 1, -1),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'not_int',\n        }\n    ]\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_test({})\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'missing', 'loc': ('foo', 'x'), 'msg': 'Field required', 'input': {}}\n    ]", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001130", "source": "def resolve(self, root: ASTNode):\n        if self.is_resolved:\n            return\n        node = _resolve_symbol(root, self.typename)\n        if node is None:\n            raise TypeResolutionError('Failed to resolve \"{}\" exposed as \"{}\"'.format(\n                self.ctype_name, self.typename\n            ))\n        self._ast_node = weakref.proxy(node)", "target": "def resolve(self, root: ASTNode) -> None:\n        errors = []\n        for item in filter(lambda item: not item.is_resolved, self):\n            try:\n                item.resolve(root)\n            except TypeResolutionError as e:\n                errors.append(str(e))\n        if len(errors) > 0:\n            raise TypeResolutionError(\n                'Failed to resolve one of \"{}\" items. Errors: {}'.format(\n                    self.full_typename, errors\n                )\n            )", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "001131", "source": "def fmt_bool(x):\n    return (\"true\" if x else \"false\")", "target": "def setUp(self):\n        self.ACDT = TzInfo(timedelta(hours=9.5).total_seconds())\n        self.EST = TzInfo(-timedelta(hours=5).total_seconds())\n        self.UTC = TzInfo(timedelta(0).total_seconds())\n        self.DT = datetime(2010, 1, 1)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "001132", "source": "def get_standard_typevars_map(cls: Any) -> dict[TypeVar, Any] | None:\n    origin = get_origin(cls)\n    if origin is None:\n        return None\n    if not hasattr(origin, '__parameters__'):\n        return None\n    args: tuple[Any, ...] = cls.__args__\n    parameters: tuple[TypeVar, ...] = origin.__parameters__\n    return dict(zip(parameters, args))", "target": "def __collect_extra_submodules(enable_debug_print=False):\n    def modules_filter(module):\n        return all((\n             not module.startswith(\"_\"),\n             not module.startswith(\"python-\"),\n             os.path.isdir(os.path.join(_extra_submodules_init_path, module))\n        ))\n    if sys.version_info[0] < 3:\n        if enable_debug_print:\n            print(\"Extra submodules is loaded only for Python 3\")\n        return []\n    __INIT_FILE_PATH = os.path.abspath(__file__)\n    _extra_submodules_init_path = os.path.dirname(__INIT_FILE_PATH)\n    return filter(modules_filter, os.listdir(_extra_submodules_init_path))", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|pydantic/pydantic", "data_split": "train", "example_id": "001133", "source": "def make_data(self, params):\n        data = _synth_classification_dataset(\n            n_samples=10000, n_features=100, n_classes=5\n        )\n        return data", "target": "def _pydantic_model_class_maker_callback(self, ctx: ClassDefContext) -> None:\n        transformer = PydanticModelTransformer(ctx.cls, ctx.reason, ctx.api, self.plugin_config)\n        transformer.transform()", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001134", "source": "def test_dict(py_and_json: PyAndJson):\n    v = py_and_json({'type': 'dict', 'keys_schema': {'type': 'datetime'}, 'values_schema': {'type': 'int'}})\n    assert v.validate_test({'2000-01-01T00:00': 2, '2000-01-02T00:00': 4}) == {\n        datetime(2000, 1, 1): 2,\n        datetime(2000, 1, 2): 4,\n    }", "target": "def test_dict(py_and_json: PyAndJson):\n    v = py_and_json({'type': 'dict', 'keys_schema': {'type': 'time'}, 'values_schema': {'type': 'int'}})\n    assert v.validate_test({'12:01:01': 2, '12:01:02': 4}) == {time(12, 1, 1): 2, time(12, 1, 2): 4}", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "001135", "source": "def test_complete_core_root_json(benchmark):\n    v = SchemaValidator(wrap_schema_in_root_model(schema()))\n    json_data = json.dumps({'root': input_data_lax()}, default=default_json_encoder)\n    benchmark(v.validate_json, json_data)", "target": "def output_csv(output_file, headers, row):\n    if os.path.exists(output_file):\n        with open(output_file) as fd:\n            lines = list(csv.reader(fd)) or [[]]\n            if headers and len(headers) > len(lines[0]):\n                lines[0] = headers\n            else:\n                headers = lines[0]\n    else:\n        lines = [headers]\n    if output_file != DEFAULT_OUTPUT_FILE:\n        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n    lines.append([(f\"{x:.6f}\" if isinstance(x, float) else x) for x in row])\n    with open(output_file, \"w\") as fd:\n        writer = csv.writer(fd, lineterminator=\"\\n\")\n        for line in lines:\n            writer.writerow(list(line) + [\"0\"] * (len(headers) - len(line)))", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001136", "source": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "target": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "001137", "source": "def get_cached_generic_type_early(parent: type[BaseModel], typevar_values: Any) -> type[BaseModel] | None:\n    return _GENERIC_TYPES_CACHE.get(_early_cache_key(parent, typevar_values))", "target": "def liger(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        from liger_kernel.transformers.cross_entropy import LigerCrossEntropyLoss\n        x, target = args\n        cross_entropy = LigerCrossEntropyLoss(reduction=\"none\")\n        return lambda: cross_entropy(x, target)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001138", "source": "def required_definition_imports(self) -> Generator[str, None, None]:\n        return self.value.required_usage_imports", "target": "def required_definition_imports(self) -> Generator[str, None, None]:\n        for item in self:\n            yield from item.required_definition_imports", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001139", "source": "def execute(cmd, shell=False):\n    try:\n        log.info(\"Executing: %s\" % cmd)\n        env = os.environ.copy()\n        env['VERBOSE'] = '1'\n        retcode = subprocess.call(cmd, shell=shell, env=env)\n        if retcode < 0:\n            raise Fail(\"Child was terminated by signal: %s\" % -retcode)\n        elif retcode > 0:\n            raise Fail(\"Child returned: %s\" % retcode)\n    except OSError as e:\n        raise Fail(\"Execution failed: %d / %s\" % (e.errno, e.strerror))", "target": "def execute(cmd, cwd=None, shell=False):\n    try:\n        log.debug(\"Executing: %s\" % cmd)\n        log.info('Executing: ' + ' '.join(cmd))\n        if cwd:\n            log.info(\"    in: %s\" % cwd)\n        retcode = subprocess.call(cmd, shell=shell, cwd=str(cwd) if cwd else None)\n        if retcode < 0:\n            raise Fail(\"Child was terminated by signal: %s\" % -retcode)\n        elif retcode > 0:\n            raise Fail(\"Child returned: %s\" % retcode)\n    except OSError as e:\n        raise Fail(\"Execution failed: %d / %s\" % (e.errno, e.strerror))", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001140", "source": "def type_format(self) -> str:\n        if TypeNode.compatible_to_runtime_usage:\n            return \"_typing.Tuple[{}]\"\n        return \"tuple[{}]\"", "target": "def type_format(self) -> str:\n        if TypeNode.compatible_to_runtime_usage:\n            return \"_typing.Union[{}]\"\n        return \"{}\"", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001141", "source": "def dec(f: Any) -> _decorators.PydanticDescriptorProxy[Any]:\n        if _decorators.is_instance_method_from_sig(f):\n            raise PydanticUserError(\n                '`@validator` cannot be applied to instance methods', code='validator-instance-method'\n            )\n        f = _decorators.ensure_classmethod_based_on_signature(f)\n        wrap = _decorators_v1.make_generic_v1_field_validator\n        validator_wrapper_info = _decorators.ValidatorDecoratorInfo(\n            fields=fields,\n            mode=mode,\n            each_item=each_item,\n            always=always,\n            check_fields=check_fields,\n        )\n        return _decorators.PydanticDescriptorProxy(f, validator_wrapper_info, shim=wrap)", "target": "def dec(f: Callable[..., Any] | classmethod[Any, Any, Any] | staticmethod[Any, Any]) -> Any:\n        if _decorators.is_instance_method_from_sig(f):\n            raise TypeError('`@root_validator` cannot be applied to instance methods')\n        res = _decorators.ensure_classmethod_based_on_signature(f)\n        dec_info = _decorators.RootValidatorDecoratorInfo(mode=mode)\n        return _decorators.PydanticDescriptorProxy(res, dec_info, shim=wrap)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001142", "source": "def rms_norm_ref(self, x, w):\n        x_f32 = x.float()\n        return (\n            x_f32\n            * torch.rsqrt(torch.mean(x_f32.square(), dim=-1, keepdim=True) + 1e-6)\n            * w\n        ).to(x.dtype)", "target": "def rms_norm_ref(self, x, w):\n        x_f32 = x.float()\n        return (\n            x_f32\n            * torch.rsqrt(torch.mean(x_f32.square(), dim=-1, keepdim=True) + 1e-6)\n            * w\n        ).to(x.dtype)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001143", "source": "def ser_x(data: Model, v: Any, info: core_schema.FieldSerializationInfo) -> str:\n        assert data['x'] == 1_000\n        return f'{v:_}-{info.field_name}'", "target": "def ser_x(data: Model, v: Any) -> str:\n        assert data['x'] == 1_000\n        return f'{v:_}'", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "001144", "source": "def gh_delete_comment(org: str, repo: str, comment_id: int) -> None:\n    url = f\"{GITHUB_API_URL}/repos/{org}/{repo}/issues/comments/{comment_id}\"\n    gh_fetch_url(url, method=\"DELETE\", reader=lambda x: x.read())", "target": "def test_alias_extra_by_name(py_and_json: PyAndJson):\n    v = py_and_json(\n        {\n            'type': 'typed-dict',\n            'extra_behavior': 'allow',\n            'fields': {\n                'field_a': {'validation_alias': 'FieldA', 'type': 'typed-dict-field', 'schema': {'type': 'int'}}\n            },\n            'config': {'validate_by_name': True},\n        },\n    )\n    assert v.validate_test({'FieldA': 1}) == {'field_a': 1}\n    assert v.validate_test({'field_a': 1}) == {'field_a': 1}", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001145", "source": "def test_union():\n    v = SchemaValidator(cs.union_schema(choices=[cs.str_schema(), cs.date_schema()]))\n    assert v.validate_python('2022-01-02') == '2022-01-02'\n    assert v.validate_python(date(2022, 1, 2)) == date(2022, 1, 2)\n    v = SchemaValidator(cs.union_schema(choices=[cs.date_schema(), cs.str_schema()]))\n    assert v.validate_python('2022-01-02') == '2022-01-02'\n    assert v.validate_python(date(2022, 1, 2)) == date(2022, 1, 2)", "target": "def test_union():\n    v = SchemaValidator(core_schema.union_schema(choices=[core_schema.str_schema(), core_schema.timedelta_schema()]))\n    assert v.validate_python('P2DT1H') == 'P2DT1H'\n    assert v.validate_python(timedelta(days=2, hours=1)) == timedelta(days=2, hours=1)\n    v = SchemaValidator(core_schema.union_schema(choices=[core_schema.timedelta_schema(), core_schema.str_schema()]))\n    assert v.validate_python('P2DT1H') == 'P2DT1H'\n    assert v.validate_python(timedelta(days=2, hours=1)) == timedelta(days=2, hours=1)", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|pydantic/pydantic", "data_split": "train", "example_id": "001146", "source": "def measure_one(data, n_components, solver, method_name=\"fit\"):\n    print(\n        f\"Benchmarking {solver=!r}, {n_components=}, {method_name=!r} on data with\"\n        f\" shape {data.shape}\"\n    )\n    pca = PCA(n_components=n_components, svd_solver=solver, random_state=0)\n    timings = []\n    elapsed = 0\n    method = getattr(pca, method_name)\n    with config_context(assume_finite=True):\n        while elapsed < 0.5:\n            tic = perf_counter()\n            method(data)\n            duration = perf_counter() - tic\n            timings.append(duration)\n            elapsed += duration\n    return np.median(timings)", "target": "def test_list_fail_fast(fail_fast, expected):\n    s = core_schema.list_schema(core_schema.int_schema(), fail_fast=fail_fast)\n    v = SchemaValidator(s)\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python([1, 'not-num', 'again'])\n    assert exc_info.value.errors(include_url=False) == expected", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001147", "source": "def compiled(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, w, dy = args\n        y = torch.compile(self.rms_norm_ref, mode=self.compile_mode, fullgraph=True)(\n            x, w\n        )\n        return lambda: torch.autograd.grad(\n            y, [x, w], grad_outputs=dy, retain_graph=True\n        )", "target": "def compiled(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, w = args\n        torch._dynamo.mark_dynamic(x, 0)\n        compiled_layernorm = torch.compile(\n            self.layernorm_ref, mode=self.compile_mode, fullgraph=True\n        )\n        return lambda: compiled_layernorm(x, w, eps=1e-6)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001148", "source": "def pydantic_general_metadata(**metadata: Any) -> BaseMetadata:\n    return _general_metadata_cls()(metadata)", "target": "def f(input_value, _info):\n        assert isinstance(input_value, dict)\n        input_value['field_a'] += b' XX'\n        return input_value", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001149", "source": "def ser_x(self, v: Any, serializer: core_schema.SerializerFunctionWrapHandler, _) -> str:\n            x = serializer(v)\n            assert self.x == 1_000\n            return f'{x:_}'", "target": "def ser_x(self, v: Any, _) -> str:\n            assert self.x == 1_000\n            return f'{v:_}'", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001150", "source": "def gen(error: bool):\n        yield 1\n        yield 2\n        if error:\n            raise RuntimeError('my error')\n        yield 3", "target": "def gen(error: bool):\n        yield 1\n        yield 2\n        if error:\n            raise RuntimeError('error')\n        yield 3", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "001151", "source": "def test_bool_error(pydantic_version):\n    v = SchemaValidator(cs.bool_schema())\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python('wrong')\n    assert str(exc_info.value) == (\n        '1 validation error for bool\\n'\n        '  Input should be a valid boolean, '\n        \"unable to interpret input [type=bool_parsing, input_value='wrong', input_type=str]\"\n        + (\n            f'\\n    For further information visit https://errors.pydantic.dev/{pydantic_version}/v/bool_parsing'\n            if os.environ.get('PYDANTIC_ERRORS_INCLUDE_URL', '1') != 'false'\n            else ''\n        )\n    )\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'bool_parsing',\n            'loc': (),\n            'msg': 'Input should be a valid boolean, unable to interpret input',\n            'input': 'wrong',\n        }\n    ]", "target": "def required_usage_imports(self) -> Generator[str, None, None]:\n        yield \"import cv2.typing\"", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001152", "source": "def relative_typename(self, root: str) -> Optional[str]:\n            if self.type_node is not None:\n                return self.type_node.relative_typename(root)\n            return None", "target": "def relative_typename(self, full_node_name: str) -> str:\n        return self.type_node.relative_typename(full_node_name)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001153", "source": "def test_core_model_json_extra(benchmark, basic_model_serializer_extra):\n    m = BasicModel(a=1, b=2, c=3, d=4, e=5, f=6, g=7, h=8, __pydantic_extra__={'i': 9})\n    assert basic_model_serializer_extra.to_json(m) == b'{\"a\":1,\"b\":2,\"c\":3,\"d\":4,\"e\":5,\"f\":6,\"g\":7,\"h\":8,\"i\":9}'\n    benchmark(basic_model_serializer_extra.to_json, m)", "target": "def check_len(v: Any) -> bool:\n            if max_len is not None:\n                return (min_len <= len(v)) and (len(v) <= max_len)\n            return min_len <= len(v)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "001154", "source": "def load_base(fn):\n    a = np.loadtxt(fn, np.float32, delimiter=',', converters={ 0 : lambda ch : ord(ch)-ord('A') })\n    samples, responses = a[:,1:], a[:,0]\n    return samples, responses", "target": "def validated_setattr(instance: PydanticDataclass, name: str, value: Any, /) -> None:\n                if frozen_:\n                    return original_setattr(instance, name, value)\n                inst_cls = type(instance)\n                attr = getattr(inst_cls, name, None)\n                if isinstance(attr, property):\n                    attr.__set__(instance, value)\n                elif isinstance(attr, functools.cached_property):\n                    instance.__dict__.__setitem__(name, value)\n                else:\n                    inst_cls.__pydantic_validator__.validate_assignment(instance, name, value)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "001155", "source": "def in_(self: _Pipeline[_InT, _OutT], values: Container[_OutT]) -> _Pipeline[_InT, _OutT]:\n        return self.constrain(_In(values))", "target": "def hessian_revrev(model, inp, strict=None):\n        return functional.hessian(model, inp, strict=False, vectorize=True)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001156", "source": "def liger(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        from liger_kernel.transformers.cross_entropy import LigerCrossEntropyLoss\n        x, target = args\n        cross_entropy = LigerCrossEntropyLoss(reduction=\"none\")\n        return lambda: cross_entropy(x, target)", "target": "def liger(self, args, kwargs) -> Any:\n        from liger_kernel.transformers.rms_norm import LigerRMSNorm\n        x, w = args\n        M, N = x.shape\n        liger_rmsnorm = LigerRMSNorm(hidden_size=N, eps=1e-6).cuda()\n        liger_rmsnorm.weight.data.copy_(w)\n        return lambda: liger_rmsnorm(x)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001157", "source": "def _prepare_once(self):\n        self.weights = [torch.randn(16, 16, requires_grad=True) for _ in range(100)]\n        self.inp = torch.randn(16, 16)", "target": "def _prepare_once(self):\n        self.x = torch.randn(4, 4, requires_grad=True)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001158", "source": "def test_dataclass_validator_reuse() -> None:\n    @dataclass\n    class Model:\n        x: str\n        y: str\n    def f(input_value: Any, info: core_schema.ValidationInfo) -> Any:\n        return f'{info.field_name}: {input_value}'\n    with pytest.warns(\n        DeprecationWarning, match='`field_name` argument on `with_info_plain_validator_function` is deprecated'\n    ):\n        validator = core_schema.with_info_plain_validator_function(f, field_name='x')\n    v = SchemaValidator(\n        core_schema.dataclass_schema(\n            Model,\n            core_schema.dataclass_args_schema(\n                'Model',\n                [\n                    core_schema.dataclass_field(name='x', schema=validator),\n                    core_schema.dataclass_field(name='y', schema=validator),\n                ],\n            ),\n            ['x', 'y'],\n        )\n    )\n    m = v.validate_python({'x': 'foo', 'y': 'bar'})\n    assert m.x == 'x: foo'\n    assert m.y == 'y: bar'", "target": "def mode(self) -> Literal['python', 'json'] | str:\n        ...", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "001159", "source": "def setdefault(self, key: str, value: Any) -> None:\n        if getattr(self, key) is None:\n            setattr(self, key, value)", "target": "def testAliasWithOffset(self) -> list[Tensor]:\n        x = torch.tensor([100, 200])\n        a = [x[0], x[1]]\n        return a", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001160", "source": "def test_serialize_as_any_wrap_serializer_applied_once() -> None:\n    class InnerModel:\n        an_inner_field: int\n    InnerModel.__pydantic_core_schema__ = core_schema.model_schema(\n        InnerModel,\n        core_schema.model_fields_schema({'an_inner_field': core_schema.model_field(core_schema.int_schema())}),\n    )\n    InnerModel.__pydantic_validator__ = SchemaValidator(InnerModel.__pydantic_core_schema__)\n    InnerModel.__pydantic_serializer__ = SchemaSerializer(InnerModel.__pydantic_core_schema__)\n    class MyModel:\n        a_field: InnerModel\n        def a_model_serializer(self, handler, info):\n            return {k + '_wrapped': v for k, v in handler(self).items()}\n    MyModel.__pydantic_core_schema__ = core_schema.model_schema(\n        MyModel,\n        core_schema.model_fields_schema({'a_field': core_schema.model_field(InnerModel.__pydantic_core_schema__)}),\n        serialization=core_schema.wrap_serializer_function_ser_schema(\n            MyModel.a_model_serializer,\n            info_arg=True,\n        ),\n    )\n    MyModel.__pydantic_validator__ = SchemaValidator(MyModel.__pydantic_core_schema__)\n    MyModel.__pydantic_serializer__ = SchemaSerializer(MyModel.__pydantic_core_schema__)\n    instance = MyModel.__pydantic_validator__.validate_python({'a_field': {'an_inner_field': 1}})\n    assert MyModel.__pydantic_serializer__.to_python(instance, serialize_as_any=True) == {\n        'a_field_wrapped': {'an_inner_field': 1},\n    }", "target": "def test_leak_model(validator):\n    def fn():\n        class Model:\n            a: int\n            @classmethod\n            def _validator(cls, v, info):\n                return v\n            @classmethod\n            def _wrap_validator(cls, v, validator, info):\n                return validator(v)\n        field_schema = core_schema.int_schema()\n        if validator == 'field':\n            field_schema = core_schema.with_info_before_validator_function(Model._validator, field_schema)\n            field_schema = core_schema.with_info_wrap_validator_function(Model._wrap_validator, field_schema)\n            field_schema = core_schema.with_info_after_validator_function(Model._validator, field_schema)\n        model_schema = core_schema.model_schema(\n            Model, core_schema.model_fields_schema({'a': core_schema.model_field(field_schema)})\n        )\n        if validator == 'model':\n            model_schema = core_schema.with_info_before_validator_function(Model._validator, model_schema)\n            model_schema = core_schema.with_info_wrap_validator_function(Model._wrap_validator, model_schema)\n            model_schema = core_schema.with_info_after_validator_function(Model._validator, model_schema)\n        Model.__pydantic_validator__ = SchemaValidator(model_schema)\n        return Model\n    klass = fn()\n    ref = weakref.ref(klass)\n    assert ref() is not None\n    del klass\n    assert_gc(lambda: ref() is None)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "001161", "source": "def test_dataclass_post_init():\n    @dataclasses.dataclass\n    class Foo:\n        a: str\n        b: bool\n        def __post_init__(self):\n            self.a = self.a.upper()\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n            ],\n        ),\n        ['a', 'b'],\n        post_init=True,\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 'hello', 'b': True})\n    assert foo.a == 'HELLO'\n    assert foo.b is True", "target": "def enumerations(self) -> Dict[str, EnumerationNode]:\n        return self._children[ASTNodeType.Enumeration]", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001162", "source": "def dataclass(\n        *,\n        init: Literal[False] = False,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool | None = None,\n        config: ConfigDict | type[object] | None = None,\n        validate_on_init: bool | None = None,\n    ) -> Callable[[type[_T]], type[PydanticDataclass]]:\n        ...", "target": "def dataclass(\n        _cls: type[_T],\n        *,\n        init: Literal[False] = False,\n        repr: bool = True,\n        eq: bool = True,\n        order: bool = False,\n        unsafe_hash: bool = False,\n        frozen: bool | None = None,\n        config: ConfigDict | type[object] | None = None,\n        validate_on_init: bool | None = None,\n    ) -> type[PydanticDataclass]: ...", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "001163", "source": "def benchmark(self):\n        for M, N in self.get_shapes():\n            print(f\"Tensor dimensions: [{M}, {N}]\")\n            torch_dtype = cutlass_torch.dtype(cutlass.BFloat16)\n            x = torch.randn(M, N, device=\"cuda\", dtype=torch_dtype, requires_grad=True)\n            w = torch.randn(N, device=\"cuda\", dtype=torch.float32, requires_grad=True)\n            dy = torch.randn(M, N, device=\"cuda\", dtype=torch_dtype)\n            self.benchmark_single_shape((x, w, dy), setting=f\"shape: [{M}, {N}]\")", "target": "def _extract_decimal_digits_info(decimal: Decimal) -> tuple[int, int]:\n    try:\n        decimal_tuple = decimal.as_tuple()\n        assert isinstance(decimal_tuple.exponent, int)\n        exponent = decimal_tuple.exponent\n        num_digits = len(decimal_tuple.digits)\n        if exponent >= 0:\n            num_digits += exponent\n            decimal_places = 0\n        else:\n            decimal_places = abs(exponent)\n            num_digits = max(num_digits, decimal_places)\n        return decimal_places, num_digits\n    except (AssertionError, AttributeError):\n        raise TypeError(f'Unable to extract decimal digits info from supplied value {decimal}')", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001164", "source": "def relative_typename(self, root: str) -> Optional[str]:\n            if self.type_node is not None:\n                return self.type_node.relative_typename(root)\n            return None", "target": "def relative_typename(self, module: str) -> str:\n        return self.type_format.format(self.types_separator.join(\n            item.relative_typename(module) for item in self\n        ))", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001165", "source": "def f(value, serializer, _info):\n        return f'result={serializer(len(value))} repr={serializer!r}'", "target": "def f(prefix, value, _info):\n        return f'{prefix}{value}'", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "001166", "source": "def visit(self, node: ast.AST) -> Any:\n        node_result = super().visit(node)\n        self.previous_node_type = type(node)\n        return node_result", "target": "def test_age_gender_infer_image(self):\n            skip_if_openvino_not_available()\n            root_path  = '/omz_intel_models/intel/age-gender-recognition-retail-0013/FP32/age-gender-recognition-retail-0013'\n            model_path = self.find_file(root_path + '.xml',   [os.environ.get('OPENCV_DNN_TEST_DATA_PATH')], required=False)\n            bin_path   = self.find_file(root_path + '.bin',   [os.environ.get('OPENCV_DNN_TEST_DATA_PATH')], required=False)\n            device_id  = 'CPU'\n            img_path = self.find_file('cv/face/david2.jpg', [os.environ.get('OPENCV_TEST_DATA_PATH')])\n            img = cv.imread(img_path)\n            def preproc(ppp):\n                ppp.input().model().set_layout(Layout(\"NCHW\"))\n                ppp.input().tensor().set_element_type(Type.u8)                            \\\n                                    .set_spatial_static_shape(img.shape[0], img.shape[1]) \\\n                                    .set_layout(Layout(\"NHWC\"))\n                ppp.input().preprocess().resize(ResizeAlgorithm.RESIZE_LINEAR)\n            ref = AgeGenderOV(model_path, bin_path, device_id)\n            ref.cfgPrePostProcessing(preproc)\n            ov_age, ov_gender = ref.apply(np.expand_dims(img, 0))\n            comp = AgeGenderGAPI(model_path, bin_path, device_id)\n            gapi_age, gapi_gender = comp.apply(img)\n            self.assertEqual(0.0, cv.norm(ov_gender, gapi_gender, cv.NORM_INF))\n            self.assertEqual(0.0, cv.norm(ov_age, gapi_age, cv.NORM_INF))", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001167", "source": "def get_batch(self, imgs_names):\n        assert type(imgs_names) is list\n        batch = np.zeros((len(imgs_names), 3, self.frame_size, self.frame_size)).astype(np.float32)\n        for i in range(len(imgs_names)):\n            img_name = imgs_names[i]\n            img_file = self.imgs_dir + img_name\n            assert os.path.exists(img_file)\n            img = cv.imread(img_file, cv.IMREAD_COLOR)\n            min_dim = min(img.shape[-3], img.shape[-2])\n            resize_ratio = self.frame_size / float(min_dim)\n            img = cv.resize(img, (0, 0), fx=resize_ratio, fy=resize_ratio)\n            cols = img.shape[1]\n            rows = img.shape[0]\n            y1 = (rows - self.frame_size) / 2\n            y2 = y1 + self.frame_size\n            x1 = (cols - self.frame_size) / 2\n            x2 = x1 + self.frame_size\n            img = img[y1:y2, x1:x2]\n            if self.bgr_to_rgb:\n                img = img[..., ::-1]\n            image_data = img[:, :, 0:3].transpose(2, 0, 1)\n            batch[i] = self.preprocess(image_data)\n        return batch", "target": "def get_batch(self, img_names):\n        assert type(img_names) is list\n        batch = np.zeros((len(img_names), 3, self.frame_size, self.frame_size)).astype(np.float32)\n        for i in range(len(img_names)):\n            img_name = img_names[i]\n            img_file = os.path.join(self.imgs_dir, img_name)\n            assert os.path.exists(img_file)\n            batch[i] = self.get_preprocessed_img(img_file)\n        return batch", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001168", "source": "def required_usage_imports(self) -> Generator[str, None, None]:\n        yield \"import cv2.typing\"", "target": "def required_usage_imports(self) -> Generator[str, None, None]:\n        yield \"import os\"", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001169", "source": "def full_typename(self) -> str:\n        return \"cv2.typing.\" + self.typename", "target": "def full_typename(self) -> str:\n        return \"cv2.typing.\" + self.typename", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|pydantic/pydantic", "data_split": "train", "example_id": "001170", "source": "def get_canonical_name_meson(target, build_path):\n    assert len(target[\"filename\"]) == 1\n    shared_library_path = Path(target[\"filename\"][0])\n    shared_library_relative_path = shared_library_path.relative_to(\n        build_path.absolute()\n    )\n    rel_path = shared_library_relative_path.as_posix()\n    pattern = r\"\\.(cpython|cp\\d+)-.+\"\n    return re.sub(pattern, \"\", str(rel_path))", "target": "def test_by_alias_and_name_config_interaction(\n    config_by_alias: Union[bool, None],\n    config_by_name: Union[bool, None],\n    runtime_by_alias: Union[bool, None],\n    runtime_by_name: Union[bool, None],\n) -> None:\n    if config_by_alias is False and config_by_name is False and runtime_by_alias is False and runtime_by_name is False:\n        pytest.skip(\"Can't have both by_alias and by_name as effectively False\")\n    core_config = {\n        **({'validate_by_alias': config_by_alias} if config_by_alias is not None else {}),\n        **({'validate_by_name': config_by_name} if config_by_name is not None else {}),\n    }\n    @dataclasses.dataclass\n    class MyDataclass:\n        my_field: int\n    schema = core_schema.dataclass_schema(\n        MyDataclass,\n        core_schema.dataclass_args_schema(\n            'MyDataclass',\n            [\n                core_schema.dataclass_field(\n                    name='my_field', schema=core_schema.int_schema(), validation_alias='my_alias'\n                ),\n            ],\n        ),\n        ['my_field'],\n        config=core_schema.CoreConfig(**core_config),\n    )\n    s = SchemaValidator(schema)\n    alias_allowed = next(x for x in (runtime_by_alias, config_by_alias, True) if x is not None)\n    name_allowed = next(x for x in (runtime_by_name, config_by_name, False) if x is not None)\n    if alias_allowed:\n        assert dataclasses.asdict(\n            s.validate_python({'my_alias': 1}, by_alias=runtime_by_alias, by_name=runtime_by_name)\n        ) == {'my_field': 1}\n    if name_allowed:\n        assert dataclasses.asdict(\n            s.validate_python({'my_field': 1}, by_alias=runtime_by_alias, by_name=runtime_by_name)\n        ) == {'my_field': 1}", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "001171", "source": "def _prepare_once(self):\n        torch._dynamo.config.capture_scalar_outputs = True\n        torch.manual_seed(0)\n        self.splits = torch.randint(10, (self.N,))", "target": "def test_function_wrap_python(benchmark):\n    def f(value, serializer, _info):\n        return f'result={serializer(len(value))}'\n    s = SchemaSerializer(\n        core_schema.int_schema(serialization=core_schema.wrap_serializer_function_ser_schema(f, info_arg=True))\n    )\n    benchmark(s.to_python, 'foo')", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001172", "source": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "target": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "001173", "source": "def test_core_schema_import_missing():\n    with pytest.raises(AttributeError, match=\"module 'pydantic_core' has no attribute 'foobar'\"):\n        core_schema.foobar", "target": "def center_crop(self, img):\n        cols = img.shape[1]\n        rows = img.shape[0]\n        y1 = round((rows - self.frame_size) / 2)\n        y2 = round(y1 + self.frame_size)\n        x1 = round((cols - self.frame_size) / 2)\n        x2 = round(x1 + self.frame_size)\n        return img[y1:y2, x1:x2]", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001174", "source": "def update_wrapper_attributes(wrapped: ValidateCallSupportedTypes, wrapper: Callable[..., Any]):\n    if inspect.iscoroutinefunction(wrapped):\n        @functools.wraps(wrapped)\n        async def wrapper_function(*args, **kwargs):\n            return await wrapper(*args, **kwargs)\n    else:\n        @functools.wraps(wrapped)\n        def wrapper_function(*args, **kwargs):\n            return wrapper(*args, **kwargs)\n    wrapper_function.__name__ = extract_function_name(wrapped)\n    wrapper_function.__qualname__ = extract_function_qualname(wrapped)\n    wrapper_function.raw_function = wrapped\n    return wrapper_function", "target": "def test_tzinfo_could_be_reused():\n    class Model:\n        value: datetime\n    v = SchemaValidator(\n        core_schema.model_schema(\n            Model, core_schema.model_fields_schema({'value': core_schema.model_field(core_schema.datetime_schema())})\n        )\n    )\n    m = v.validate_python({'value': '2015-10-21T15:28:00.000000+01:00'})\n    target = datetime(1955, 11, 12, 14, 38, tzinfo=m.value.tzinfo)\n    assert target == datetime(1955, 11, 12, 14, 38, tzinfo=timezone(timedelta(hours=1)))\n    now = datetime.now(tz=m.value.tzinfo)\n    assert isinstance(now, datetime)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "001175", "source": "def test_default_matx_argument(self):\n        res = cv.utils.dumpVec2i()\n        self.assertEqual(res, \"Vec2i(42, 24)\",\n                         msg=\"Default argument is not properly handled\")\n        res = cv.utils.dumpVec2i((12, 21))\n        self.assertEqual(res, \"Vec2i(12, 21)\")", "target": "def make_v1_generic_root_validator(\n    validator: V1RootValidatorFunction, pre: bool\n) -> V2CoreBeforeRootValidator | V2CoreAfterRootValidator:\n    if pre is True:\n        def _wrapper1(values: RootValidatorValues, _: core_schema.ValidationInfo) -> RootValidatorValues:\n            return validator(values)\n        return _wrapper1\n    def _wrapper2(fields_tuple: RootValidatorFieldsTuple, _: core_schema.ValidationInfo) -> RootValidatorFieldsTuple:\n        if len(fields_tuple) == 2:\n            values, init_vars = fields_tuple\n            values = validator(values)\n            return values, init_vars\n        else:\n            model_dict, model_extra, fields_set = fields_tuple\n            if model_extra:\n                fields = set(model_dict.keys())\n                model_dict.update(model_extra)\n                model_dict_new = validator(model_dict)\n                for k in list(model_dict_new.keys()):\n                    if k not in fields:\n                        model_extra[k] = model_dict_new.pop(k)\n            else:\n                model_dict_new = validator(model_dict)\n            return model_dict_new, model_extra, fields_set\n    return _wrapper2", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001176", "source": "def test_aliases_path_multiple(py_and_json: PyAndJson, input_value, expected):\n    v = py_and_json(\n        {\n            'type': 'model-fields',\n            'fields': {\n                'field_a': {\n                    'validation_alias': [['foo', 'bar', 'bat'], ['foo', 3], ['spam']],\n                    'type': 'model-field',\n                    'schema': {'type': 'int'},\n                }\n            },\n        },\n        {'loc_by_alias': False},\n    )\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=expected.message):\n            val = v.validate_test(input_value)\n            print(f'UNEXPECTED OUTPUT: {val!r}')\n    else:\n        output = v.validate_test(input_value)\n        assert output == expected", "target": "def test_aliases_path_multiple(py_and_json: PyAndJson, input_value, expected):\n    v = py_and_json(\n        {\n            'type': 'typed-dict',\n            'fields': {\n                'field_a': {\n                    'validation_alias': [['foo', 'bar', 'bat'], ['foo', 3], ['spam']],\n                    'type': 'typed-dict-field',\n                    'schema': {'type': 'int'},\n                }\n            },\n            'config': {'loc_by_alias': False},\n        }\n    )\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=expected.message):\n            val = v.validate_test(input_value)\n            print(f'UNEXPECTED OUTPUT: {val!r}')\n    else:\n        output = v.validate_test(input_value)\n        assert output == expected", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001177", "source": "def forward(self, inputs):\n        output = self.pool(F.relu(self.conv(inputs)))\n        output = output.view(1)\n        return output", "target": "def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = F.max_pool2d(x, 2)\n        x = torch.flatten(x, 1)\n        output = self.fc1(x)\n        return output", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "001178", "source": "def test_str_config():\n    v = SchemaValidator(\n        core_schema.typed_dict_schema(\n            fields={'field_a': core_schema.typed_dict_field(schema=core_schema.str_schema())},\n            config=CoreConfig(str_max_length=5),\n        )\n    )\n    assert v.validate_python({'field_a': 'test'}) == {'field_a': 'test'}\n    with pytest.raises(ValidationError, match='String should have at most 5 characters'):\n        v.validate_python({'field_a': 'test long'})", "target": "def get_release_version(self) -> str:\n        if not get_tag():\n            raise NoGitTagException(\n                \"Not on a git tag, are you sure you want a release version?\"\n            )\n        return f\"{get_tag()}{self.get_post_build_suffix()}\"", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|opencv/opencv", "data_split": "train", "example_id": "001179", "source": "def _work(self) -> None:\n        @torch.compile(\n            backend=\"inductor\",\n            fullgraph=True,\n            dynamic=self._dynamic,\n        )\n        def f(a, b):\n            z = torch.mm(a, b)\n            for i in range(200):\n                z = torch.mm(z, b)\n            return z\n        with fresh_cache(), torch._inductor.config.patch(max_autotune=True):\n            f(self.a, self.b)", "target": "def config(self):\n        cmd = self.get_cmake_cmd()\n        cmd.append(self.opencv_dir)\n        execute(cmd)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001180", "source": "def add_class(self, name: str,\n                  bases: Sequence[\"weakref.ProxyType[ClassNode]\"] = (),\n                  properties: Sequence[ClassProperty] = ()) -> \"ClassNode\":\n        return self._add_child(ClassNode, name, bases=bases,\n                               properties=properties)", "target": "def add_class(self, name: str,\n                  bases: Sequence[\"weakref.ProxyType[ClassNode]\"] = (),\n                  properties: Sequence[ClassProperty] = ()) -> \"ClassNode\":\n        return self._add_child(ClassNode, name, bases=bases,\n                               properties=properties)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "001181", "source": "def test_validation_error_include_context():\n    v = SchemaValidator(core_schema.list_schema(max_length=2))\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python([1, 2, 3])\n    assert exc_info.value.title == 'list[any]'\n    assert exc_info.value.error_count() == 1\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'too_long',\n            'loc': (),\n            'msg': 'List should have at most 2 items after validation, not 3',\n            'input': [1, 2, 3],\n            'ctx': {'field_type': 'List', 'max_length': 2, 'actual_length': 3},\n        }\n    ]\n    assert exc_info.value.errors(include_url=False, include_context=False) == [\n        {\n            'type': 'too_long',\n            'loc': (),\n            'msg': 'List should have at most 2 items after validation, not 3',\n            'input': [1, 2, 3],\n        }\n    ]", "target": "def test_parse_vector_int_not_convertible(self):\n        np.random.seed(123098765)\n        arr = np.random.randint(-20, 20, 40).astype(np.float32).reshape(10, 2, 2)\n        int_min, int_max = get_limits(ctypes.c_int)\n        test_dict = {1: 2, 3: 10, 10: 20}\n        for not_convertible in ((int_min, 1, 2.5, 3, int_max), [True, 50], 'test', test_dict,\n                                reversed([1, 2, 3]),\n                                np.array([int_min, -10, 24, [1, 2]], dtype=object),\n                                np.array([[1, 2], [3, 4]]), arr[:, 0, 1],):\n            with self.assertRaises(TypeError, msg=get_no_exception_msg(not_convertible)):\n                _ = cv.utils.dumpVectorOfInt(not_convertible)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001182", "source": "def typename(self) -> str:\n        return \"None\"", "target": "def typename(self) -> str:\n        return self._typename", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001183", "source": "def test_function_wrap_field_serializer_to_json():\n    @dataclasses.dataclass\n    class Model:\n        x: int\n        def ser_x(self, v: Any, serializer: core_schema.SerializerFunctionWrapHandler, _) -> str:\n            assert self.x == 1_000\n            x = serializer(v)\n            return f'{x:_}'\n    s = SchemaSerializer(\n        core_schema.model_schema(\n            Model,\n            core_schema.model_fields_schema(\n                {\n                    'x': core_schema.model_field(\n                        core_schema.int_schema(\n                            serialization=core_schema.wrap_serializer_function_ser_schema(\n                                Model.ser_x, is_field_serializer=True, info_arg=True, schema=core_schema.any_schema()\n                            )\n                        )\n                    )\n                }\n            ),\n        )\n    )\n    assert json.loads(s.to_json(Model(x=1000))) == {'x': '1_000'}", "target": "def test_function_wrap_field_serializer_to_json():\n    class Model(TypedDict):\n        x: int\n    def ser_x(\n        data: Model,\n        v: Any,\n        serializer: core_schema.SerializerFunctionWrapHandler,\n        info: core_schema.FieldSerializationInfo,\n    ) -> str:\n        assert data['x'] == 1_000\n        x = serializer(v)\n        return f'{x:_}-{info.field_name}'\n    s = SchemaSerializer(\n        core_schema.typed_dict_schema(\n            {\n                'x': core_schema.typed_dict_field(\n                    core_schema.int_schema(\n                        serialization=core_schema.wrap_serializer_function_ser_schema(\n                            ser_x, is_field_serializer=True, info_arg=True, schema=core_schema.any_schema()\n                        )\n                    )\n                )\n            }\n        )\n    )\n    assert json.loads(s.to_json(Model(x=1000))) == {'x': '1_000-x'}", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001184", "source": "def preprocess(self, img):\n        img = cv2.resize(img, (PYTORCH_RSZ_WIDTH, PYTORCH_RSZ_HEIGHT))\n        img = self.center_crop(img)\n        if self.preprocess_input:\n            return self.presprocess_input(img)\n        return get_pytorch_preprocess(img)", "target": "def preprocess(self, img):\n        img = self.initial_preprocess(img)\n        return self.preprocess_input(img)", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001185", "source": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "target": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001186", "source": "def test_use_after():\n    v = SchemaValidator(\n        core_schema.tuple_positional_schema(\n            [\n                core_schema.definitions_schema(\n                    core_schema.definition_reference_schema('foobar'), [core_schema.int_schema(ref='foobar')]\n                ),\n                core_schema.definition_reference_schema('foobar'),\n            ]\n        )\n    )\n    assert v.validate_python(['1', '2']) == (1, 2)", "target": "def basic_model_serializer_fixture():\n    return SchemaSerializer(\n        core_schema.model_schema(\n            BasicModel,\n            core_schema.model_fields_schema(\n                {\n                    'a': core_schema.model_field(core_schema.int_schema()),\n                    'b': core_schema.model_field(core_schema.int_schema()),\n                    'c': core_schema.model_field(core_schema.int_schema()),\n                    'd': core_schema.model_field(core_schema.int_schema()),\n                    'e': core_schema.model_field(core_schema.int_schema()),\n                    'f': core_schema.model_field(core_schema.int_schema()),\n                    'g': core_schema.model_field(core_schema.int_schema()),\n                    'h': core_schema.model_field(core_schema.int_schema()),\n                }\n            ),\n        )\n    )", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|opencv/opencv", "data_split": "train", "example_id": "001187", "source": "def update_full_export_name(class_node: ClassNode) -> None:\n        nonlocal enum_export_name\n        enum_export_name = class_node.export_name + \"_\" + enum_export_name", "target": "def test_letter_recog(self):\n        eps = 0.01\n        models = [RTrees, KNearest, Boost, SVM, MLP]\n        models = dict( [(cls.__name__.lower(), cls) for cls in models] )\n        testErrors = {RTrees: (98.930000, 92.390000), KNearest: (94.960000, 92.010000),\n         Boost: (85.970000, 74.920000), SVM: (99.780000, 95.680000), MLP: (90.060000, 87.410000)}\n        for model in models:\n            Model = models[model]\n            classifier = Model()\n            samples, responses = load_base(self.repoPath + '/samples/data/letter-recognition.data')\n            train_n = int(len(samples)*classifier.train_ratio)\n            classifier.train(samples[:train_n], responses[:train_n])\n            train_rate = np.mean(classifier.predict(samples[:train_n]) == responses[:train_n].astype(int))\n            test_rate  = np.mean(classifier.predict(samples[train_n:]) == responses[train_n:].astype(int))\n            self.assertLess(train_rate - testErrors[Model][0], eps)\n            self.assertLess(test_rate - testErrors[Model][1], eps)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001188", "source": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "target": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "001189", "source": "def timed(fn):\n    start = torch.cuda.Event(enable_timing=True)\n    end = torch.cuda.Event(enable_timing=True)\n    start.record()\n    result = fn()\n    end.record()\n    torch.cuda.synchronize()\n    return result, start.elapsed_time(end) / 1000", "target": "def rebuild(self) -> ValSer | None:\n        if self._attempt_rebuild:\n            val_ser = self._attempt_rebuild()\n            if val_ser is not None:\n                return val_ser\n            else:\n                raise PydanticUserError(self._error_message, code=self._code)\n        return None", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001190", "source": "def eager(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        (x,) = args\n        return lambda: F.softmax(x, dim=-1)", "target": "def eager(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, w, dy = args\n        y = self.rms_norm_ref(x, w)\n        return lambda: torch.autograd.grad(\n            y, [x, w], grad_outputs=dy, retain_graph=True\n        )", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001191", "source": "def test_internal_error(py_and_json: PyAndJson) -> None:\n    v = py_and_json(\n        cs.arguments_v3_schema(\n            [\n                cs.arguments_v3_parameter(name='a', schema=cs.int_schema(), mode='positional_only'),\n                cs.arguments_v3_parameter(\n                    name='b', schema=cs.no_info_plain_validator_function(double_or_bust), mode='positional_only'\n                ),\n            ]\n        )\n    )\n    assert v.validate_test(ArgsKwargs((1, 2))) == ((1, 4), {})\n    with pytest.raises(RuntimeError, match='bust'):\n        v.validate_test(ArgsKwargs((1, 1)))", "target": "def test_internal_error(py_and_json: PyAndJson):\n    v = py_and_json(\n        {\n            'type': 'arguments',\n            'arguments_schema': [\n                {'name': 'a', 'mode': 'positional_only', 'schema': {'type': 'int'}},\n                {\n                    'name': 'b',\n                    'mode': 'positional_only',\n                    'schema': core_schema.with_info_plain_validator_function(double_or_bust),\n                },\n            ],\n        }\n    )\n    assert v.validate_test((1, 2)) == ((1, 4), {})\n    with pytest.raises(RuntimeError, match='bust'):\n        v.validate_test((1, 1))", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|opencv/opencv", "data_split": "train", "example_id": "001192", "source": "def main() -> None:\n    args = parse_args()\n    try:\n        job_id, job_name = find_job_id_name(args)\n        set_output(\"job-id\", job_id)\n        set_output(\"job-name\", job_name)\n    except Exception as e:\n        print(repr(e), file=sys.stderr)\n        print(f\"workflow-{args.workflow_run_id}\")", "target": "def cvt_bgr_to_yuv_color(self, bgr):\n            y = bgr[2] *  0.299000 + bgr[1] *  0.587000 + bgr[0] *  0.114000;\n            u = bgr[2] * -0.168736 + bgr[1] * -0.331264 + bgr[0] *  0.500000 + 128;\n            v = bgr[2] *  0.500000 + bgr[1] * -0.418688 + bgr[0] * -0.081312 + 128;\n            return (y, u, v)", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "001193", "source": "def compute_time(t_start, delta):\n    mu_second = 0.0 + 10**6\n    return delta.seconds + delta.microseconds / mu_second", "target": "def compute_time(t_start, delta):\n    mu_second = 0.0 + 10**6\n    return delta.seconds + delta.microseconds / mu_second", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001194", "source": "def required_definition_imports(self) -> Generator[str, None, None]:\n        yield from ()", "target": "def required_definition_imports(self) -> Generator[str, None, None]:\n        yield from self.positive_branch_type.required_usage_imports\n        yield from self.negative_branch_type.required_usage_imports\n        yield from self._condition_required_imports", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001195", "source": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "target": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001196", "source": "def ser_x(self, v: Any, _) -> str:\n            assert self.x == 1_000\n            return f'{v:_}'", "target": "def ser_x(data: Model, v: Any, _) -> str:\n        assert data['x'] == 1_000\n        return f'{v:_}'", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001197", "source": "def constrain(self: _Pipeline[_InT, _OutT], constraint: _NotEq) -> _Pipeline[_InT, _OutT]: ...", "target": "def constrain(self: _Pipeline[_InT, _OutT], constraint: _In) -> _Pipeline[_InT, _OutT]: ...", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|scikit-learn/scikit-learn", "data_split": "train", "example_id": "001198", "source": "def test_schema_serializer_not_reused_when_unpickling() -> None:\n    s = SchemaSerializer(\n        core_schema.model_schema(\n            cls=Model,\n            schema=core_schema.model_fields_schema(fields={}, model_name='Model'),\n            config={'title': 'Model'},\n            ref='Model:123',\n        )\n    )\n    Model.__pydantic_serializer__ = s\n    assert 'Prebuilt' not in str(Model.__pydantic_serializer__)\n    reconstructed = pickle.loads(pickle.dumps(Model.__pydantic_serializer__))\n    assert 'Prebuilt' not in str(reconstructed)", "target": "def make_estimator(self, params):\n        fit_algorithm, n_jobs = params\n        estimator = DictionaryLearning(\n            n_components=15,\n            fit_algorithm=fit_algorithm,\n            alpha=0.1,\n            transform_alpha=1,\n            max_iter=20,\n            tol=1e-16,\n            random_state=0,\n            n_jobs=n_jobs,\n        )\n        return estimator", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001199", "source": "def constrain(self: _Pipeline[_InT, _NewOutLt], constraint: annotated_types.Lt) -> _Pipeline[_InT, _NewOutLt]: ...", "target": "def constrain(self: _Pipeline[_InT, _OutT], constraint: _NotEq) -> _Pipeline[_InT, _OutT]: ...", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001200", "source": "def get_output(self, input_blob):\n        tensor = torch.FloatTensor(input_blob)\n        out = self.net.forward(tensor).numpy()\n        return out", "target": "def get_output(self, input_blob):\n        self.net.setBlob(\"\", input_blob)\n        self.net.forward()\n        return self.net.getBlob(self.net.getLayerNames()[-1])", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001201", "source": "def test_by_alias_and_name_config_interaction(config, runtime, expected) -> None:\n    @dataclasses.dataclass\n    class Foo:\n        my_field: str\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(\n                    name='my_field', schema=core_schema.str_schema(), serialization_alias='my_alias'\n                ),\n            ],\n        ),\n        ['my_field'],\n        config=core_schema.CoreConfig(serialize_by_alias=config or False),\n    )\n    s = SchemaSerializer(schema)\n    assert s.to_python(Foo(my_field='hello'), by_alias=runtime) == expected", "target": "def test_by_alias_and_name_config_interaction(config, runtime, expected) -> None:\n    class Model(TypedDict):\n        my_field: int\n    schema = core_schema.typed_dict_schema(\n        {\n            'my_field': core_schema.typed_dict_field(core_schema.int_schema(), serialization_alias='my_alias'),\n        },\n    )\n    s = SchemaSerializer(schema, config=core_schema.CoreConfig(serialize_by_alias=config or False))\n    assert s.to_python(Model(my_field=1), by_alias=runtime) == expected", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001202", "source": "def test_field_serializer_cached_property():\n    @dataclasses.dataclass\n    class Model:\n        x: int\n        y: int\n        @cached_property\n        def x_formatted(self) -> str:\n            return f'{self.x:_}'\n        @cached_property\n        def y_formatted(self) -> str:\n            return f'{self.y:_}'\n        def ser_x(self, v: Any, _) -> str:\n            assert self.x == 1_000 == v\n            return self.x_formatted\n        def ser_y(self, v: Any, _) -> str:\n            assert self.y == 2_000 == v\n            return self.y_formatted\n    s = SchemaSerializer(\n        core_schema.model_schema(\n            Model,\n            core_schema.model_fields_schema(\n                {\n                    'x': core_schema.model_field(\n                        core_schema.int_schema(\n                            serialization=core_schema.plain_serializer_function_ser_schema(\n                                Model.ser_x, is_field_serializer=True, info_arg=True\n                            )\n                        )\n                    ),\n                    'y': core_schema.model_field(\n                        core_schema.int_schema(\n                            serialization=core_schema.plain_serializer_function_ser_schema(\n                                Model.ser_y, is_field_serializer=True, info_arg=True\n                            )\n                        )\n                    ),\n                },\n                computed_fields=[core_schema.computed_field('y_formatted', core_schema.str_schema())],\n            ),\n        )\n    )\n    assert s.to_python(Model(x=1000, y=2000)) == {'x': '1_000', 'y': '2_000', 'y_formatted': '2_000'}\n    assert s.to_json(Model(x=1000, y=2000)) == b'{\"x\":\"1_000\",\"y\":\"2_000\",\"y_formatted\":\"2_000\"}'", "target": "def resolve_ref_schema(self, maybe_ref_json_schema: JsonSchemaValue) -> JsonSchemaValue:\n        if '$ref' not in maybe_ref_json_schema:\n            return maybe_ref_json_schema\n        ref = maybe_ref_json_schema['$ref']\n        json_schema = self.generate_json_schema.get_schema_from_definitions(ref)\n        if json_schema is None:\n            raise LookupError(\n                f'Could not find a ref for {ref}.'\n                ' Maybe you tried to call resolve_ref_schema from within a recursive model?'\n            )\n        return json_schema", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001203", "source": "def test_function_before():\n    s = SchemaSerializer(\n        core_schema.with_info_before_validator_function(lambda v, info: v + 1, core_schema.int_schema())\n    )\n    assert plain_repr(s) == 'SchemaSerializer(serializer=Int(IntSerializer),definitions=[])'", "target": "def test_function_before():\n    def f(input_value, _info):\n        return input_value + ' Changed'\n    v = SchemaValidator(core_schema.with_info_before_validator_function(f, core_schema.str_schema()))\n    assert v.validate_python('input value') == 'input value Changed'", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "001204", "source": "def test_repr():\n    v = SchemaValidator(cs.union_schema(choices=[cs.int_schema(), cs.is_instance_schema(cls=Foo)]))\n    assert v.isinstance_python(4) is True\n    assert v.isinstance_python(Bar()) is True\n    assert v.isinstance_python('foo') is False\n    with pytest.raises(ValidationError, match=r'is-instance\\[Foo\\]\\s+Input should be an instance of Foo'):\n        v.validate_python('foo')", "target": "def _get_s3_key_prefix(\n    pr_identifier: PRIdentifier,\n    repo: GithubRepo,\n    job_identifier: str,\n    sha: str = \"\",\n    test_config: str = \"\",\n    shard: str = \"\",\n) -> str:\n    prefix = f\"{PYTEST_CACHE_KEY_PREFIX}/{repo.owner}/{repo.name}/{pr_identifier}/{sanitize_for_s3(job_identifier)}\"\n    if sha:\n        prefix += f\"/{sha}\"\n    if test_config:\n        prefix += f\"/{sanitize_for_s3(test_config)}\"\n    if shard:\n        prefix += f\"/{shard}\"\n    return prefix", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|opencv/opencv", "data_split": "train", "example_id": "001205", "source": "def find_class_node(root: NamespaceNode, class_symbol: SymbolName,\n                    create_missing_namespaces: bool = False) -> ClassNode:\n    scope = find_scope(root, class_symbol, create_missing_namespaces)\n    if class_symbol.name not in scope.classes:\n        raise SymbolNotFoundError(\n            \"Can't find {} in its scope\".format(class_symbol)\n        )\n    return scope.classes[class_symbol.name]", "target": "def prepare_sources(self):\n        if self.config.dldt_src_dir:\n            log.info('Using DLDT custom repository: %s', self.srcdir)\n            return\n        def do_clone(srcdir, noFetch):\n            git_checkout(srcdir, self.config.dldt_src_url, self.config.dldt_src_branch, self.config.dldt_src_commit,\n                    ['-n', '--depth=100', '--no-single-branch', '--recurse-submodules'] +\n                    (self.config.dldt_src_git_clone_extra or []),\n                    noFetch=noFetch\n            )\n        if not os.path.exists(str(self.srcdir / '.git')):\n            log.info('DLDT git checkout through \"reference\" copy.')\n            reference_dir = self.config.dldt_reference_dir\n            if reference_dir is None:\n                reference_dir = prepare_dir(os.path.join(self.config.build_cache_dir, 'dldt-git-reference-repository'))\n                do_clone(reference_dir, False)\n                log.info('DLDT reference git checkout completed. Copying...')\n            else:\n                log.info('Using DLDT reference repository. Copying...')\n            copytree(reference_dir, self.srcdir)\n            do_clone(self.srcdir, True)\n        else:\n            do_clone(self.srcdir, False)\n        log.info('DLDT git checkout completed. Patching...')\n        def applyPatch(patch_file, subdir = None):\n            if subdir:\n                log.info('Patching \"%s\": %s' % (subdir, patch_file))\n            else:\n                log.info('Patching: %s' % (patch_file))\n            git_apply_patch(self.srcdir / subdir if subdir else self.srcdir, self.cpath / patch_file)\n        exec(compile(self.patch_file_contents, self.patch_file, 'exec'))\n        log.info('DLDT patches applied')", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|opencv/opencv", "data_split": "train", "example_id": "001206", "source": "def hardswish(a):\n    return a * (a + 3).clamp(0.0, 6.0) / 6", "target": "def outMeta(desc):\n                raise NotImplementedError(\"outMeta isn't implemented\")", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001207", "source": "def typename(self) -> str:\n        return \"\"", "target": "def typename(self) -> str:\n        return \"_typing.Any\"", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001208", "source": "def predict(self, samples):\n        _retval, results, _neigh_resp, _dists = self.model.findNearest(samples, self.k)\n        return results.ravel()", "target": "def predict(self, samples):\n        _ret, resp = self.model.predict(samples)\n        return resp.argmax(-1)", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|opencv/opencv", "data_split": "train", "example_id": "001209", "source": "def wrapped_func(x, y):\n            dst_dtype = x.dtype\n            if apply_saturation:\n                if np.issubdtype(x.dtype, np.integer):\n                    x = x.astype(np.int64)\n            if not isinstance(y, (float, int)):\n                if len(y) > x.shape[-1]:\n                    y = y[:x.shape[-1]]\n                else:\n                    y = rpad(y, x.shape[-1], pad_value=0)\n            dst = func(x, y)\n            if apply_saturation:\n                min_val, max_val = get_limits(dst_dtype)\n                dst = np.clip(dst, min_val, max_val)\n            return dst.astype(dst_dtype)", "target": "def resolve_type_nodes(self, root: ASTNode):\n        def has_unresolved_type_node(item) -> bool:\n            return item.type_node is not None and not item.type_node.is_resolved\n        errors = []\n        for overload in self.overloads:\n            for arg in filter(has_unresolved_type_node, overload.arguments):\n                try:\n                    arg.type_node.resolve(root)\n                except TypeResolutionError as e:\n                    errors.append(\n                        'Failed to resolve \"{}\" argument: {}'.format(arg.name, e)\n                    )\n            if overload.return_type is not None and \\\n                    has_unresolved_type_node(overload.return_type):\n                try:\n                    overload.return_type.type_node.resolve(root)\n                except TypeResolutionError as e:\n                    errors.append('Failed to resolve return type: {}'.format(e))\n        if len(errors) > 0:\n            raise TypeResolutionError(\n                'Failed to resolve \"{}\" function against \"{}\". Errors: {}'.format(\n                    self.full_export_name, root.full_export_name,\n                    \", \".join(\"[{}]: {}\".format(i, e) for i, e in enumerate(errors))\n                )\n            )", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "001210", "source": "def compute_bench(samples_range, features_range, n_iter=3, rank=50):\n    it = 0\n    results = defaultdict(lambda: [])\n    max_it = len(samples_range) * len(features_range)\n    for n_samples in samples_range:\n        for n_features in features_range:\n            it += 1\n            print(\"====================\")\n            print(\"Iteration %03d of %03d\" % (it, max_it))\n            print(\"====================\")\n            X = make_low_rank_matrix(\n                n_samples, n_features, effective_rank=rank, tail_strength=0.2\n            )\n            gc.collect()\n            print(\"benchmarking scipy svd: \")\n            tstart = time()\n            svd(X, full_matrices=False)\n            results[\"scipy svd\"].append(time() - tstart)\n            gc.collect()\n            print(\"benchmarking scikit-learn randomized_svd: n_iter=0\")\n            tstart = time()\n            randomized_svd(X, rank, n_iter=0)\n            results[\"scikit-learn randomized_svd (n_iter=0)\"].append(time() - tstart)\n            gc.collect()\n            print(\"benchmarking scikit-learn randomized_svd: n_iter=%d \" % n_iter)\n            tstart = time()\n            randomized_svd(X, rank, n_iter=n_iter)\n            results[\"scikit-learn randomized_svd (n_iter=%d)\" % n_iter].append(\n                time() - tstart\n            )\n    return results", "target": "def compute_bench(samples_range, features_range):\n    it = 0\n    results = defaultdict(lambda: [])\n    chunk = 100\n    max_it = len(samples_range) * len(features_range)\n    for n_samples in samples_range:\n        for n_features in features_range:\n            it += 1\n            print(\"==============================\")\n            print(\"Iteration %03d of %03d\" % (it, max_it))\n            print(\"==============================\")\n            print()\n            data = nr.randint(-50, 51, (n_samples, n_features))\n            print(\"K-Means\")\n            tstart = time()\n            kmeans = KMeans(init=\"k-means++\", n_clusters=10).fit(data)\n            delta = time() - tstart\n            print(\"Speed: %0.3fs\" % delta)\n            print(\"Inertia: %0.5f\" % kmeans.inertia_)\n            print()\n            results[\"kmeans_speed\"].append(delta)\n            results[\"kmeans_quality\"].append(kmeans.inertia_)\n            print(\"Fast K-Means\")\n            mbkmeans = MiniBatchKMeans(\n                init=\"k-means++\", n_clusters=10, batch_size=chunk\n            )\n            tstart = time()\n            mbkmeans.fit(data)\n            delta = time() - tstart\n            print(\"Speed: %0.3fs\" % delta)\n            print(\"Inertia: %f\" % mbkmeans.inertia_)\n            print()\n            print()\n            results[\"MiniBatchKMeans Speed\"].append(delta)\n            results[\"MiniBatchKMeans Quality\"].append(mbkmeans.inertia_)\n    return results", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001211", "source": "def validate_b(cls, v: bytes, info: core_schema.ValidationInfo) -> bytes:\n            assert v == b'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return b'hello world!'", "target": "def validate_b(cls, v: str, info: core_schema.ValidationInfo) -> str:\n            assert v == 'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return 'hello world!'", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001212", "source": "def is_resolved(self) -> bool:\n        return self.value.is_resolved", "target": "def is_resolved(self) -> bool:\n        return self._ast_node is not None or self._module_name is not None", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "train", "example_id": "001213", "source": "def test_env_path_returns_path_when_present(self):\n        tmp = Path(\"./b\").resolve()\n        with patch.dict(os.environ, {\"P\": str(tmp)}, clear=True):\n            p = m.env_path(\"P\", None, resolve=True)\n            self.assertEqual(p, tmp)", "target": "def test_benchmark_infra_runs(self) -> None:\n        original_dir = setup_torchbench_cwd()\n        try:\n            args = parse_args(\n                [\n                    \"-dcpu\",\n                    \"--inductor\",\n                    \"--training\",\n                    \"--performance\",\n                    \"--only=BERT_pytorch\",\n                    \"-n1\",\n                    \"--batch-size=1\",\n                ]\n            )\n            run(TorchBenchmarkRunner(), args, original_dir)\n        finally:\n            os.chdir(original_dir)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "001214", "source": "def f(inp, *weights):\n            x = inp\n            for w in weights:\n                x = torch.matmul(w, x).sin().sin()\n            return x", "target": "def wrap_function(input_value, validator, info):\n            try:\n                return validator(input_value)\n            except ValidationError:\n                if isinstance(input_value, list):\n                    return str(len(input_value))\n                else:\n                    return repr(input_value)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001215", "source": "def test_by_alias_and_name_config_interaction(config, runtime, expected) -> None:\n    class Model(TypedDict):\n        my_field: int\n    schema = core_schema.typed_dict_schema(\n        {\n            'my_field': core_schema.typed_dict_field(core_schema.int_schema(), serialization_alias='my_alias'),\n        },\n    )\n    s = SchemaSerializer(schema, config=core_schema.CoreConfig(serialize_by_alias=config or False))\n    assert s.to_python(Model(my_field=1), by_alias=runtime) == expected", "target": "def test_by_alias_and_name_config_interaction(\n    config_by_alias: Union[bool, None],\n    config_by_name: Union[bool, None],\n    runtime_by_alias: Union[bool, None],\n    runtime_by_name: Union[bool, None],\n) -> None:\n    if config_by_alias is False and config_by_name is False and runtime_by_alias is False and runtime_by_name is False:\n        pytest.skip(\"Can't have both by_alias and by_name as effectively False\")\n    core_config = {\n        **({'validate_by_alias': config_by_alias} if config_by_alias is not None else {}),\n        **({'validate_by_name': config_by_name} if config_by_name is not None else {}),\n    }\n    schema = core_schema.typed_dict_schema(\n        fields={\n            'my_field': core_schema.typed_dict_field(schema=core_schema.int_schema(), validation_alias='my_alias'),\n        },\n        config=core_schema.CoreConfig(**core_config),\n    )\n    s = SchemaValidator(schema)\n    alias_allowed = next(x for x in (runtime_by_alias, config_by_alias, True) if x is not None)\n    name_allowed = next(x for x in (runtime_by_name, config_by_name, False) if x is not None)\n    if alias_allowed:\n        assert s.validate_python({'my_alias': 1}, by_alias=runtime_by_alias, by_name=runtime_by_name) == {'my_field': 1}\n    if name_allowed:\n        assert s.validate_python({'my_field': 1}, by_alias=runtime_by_alias, by_name=runtime_by_name) == {'my_field': 1}", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001216", "source": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "target": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "001217", "source": "def test_parse_vector_int_not_convertible(self):\n        np.random.seed(123098765)\n        arr = np.random.randint(-20, 20, 40).astype(np.float32).reshape(10, 2, 2)\n        int_min, int_max = get_limits(ctypes.c_int)\n        test_dict = {1: 2, 3: 10, 10: 20}\n        for not_convertible in ((int_min, 1, 2.5, 3, int_max), [True, 50], 'test', test_dict,\n                                reversed([1, 2, 3]),\n                                np.array([int_min, -10, 24, [1, 2]], dtype=object),\n                                np.array([[1, 2], [3, 4]]), arr[:, 0, 1],):\n            with self.assertRaises(TypeError, msg=get_no_exception_msg(not_convertible)):\n                _ = cv.utils.dumpVectorOfInt(not_convertible)", "target": "def test_decimal_not_json(input_value, expected):\n    v = SchemaValidator(cs.decimal_schema())\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            v.validate_python(input_value)\n    else:\n        output = v.validate_python(input_value)\n        assert output == expected\n        assert isinstance(output, Decimal)", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|opencv/opencv", "data_split": "train", "example_id": "001218", "source": "def get_file_size(version):\n    api_url = ROOT_URL + \"%s/_downloads\" % version\n    for path_details in json_urlread(api_url):\n        file_extension = get_file_extension(version)\n        file_path = f\"scikit-learn-docs.{file_extension}\"\n        if path_details[\"name\"] == file_path:\n            return human_readable_data_quantity(path_details[\"size\"], 1000)", "target": "def initial_preprocess(self, img):\n        min_dim = min(img.shape[-3], img.shape[-2])\n        resize_ratio = self.frame_size / float(min_dim)\n        img = cv2.resize(img, (0, 0), fx=resize_ratio, fy=resize_ratio)\n        img = self.center_crop(img)\n        return img", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|opencv/opencv", "data_split": "train", "example_id": "001219", "source": "def test_digits(self):\n        digits, labels = self.load_digits(DIGITS_FN)\n        rand = np.random.RandomState(321)\n        shuffle = rand.permutation(len(digits))\n        digits, labels = digits[shuffle], labels[shuffle]\n        digits2 = list(map(deskew, digits))\n        samples = preprocess_hog(digits2)\n        train_n = int(0.9*len(samples))\n        _digits_train, digits_test = np.split(digits2, [train_n])\n        samples_train, samples_test = np.split(samples, [train_n])\n        labels_train, labels_test = np.split(labels, [train_n])\n        errors = list()\n        confusionMatrixes = list()\n        model = KNearest(k=4)\n        model.train(samples_train, labels_train)\n        error, confusion = evaluate_model(model, digits_test, samples_test, labels_test)\n        errors.append(error)\n        confusionMatrixes.append(confusion)\n        model = SVM(C=2.67, gamma=5.383)\n        model.train(samples_train, labels_train)\n        error, confusion = evaluate_model(model, digits_test, samples_test, labels_test)\n        errors.append(error)\n        confusionMatrixes.append(confusion)\n        eps = 0.001\n        normEps = len(samples_test) * 0.02\n        confusionKNN = [[45,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n         [ 0, 57,  0,  0,  0,  0,  0,  0,  0,  0],\n         [ 0,  0, 59,  1,  0,  0,  0,  0,  1,  0],\n         [ 0,  0,  0, 43,  0,  0,  0,  1,  0,  0],\n         [ 0,  0,  0,  0, 38,  0,  2,  0,  0,  0],\n         [ 0,  0,  0,  2,  0, 48,  0,  0,  1,  0],\n         [ 0,  1,  0,  0,  0,  0, 51,  0,  0,  0],\n         [ 0,  0,  1,  0,  0,  0,  0, 54,  0,  0],\n         [ 0,  0,  0,  0,  0,  1,  0,  0, 46,  0],\n         [ 1,  1,  0,  1,  1,  0,  0,  0,  2, 42]]\n        confusionSVM = [[45,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n          [ 0, 57,  0,  0,  0,  0,  0,  0,  0,  0],\n          [ 0,  0, 59,  2,  0,  0,  0,  0,  0,  0],\n          [ 0,  0,  0, 43,  0,  0,  0,  1,  0,  0],\n          [ 0,  0,  0,  0, 40,  0,  0,  0,  0,  0],\n          [ 0,  0,  0,  1,  0, 50,  0,  0,  0,  0],\n          [ 0,  0,  0,  0,  1,  0,  51, 0,  0,  0],\n          [ 0,  0,  1,  0,  0,  0,  0,  54, 0,  0],\n          [ 0,  0,  0,  0,  0,  0,  0,  0, 47,  0],\n          [ 0,  1,  0,  1,  0,  0,  0,  0,  1, 45]]\n        self.assertLess(cv.norm(confusionMatrixes[0] - confusionKNN, cv.NORM_L1), normEps)\n        self.assertLess(cv.norm(confusionMatrixes[1] - confusionSVM, cv.NORM_L1), normEps)\n        self.assertLess(errors[0] - 0.034, eps)\n        self.assertLess(errors[1] - 0.018, eps)", "target": "def load_html_file(file_dir):\n    with open(file_dir, 'rb') as fp:\n        data = fp.read()\n    if os.name == 'nt' or sys.version_info[0] == 3:\n        data = data.decode(encoding='utf-8', errors='strict')\n    data = re.sub(r'(\\>)([ ]+)', lambda match: match.group(1) + ('!space!' * len(match.group(2))), data)\n    data = re.sub(r'([ ]+)(\\<)', lambda match: ('!space!' * len(match.group(1))) + match.group(2), data)\n    if os.name == 'nt' or sys.version_info[0] == 3:\n        data = data.encode('utf-8', 'ignore')\n    soup = BeautifulSoup(data, 'html.parser')\n    return soup", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001220", "source": "def resolve_type_nodes(self, root: Optional[ASTNode] = None) -> None:\n        errors = []\n        for child in itertools.chain(self.functions.values(),\n                                     self.classes.values(),\n                                     self.namespaces.values()):\n            try:\n                try:\n                    child.resolve_type_nodes(self)\n                except TypeResolutionError:\n                    if root is not None:\n                        child.resolve_type_nodes(root)\n                    else:\n                        raise\n            except TypeResolutionError as e:\n                errors.append(str(e))\n        if len(errors) > 0:\n            raise TypeResolutionError(\n                'Failed to resolve \"{}\" namespace against \"{}\". '\n                'Errors: {}'.format(\n                    self.full_export_name,\n                    root if root is None else root.full_export_name,\n                    errors\n                )\n            )", "target": "def resolve_type_nodes(self, root: ASTNode) -> None:\n        errors = []\n        for child in itertools.chain(self.properties,\n                                     self.functions.values(),\n                                     self.classes.values()):\n            try:\n                try:\n                    child.resolve_type_nodes(self)\n                except TypeResolutionError:\n                    child.resolve_type_nodes(root)\n            except TypeResolutionError as e:\n                errors.append(str(e))\n        if len(errors) > 0:\n            raise TypeResolutionError(\n                'Failed to resolve \"{}\" class against \"{}\". Errors: {}'.format(\n                    self.full_export_name, root.full_export_name, errors\n                )\n            )", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "001221", "source": "def test_function_plain_field_serializer_to_json():\n    class Model(RootModel):\n        def ser_root(self, v: Any, _) -> str:\n            assert self.root == 1_000\n            return f'{v:_}'\n    s = SchemaSerializer(\n        core_schema.model_schema(\n            Model,\n            core_schema.int_schema(\n                serialization=core_schema.plain_serializer_function_ser_schema(\n                    Model.ser_root, is_field_serializer=True, info_arg=True\n                )\n            ),\n            root_model=True,\n        )\n    )\n    assert json.loads(s.to_json(Model(1000))) == '1_000'", "target": "def test_detect_and_decode_multi(self):\n        img = cv.imread(os.path.join(self.extraTestDataPath, 'cv/qrcode/multiple/6_qrcodes.png'))\n        self.assertFalse(img is None)\n        detector = cv.QRCodeDetector()\n        retval, decoded_data, points, straight_qrcode = detector.detectAndDecodeMulti(img)\n        self.assertTrue(retval)\n        self.assertEqual(len(decoded_data), 6)\n        self.assertTrue(\"TWO STEPS FORWARD\" in decoded_data)\n        self.assertTrue(\"EXTRA\" in decoded_data)\n        self.assertTrue(\"SKIP\" in decoded_data)\n        self.assertTrue(\"STEP FORWARD\" in decoded_data)\n        self.assertTrue(\"STEP BACK\" in decoded_data)\n        self.assertTrue(\"QUESTION\" in decoded_data)\n        self.assertEqual(points.shape, (6, 4, 2))", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001222", "source": "def children_types(self) -> Tuple[ASTNodeType, ...]:\n        return (ASTNodeType.Namespace, ASTNodeType.Class, ASTNodeType.Function,\n                ASTNodeType.Enumeration, ASTNodeType.Constant)", "target": "def children_types(self) -> Tuple[ASTNodeType, ...]:\n        return ()", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001223", "source": "def test_constraints_schema_validation_error(constraint: str) -> None:\n    with pytest.raises(SchemaError, match=f\"'{constraint}' must be coercible to a Decimal instance\"):\n        SchemaValidator(cs.decimal_schema(**{constraint: 'bad_value'}))", "target": "def test_constraints_schema_validation_error(constraint: str) -> None:\n    with pytest.raises(SchemaError, match=f\"'{constraint}' must be coercible to a datetime instance\"):\n        SchemaValidator(cs.datetime_schema(**{constraint: 'bad_value'}))", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001224", "source": "def required_usage_imports(self) -> Generator[str, None, None]:\n        for item in self:\n            yield from item.required_usage_imports", "target": "def required_usage_imports(self) -> Generator[str, None, None]:\n        if TypeNode.compatible_to_runtime_usage:\n            yield \"import typing as _typing\"\n        yield from super().required_usage_imports", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001225", "source": "def test_union_nullable_bool_int():\n    v = SchemaValidator(\n        core_schema.union_schema(\n            choices=[\n                core_schema.nullable_schema(schema=core_schema.bool_schema()),\n                core_schema.nullable_schema(schema=core_schema.int_schema()),\n            ]\n        )\n    )\n    assert v.validate_python(None) is None\n    assert v.validate_python(True) is True\n    assert v.validate_python(1) == 1", "target": "def test_int_not_coerced_to_enum():\n    class BinaryEnum(IntEnum):\n        ZERO = 0\n        ONE = 1\n    enum_schema = core_schema.lax_or_strict_schema(\n        core_schema.no_info_after_validator_function(BinaryEnum, core_schema.int_schema()),\n        core_schema.is_instance_schema(BinaryEnum),\n    )\n    schema = core_schema.union_schema([enum_schema, core_schema.int_schema()])\n    validator = SchemaValidator(schema)\n    assert validator.validate_python(0) is not BinaryEnum.ZERO\n    assert validator.validate_python(1) is not BinaryEnum.ONE\n    assert validator.validate_python(BinaryEnum.ZERO) is BinaryEnum.ZERO\n    assert validator.validate_python(BinaryEnum.ONE) is BinaryEnum.ONE", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001226", "source": "def _work(self):\n        with (\n            fresh_cache(),\n            torch._inductor.config.patch(force_shape_pad=self._force_shape_pad),\n        ):\n            opt_m = torch.compile(backend=self.backend(), dynamic=self.is_dynamic())(\n                self.m.cuda() if self._is_gpu else self.m\n            )\n            opt_m(self.input)", "target": "def _work(self):\n        with (\n            torch._dynamo.config.patch(\"enable_cpp_symbolic_shape_guards\", False),\n            torch._export.config.patch(use_new_tracer_experimental=True),\n            CompileTimeInstructionCounter.record(),\n        ):\n            torch.export.export(self.m, (self.input,), strict=True)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "001227", "source": "def test_dict_py():\n    v = SchemaValidator(\n        core_schema.dict_schema(keys_schema=core_schema.timedelta_schema(), values_schema=core_schema.int_schema())\n    )\n    assert v.validate_python({timedelta(days=2, hours=1): 2, timedelta(days=2, hours=2): 4}) == {\n        timedelta(days=2, hours=1): 2,\n        timedelta(days=2, hours=2): 4,\n    }", "target": "def bench(name, fn):\n    torch._dynamo.reset()\n    inps = [[torch.randn(i) for _ in range(100)] for i in range(10, 101, 10)]\n    def run_fn():\n        for inp in inps:\n            fn(*inp)\n    start = time.perf_counter()\n    for _ in range(3):\n        run_fn()\n    end = time.perf_counter()\n    results = timeit.repeat(lambda: run_fn(), number=1000, repeat=10)\n    print(f\"{name} {np.median(results) * 1000:.1f}us (warmup={end - start:.1f}s)\")", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001228", "source": "def f(value, serializer, _info):\n        return f'result={serializer(len(value))} repr={serializer!r}'", "target": "def test_schema_validator() -> None:\n    SchemaValidator({'type': 'int'})", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "train", "example_id": "001229", "source": "def main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--field\", \"-f\", default=\"speedup\", type=str)\n    parser.add_argument(\"--output\", \"-o\", type=str)\n    parser.add_argument(\"inputs\", nargs=\"*\")\n    args = parser.parse_args()\n    prefix = longest_common_prefix([Path(inp).stem for inp in args.inputs])\n    frames = []\n    fields = []\n    for inp in args.inputs:\n        field = Path(inp).stem[len(prefix) :]\n        fields.append(field)\n        frames.append(\n            pd.read_csv(inp)\n            .filter([\"name\", args.field])\n            .rename(columns={args.field: field})\n        )\n    df = frames[0]\n    for other in frames[1:]:\n        df = df.merge(other, how=\"outer\", on=\"name\")\n    df = df.fillna(0)\n    df = df[functools.reduce(operator.or_, [df[f] != 0 for f in fields])]\n    prefix = prefix.strip(\"_\") or \"output\"\n    output = args.output or f\"{prefix}.csv\"\n    print(f\"Writing {output}\")\n    df.to_csv(output, index=False)", "target": "def bench(name):\n            nnc_op = torch.jit.trace(operator, args)\n            result = benchmark(nnc_op, args)\n            print(\n                \",\".join(\n                    [\n                        name,\n                        args[0].device.type,\n                        operator.__name__,\n                        shape.__name__,\n                        micros(result),\n                    ]\n                )\n            )\n            sys.stdout.flush()", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001230", "source": "def predict(self, samples):\n        _ret, resp = self.model.predict(samples)\n        return resp.ravel()", "target": "def predict(self, samples):\n        _ret, resp = self.model.predict(samples)\n        return resp.ravel()", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001231", "source": "def test_dataclass_args(py_and_json: PyAndJson, input_value, expected):\n    schema = core_schema.dataclass_args_schema(\n        'MyDataclass',\n        [\n            core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), kw_only=False),\n            core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), kw_only=False),\n        ],\n    )\n    v = py_and_json(schema)\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)) as exc_info:\n            v.validate_test(input_value)\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        assert v.validate_test(input_value) == expected", "target": "def test_validate_assignment_allow_extra():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={'field_a': core_schema.model_field(schema=core_schema.str_schema())}, extra_behavior='allow'\n        )\n    )\n    assert v.validate_python({'field_a': 'test'}) == ({'field_a': 'test'}, {}, {'field_a'})\n    assert v.validate_assignment({'field_a': 'test'}, 'other_field', 456) == (\n        {'field_a': 'test'},\n        {'other_field': 456},\n        {'other_field'},\n    )", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001232", "source": "def test_date_strict(input_value, expected, strict_mode_type):\n    v = SchemaValidator(cs.date_schema(strict=strict_mode_type.schema))\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            v.validate_python(input_value, **strict_mode_type.validator_args)\n    else:\n        output = v.validate_python(input_value, **strict_mode_type.validator_args)\n        assert output == expected", "target": "def test_null():\n    assert SchemaValidator(core_schema.none_schema()).validate_json('null') is None", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001233", "source": "def constrain(\n        self: _Pipeline[_InT, _NewOutDatetime], constraint: annotated_types.Timezone\n    ) -> _Pipeline[_InT, _NewOutDatetime]: ...", "target": "def constrain(self: _Pipeline[_InT, _OutT], constraint: _NotEq) -> _Pipeline[_InT, _OutT]: ...", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "001234", "source": "def test_model_field_validator_reuse() -> None:\n    class Model:\n        x: str\n        y: str\n    def f(input_value: Any, info: core_schema.ValidationInfo) -> Any:\n        return f'{info.field_name}: {input_value}'\n    with pytest.warns(\n        DeprecationWarning, match='`field_name` argument on `with_info_plain_validator_function` is deprecated'\n    ):\n        validator = core_schema.with_info_plain_validator_function(f, field_name='x')\n    v = SchemaValidator(\n        core_schema.model_schema(\n            Model,\n            core_schema.model_fields_schema(\n                {\n                    'x': core_schema.model_field(validator),\n                    'y': core_schema.model_field(validator),\n                }\n            ),\n        )\n    )\n    m = v.validate_python({'x': 'foo', 'y': 'bar'})\n    assert m.x == 'x: foo'\n    assert m.y == 'y: bar'", "target": "def main():\n    duration = int(sys.argv[1])\n    scribe.open_source_signpost(\n        subsystem=\"pr_time_benchmarks\",\n        name=\"duration\",\n        parameters=json.dumps(duration),\n    )", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|opencv/opencv", "data_split": "train", "example_id": "001235", "source": "def plot_digits(X, title):\n    fig, axs = plt.subplots(nrows=10, ncols=10, figsize=(8, 8))\n    for img, ax in zip(X, axs.ravel()):\n        ax.imshow(img.reshape((16, 16)), cmap=\"Greys\")\n        ax.axis(\"off\")\n    fig.suptitle(title, fontsize=24)", "target": "def _find_argument_index(arguments: Sequence[FunctionNode.Arg],\n                         name: str) -> Optional[int]:\n    for i, arg in enumerate(arguments):\n        if arg.name == name:\n            return i\n    return None", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|pytorch/pytorch", "data_split": "train", "example_id": "001236", "source": "def bench(args):\n    results_dir = Path(args.bench_results)\n    branch = args.branch\n    random_state = 1\n    results = defaultdict(list)\n    n_samples_train = 1000\n    for n_samples_test in [\n        1000,\n        10000,\n        50000,\n    ]:\n        for n_features in [10, 100, 1000]:\n            for contamination in [0.01, 0.1, 0.5]:\n                for n_jobs in [1, 2, 3, 4]:\n                    X_train, X_test = get_data(\n                        n_samples_train,\n                        n_samples_test,\n                        n_features,\n                        contamination,\n                        random_state,\n                    )\n                    print(\"--- Fitting the IsolationForest estimator...\")\n                    model = IsolationForest(n_jobs=-1, random_state=random_state)\n                    tstart = time()\n                    model.fit(X_train)\n                    fit_time = time() - tstart\n                    for _ in range(1000):\n                        1 + 1\n                    with parallel_config(\"threading\", n_jobs=n_jobs):\n                        tstart = time()\n                        model.decision_function(X_test)\n                        predict_time = time() - tstart\n                    results[\"predict_time\"].append(predict_time)\n                    results[\"fit_time\"].append(fit_time)\n                    results[\"n_samples_train\"].append(n_samples_train)\n                    results[\"n_samples_test\"].append(n_samples_test)\n                    results[\"n_features\"].append(n_features)\n                    results[\"contamination\"].append(contamination)\n                    results[\"n_jobs\"].append(n_jobs)\n    df = pd.DataFrame(results)\n    df.to_csv(results_dir / f\"{branch}.csv\", index=False)", "target": "def get_nccl_wheel_version(arch_version: str) -> str:\n    requirements = map(\n        str.strip, re.split(\"[;|]\", PYTORCH_EXTRA_INSTALL_REQUIREMENTS[arch_version])\n    )\n    return next(x for x in requirements if x.startswith(\"nvidia-nccl\")).split(\"==\")[1]", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001237", "source": "def test_alias():\n    s = SchemaSerializer(\n        core_schema.model_schema(\n            BasicModel,\n            core_schema.model_fields_schema(\n                {\n                    'cat': core_schema.model_field(core_schema.int_schema(), serialization_alias='Meow'),\n                    'dog': core_schema.model_field(core_schema.int_schema(), serialization_alias='Woof'),\n                    'bird': core_schema.model_field(core_schema.int_schema()),\n                }\n            ),\n        )\n    )\n    value = BasicModel(cat=0, dog=1, bird=2)\n    assert s.to_python(value, by_alias=True) == IsStrictDict(Meow=0, Woof=1, bird=2)", "target": "def test_alias():\n    s = SchemaSerializer(\n        core_schema.typed_dict_schema(\n            {\n                'cat': core_schema.typed_dict_field(core_schema.int_schema(), serialization_alias='Meow'),\n                'dog': core_schema.typed_dict_field(core_schema.int_schema(), serialization_alias='Woof'),\n                'bird': core_schema.typed_dict_field(core_schema.int_schema()),\n            }\n        )\n    )\n    value = {'cat': 0, 'dog': 1, 'bird': 2}\n    assert s.to_python(value, by_alias=True) == IsStrictDict(Meow=0, Woof=1, bird=2)\n    assert s.to_python(value, exclude={'dog'}, by_alias=True) == IsStrictDict(Meow=0, bird=2)\n    assert s.to_python(value, by_alias=False) == IsStrictDict(cat=0, dog=1, bird=2)\n    assert s.to_python(value, mode='json', by_alias=True) == IsStrictDict(Meow=0, Woof=1, bird=2)\n    assert s.to_python(value, mode='json', include={'cat'}, by_alias=True) == IsStrictDict(Meow=0)\n    assert s.to_python(value, mode='json', by_alias=False) == IsStrictDict(cat=0, dog=1, bird=2)\n    assert json.loads(s.to_json(value, by_alias=True)) == IsStrictDict(Meow=0, Woof=1, bird=2)\n    assert json.loads(s.to_json(value, include={'cat', 'bird'}, by_alias=True)) == IsStrictDict(Meow=0, bird=2)\n    assert json.loads(s.to_json(value, by_alias=False)) == IsStrictDict(cat=0, dog=1, bird=2)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001238", "source": "def ser_x(self, v: Any, serializer: core_schema.SerializerFunctionWrapHandler, _) -> str:\n            x = serializer(v)\n            assert self.x == 1_000\n            return f'{x:_}'", "target": "def ser_x(self, v: Any, serializer: core_schema.SerializerFunctionWrapHandler, _) -> str:\n            assert self.x == 1_000\n            x = serializer(v)\n            return f'{x:_}'", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001239", "source": "def types_separator(self) -> str:\n        return \"\"", "target": "def types_separator(self) -> str:\n        return \", \"", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001240", "source": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "target": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "001241", "source": "def _validator(cls, v, info):\n                return v", "target": "def import_path():\n    import sys\n    if sys.version_info[0] < 3 or sys.version_info[1] < 6:\n        raise unittest.SkipTest('Python 3.6+ required')\n    from pathlib import Path\n    return Path", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001242", "source": "def benchmark(self):\n        for M, N in self.get_shapes():\n            print(f\"Tensor dimensions: [{M}, {N}]\")\n            torch_dtype = cutlass_torch.dtype(cutlass.BFloat16)\n            x = torch.randn(M, N, device=\"cuda\", dtype=torch_dtype)\n            w = torch.randn(N, device=\"cuda\", dtype=torch.float32)\n            self.benchmark_single_shape((x, w), setting=f\"shape: [{M}, {N}]\")", "target": "def benchmark(self):\n        for M, N in self.get_shapes():\n            print(f\"Tensor dimensions: [{M}, {N}]\")\n            torch_dtype = cutlass_torch.dtype(cutlass.BFloat16)\n            x = torch.randn(M, N, device=\"cuda\", dtype=torch_dtype, requires_grad=True)\n            w = torch.randn(N, device=\"cuda\", dtype=torch.float32, requires_grad=True)\n            dy = torch.randn(M, N, device=\"cuda\", dtype=torch_dtype)\n            self.benchmark_single_shape((x, w, dy), setting=f\"shape: [{M}, {N}]\")", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001243", "source": "def test_json_error():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'field_a': core_schema.model_field(\n                    schema=core_schema.list_schema(items_schema=core_schema.int_schema())\n                )\n            }\n        )\n    )\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_json('{\"field_a\": [123, \"wrong\"]}')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('field_a', 1),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'wrong',\n        }\n    ]", "target": "def test_json_error():\n    v = SchemaValidator(\n        core_schema.typed_dict_schema(\n            fields={\n                'field_a': core_schema.typed_dict_field(\n                    schema=core_schema.list_schema(items_schema=core_schema.int_schema())\n                )\n            }\n        )\n    )\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_json('{\"field_a\": [123, \"wrong\"]}')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('field_a', 1),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'wrong',\n        }\n    ]", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001244", "source": "def test_validation_error_loc_overrides():\n    class CustomLocOverridesError(ValidationError):\n        @override\n        def errors(\n            self, *, include_url: bool = True, include_context: bool = True, include_input: bool = True\n        ) -> list[ErrorDetails]:\n            errors = super().errors(\n                include_url=include_url, include_context=include_context, include_input=include_input\n            )\n            return [{**error, 'loc': error['loc'][1:]} for error in errors]\n    with pytest.raises(CustomLocOverridesError) as exception_info:\n        raise CustomLocOverridesError.from_exception_data(\n            'My CustomError',\n            [\n                InitErrorDetails(\n                    type='value_error',\n                    loc=(\n                        'hide_this',\n                        'myField',\n                    ),\n                    msg='This is my custom error.',\n                    input='something invalid',\n                    ctx={\n                        'myField': 'something invalid',\n                        'error': \"'something invalid' is not a valid value for 'myField'\",\n                    },\n                ),\n                InitErrorDetails(\n                    type='value_error',\n                    loc=(\n                        'hide_this',\n                        'myFieldToo',\n                    ),\n                    msg='This is my custom error.',\n                    input='something invalid',\n                    ctx={\n                        'myFieldToo': 'something invalid',\n                        'error': \"'something invalid' is not a valid value for 'myFieldToo'\",\n                    },\n                ),\n            ],\n        )\n    TestCase().assertCountEqual(\n        exception_info.value.errors(),\n        [\n            {\n                'type': 'value_error',\n                'loc': ('myField',),\n                'msg': \"Value error, 'something invalid' is not a valid value for 'myField'\",\n                'input': 'something invalid',\n                'ctx': {\n                    'error': \"'something invalid' is not a valid value for 'myField'\",\n                    'myField': 'something invalid',\n                },\n                'url': ANY,\n            },\n            {\n                'type': 'value_error',\n                'loc': ('myFieldToo',),\n                'msg': \"Value error, 'something invalid' is not a valid value for 'myFieldToo'\",\n                'input': 'something invalid',\n                'ctx': {\n                    'error': \"'something invalid' is not a valid value for 'myFieldToo'\",\n                    'myFieldToo': 'something invalid',\n                },\n                'url': ANY,\n            },\n        ],\n    )", "target": "def test_repr():\n    v = SchemaValidator(cs.frozenset_schema(strict=True, min_length=42))\n    assert plain_repr(v) == (\n        'SchemaValidator('\n        'title=\"frozenset[any]\",'\n        'validator=FrozenSet(FrozenSetValidator{'\n        'strict:true,item_validator:Any(AnyValidator),min_length:Some(42),max_length:None,'\n        'name:\"frozenset[any]\",'\n        'fail_fast:false'\n        '}),'\n        'definitions=[],'\n        'cache_strings=True)'\n    )", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001245", "source": "def test_filter_runtime():\n    s = SchemaSerializer(\n        core_schema.dict_schema(\n            core_schema.any_schema(), serialization=core_schema.filter_dict_schema(exclude={'0', '1'})\n        )\n    )\n    assert s.to_python({'0': 0, '1': 1, '2': 2, '3': 3}, include={'1', '2'}) == {'1': 1, '2': 2}\n    assert s.to_python({'0': 0, '1': 1, '2': 2, '3': 3}, include={'1', '2'}, exclude={'2', '3'}) == {'1': 1}", "target": "def test_filter_runtime():\n    v = SchemaSerializer(\n        core_schema.list_schema(core_schema.any_schema(), serialization=core_schema.filter_seq_schema(exclude={0, 1}))\n    )\n    assert v.to_python([0, 1, 2, 3]) == [2, 3]\n    assert v.to_python([0, 1, 2, 3], include={1, 2}) == [1, 2]", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001246", "source": "def get_output(self, input_blob):\n        if self.need_reshape:\n            self.net.blobs[self.in_blob_name].reshape(*input_blob.shape)\n        return self.net.forward_all(**{self.in_blob_name: input_blob})[self.out_blob_name]", "target": "def get_output(self, input_blob):\n        self.net.setInput(input_blob, self.in_blob_name)\n        return self.net.forward(self.out_blob_name)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001247", "source": "def forward(self, x):\n        for _ in range(self._n):\n            x = fn9(x)\n        return x", "target": "def forward(self, x):\n                total = sum(t.item() for t in x)\n                return total // 2", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "001248", "source": "def get_custom_getitem():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={'field_a': core_schema.model_field(validation_alias=['foo'], schema=core_schema.int_schema())}\n        )\n    )\n    assert v.validate_python(GetItemThing()) == ({'field_a': 321}, {}, {'field_a'})\n    assert v.validate_python({'bar': GetItemThing()}) == ({'field_a': 321}, {}, {'field_a'})", "target": "def update(self, input_pos, k_val, v_val):\n        assert input_pos.shape[0] == k_val.shape[2]\n        k_out = self.k_cache\n        v_out = self.v_cache\n        k_out[:, :, input_pos] = k_val\n        v_out[:, :, input_pos] = v_val\n        return k_out, v_out", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001249", "source": "def get_shapes(self) -> tuple[tuple[int, ...], ...]:\n        return (\n            (32768, 256),\n            (32768, 512),\n            (32768, 1024),\n            (32768, 2048),\n            (32768, 4096),\n            (32768, 8192),\n            (32768, 16384),\n            (32768, 32768),\n            (32768, 65536),\n            (16384, 131072),\n            (8192, 262144),\n        )", "target": "def get_shapes(self) -> tuple[tuple[int, ...], ...]:\n        return (\n            (32768, 256),\n            (32768, 512),\n            (32768, 1024),\n            (32768, 2048),\n            (32768, 4096),\n            (32768, 8192),\n            (32768, 16384),\n            (32768, 32768),\n            (32768, 65536),\n        ) + extra_shapes_for_norm", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|opencv/opencv", "data_split": "train", "example_id": "001250", "source": "def run(self):\n        inputs = VllmBuildParameters()\n        logger.info(\"Running vllm build with inputs: %s\", inputs)\n        vllm_commit = clone_vllm()\n        self.cp_torch_cleaning_script(inputs)\n        self.cp_dockerfile_if_exist(inputs)\n        self.cp_torch_whls_if_exist(inputs)\n        ensure_dir_exists(Path(inputs.output_dir))\n        cmd = self._generate_docker_build_cmd(inputs)\n        logger.info(\"Running docker build: \\n %s\", cmd)\n        try:\n            run_command(cmd, cwd=\"vllm\", env=os.environ.copy())\n        finally:\n            self.genearte_vllm_build_summary(vllm_commit, inputs)", "target": "def string_or_pathlike_(ctype_name: str = \"string\") -> UnionTypeNode:\n        return UnionTypeNode(\n            ctype_name,\n            items=(\n                PrimitiveTypeNode.str_(ctype_name),\n                PathLikeTypeNode(ctype_name)\n            )\n        )", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|opencv/opencv", "data_split": "train", "example_id": "001251", "source": "def test_numpy_writeable_flag_is_preserved(self):\n        array = np.zeros((10, 10, 1), dtype=np.uint8)\n        array.setflags(write=False)\n        with self.assertRaises(Exception):\n            cv.rectangle(array, (0, 0), (5, 5), (255), 2)", "target": "def process(self, frameworks, data_fetcher):\n        samples_handled = 0\n        conf_mats = [np.zeros((data_fetcher.get_num_classes(), data_fetcher.get_num_classes())) for i in range(len(frameworks))]\n        blobs_l1_diff = [0] * len(frameworks)\n        blobs_l1_diff_count = [0] * len(frameworks)\n        blobs_l_inf_diff = [sys.float_info.min] * len(frameworks)\n        inference_time = [0.0] * len(frameworks)\n        for in_blob_dict, gt in data_fetcher:\n            frameworks_out = []\n            samples_handled += 1\n            for i in range(len(frameworks)):\n                start = time.time()\n                framework_name = frameworks[i].get_name()\n                out = frameworks[i].get_output(in_blob_dict[framework_name])\n                end = time.time()\n                segm = eval_segm_result(out)\n                conf_mats[i] += get_conf_mat(gt, segm[0])\n                frameworks_out.append(out)\n                inference_time[i] += end - start\n                pix_acc, mean_acc, miou = get_metrics(conf_mats[i])\n                name = frameworks[i].get_name()\n                print(samples_handled, 'Pixel accuracy, %s:' % name, 100 * pix_acc, file=self.log)\n                print(samples_handled, 'Mean accuracy, %s:' % name, 100 * mean_acc, file=self.log)\n                print(samples_handled, 'Mean IOU, %s:' % name, 100 * miou, file=self.log)\n                print(\"Inference time, ms \", \\\n                    frameworks[i].get_name(), inference_time[i] / samples_handled * 1000, file=self.log)\n            for i in range(1, len(frameworks)):\n                log_str = frameworks[0].get_name() + \" vs \" + frameworks[i].get_name() + ':'\n                diff = np.abs(frameworks_out[0] - frameworks_out[i])\n                l1_diff = np.sum(diff) / diff.size\n                print(samples_handled, \"L1 difference\", log_str, l1_diff, file=self.log)\n                blobs_l1_diff[i] += l1_diff\n                blobs_l1_diff_count[i] += 1\n                if np.max(diff) > blobs_l_inf_diff[i]:\n                    blobs_l_inf_diff[i] = np.max(diff)\n                print(samples_handled, \"L_INF difference\", log_str, blobs_l_inf_diff[i], file=self.log)\n            self.log.flush()\n        for i in range(1, len(blobs_l1_diff)):\n            log_str = frameworks[0].get_name() + \" vs \" + frameworks[i].get_name() + ':'\n            print('Final l1 diff', log_str, blobs_l1_diff[i] / blobs_l1_diff_count[i], file=self.log)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001252", "source": "def test_uuid(input_value, expected):\n    v = SchemaValidator(core_schema.uuid_schema())\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            result = v.validate_python(input_value)\n            print(f'input_value={input_value} result={result}')\n    else:\n        output = v.validate_python(input_value)\n        assert output == expected\n        assert isinstance(output, UUID)", "target": "def datetime_tz_aware(self: _Pipeline[_InT, datetime.datetime]) -> _Pipeline[_InT, datetime.datetime]:\n        return self.constrain(annotated_types.Timezone(...))", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "001253", "source": "def get_shapes(self) -> tuple[tuple[int, ...], ...]:\n        return (\n            (32768, 256),\n            (32768, 512),\n            (32768, 1024),\n            (32768, 2048),\n            (32768, 4096),\n            (32768, 8192),\n            (32768, 16384),\n            (32768, 32768),\n            (32768, 65536),\n        ) + extra_shapes_for_norm", "target": "def test_dataclass_self_init():\n    @dataclasses.dataclass(init=False)\n    class Foo:\n        a: str\n        b: bool\n        def __init__(self, *args, **kwargs):\n            v.validate_python(ArgsKwargs(args, kwargs), self_instance=self)\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), kw_only=False),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), kw_only=False),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    foo = Foo(b'hello', 'True')\n    assert dataclasses.is_dataclass(foo)\n    assert dataclasses.asdict(foo) == {'a': 'hello', 'b': True}", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001254", "source": "def process(self, frameworks, data_fetcher):\n        sorted_imgs_names = sorted(self.img_classes.keys())\n        correct_answers = [0] * len(frameworks)\n        samples_handled = 0\n        blobs_l1_diff = [0] * len(frameworks)\n        blobs_l1_diff_count = [0] * len(frameworks)\n        blobs_l_inf_diff = [sys.float_info.min] * len(frameworks)\n        inference_time = [0.0] * len(frameworks)\n        for x in xrange(0, len(sorted_imgs_names), self.batch_size):\n            sublist = sorted_imgs_names[x:x + self.batch_size]\n            batch = data_fetcher.get_batch(sublist)\n            samples_handled += len(sublist)\n            frameworks_out = []\n            fw_accuracy = []\n            for i in range(len(frameworks)):\n                start = time.time()\n                out = frameworks[i].get_output(batch)\n                end = time.time()\n                correct_answers[i] += get_correct_answers(sublist, self.img_classes, out)\n                fw_accuracy.append(100 * correct_answers[i] / float(samples_handled))\n                frameworks_out.append(out)\n                inference_time[i] += end - start\n                print(samples_handled, 'Accuracy for', frameworks[i].get_name() + ':', fw_accuracy[i], file=self.log)\n                print(\"Inference time, ms \", \\\n                    frameworks[i].get_name(), inference_time[i] / samples_handled * 1000, file=self.log)\n            for i in range(1, len(frameworks)):\n                log_str = frameworks[0].get_name() + \" vs \" + frameworks[i].get_name() + ':'\n                diff = np.abs(frameworks_out[0] - frameworks_out[i])\n                l1_diff = np.sum(diff) / diff.size\n                print(samples_handled, \"L1 difference\", log_str, l1_diff, file=self.log)\n                blobs_l1_diff[i] += l1_diff\n                blobs_l1_diff_count[i] += 1\n                if np.max(diff) > blobs_l_inf_diff[i]:\n                    blobs_l_inf_diff[i] = np.max(diff)\n                print(samples_handled, \"L_INF difference\", log_str, blobs_l_inf_diff[i], file=self.log)\n            self.log.flush()\n        for i in range(1, len(blobs_l1_diff)):\n            log_str = frameworks[0].get_name() + \" vs \" + frameworks[i].get_name() + ':'\n            print('Final l1 diff', log_str, blobs_l1_diff[i] / blobs_l1_diff_count[i], file=self.log)", "target": "def process(self, frameworks, data_fetcher):\n        sorted_imgs_names = sorted(self.img_classes.keys())\n        correct_answers = [0] * len(frameworks)\n        samples_handled = 0\n        blobs_l1_diff = [0] * len(frameworks)\n        blobs_l1_diff_count = [0] * len(frameworks)\n        blobs_l_inf_diff = [sys.float_info.min] * len(frameworks)\n        inference_time = [0.0] * len(frameworks)\n        for x in range(0, len(sorted_imgs_names), self.batch_size):\n            sublist = sorted_imgs_names[x:x + self.batch_size]\n            batch = data_fetcher.get_batch(sublist)\n            samples_handled += len(sublist)\n            fw_accuracy = []\n            fw_time = []\n            frameworks_out = []\n            for i in range(len(frameworks)):\n                start = time.time()\n                out = frameworks[i].get_output(batch)\n                end = time.time()\n                correct_answers[i] += self.get_correct_answers(sublist, out)\n                fw_accuracy.append(100 * correct_answers[i] / float(samples_handled))\n                frameworks_out.append(out)\n                inference_time[i] += end - start\n                fw_time.append(inference_time[i] / samples_handled * 1000)\n                print(samples_handled, 'Accuracy for', frameworks[i].get_name() + ':', fw_accuracy[i], file=self.log)\n                print(\"Inference time, ms \", frameworks[i].get_name(), fw_time[i], file=self.log)\n                self.general_quality_metric.append(fw_accuracy)\n                self.general_inference_time.append(fw_time)\n            for i in range(1, len(frameworks)):\n                log_str = frameworks[0].get_name() + \" vs \" + frameworks[i].get_name() + ':'\n                diff = np.abs(frameworks_out[0] - frameworks_out[i])\n                l1_diff = np.sum(diff) / diff.size\n                print(samples_handled, \"L1 difference\", log_str, l1_diff, file=self.log)\n                blobs_l1_diff[i] += l1_diff\n                blobs_l1_diff_count[i] += 1\n                if np.max(diff) > blobs_l_inf_diff[i]:\n                    blobs_l_inf_diff[i] = np.max(diff)\n                print(samples_handled, \"L_INF difference\", log_str, blobs_l_inf_diff[i], file=self.log)\n            self.log.flush()\n        for i in range(1, len(blobs_l1_diff)):\n            log_str = frameworks[0].get_name() + \" vs \" + frameworks[i].get_name() + ':'\n            print('Final l1 diff', log_str, blobs_l1_diff[i] / blobs_l1_diff_count[i], file=self.log)\n        print(\n            get_final_summary_info(\n                self.general_quality_metric,\n                self.general_inference_time,\n                \"accuracy\"\n            ),\n            file=self.log\n        )", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001255", "source": "def benchmark(self):\n        for M, N in self.get_shapes():\n            print(f\"Tensor dimensions: [{M}, {N}]\")\n            torch_dtype = cutlass_torch.dtype(cutlass.BFloat16)\n            x = 0.1 * torch.randn(\n                M, N, device=\"cuda\", dtype=torch_dtype, requires_grad=True\n            )\n            target = torch.randint(0, N, (M,), device=\"cuda\", dtype=torch.int64)\n            dloss = torch.randn(M, device=\"cuda\", dtype=torch.float32)\n            self.benchmark_single_shape(\n                (x, target, dloss), setting=f\"shape: [{M}, {N}]\"\n            )", "target": "def benchmark(self):\n        for M, N in self.get_shapes():\n            print(f\"Tensor dimensions: [{M}, {N}]\")\n            torch_dtype = cutlass_torch.dtype(cutlass.BFloat16)\n            x = torch.randn(M, N, device=\"cuda\", dtype=torch_dtype, requires_grad=True)\n            w = torch.randn(N, device=\"cuda\", dtype=torch.float32, requires_grad=True)\n            dy = torch.randn(M, N, device=\"cuda\", dtype=torch_dtype)\n            self.benchmark_single_shape((x, w, dy), setting=f\"shape: [{M}, {N}]\")", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001256", "source": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "target": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "001257", "source": "def projectMarker(img, board, markerIndex, cameraMatrix, rvec, tvec, markerBorder):\n    markerSizePixels = 100\n    markerImg = cv.aruco.generateImageMarker(board.getDictionary(), board.getIds()[markerIndex], markerSizePixels, borderBits=markerBorder)\n    distCoeffs = np.zeros((5, 1), np.float64)\n    maxCoord = board.getRightBottomCorner()\n    objPoints = board.getObjPoints()[markerIndex]\n    for i in range(len(objPoints)):\n        objPoints[i][0] -= maxCoord[0] / 2\n        objPoints[i][1] -= maxCoord[1] / 2\n        objPoints[i][2] -= maxCoord[2] / 2\n    corners, _ = cv.projectPoints(objPoints, rvec, tvec, cameraMatrix, distCoeffs)\n    originalCorners = np.array([\n        [0, 0],\n        [markerSizePixels, 0],\n        [markerSizePixels, markerSizePixels],\n        [0, markerSizePixels],\n    ], np.float32)\n    transformation = cv.getPerspectiveTransform(originalCorners, corners)\n    borderValue = 127\n    aux = cv.warpPerspective(markerImg, transformation, img.shape, None, cv.INTER_NEAREST, cv.BORDER_CONSTANT, borderValue)\n    assert(img.shape == aux.shape)\n    mask = (aux == borderValue).astype(np.uint8)\n    img = img * mask + aux * (1 - mask)\n    return img", "target": "def test_null():\n    assert SchemaValidator(core_schema.none_schema()).validate_json('null') is None", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|opencv/opencv", "data_split": "train", "example_id": "001258", "source": "def copy_sysroot(self, builderDLDT):\n        log.info('Copy sysroot files')\n        copytree(builderDLDT.sysrootdir / 'bin', self.install_dir / 'bin')\n        copytree(builderDLDT.sysrootdir / 'etc', self.install_dir / 'etc')\n        log.info('Copy sysroot files - DONE')", "target": "def runTracker(self):\n        foregroundPointsNum = 0\n        while True:\n            frame = self.render.getNextFrame()\n            frame_gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n            if len(self.tracks) > 0:\n                img0, img1 = self.prev_gray, frame_gray\n                p0 = np.float32([tr[-1][0] for tr in self.tracks]).reshape(-1, 1, 2)\n                p1, _st, _err = cv.calcOpticalFlowPyrLK(img0, img1, p0, None, **lk_params)\n                p0r, _st, _err = cv.calcOpticalFlowPyrLK(img1, img0, p1, None, **lk_params)\n                d = abs(p0-p0r).reshape(-1, 2).max(-1)\n                good = d < 1\n                new_tracks = []\n                for tr, (x, y), good_flag in zip(self.tracks, p1.reshape(-1, 2), good):\n                    if not good_flag:\n                        continue\n                    tr.append([(x, y), self.frame_idx])\n                    if len(tr) > self.track_len:\n                        del tr[0]\n                    new_tracks.append(tr)\n                self.tracks = new_tracks\n            if self.frame_idx % self.detect_interval == 0:\n                goodTracksCount = 0\n                for tr in self.tracks:\n                    oldRect = self.render.getRectInTime(self.render.timeStep * tr[0][1])\n                    newRect = self.render.getRectInTime(self.render.timeStep * tr[-1][1])\n                    if isPointInRect(tr[0][0], oldRect) and isPointInRect(tr[-1][0], newRect):\n                        goodTracksCount += 1\n                if self.frame_idx == self.detect_interval:\n                    foregroundPointsNum = goodTracksCount\n                fgIndex = float(foregroundPointsNum) / (foregroundPointsNum + 1)\n                fgRate = float(goodTracksCount) / (len(self.tracks) + 1)\n                if self.frame_idx > 0:\n                    self.assertGreater(fgIndex, 0.9)\n                    self.assertGreater(fgRate, 0.2)\n                mask = np.zeros_like(frame_gray)\n                mask[:] = 255\n                for x, y in [np.int32(tr[-1][0]) for tr in self.tracks]:\n                    cv.circle(mask, (x, y), 5, 0, -1)\n                p = cv.goodFeaturesToTrack(frame_gray, mask = mask, **feature_params)\n                if p is not None:\n                    for x, y in np.float32(p).reshape(-1, 2):\n                        self.tracks.append([[(x, y), self.frame_idx]])\n            self.frame_idx += 1\n            self.prev_gray = frame_gray\n            if self.frame_idx > 300:\n                break", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001259", "source": "def get_output(self, input_blob):\n        return super(DnnTfInceptionModel, self).get_output(input_blob)[..., 1:1001]", "target": "def get_output(self, input_blob):\n        tensor = torch.FloatTensor(input_blob)\n        out = self.net.forward(tensor).numpy()\n        return out", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001260", "source": "def handler() -> T | None:\n            if adapter.rebuild(raise_errors=False, _parent_namespace_depth=5) is not False:\n                return attr_fn(adapter)\n            return None", "target": "def handler() -> T | None:\n            if rebuild_dataclass(cls, raise_errors=False, _parent_namespace_depth=5) is not False:\n                return attr_fn(cls)\n            return None", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001261", "source": "def test_alias(py_and_json: PyAndJson, input_value, expected):\n    v = py_and_json(\n        {\n            'type': 'arguments',\n            'arguments_schema': [\n                {'name': 'a', 'mode': 'positional_or_keyword', 'schema': {'type': 'int'}, 'alias': 'Foo'}\n            ],\n        }\n    )\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            v.validate_test(input_value)\n    else:\n        assert v.validate_test(input_value) == expected", "target": "def test_alias(py_and_json: PyAndJson):\n    v = py_and_json(\n        {\n            'type': 'typed-dict',\n            'fields': {\n                'field_a': {'validation_alias': 'FieldA', 'type': 'typed-dict-field', 'schema': {'type': 'int'}}\n            },\n        }\n    )\n    assert v.validate_test({'FieldA': '123'}) == {'field_a': 123}\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'foobar': '123'})\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'field_a': '123'})", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "001262", "source": "def applyPatch(patch_file, subdir = None):\n            if subdir:\n                log.info('Patching \"%s\": %s' % (subdir, patch_file))\n            else:\n                log.info('Patching: %s' % (patch_file))\n            git_apply_patch(self.srcdir / subdir if subdir else self.srcdir, self.cpath / patch_file)", "target": "def test_var_kwargs_invalid_dict(py_and_json: PyAndJson) -> None:\n    v = py_and_json(\n        cs.arguments_v3_schema(\n            [\n                cs.arguments_v3_parameter(name='kwargs', schema=cs.int_schema(), mode='var_kwargs_uniform'),\n            ]\n        )\n    )\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_test({'kwargs': 'not_a_dict'})\n    error = exc_info.value.errors()[0]\n    assert error['type'] == 'dict_type'\n    assert error['loc'] == ('kwargs',)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "train", "example_id": "001263", "source": "def get_git_repo_dir() -> str:\n    from pathlib import Path\n    return os.getenv(\"GIT_REPO_DIR\", str(Path(__file__).resolve().parents[2]))", "target": "def smoke_test_conv2d() -> None:\n    import torch.nn as nn\n    print(\"Testing smoke_test_conv2d\")\n    m = nn.Conv2d(16, 33, 3, stride=2)\n    m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))\n    assert m is not None\n    basic_conv = nn.Conv2d(\n        16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1)\n    )\n    input = torch.randn(20, 16, 50, 100)\n    output = basic_conv(input)\n    if is_cuda_system:\n        print(\"Testing smoke_test_conv2d with cuda\")\n        conv = nn.Conv2d(3, 3, 3).cuda()\n        x = torch.randn(1, 3, 24, 24, device=\"cuda\")\n        with torch.cuda.amp.autocast():\n            out = conv(x)\n        assert out is not None\n        supported_dtypes = [torch.float16, torch.float32, torch.float64]\n        for dtype in supported_dtypes:\n            print(f\"Testing smoke_test_conv2d with cuda for {dtype}\")\n            conv = basic_conv.to(dtype).cuda()\n            input = torch.randn(20, 16, 50, 100, device=\"cuda\").type(dtype)\n            output = conv(input)\n            assert output is not None", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "001264", "source": "def fn():\n        def validate(v, info):\n            return v\n        schema = core_schema.with_info_plain_validator_function(validate)\n        schema = core_schema.typed_dict_schema(\n            {'f': core_schema.typed_dict_field(schema)}, extra_behavior='allow', extras_schema=schema\n        )\n        validate.__pydantic_validator__ = SchemaValidator(schema)\n        return validate", "target": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001265", "source": "def ser_x(self, v: Any, _) -> str:\n            assert self.x == 1_000\n            return f'{v:_}'", "target": "def ser_x(self, v: Any, _) -> str:\n            assert self.x == 1_000 == v\n            return self.x_formatted", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001266", "source": "def test_tz_constraint_too_high():\n    with pytest.raises(SchemaError, match='OverflowError: Python int too large.*'):\n        SchemaValidator(core_schema.time_schema(tz_constraint=2**64))", "target": "def test_kwargs_uniform(py_and_json: PyAndJson, input_value, expected):\n    v = py_and_json(\n        {\n            'type': 'arguments',\n            'arguments_schema': [\n                {'name': 'a', 'mode': 'positional_only', 'schema': {'type': 'int'}},\n                {'name': 'b', 'mode': 'positional_or_keyword', 'schema': {'type': 'int'}},\n            ],\n            'var_kwargs_schema': {'type': 'str'},\n        }\n    )\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            v.validate_test(input_value)\n    else:\n        assert v.validate_test(input_value) == expected", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001267", "source": "def description(self):\n        return \"information at https://github.com/pytorch/pytorch/issues/134133\"", "target": "def description(self):\n        return \"information at https://github.com/pytorch/pytorch/pull/129893\"", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001268", "source": "def get_shapes(self) -> tuple[tuple[int, ...], ...]:\n        return (\n            (32768, 256),\n            (32768, 512),\n            (32768, 1024),\n            (32768, 2048),\n            (32768, 4096),\n            (32768, 8192),\n            (32768, 16384),\n            (32768, 32768),\n            (32768, 65536),\n            (16384, 131072),\n            (8192, 262144),\n        ) + extra_shapes_for_norm", "target": "def get_shapes(self) -> tuple[tuple[int, ...], ...]:\n        return (\n            (32768, 256),\n            (32768, 512),\n            (32768, 1024),\n            (32768, 2048),\n            (32768, 4096),\n            (32768, 8192),\n            (32768, 16384),\n            (32768, 32768),\n            (32768, 65536),\n        ) + extra_shapes_for_norm", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|pydantic/pydantic", "data_split": "train", "example_id": "001269", "source": "def _synth_regression_dataset(n_samples=100000, n_features=100, dtype=np.float32):\n    X, y = make_regression(\n        n_samples=n_samples,\n        n_features=n_features,\n        n_informative=n_features // 10,\n        noise=50,\n        random_state=0,\n    )\n    X = X.astype(dtype, copy=False)\n    X = StandardScaler().fit_transform(X)\n    X, X_val, y, y_val = train_test_split(X, y, test_size=0.1, random_state=0)\n    return X, X_val, y, y_val", "target": "def test_aware():\n    v = SchemaValidator(core_schema.time_schema(tz_constraint='aware'))\n    value = time(12, 13, 15, tzinfo=timezone.utc)\n    assert value is v.validate_python(value)\n    assert v.validate_python('12:13:14Z') == time(12, 13, 14, tzinfo=timezone.utc)\n    value = time(12, 13, 15)\n    with pytest.raises(ValidationError, match=r'Input should have timezone info'):\n        v.validate_python(value)\n    with pytest.raises(ValidationError, match=r'Input should have timezone info'):\n        v.validate_python('12:13:14')", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001270", "source": "def full_typename(self) -> str:\n        return self.typename", "target": "def full_typename(self) -> str:\n        return self.type_format.format(self.types_separator.join(\n            item.full_typename for item in self\n        ))", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|scikit-learn/scikit-learn", "data_split": "train", "example_id": "001271", "source": "def schema_validator(self) -> SchemaValidator:\n        return SchemaValidator(\n            schema=core_schema.union_schema(\n                choices=[\n                    core_schema.model_schema(\n                        cls=self.ModelA,\n                        schema=core_schema.model_fields_schema(\n                            fields={\n                                'a': core_schema.model_field(schema=core_schema.int_schema()),\n                                'b': core_schema.model_field(schema=core_schema.str_schema()),\n                            }\n                        ),\n                    ),\n                    core_schema.model_schema(\n                        cls=self.ModelB,\n                        schema=core_schema.model_fields_schema(\n                            fields={\n                                'c': core_schema.model_field(schema=core_schema.int_schema()),\n                                'd': core_schema.model_field(schema=core_schema.str_schema()),\n                            }\n                        ),\n                    ),\n                ]\n            )\n        )", "target": "def make_data(random_state, n_samples_per_center, grid_size, scale):\n    random_state = check_random_state(random_state)\n    centers = np.array([[i, j] for i in range(grid_size) for j in range(grid_size)])\n    n_clusters_true, n_features = centers.shape\n    noise = random_state.normal(\n        scale=scale, size=(n_samples_per_center, centers.shape[1])\n    )\n    X = np.concatenate([c + noise for c in centers])\n    y = np.concatenate([[i] * n_samples_per_center for i in range(n_clusters_true)])\n    return shuffle(X, y, random_state=random_state)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001272", "source": "def dynamic_rnn(\n        input: Tensor,\n        hidden: tuple[Tensor, Tensor],\n        w_ih: Tensor,\n        w_hh: Tensor,\n        b_ih: Tensor,\n        b_hh: Tensor,\n    ) -> tuple[Tensor, tuple[Tensor, Tensor]]:\n        hx, cx = hidden\n        outputs = []\n        inputs = torch.matmul(input, w_ih.t()).unbind(0)\n        hy, cy = hx[0], cx[0]\n        for seq_idx in range(len(inputs)):\n            hy, cy = premul_cell(inputs[seq_idx], (hy, cy), w_hh, b_ih, b_hh)\n            outputs += [hy]\n        return torch.stack(outputs), (hy.unsqueeze(0), cy.unsqueeze(0))", "target": "def dynamic_rnn(\n        input: Tensor, hidden: tuple[Tensor, Tensor], params: list[Tensor]\n    ) -> tuple[Tensor, tuple[Tensor, Tensor]]:\n        params_stride = 4\n        hx, cx = hidden\n        hy, cy = hidden\n        inputs, outputs = input.unbind(0), []\n        for layer in range(hx.size(0)):\n            hy = hx[layer]\n            cy = cx[layer]\n            base_idx = layer * params_stride\n            w_ih = params[base_idx]\n            w_hh = params[base_idx + 1]\n            b_ih = params[base_idx + 2]\n            b_hh = params[base_idx + 3]\n            for seq_idx in range(len(inputs)):\n                hy, cy = cell(inputs[seq_idx], (hy, cy), w_ih, w_hh, b_ih, b_hh)\n                outputs += [hy]\n            inputs, outputs = outputs, []\n        return torch.stack(inputs), (hy.unsqueeze(0), cy.unsqueeze(0))", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "001273", "source": "def run(img, size):\n                    return cv.resize(img, size)", "target": "def test_tz_constraint_too_high():\n    with pytest.raises(SchemaError, match='OverflowError: Python int too large.*'):\n        SchemaValidator(core_schema.datetime_schema(tz_constraint=2**64))", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001274", "source": "def get_type_ref(type_: Any, args_override: tuple[type[Any], ...] | None = None) -> str:\n    origin = get_origin(type_) or type_\n    args = get_args(type_) if is_generic_alias(type_) else (args_override or ())\n    generic_metadata = getattr(type_, '__pydantic_generic_metadata__', None)\n    if generic_metadata:\n        origin = generic_metadata['origin'] or origin\n        args = generic_metadata['args'] or args\n    module_name = getattr(origin, '__module__', '<No __module__>')\n    if typing_objects.is_typealiastype(origin):\n        type_ref = f'{module_name}.{origin.__name__}:{id(origin)}'\n    else:\n        try:\n            qualname = getattr(origin, '__qualname__', f'<No __qualname__: {origin}>')\n        except Exception:\n            qualname = getattr(origin, '__qualname__', '<No __qualname__>')\n        type_ref = f'{module_name}.{qualname}:{id(origin)}'\n    arg_refs: list[str] = []\n    for arg in args:\n        if isinstance(arg, str):\n            arg_ref = f'{arg}:str-{id(arg)}'\n        else:\n            arg_ref = f'{_repr.display_as_type(arg)}:{id(arg)}'\n        arg_refs.append(arg_ref)\n    if arg_refs:\n        type_ref = f'{type_ref}[{\",\".join(arg_refs)}]'\n    return type_ref", "target": "def gen(error: bool):\n        yield 1\n        yield 2\n        if error:\n            raise RuntimeError('error')\n        yield 3", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001275", "source": "def types_separator(self) -> str:\n        return \", \"", "target": "def types_separator(self) -> str:\n        return \", \"", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|scikit-learn/scikit-learn", "data_split": "train", "example_id": "001276", "source": "def test_reuse_plain_validator_ok() -> None:\n    class InnerModel:\n        x: str\n        def __init__(self, x: str) -> None:\n            self.x = x\n    def validate_inner(data) -> InnerModel:\n        data['x'] = data['x'] + ' modified'\n        return InnerModel(**data)\n    inner_schema = core_schema.no_info_plain_validator_function(validate_inner)\n    inner_validator = SchemaValidator(inner_schema)\n    InnerModel.__pydantic_complete__ = True\n    InnerModel.__pydantic_validator__ = inner_validator\n    class OuterModel:\n        inner: InnerModel\n        def __init__(self, inner: InnerModel) -> None:\n            self.inner = inner\n    outer_schema = core_schema.model_schema(\n        OuterModel,\n        schema=core_schema.model_fields_schema(\n            {\n                'inner': core_schema.model_field(\n                    schema=core_schema.model_schema(\n                        InnerModel,\n                        schema=core_schema.model_fields_schema(\n                            {'x': core_schema.model_field(schema=core_schema.str_schema())},\n                        ),\n                    )\n                )\n            }\n        ),\n    )\n    outer_validator = SchemaValidator(outer_schema)\n    result_inner = inner_validator.validate_python({'x': 'hello'})\n    assert result_inner.x == 'hello modified'\n    assert 'FunctionPlainValidator' in repr(inner_validator)\n    result_outer = outer_validator.validate_python({'inner': {'x': 'hello'}})\n    assert result_outer.inner.x == 'hello modified'\n    assert 'PrebuiltValidator' in repr(outer_validator)", "target": "def bench_sample(sampling, n_population, n_samples):\n    gc.collect()\n    t_start = datetime.now()\n    sampling(n_population, n_samples)\n    delta = datetime.now() - t_start\n    time = compute_time(t_start, delta)\n    return time", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001277", "source": "def test_chain():\n    s = SchemaSerializer(core_schema.chain_schema([core_schema.str_schema(), core_schema.int_schema()]))\n    assert plain_repr(s) == 'SchemaSerializer(serializer=Int(IntSerializer),definitions=[])'\n    assert s.to_python(1) == 1\n    assert s.to_json(1) == b'1'", "target": "def test_chain():\n    validator = SchemaValidator(\n        cs.chain_schema(\n            steps=[cs.str_schema(), core_schema.with_info_plain_validator_function(lambda v, info: Decimal(v))]\n        )\n    )\n    assert validator.validate_python('1.44') == Decimal('1.44')\n    assert validator.validate_python(b'1.44') == Decimal('1.44')", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|pydantic/pydantic", "data_split": "train", "example_id": "001278", "source": "def setup_working_with_text_data():\n    check_skip_network()\n    cache_path = _pkl_filepath(get_data_home(), CACHE_NAME)\n    if not exists(cache_path):\n        raise SkipTest(\"Skipping dataset loading doctests\")", "target": "def test_non_finite_json_values(py_and_json: PyAndJson, input_value, allow_inf_nan, expected):\n    v = py_and_json({'type': 'float', 'allow_inf_nan': allow_inf_nan})\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            v.validate_test(input_value)\n    else:\n        assert v.validate_test(input_value) == expected", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001279", "source": "def test_aware():\n    v = SchemaValidator(core_schema.datetime_schema(tz_constraint='aware'))\n    value = datetime.now(tz=timezone.utc)\n    assert value is v.validate_python(value)\n    assert v.validate_python('2022-06-08T12:13:14Z') == datetime(2022, 6, 8, 12, 13, 14, tzinfo=timezone.utc)\n    value = datetime.now()\n    with pytest.raises(ValidationError, match=r'Input should have timezone info \\[type=timezone_aware,'):\n        v.validate_python(value)\n    with pytest.raises(ValidationError, match=r'Input should have timezone info \\[type=timezone_aware,'):\n        v.validate_python('2022-06-08T12:13:14')", "target": "def test_aware():\n    v = SchemaValidator(core_schema.time_schema(tz_constraint='aware'))\n    value = time(12, 13, 15, tzinfo=timezone.utc)\n    assert value is v.validate_python(value)\n    assert v.validate_python('12:13:14Z') == time(12, 13, 14, tzinfo=timezone.utc)\n    value = time(12, 13, 15)\n    with pytest.raises(ValidationError, match=r'Input should have timezone info'):\n        v.validate_python(value)\n    with pytest.raises(ValidationError, match=r'Input should have timezone info'):\n        v.validate_python('12:13:14')", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001280", "source": "def quack(self, args, kwargs=None) -> Any:\n        from quack.cross_entropy import cross_entropy\n        assert kwargs is None\n        x, target, dloss = args\n        loss = cross_entropy(x, target)\n        return lambda: torch.autograd.grad(\n            loss, x, grad_outputs=dloss, retain_graph=True\n        )", "target": "def quack(self, args, kwargs=None) -> Any:\n        from quack.rmsnorm import _rmsnorm_fwd\n        x, w = args\n        y = torch.empty_like(x)\n        def quack_fwd():\n            _rmsnorm_fwd(\n                x,\n                w,\n                out=y,\n                bias=None,\n                rstd=None,\n                residual=None,\n                residual_out=None,\n                eps=1e-6,\n            )\n            return y\n        return quack_fwd", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pytorch/pytorch", "data_split": "train", "example_id": "001281", "source": "def int_(cls, ctype_name: str, export_name: Optional[str] = None,\n             doc: Optional[str] = None, required_modules: Tuple[str, ...] = ()):\n        return cls(ctype_name, PrimitiveTypeNode.int_(), export_name, doc, required_modules)", "target": "def torch_mm(a, b):\n    return torch.mm(a, b)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|opencv/opencv", "data_split": "train", "example_id": "001282", "source": "def smoke_test_cuda(\n    package: str,\n    runtime_error_check: str,\n    torch_compile_check: str,\n    pypi_pkg_check: str,\n) -> None:\n    if not torch.cuda.is_available() and is_cuda_system:\n        raise RuntimeError(f\"Expected CUDA {gpu_arch_ver}. However CUDA is not loaded.\")\n    if package == \"all\" and is_cuda_system:\n        for module in MODULES:\n            imported_module = importlib.import_module(module[\"name\"])\n            version = \"N/A\"\n            if module[\"extension\"] == \"extension\":\n                version = imported_module.extension._check_cuda_version()\n            else:\n                version = imported_module._extension._check_cuda_version()\n            print(f\"{module['name']} CUDA: {version}\")\n    if (\n        torch_compile_check == \"enabled\"\n        and sys.version_info < (3, 14, 0)\n        and target_os in [\"linux\", \"linux-aarch64\", \"macos-arm64\", \"darwin\"]\n    ):\n        smoke_test_compile(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    if torch.cuda.is_available():\n        if torch.version.cuda != gpu_arch_ver:\n            raise RuntimeError(\n                f\"Wrong CUDA version. Loaded: {torch.version.cuda} Expected: {gpu_arch_ver}\"\n            )\n        print(f\"torch cuda: {torch.version.cuda}\")\n        torch.cuda.init()\n        print(\"CUDA initialized successfully\")\n        print(f\"Number of CUDA devices: {torch.cuda.device_count()}\")\n        for i in range(torch.cuda.device_count()):\n            print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")\n        print(f\"cuDNN enabled? {torch.backends.cudnn.enabled}\")\n        torch_cudnn_version = cudnn_to_version_str(torch.backends.cudnn.version())\n        print(f\"Torch cuDNN version: {torch_cudnn_version}\")\n        if sys.platform in [\"linux\", \"linux2\"]:\n            torch_nccl_version = \".\".join(str(v) for v in torch.cuda.nccl.version())\n            print(f\"Torch nccl; version: {torch_nccl_version}\")\n        if pypi_pkg_check == \"enabled\" and sys.platform in [\"linux\", \"linux2\"]:\n            compare_pypi_to_torch_versions(\n                \"cudnn\", find_pypi_package_version(\"nvidia-cudnn\"), torch_cudnn_version\n            )\n            compare_pypi_to_torch_versions(\n                \"nccl\", find_pypi_package_version(\"nvidia-nccl\"), torch_nccl_version\n            )\n        if runtime_error_check == \"enabled\":\n            test_cuda_runtime_errors_captured()", "target": "def test_letter_recog(self):\n        eps = 0.01\n        models = [RTrees, KNearest, Boost, SVM, MLP]\n        models = dict( [(cls.__name__.lower(), cls) for cls in models] )\n        testErrors = {RTrees: (98.930000, 92.390000), KNearest: (94.960000, 92.010000),\n         Boost: (85.970000, 74.920000), SVM: (99.780000, 95.680000), MLP: (90.060000, 87.410000)}\n        for model in models:\n            Model = models[model]\n            classifier = Model()\n            samples, responses = load_base(self.repoPath + '/samples/data/letter-recognition.data')\n            train_n = int(len(samples)*classifier.train_ratio)\n            classifier.train(samples[:train_n], responses[:train_n])\n            train_rate = np.mean(classifier.predict(samples[:train_n]) == responses[:train_n].astype(int))\n            test_rate  = np.mean(classifier.predict(samples[train_n:]) == responses[train_n:].astype(int))\n            self.assertLess(train_rate - testErrors[Model][0], eps)\n            self.assertLess(test_rate - testErrors[Model][1], eps)", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "001283", "source": "def make_data(self, params):\n        representation, solver, n_jobs = params\n        if Benchmark.data_size == \"large\":\n            if representation == \"sparse\":\n                data = _20newsgroups_highdim_dataset(n_samples=10000)\n            else:\n                data = _20newsgroups_lowdim_dataset(n_components=1e3)\n        else:\n            if representation == \"sparse\":\n                data = _20newsgroups_highdim_dataset(n_samples=2500)\n            else:\n                data = _20newsgroups_lowdim_dataset()\n        return data", "target": "def make_data(self, params):\n        representation, solver = params\n        if representation == \"dense\":\n            data = _synth_regression_dataset(n_samples=500000, n_features=100)\n        else:\n            data = _synth_regression_sparse_dataset(\n                n_samples=100000, n_features=10000, density=0.005\n            )\n        return data", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001284", "source": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "target": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|opencv/opencv", "data_split": "train", "example_id": "001285", "source": "def type_format(self) -> str:\n        return \"_typing.Sequence[{}]\"", "target": "def test_regression_16040_2(self):\n        obj_points = np.array([[0, 0, 0], [0, 1, 0], [1, 1, 0], [1, 0, 0]], dtype=np.float32)\n        img_points = np.array(\n            [[[700, 400], [700, 600], [900, 600], [900, 400]]], dtype=np.float32\n        )\n        cameraMatrix = np.array(\n            [[712.0634, 0, 800], [0, 712.540, 500], [0, 0, 1]], dtype=np.float32\n        )\n        distCoeffs = np.array([[0, 0, 0, 0]], dtype=np.float32)\n        r = np.array([], dtype=np.float32)\n        x, r, t, e = cv.solvePnPGeneric(\n            obj_points, img_points, cameraMatrix, distCoeffs, reprojectionError=r\n        )", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001286", "source": "def test_extra_custom_serializer():\n    @dataclasses.dataclass\n    class Model:\n        pass\n    schema = core_schema.dataclass_schema(\n        Model,\n        core_schema.dataclass_args_schema(\n            'Model',\n            [],\n            extra_behavior='allow',\n        ),\n        [],\n    )\n    s = SchemaSerializer(schema)\n    v = SchemaValidator(schema)\n    m = v.validate_python({'extra': 'extra'})\n    assert s.to_python(m) == {'extra': 'extra bam!'}", "target": "def test_extra_custom_serializer():\n    class Model:\n        __slots__ = ('__pydantic_extra__', '__dict__')\n        __pydantic_extra__: dict[str, Any]\n    schema = core_schema.model_schema(\n        Model,\n        core_schema.model_fields_schema(\n            {},\n            extra_behavior='allow',\n            extras_schema=core_schema.any_schema(\n                serialization=core_schema.plain_serializer_function_ser_schema(lambda v: v + ' bam!')\n            ),\n        ),\n        extra_behavior='allow',\n    )\n    s = SchemaSerializer(schema)\n    m = Model()\n    m.__pydantic_extra__ = {'extra': 'extra'}\n    assert s.to_python(m) == {'extra': 'extra bam!'}", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001287", "source": "def f(inp, *weights):\n            x = inp\n            for w in weights:\n                x = torch.matmul(w, x).sin().sin()\n            return x", "target": "def f(a, b):\n            xs = b.tolist()\n            for x in xs:\n                torch._check(x >= 0)\n                torch._check(x <= self.N)\n            return a.split(xs)", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001288", "source": "def train(self, samples, responses):\n        self.model.train(samples, cv.ml.ROW_SAMPLE, responses)", "target": "def train(self, samples, responses):\n        self.model.setType(cv.ml.SVM_C_SVC)\n        self.model.setC(1)\n        self.model.setKernel(cv.ml.SVM_RBF)\n        self.model.setGamma(.1)\n        self.model.train(samples, cv.ml.ROW_SAMPLE, responses.astype(int))", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001289", "source": "def typename(self) -> str:\n            return self.type_node.full_typename", "target": "def typename(self) -> str:\n        return self.type_node.full_typename", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "001290", "source": "def test_function_wrap_location():\n    def f(input_value, validator, info):\n        return validator(input_value, outer_location='foo') + 2\n    v = SchemaValidator(core_schema.with_info_wrap_validator_function(f, core_schema.int_schema()))\n    assert v.validate_python(4) == 6\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python('wrong')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('foo',),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'wrong',\n        }\n    ]", "target": "def load_hook(self, state_dict, prefix, *args):\n        if prefix + \"wq.weight\" in state_dict:\n            wq = state_dict.pop(prefix + \"wq.weight\")\n            wk = state_dict.pop(prefix + \"wk.weight\")\n            wv = state_dict.pop(prefix + \"wv.weight\")\n            state_dict[prefix + \"wqkv.weight\"] = torch.cat([wq, wk, wv])", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001291", "source": "def forward(self, inputs):\n        output = self.pool(F.relu(self.conv(inputs)))\n        output = output.view(1)\n        return output", "target": "def forward(self, x):\n        return torch.mm(x, self.weight)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001292", "source": "def validate_b(cls, v: bytes, info: core_schema.ValidationInfo) -> str:\n            assert v == b'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return 'hello world!'", "target": "def validate_b(\n            cls, v: bytes, nxt: core_schema.ValidatorFunctionWrapHandler, info: core_schema.ValidationInfo\n        ) -> bytes:\n            assert v == b'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return nxt(b'hello world!')", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "001293", "source": "def run_fn():\n        for inp in inps:\n            fn(*inp)", "target": "def test_int_kwargs(py_and_json: PyAndJson, kwargs: dict[str, Any], input_value, expected):\n    v = py_and_json({'type': 'int', **kwargs})\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)) as exc_info:\n            v.validate_test(input_value)\n        errors = exc_info.value.errors(include_url=False)\n        assert len(errors) == 1\n        if 'ctx' in errors[0]:\n            assert errors[0]['ctx'] == kwargs\n    else:\n        output = v.validate_test(input_value)\n        assert output == expected\n        assert isinstance(output, int)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001294", "source": "def type_format(self) -> str:\n        return \"\"", "target": "def type_format(self) -> str:\n        if TypeNode.compatible_to_runtime_usage:\n            return \"_typing.Dict[{}]\"\n        return \"dict[{}]\"", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "001295", "source": "def layernorm_ref(self, x: torch.Tensor, w: torch.Tensor, eps: float = 1e-6):\n        x_f32 = x.float()\n        return F.layer_norm(x_f32, w.shape, w, None, eps).to(x.dtype)", "target": "def test_function_args_any(input_value, expected):\n    def my_function(a, b, c):\n        return a + b + c\n    v = SchemaValidator(cs.call_schema(function=my_function, arguments=cs.any_schema(), return_schema=cs.int_schema()))\n    if isinstance(expected, Exception):\n        with pytest.raises(type(expected), match=re.escape(str(expected))):\n            v.validate_python(input_value)\n    else:\n        assert v.validate_python(input_value) == expected", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001296", "source": "def check_tz_naive(v: object) -> bool:\n                    assert isinstance(v, datetime.datetime)\n                    return v.tzinfo is None", "target": "def test_date_past_future_today():\n    v = SchemaValidator(core_schema.date_schema(now_op='past'))\n    today = datetime.now(timezone.utc).date()\n    assert v.isinstance_python(today) is False\n    assert v.isinstance_python(today - timedelta(days=1)) is True\n    assert v.isinstance_python(today + timedelta(days=1)) is False\n    v = SchemaValidator(core_schema.date_schema(now_op='future'))\n    assert v.isinstance_python(today) is False\n    assert v.isinstance_python(today - timedelta(days=1)) is False\n    assert v.isinstance_python(today + timedelta(days=1)) is True", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001297", "source": "def test_only_validate_by_name(py_and_json) -> None:\n    v = py_and_json(\n        cs.arguments_v3_schema(\n            [\n                cs.arguments_v3_parameter(\n                    name='a', schema=cs.str_schema(), alias='FieldA', mode='positional_or_keyword'\n                ),\n            ],\n            validate_by_name=True,\n            validate_by_alias=False,\n        )\n    )\n    assert v.validate_test(ArgsKwargs((), {'a': 'hello'})) == ((), {'a': 'hello'})\n    assert v.validate_test({'a': 'hello'}) == (('hello',), {})\n    with pytest.raises(ValidationError, match=r'a\\n +Missing required argument \\[type=missing_argument,'):\n        assert v.validate_test(ArgsKwargs((), {'FieldA': 'hello'}))\n    with pytest.raises(ValidationError, match=r'a\\n +Missing required argument \\[type=missing_argument,'):\n        assert v.validate_test({'FieldA': 'hello'})", "target": "def test_only_validate_by_name(py_and_json) -> None:\n    schema = core_schema.arguments_schema(\n        [\n            core_schema.arguments_parameter(name='a', schema=core_schema.str_schema(), alias='FieldA'),\n        ],\n        validate_by_name=True,\n        validate_by_alias=False,\n    )\n    v = py_and_json(schema)\n    assert v.validate_test(ArgsKwargs((), {'a': 'hello'})) == ((), {'a': 'hello'})\n    with pytest.raises(ValidationError, match=r'a\\n +Missing required argument \\[type=missing_argument,'):\n        assert v.validate_test(ArgsKwargs((), {'FieldA': 'hello'}))", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001298", "source": "def full_typename(self) -> str:\n        return \"cv2.typing.\" + self.typename", "target": "def full_typename(self) -> str:\n        return self.type_format.format(self.types_separator.join(\n            item.full_typename for item in self\n        ))", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001299", "source": "def extract_function_name(func: ValidateCallSupportedTypes) -> str:\n    return f'partial({func.func.__name__})' if isinstance(func, functools.partial) else func.__name__", "target": "def test_datetime_past(py_and_json: PyAndJson, input_value, expected):\n    v = py_and_json(core_schema.datetime_schema(now_utc_offset=0, now_op='past'))\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            v.validate_test(input_value)\n    else:\n        output = v.validate_test(input_value)\n        assert output == expected", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "001300", "source": "def test_generator_error():\n    def gen(error: bool):\n        yield 1\n        yield 2\n        if error:\n            raise RuntimeError('error')\n        yield 3\n    v = SchemaValidator(cs.list_schema(items_schema=cs.int_schema()))\n    assert v.validate_python(gen(False)) == [1, 2, 3]\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python(gen(True))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'iteration_error',\n            'loc': (2,),\n            'msg': 'Error iterating over object, error: RuntimeError: error',\n            'input': HasRepr(IsStr(regex='<generator object test_generator_error.<locals>.gen at 0x[0-9a-fA-F]+>')),\n            'ctx': {'error': 'RuntimeError: error'},\n        }\n    ]", "target": "def lazy_setup_lint(ctx, parent_callback, **kwargs):\n    if hashes := _updated_hashes(*LINTRUNNER_CACHE_INFO):\n        click.echo(\n            \"Changes detected in lint configuration files. Setting up linting tools...\"\n        )\n        parent_callback(**kwargs)\n        hash_file = LINTRUNNER_CACHE_INFO[0]\n        hash_file.parent.mkdir(parents=True, exist_ok=True)\n        with hash_file.open(\"w\") as f:\n            for file, hash in hashes.items():\n                f.write(f\"{hash}  {file}\\n\")\n        click.echo(\"Linting tools set up and hashes updated.\")\n    else:\n        click.echo(\"No changes detected in lint configuration files. Skipping setup.\")\n    click.echo(\"Regenerating version...\")\n    ctx.invoke(regenerate_version)\n    click.echo(\"Regenerating type stubs...\")\n    ctx.invoke(regenerate_type_stubs)\n    click.echo(\"Done.\")\n    _check_linters()", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001301", "source": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "target": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001302", "source": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "target": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001303", "source": "def type_format(self) -> str:\n        return \"\"", "target": "def type_format(self) -> str:\n        return \"_typing.Type[{}]\"", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001304", "source": "def test_repeat_after():\n    with pytest.raises(SchemaError, match='SchemaError: Duplicate ref: `foobar`'):\n        SchemaSerializer(\n            core_schema.definitions_schema(\n                core_schema.tuple_positional_schema(\n                    [\n                        core_schema.definitions_schema(\n                            core_schema.definition_reference_schema('foobar'), [core_schema.int_schema(ref='foobar')]\n                        ),\n                        core_schema.definition_reference_schema('foobar'),\n                    ]\n                ),\n                [core_schema.int_schema(ref='foobar')],\n            )\n        )", "target": "def test_repeat_after():\n    with pytest.raises(SchemaError, match='SchemaError: Duplicate ref: `foobar`'):\n        SchemaValidator(\n            schema=core_schema.definitions_schema(\n                core_schema.tuple_positional_schema(\n                    [\n                        core_schema.definitions_schema(\n                            core_schema.definition_reference_schema('foobar'), [core_schema.int_schema(ref='foobar')]\n                        ),\n                        core_schema.definition_reference_schema('foobar'),\n                    ]\n                ),\n                [core_schema.int_schema(ref='foobar')],\n            )\n        )", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|opencv/opencv", "data_split": "train", "example_id": "001305", "source": "def tearDown(self):\n        import os\n        import shutil\n        import tempfile\n        try:\n            os.chdir(self._cwd_backup)\n        except OSError:\n            os.chdir(tempfile.gettempdir())\n        try:\n            shutil.rmtree(self._temp_dir, ignore_errors=True)\n        except Exception:\n            pass\n        to_del = set(os.environ.keys()) - set(self._env_backup.keys())\n        for k in to_del:\n            os.environ.pop(k, None)\n        for k, v in self._env_backup.items():\n            os.environ[k] = v", "target": "def import_path():\n    import sys\n    if sys.version_info[0] < 3 or sys.version_info[1] < 6:\n        raise unittest.SkipTest('Python 3.6+ required')\n    from pathlib import Path\n    return Path", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001306", "source": "def get_memory_bytes(self, args, kwargs) -> int:\n        x, w = args\n        M, N = x.shape\n        return 2 * M * N * x.dtype.itemsize + N * w.dtype.itemsize", "target": "def get_memory_bytes(self, args, kwargs) -> int:\n        x, w = args\n        M, N = x.shape\n        return 2 * M * N * x.dtype.itemsize + N * w.dtype.itemsize", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pytorch/pytorch", "data_split": "train", "example_id": "001307", "source": "def _find_argument_index(arguments: Sequence[FunctionNode.Arg],\n                         name: str) -> Optional[int]:\n    for i, arg in enumerate(arguments):\n        if arg.name == name:\n            return i\n    return None", "target": "def compute_speedups(args, models, example_inputs):\n    expected = models[0](*example_inputs)\n    for model in models[1:]:\n        actual = model(*example_inputs)\n        assert same(actual, expected), expected[0] - actual[0]\n    timings = np.zeros((args.repeat, len(models)), np.float64)\n    for rep in range(args.repeat):\n        for m, model in enumerate(models):\n            timings[rep, m] = timed(model, example_inputs)\n    median = np.median(timings, axis=0)\n    return (median[0] / median[1:]).tolist()", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001308", "source": "def liger(self, args, kwargs=None) -> Any:\n        from liger_kernel.transformers.softmax import LigerSoftmax\n        assert kwargs is None\n        x, dy = args\n        softmax = LigerSoftmax().to(\"cuda\")\n        y = softmax(x)\n        return lambda: torch.autograd.grad(y, x, grad_outputs=dy, retain_graph=True)", "target": "def liger(self, args, kwargs) -> Any:\n        from liger_kernel.transformers.rms_norm import LigerRMSNorm\n        x, w = args\n        M, N = x.shape\n        liger_rmsnorm = LigerRMSNorm(hidden_size=N, eps=1e-6).cuda()\n        liger_rmsnorm.weight.data.copy_(w)\n        return lambda: liger_rmsnorm(x)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001309", "source": "def set_output(name: str, val: Any) -> None:\n    print(f\"Setting output {name}={val}\")\n    if os.getenv(\"GITHUB_OUTPUT\"):\n        with open(str(os.getenv(\"GITHUB_OUTPUT\")), \"a\") as env:\n            print(f\"{name}={val}\", file=env)\n    else:\n        print(f\"::set-output name={name}::{val}\")", "target": "def set_output(name: str, val: str) -> None:\n    print(f\"Setting output {name}={val}\")\n    if os.getenv(\"GITHUB_OUTPUT\"):\n        with open(str(os.getenv(\"GITHUB_OUTPUT\")), \"a\") as env:\n            print(f\"{name}={val}\", file=env)\n    else:\n        print(f\"::set-output name={name}::{val}\")", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001310", "source": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "target": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|opencv/opencv", "data_split": "train", "example_id": "001311", "source": "def test_charuco_detector_accuracy(self):\n        iteration = 0\n        cameraMatrix = np.eye(3, 3, dtype=np.float64)\n        imgSize = (500, 500)\n        params = cv.aruco.DetectorParameters()\n        params.minDistanceToBorder = 3\n        board = cv.aruco.CharucoBoard((4, 4), 0.03, 0.015, cv.aruco.getPredefinedDictionary(cv.aruco.DICT_6X6_250))\n        detector = cv.aruco.CharucoDetector(board, detectorParams=params)\n        cameraMatrix[0, 0] = cameraMatrix[1, 1] = 600\n        cameraMatrix[0, 2] = imgSize[0] / 2\n        cameraMatrix[1, 2] = imgSize[1] / 2\n        distCoeffs = np.zeros((5, 1), dtype=np.float64)\n        for distance in [0.2, 0.4]:\n            for yaw in range(-55, 51, 25):\n                for pitch in range(-55, 51, 25):\n                    markerBorder = iteration % 2 + 1\n                    iteration += 1\n                    img, rvec, tvec = projectCharucoBoard(board, cameraMatrix, yaw * pi / 180, pitch * pi / 180, distance, imgSize, markerBorder)\n                    params.markerBorderBits = markerBorder\n                    detector.setDetectorParameters(params)\n                    if (iteration % 2 != 0):\n                        charucoParameters = cv.aruco.CharucoParameters()\n                        charucoParameters.cameraMatrix = cameraMatrix\n                        charucoParameters.distCoeffs = distCoeffs\n                        detector.setCharucoParameters(charucoParameters)\n                    charucoCorners, charucoIds, corners, ids = detector.detectBoard(img)\n                    self.assertGreater(len(ids), 0)\n                    copyChessboardCorners = board.getChessboardCorners()\n                    copyChessboardCorners -= np.array(board.getRightBottomCorner()) / 2\n                    projectedCharucoCorners, _ = cv.projectPoints(copyChessboardCorners, rvec, tvec, cameraMatrix, distCoeffs)\n                    if charucoIds is None:\n                        self.assertEqual(iteration, 46)\n                        continue\n                    for i in range(len(charucoIds)):\n                        currentId = charucoIds[i]\n                        self.assertLess(currentId, len(board.getChessboardCorners()))\n                        reprErr = cv.norm(charucoCorners[i] - projectedCharucoCorners[currentId])\n                        self.assertLessEqual(reprErr, 5)", "target": "def test_write_read_dictionary(self):\n        try:\n            aruco_dict = cv.aruco.getPredefinedDictionary(cv.aruco.DICT_5X5_50)\n            markers_gold = aruco_dict.bytesList\n            fd, filename = tempfile.mkstemp(prefix=\"opencv_python_aruco_dict_\", suffix=\".yml\")\n            os.close(fd)\n            fs_write = cv.FileStorage(filename, cv.FileStorage_WRITE)\n            aruco_dict.writeDictionary(fs_write)\n            fs_write.release()\n            aruco_dict = cv.aruco.getPredefinedDictionary(cv.aruco.DICT_6X6_250)\n            fs_read = cv.FileStorage(filename, cv.FileStorage_READ)\n            aruco_dict.readDictionary(fs_read.root())\n            fs_read.release()\n            self.assertEqual(aruco_dict.markerSize, 5)\n            self.assertEqual(aruco_dict.maxCorrectionBits, 3)\n            np.testing.assert_array_equal(aruco_dict.bytesList, markers_gold)\n        finally:\n            if os.path.exists(filename):\n                os.remove(filename)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001312", "source": "def constrain(self: _Pipeline[_InT, _NewOutLt], constraint: annotated_types.Lt) -> _Pipeline[_InT, _NewOutLt]: ...", "target": "def constrain(\n        self: _Pipeline[_InT, _NewOutLen], constraint: annotated_types.Len\n    ) -> _Pipeline[_InT, _NewOutLen]: ...", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001313", "source": "def set_model_mocks(cls: type[BaseModel], undefined_name: str = 'all referenced types') -> None:\n    undefined_type_error_message = (\n        f'`{cls.__name__}` is not fully defined; you should define {undefined_name},'\n        f' then call `{cls.__name__}.model_rebuild()`.'\n    )\n    def attempt_rebuild_fn(attr_fn: Callable[[type[BaseModel]], T]) -> Callable[[], T | None]:\n        def handler() -> T | None:\n            if cls.model_rebuild(raise_errors=False, _parent_namespace_depth=5) is not False:\n                return attr_fn(cls)\n            return None\n        return handler\n    cls.__pydantic_core_schema__ = MockCoreSchema(\n        undefined_type_error_message,\n        code='class-not-fully-defined',\n        attempt_rebuild=attempt_rebuild_fn(lambda c: c.__pydantic_core_schema__),\n    )\n    cls.__pydantic_validator__ = MockValSer(\n        undefined_type_error_message,\n        code='class-not-fully-defined',\n        val_or_ser='validator',\n        attempt_rebuild=attempt_rebuild_fn(lambda c: c.__pydantic_validator__),\n    )\n    cls.__pydantic_serializer__ = MockValSer(\n        undefined_type_error_message,\n        code='class-not-fully-defined',\n        val_or_ser='serializer',\n        attempt_rebuild=attempt_rebuild_fn(lambda c: c.__pydantic_serializer__),\n    )", "target": "def args(*args, **kwargs):\n    return args, kwargs", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001314", "source": "def test_include():\n    v = SchemaSerializer(\n        core_schema.generator_schema(\n            core_schema.any_schema(), serialization=core_schema.filter_seq_schema(include={1, 3, 5})\n        )\n    )\n    assert v.to_python(gen_ok(0, 1, 2, 3), mode='json') == [1, 3]\n    assert list(v.to_python(gen_ok(0, 1, 2, 3))) == [1, 3]\n    assert v.to_python(gen_ok('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'), mode='json') == ['b', 'd', 'f']\n    assert v.to_python(gen_ok('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'), mode='json') == ['b', 'd', 'f']\n    assert v.to_json(gen_ok('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h')) == b'[\"b\",\"d\",\"f\"]'\n    assert v.to_python(gen_ok('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'), include={6}, mode='json') == ['b', 'd', 'f', 'g']\n    assert list(v.to_python(gen_ok('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'), include={6})) == ['b', 'd', 'f', 'g']\n    assert v.to_json(gen_ok('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'), include={6}) == b'[\"b\",\"d\",\"f\",\"g\"]'\n    assert v.to_python(gen_ok('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'), include={6: None}, mode='json') == [\n        'b',\n        'd',\n        'f',\n        'g',\n    ]\n    with pytest.raises(ValueError, match='Negative indices cannot be used to exclude items on unsized iterables'):\n        v.to_python(gen_ok('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'), include={-1: None, -2: None}, mode='json')\n    v.to_python(gen_ok('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'), include={'__all__': None}, mode='json')", "target": "def test_include(schema_func, seq_f):\n    v = SchemaSerializer(\n        schema_func(core_schema.any_schema(), serialization=core_schema.filter_seq_schema(include={1, 3, 5}))\n    )\n    assert v.to_python(seq_f(0, 1, 2, 3)) == seq_f(1, 3)\n    assert v.to_python(seq_f('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h')) == seq_f('b', 'd', 'f')\n    assert v.to_python(seq_f('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'), mode='json') == ['b', 'd', 'f']\n    assert v.to_json(seq_f('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h')) == b'[\"b\",\"d\",\"f\"]'\n    assert v.to_python(seq_f('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'), include={6}) == seq_f('b', 'd', 'f', 'g')\n    assert v.to_python(seq_f('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'), include=[6]) == seq_f('b', 'd', 'f', 'g')\n    assert v.to_json(seq_f('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'), include={6}) == b'[\"b\",\"d\",\"f\",\"g\"]'\n    assert v.to_python(seq_f('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'), include={6: None}) == seq_f('b', 'd', 'f', 'g')\n    assert v.to_python(seq_f('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'), include={-1: None, -2: None}, mode='json') == [\n        'b',\n        'd',\n        'f',\n        'g',\n        'h',\n    ]", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001315", "source": "def compiled(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, w = args\n        torch._dynamo.mark_dynamic(x, 0)\n        compiled_layernorm = torch.compile(\n            self.layernorm_ref, mode=self.compile_mode, fullgraph=True\n        )\n        return lambda: compiled_layernorm(x, w, eps=1e-6)", "target": "def compiled(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, w, dy = args\n        compiled_layernorm = torch.compile(\n            self.layernorm_ref, mode=self.compile_mode, fullgraph=True\n        )\n        y = compiled_layernorm(x, w)\n        return lambda: torch.autograd.grad(\n            y, [x, w], grad_outputs=dy, retain_graph=True\n        )", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|pydantic/pydantic", "data_split": "train", "example_id": "001316", "source": "def benchmark(\n    metrics=tuple(v for k, v in sorted(METRICS.items())),\n    formats=tuple(v for k, v in sorted(FORMATS.items())),\n    samples=1000,\n    classes=4,\n    density=0.2,\n    n_times=5,\n):\n    metrics = np.atleast_1d(metrics)\n    samples = np.atleast_1d(samples)\n    classes = np.atleast_1d(classes)\n    density = np.atleast_1d(density)\n    formats = np.atleast_1d(formats)\n    out = np.zeros(\n        (len(metrics), len(formats), len(samples), len(classes), len(density)),\n        dtype=float,\n    )\n    it = itertools.product(samples, classes, density)\n    for i, (s, c, d) in enumerate(it):\n        _, y_true = make_multilabel_classification(\n            n_samples=s, n_features=1, n_classes=c, n_labels=d * c, random_state=42\n        )\n        _, y_pred = make_multilabel_classification(\n            n_samples=s, n_features=1, n_classes=c, n_labels=d * c, random_state=84\n        )\n        for j, f in enumerate(formats):\n            f_true = f(y_true)\n            f_pred = f(y_pred)\n            for k, metric in enumerate(metrics):\n                t = timeit(partial(metric, f_true, f_pred), number=n_times)\n                out[k, j].flat[i] = t\n    return out", "target": "def _apply_step(step: _Step, s: cs.CoreSchema | None, handler: GetCoreSchemaHandler, source_type: Any) -> cs.CoreSchema:\n    if isinstance(step, _ValidateAs):\n        s = _apply_parse(s, step.tp, step.strict, handler, source_type)\n    elif isinstance(step, _ValidateAsDefer):\n        s = _apply_parse(s, step.tp, False, handler, source_type)\n    elif isinstance(step, _Transform):\n        s = _apply_transform(s, step.func, handler)\n    elif isinstance(step, _Constraint):\n        s = _apply_constraint(s, step.constraint)\n    elif isinstance(step, _PipelineOr):\n        s = cs.union_schema([handler(step.left), handler(step.right)])\n    else:\n        assert isinstance(step, _PipelineAnd)\n        s = cs.chain_schema([handler(step.left), handler(step.right)])\n    return s", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001317", "source": "def forward(self, x, y):\n        return (x + y,)", "target": "def forward(self, x):\n        for _ in range(self._n):\n            x = fn9(x)\n        return x", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001318", "source": "def get_output(self, input_blob):\n        return super(DnnTfInceptionModel, self).get_output(input_blob)[..., 1:1001]", "target": "def get_output(self, input_blob):\n        self.net.setInput(input_blob, self.in_blob_name)\n        return self.net.forward(self.out_blob_name)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001319", "source": "def test_float_no_remainder():\n    v = SchemaValidator(core_schema.int_schema())\n    assert v.validate_json('123.0') == 123", "target": "def test_extra_behavior_allow_keys_validation() -> None:\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            {}, extra_behavior='allow', extras_keys_schema=core_schema.str_schema(max_length=3)\n        )\n    )\n    m, model_extra, fields_set = v.validate_python({'ext': 123})\n    assert m == {}\n    assert model_extra == {'ext': 123}\n    assert fields_set == {'ext'}\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python({'extra_too_long': 123})\n    assert exc_info.value.errors()[0]['type'] == 'string_too_long'", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001320", "source": "def test_on_error_raise_by_default(self, py_and_json: PyAndJson):\n        v = py_and_json({'type': 'model-fields', 'fields': {'x': {'type': 'model-field', 'schema': {'type': 'str'}}}})\n        assert v.validate_test({'x': 'foo'}) == ({'x': 'foo'}, None, {'x'})\n        with pytest.raises(ValidationError) as exc_info:\n            v.validate_test({'x': ['foo']})\n        assert exc_info.value.errors(include_url=False) == [\n            {'input': ['foo'], 'type': 'string_type', 'loc': ('x',), 'msg': 'Input should be a valid string'}\n        ]", "target": "def test_on_error_raise_by_default(self, py_and_json: PyAndJson):\n        v = py_and_json(\n            {'type': 'typed-dict', 'fields': {'x': {'type': 'typed-dict-field', 'schema': {'type': 'str'}}}}\n        )\n        assert v.validate_test({'x': 'foo'}) == {'x': 'foo'}\n        with pytest.raises(ValidationError) as exc_info:\n            v.validate_test({'x': ['foo']})\n        assert exc_info.value.errors(include_url=False) == [\n            {'input': ['foo'], 'type': 'string_type', 'loc': ('x',), 'msg': 'Input should be a valid string'}\n        ]", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001321", "source": "def test_string_with_underscores() -> None:\n    v = SchemaValidator(cs.float_schema())\n    assert v.validate_python('1_000_000.0') == 1_000_000.0\n    assert v.validate_json('\"1_000_000.0\"') == 1_000_000.0\n    for edge_case in ('_1', '_1.0', '1__0', '1.1__1', '1_0.0_', '1._', '1_0__0.0'):\n        with pytest.raises(ValidationError):\n            v.validate_python(edge_case)\n        with pytest.raises(ValidationError):\n            v.validate_json(f'\"{edge_case}\"')", "target": "def dec(f: FieldSerializer) -> _decorators.PydanticDescriptorProxy[Any]:\n        dec_info = _decorators.FieldSerializerDecoratorInfo(\n            fields=fields,\n            mode=mode,\n            return_type=return_type,\n            when_used=when_used,\n            check_fields=check_fields,\n        )\n        return _decorators.PydanticDescriptorProxy(f, dec_info)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|opencv/opencv", "data_split": "train", "example_id": "001322", "source": "def scale(x, name):\n    with tf.variable_scope(name):\n        layer = dnnLayer(name)\n        w = tf.Variable(layer.blobs[0].flatten(), dtype=dtype, name='mul')\n        if len(layer.blobs) > 1:\n            b = tf.Variable(layer.blobs[1].flatten(), dtype=dtype, name='add')\n            return tf.nn.bias_add(tf.multiply(x, w), b)\n        else:\n            return tf.multiply(x, w, name)", "target": "def typename(self) -> str:\n        return self.alias_export_name", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001323", "source": "def predict(self, samples):\n        _retval, results, _neigh_resp, _dists = self.model.findNearest(samples, self.k)\n        return results.ravel()", "target": "def predict(self, samples):\n        new_samples = self.unroll_samples(samples)\n        _ret, resp = self.model.predict(new_samples)\n        return resp.ravel().reshape(-1, self.class_n).argmax(1)", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "001324", "source": "def make_estimator(self, params):\n        (representation,) = params\n        max_iter = 60 if representation == \"dense\" else 300\n        estimator = SGDRegressor(max_iter=max_iter, tol=None, random_state=0)\n        return estimator", "target": "def make_estimator(self, params):\n        representation, precompute = params\n        estimator = Lasso(precompute=precompute, alpha=0.001, random_state=0)\n        return estimator", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001325", "source": "def test_long_json():\n    v = SchemaValidator(cs.int_schema())\n    assert v.validate_json('-' + '1' * 400) == int('-' + '1' * 400)\n    with pytest.raises(ValidationError, match=r'expected ident at line 1 column 2 \\[type=json_invalid,'):\n        v.validate_json('nan')", "target": "def to_argument(\n        self,\n        current_info: TypeInfo,\n        typed: bool,\n        model_strict: bool,\n        force_optional: bool,\n        use_alias: bool,\n        api: SemanticAnalyzerPluginInterface,\n        force_typevars_invariant: bool,\n        is_root_model_root: bool,\n    ) -> Argument:\n        variable = self.to_var(current_info, api, use_alias, force_typevars_invariant)\n        strict = model_strict if self.strict is None else self.strict\n        if typed or strict:\n            type_annotation = self.expand_type(current_info, api, include_root_type=True)\n        else:\n            type_annotation = AnyType(TypeOfAny.explicit)\n        return Argument(\n            variable=variable,\n            type_annotation=type_annotation,\n            initializer=None,\n            kind=ARG_OPT\n            if is_root_model_root\n            else (ARG_NAMED_OPT if force_optional or self.has_default else ARG_NAMED),\n        )", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001326", "source": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "target": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001327", "source": "def typename(self) -> str:\n        return self._typename", "target": "def typename(self) -> str:\n        return self.alias_export_name", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|opencv/opencv", "data_split": "train", "example_id": "001328", "source": "def make_roi(self, img, roi):\n            return img[roi[1]:roi[1] + roi[3], roi[0]:roi[0] + roi[2], ...]", "target": "def test_kmeans_2d(self):\n            count     = 100\n            sz        = (count, 2)\n            amount    = sz[0]\n            K         = 5\n            flags     = cv.KMEANS_RANDOM_CENTERS\n            attempts  = 1\n            criteria  = (cv.TERM_CRITERIA_MAX_ITER + cv.TERM_CRITERIA_EPS, 30, 0)\n            in_vector = self.generate_random_points(sz)\n            in_labels = []\n            data        = cv.GArrayT(cv.gapi.CV_POINT2F)\n            best_labels = cv.GArrayT(cv.gapi.CV_INT)\n            compactness, out_labels, centers = cv.gapi.kmeans(data, K, best_labels, criteria, attempts, flags)\n            comp = cv.GComputation(cv.GIn(data, best_labels), cv.GOut(compactness, out_labels, centers))\n            compact, labels, centers = comp.apply(cv.gin(in_vector, in_labels))\n            self.assertTrue(compact >= 0)\n            self.assertEqual(amount, len(labels))\n            self.assertEqual(K, len(centers))", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001329", "source": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "target": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "001330", "source": "def download_pytest_cache(\n    pr_identifier: PRIdentifier,\n    repo: GithubRepo,\n    job_identifier: str,\n    dest_cache_dir: Path,\n    temp_dir: Path,\n    bucket: str = BUCKET,\n) -> None:\n    if not bucket:\n        bucket = BUCKET\n    if not isinstance(pr_identifier, PRIdentifier):\n        raise ValueError(\n            f\"pr_identifier must be of type PRIdentifier, not {type(pr_identifier)}\"\n        )\n    obj_key_prefix = _get_s3_key_prefix(pr_identifier, repo, job_identifier)\n    zip_download_dir = temp_dir / CACHE_ZIP_DOWNLOADS / obj_key_prefix\n    downloads = download_s3_objects_with_prefix(\n        bucket, obj_key_prefix, zip_download_dir\n    )\n    for downloaded_zip in downloads:\n        cache_dir_for_shard = (\n            temp_dir / UNZIPPED_CACHES / os.urandom(16).hex() / PYTEST_CACHE_DIR_NAME\n        )\n        unzip_folder(downloaded_zip, cache_dir_for_shard)\n        print(f\"Merging cache from {downloaded_zip}\")\n        _merge_pytest_caches(cache_dir_for_shard, dest_cache_dir)", "target": "def test_json_none():\n    v = SchemaValidator(cs.none_schema())\n    assert v.validate_json('null') is None\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_json('1')\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'none_required', 'loc': (), 'msg': 'Input should be null', 'input': 1}\n    ]", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "001331", "source": "def test_houghcircles(self):\n        fn = \"samples/data/board.jpg\"\n        src = self.get_sample(fn, 1)\n        img = cv.cvtColor(src, cv.COLOR_BGR2GRAY)\n        img = cv.medianBlur(img, 5)\n        circles = cv.HoughCircles(img, cv.HOUGH_GRADIENT, 1, 10, np.array([]), 100, 30, 1, 30)[0]\n        testCircles = [[38, 181, 17.6],\n        [99.7, 166, 13.12],\n        [142.7, 160, 13.52],\n        [223.6, 110, 8.62],\n        [79.1, 206.7, 8.62],\n        [47.5, 351.6, 11.64],\n        [189.5, 354.4, 11.64],\n        [189.8, 298.9, 10.64],\n        [189.5, 252.4, 14.62],\n        [252.5, 393.4, 15.62],\n        [602.9, 467.5, 11.42],\n        [222, 210.4, 9.12],\n        [263.1, 216.7, 9.12],\n        [359.8, 222.6, 9.12],\n        [518.9, 120.9, 9.12],\n        [413.8, 113.4, 9.12],\n        [489, 127.2, 9.12],\n        [448.4, 121.3, 9.12],\n        [384.6, 128.9, 8.62]]\n        matches_counter = 0\n        for i in range(len(testCircles)):\n            for j in range(len(circles)):\n                tstCircle = circleApproximation(testCircles[i])\n                circle = circleApproximation(circles[j])\n                if convContoursIntersectiponRate(tstCircle, circle) > 0.6:\n                    matches_counter += 1\n        self.assertGreater(float(matches_counter) / len(testCircles), .5)\n        self.assertLess(float(len(circles) - matches_counter) / len(circles), .75)\n        circles_acc = cv.HoughCirclesWithAccumulator(\n            image=img,\n            method=cv.HOUGH_GRADIENT,\n            dp=1,\n            minDist=10,\n            circles=np.array([]),\n            param1=150,\n            param2=45,\n            minRadius=1,\n            maxRadius=30)\n        self.assertEqual(circles_acc.shape, (1, 2, 4))\n        self.assertEqual(circles_acc[0, 0, 3], 66.)\n        self.assertEqual(circles_acc[0, 1, 3], 62.)", "target": "def test_defs_with_dict():\n    s = SchemaSerializer(\n        core_schema.definitions_schema(\n            schema=core_schema.typed_dict_schema(\n                {\n                    'foo': core_schema.typed_dict_field(\n                        core_schema.dict_schema(\n                            keys_schema=core_schema.definition_reference_schema('key'),\n                            values_schema=core_schema.definition_reference_schema('val'),\n                        )\n                    )\n                }\n            ),\n            definitions=[core_schema.str_schema(ref='key'), core_schema.str_schema(ref='val')],\n        )\n    )\n    assert s.to_json({'foo': {'key': 'val'}}) == b'{\"foo\":{\"key\":\"val\"}}'\n    assert s.to_python({'foo': {'key': 'val'}}) == {'foo': {'key': 'val'}}", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "001332", "source": "def test_filter_args_nested(params):\n    s = SchemaSerializer(core_schema.dict_schema(core_schema.str_schema(), core_schema.list_schema()))\n    include, exclude, expected = params['include'], params['exclude'], params['expected']\n    value = {'0': [0], '1': [0, 1], '2': [0, 1, 2], '3': [0, 1, 2, 3]}\n    assert s.to_python(value, include=include, exclude=exclude) == expected\n    assert s.to_python(value, mode='json', include=include, exclude=exclude) == expected\n    assert json.loads(s.to_json(value, include=include, exclude=exclude)) == expected", "target": "def regenerate_type_stubs():\n    for name, hash_file, files_to_hash, cmd in TYPE_STUBS:\n        if hash_file:\n            if hashes := _updated_hashes(hash_file, files_to_hash):\n                click.echo(\n                    f\"Changes detected in type stub files for {name}. Regenerating...\"\n                )\n                spin.util.run(cmd)\n                hash_file.parent.mkdir(parents=True, exist_ok=True)\n                with hash_file.open(\"w\") as f:\n                    for file, hash in hashes.items():\n                        f.write(f\"{hash}  {file}\\n\")\n                click.echo(\"Type stubs and hashes updated.\")\n            else:\n                click.echo(f\"No changes detected in type stub files for {name}.\")\n        else:\n            click.echo(f\"No hash file for {name}. Regenerating...\")\n            spin.util.run(cmd)\n            click.echo(\"Type stubs regenerated.\")", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001333", "source": "def test_constraints_schema_validation_error(constraint: str) -> None:\n    with pytest.raises(SchemaError, match=f\"'{constraint}' must be coercible to a datetime instance\"):\n        SchemaValidator(cs.datetime_schema(**{constraint: 'bad_value'}))", "target": "def test_constraints_schema_validation_error(constraint: str) -> None:\n    with pytest.raises(SchemaError, match=f\"'{constraint}' must be coercible to a time instance\"):\n        SchemaValidator(core_schema.time_schema(**{constraint: 'bad_value'}))", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001334", "source": "def gen_error(*things):\n    yield from things\n    raise ValueError('oops')", "target": "def test_extra_behavior_allow(\n    config: Union[core_schema.CoreConfig, None],\n    schema_extra_behavior_kw: dict[str, Any],\n    extras_schema_kw: dict[str, Any],\n    expected_extra_value: Any,\n):\n    v = SchemaValidator(\n        core_schema.typed_dict_schema(\n            {'f': core_schema.typed_dict_field(core_schema.str_schema())},\n            **schema_extra_behavior_kw,\n            **extras_schema_kw,\n            config=config,\n        )\n    )\n    m: dict[str, Any] = v.validate_python({'f': 'x', 'extra_field': '123'})\n    assert m == {'f': 'x', 'extra_field': expected_extra_value}", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001335", "source": "def f(*args):\n            outs = [torch.add(x, x) for x in args]\n            return outs", "target": "def f(a, b):\n            xs = b.tolist()\n            for x in xs:\n                torch._check(x >= 0)\n                torch._check(x <= self.N)\n            return a.split(xs)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001336", "source": "def test_dataclass_serialization_python(benchmark):\n    s = SchemaSerializer(dataclass_schema)\n    dc = Foo(a='hello', b=b'more', c=123, d=1.23)\n    assert s.to_python(dc) == {'a': 'hello', 'b': b'more', 'c': 123, 'd': 1.23}\n    benchmark(s.to_python, dc)", "target": "def col_values(self, row: Row) -> list[str]:\n        o = self.open_nowrap_span\n        c = self.close_nowrap_span\n        return [\n            f'{o}`{row.field_type_str}`{c}',\n            f'{o}`{row.input_type_str}`{c}',\n            '' if row.strict else '',\n            f'{o}{row.input_source_str}{c}',\n            row.condition if row.condition else '',\n        ]", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "train", "example_id": "001337", "source": "def with_nnc():\n    torch._C._jit_override_can_fuse_on_cpu(True)\n    torch._C._jit_override_can_fuse_on_gpu(True)\n    torch._C._jit_set_texpr_fuser_enabled(True)\n    torch._C._jit_set_nvfuser_enabled(False)\n    torch._C._jit_set_profiling_executor(True)\n    torch._C._jit_set_profiling_mode(True)", "target": "def print_detailed_memory():\n    process = psutil.Process()\n    print(\"\\nDetailed memory information:\")\n    try:\n        print(\n            f\"  USS (Unique Set Size): {process.memory_full_info().uss / (1024 * 1024):.2f} MB\"\n        )\n        print(\n            f\"  PSS (Proportional Set Size): {process.memory_full_info().pss / (1024 * 1024):.2f} MB\"\n        )\n        print(\n            f\"  RSS (Resident Set Size): {process.memory_info().rss / (1024 * 1024):.2f} MB\"\n        )\n    except Exception:\n        print(\"  Detailed memory info not available\")", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001338", "source": "def name(self) -> str:\n        prefix = f\"{self.category()}_{self.backend()}\"\n        if self.is_dynamic():\n            prefix += \"_dynamic\"\n        if self.device() == \"cuda\":\n            prefix += \"_gpu\"\n        return prefix", "target": "def name(self):\n        prefix = f\"{self.category()}_{self.backend()}\"\n        if self._requires_grad:\n            prefix += \"_requires_grad\"\n        if self._inference_mode:\n            prefix += \"_inference_mode\"\n        if self._backward:\n            prefix += \"_backward\"\n        if self.is_dynamic():\n            prefix += \"_dynamic\"\n        return prefix", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001339", "source": "def test_length_ctx():\n    v = SchemaValidator(cs.bytes_schema(min_length=2, max_length=3))\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python(b'1')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'bytes_too_short',\n            'loc': (),\n            'msg': 'Data should have at least 2 bytes',\n            'input': b'1',\n            'ctx': {'min_length': 2},\n        }\n    ]\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python(b'1234')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'bytes_too_long',\n            'loc': (),\n            'msg': 'Data should have at most 3 bytes',\n            'input': b'1234',\n            'ctx': {'max_length': 3},\n        }\n    ]", "target": "def test_url_pickle(value):\n    pickled = pickle.dumps(value)\n    unpickled = pickle.loads(pickled)\n    assert value == unpickled", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001340", "source": "def dynamic_rnn(\n        input: Tensor,\n        hidden: tuple[Tensor, Tensor],\n        w_ih: Tensor,\n        w_hh: Tensor,\n        b_ih: Tensor,\n        b_hh: Tensor,\n    ) -> tuple[Tensor, tuple[Tensor, Tensor]]:\n        hx, cx = hidden\n        outputs = []\n        inpSize = input.size()\n        inpSize = input.size()\n        inputs = torch.mm(input.view(-1, inpSize[2]), w_ih.t()) + b_ih\n        inputs = inputs.view(inpSize[0], inpSize[1], -1).unbind(0)\n        hy, cy = hx[0], cx[0]\n        for seq_idx in range(len(inputs)):\n            hy, cy = premul_cell(inputs[seq_idx], (hy, cy), w_hh, b_hh)\n            outputs += [hy]\n        return torch.stack(outputs), (hy.unsqueeze(0), cy.unsqueeze(0))", "target": "def dynamic_rnn(input, hx, cx, w_ih, w_hh, b_ih, b_hh):\n        hy = hx\n        cy = cx\n        inputs = input.unbind(0)\n        for seq_idx in range(len(inputs)):\n            hy, cy = cell(inputs[seq_idx], hy, cy, w_ih, w_hh, b_ih, b_hh)\n        return hy, cy", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001341", "source": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "target": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001342", "source": "def mode(self) -> Literal['python', 'json'] | str:\n        ...", "target": "def mode(self) -> Literal['python', 'json']:\n        ...", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001343", "source": "def compiled(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, dy = args\n        compiled_softmax = torch.compile(\n            lambda x: F.softmax(x, dim=-1), mode=self.compile_mode, fullgraph=True\n        )\n        y = compiled_softmax(x)\n        return lambda: torch.autograd.grad(y, x, grad_outputs=dy, retain_graph=True)", "target": "def compiled(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, w = args\n        torch._dynamo.mark_dynamic(x, 0)\n        compiled_layernorm = torch.compile(\n            self.layernorm_ref, mode=self.compile_mode, fullgraph=True\n        )\n        return lambda: compiled_layernorm(x, w, eps=1e-6)", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "001344", "source": "def deskew(img):\n    m = cv.moments(img)\n    if abs(m['mu02']) < 1e-2:\n        return img.copy()\n    skew = m['mu11']/m['mu02']\n    M = np.float32([[1, skew, -0.5*SZ*skew], [0, 1, 0]])\n    img = cv.warpAffine(img, M, (SZ, SZ), flags=cv.WARP_INVERSE_MAP | cv.INTER_LINEAR)\n    return img", "target": "def test_empty_choices():\n    msg = r'Error building \"union\" validator:\\s+SchemaError: One or more union choices required'\n    with pytest.raises(SchemaError, match=msg):\n        SchemaValidator(core_schema.union_schema(choices=[]))", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001345", "source": "def ser_x(data: Model, v: Any, info: core_schema.FieldSerializationInfo) -> str:\n        assert data['x'] == 1_000\n        return f'{v:_}-{info.field_name}'", "target": "def ser_x(data: Model, v: Any, serializer: core_schema.SerializerFunctionWrapHandler) -> str:\n        assert data['x'] == 1_000\n        x = serializer(v)\n        return f'{x:_}'", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "001346", "source": "def test_int_array(self):\n        a = np.array([-1, 2, -3, 4, -5])\n        absa0 = np.abs(a)\n        self.assertTrue(cv.norm(a, cv.NORM_L1) == 15)\n        absa1 = cv.absdiff(a, 0)\n        self.assertEqual(cv.norm(absa1, absa0, cv.NORM_INF), 0)", "target": "def update_not_none(mapping: dict[Any, Any], **update: Any) -> None:\n    mapping.update({k: v for k, v in update.items() if v is not None})", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "001347", "source": "def test_gh_get_labels_raises_with_no_pages(\n        self,\n        mock_request_for_labels: Any,\n        get_last_page_num_from_header: Any,\n    ) -> None:\n        with self.assertRaises(AssertionError) as err:\n            gh_get_labels(\"foo\", \"bar\")\n        self.assertIn(\"number of pages of labels\", str(err.exception))", "target": "def test_filter_args(params):\n    s = SchemaSerializer(core_schema.list_schema())\n    include, exclude, expected = params['include'], params['exclude'], params['expected']\n    value = ['0', '1', '2', '3']\n    assert s.to_python(value, include=include, exclude=exclude) == expected\n    assert s.to_python(value, mode='json', include=include, exclude=exclude) == expected\n    assert json.loads(s.to_json(value, include=include, exclude=exclude)) == expected", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "001348", "source": "def parse_obj_as(type_: type[T], obj: Any, type_name: NameFactory | None = None) -> T:\n    warnings.warn(\n        '`parse_obj_as` is deprecated. Use `pydantic.TypeAdapter.validate_python` instead.',\n        category=PydanticDeprecatedSince20,\n        stacklevel=2,\n    )\n    if type_name is not None:\n        warnings.warn(\n            'The type_name parameter is deprecated. parse_obj_as no longer creates temporary models',\n            DeprecationWarning,\n            stacklevel=2,\n        )\n    return TypeAdapter(type_).validate_python(obj)", "target": "def _norm(self, x):\n        return x * torch.rsqrt(torch.mean(x * x, dim=-1, keepdim=True) + self.eps)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001349", "source": "def get_output(self, input_blob):\n        self.net.setBlob(\"\", input_blob)\n        self.net.forward()\n        return self.net.getBlob(self.net.getLayerNames()[-1])", "target": "def get_output(self, input_blob):\n        self.net.setInput(input_blob, self.in_blob_name)\n        return self.net.forward(self.out_blob_name)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001350", "source": "def test_timedelta_key():\n    v = SchemaSerializer(core_schema.dict_schema(core_schema.timedelta_schema(), core_schema.int_schema()))\n    assert v.to_python({timedelta(days=2, hours=3, minutes=4): 1}) == {timedelta(days=2, hours=3, minutes=4): 1}\n    assert v.to_python({timedelta(days=2, hours=3, minutes=4): 1}, mode='json') == {'P2DT3H4M': 1}\n    assert v.to_json({timedelta(days=2, hours=3, minutes=4): 1}) == b'{\"P2DT3H4M\":1}'", "target": "def test_complete_core_error(benchmark):\n    v = SchemaValidator(schema())\n    data = input_data_wrong()\n    @benchmark\n    def f():\n        try:\n            v.validate_python(data)\n        except ValueError:\n            pass\n        else:\n            raise RuntimeError('expected ValueError')", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001351", "source": "def typename(self) -> Optional[str]:\n            return getattr(self.type_node, \"full_typename\", None)", "target": "def typename(self) -> str:\n            return self.type_node.full_typename", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001352", "source": "def test_union():\n    v = SchemaValidator(cs.union_schema(choices=[cs.str_schema(strict=True), cs.bytes_schema(strict=True)]))\n    assert v.validate_python('oh, a string') == 'oh, a string'\n    assert v.validate_python(b'oh, bytes') == b'oh, bytes'", "target": "def test_union():\n    v = SchemaValidator(cs.union_schema(choices=[cs.str_schema(), cs.datetime_schema()]))\n    assert v.validate_python('2022-01-02T00:00') == '2022-01-02T00:00'\n    assert v.validate_python(datetime(2022, 1, 2)) == datetime(2022, 1, 2)\n    v = SchemaValidator(cs.union_schema(choices=[cs.datetime_schema(), cs.str_schema()]))\n    assert v.validate_python('2022-01-02T00:00') == '2022-01-02T00:00'\n    assert v.validate_python(datetime(2022, 1, 2)) == datetime(2022, 1, 2)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "001353", "source": "def test_dict_list():\n    v = SchemaValidator(\n        core_schema.dict_schema(core_schema.int_schema(), core_schema.list_schema(core_schema.int_schema(ge=10)))\n    )\n    assert v.validate_python({'1': [20, 30], 3: [40, '50']}, allow_partial=True) == snapshot({1: [20, 30], 3: [40, 50]})\n    assert v.validate_python({'1': [20, 30], 3: [40, 5]}, allow_partial=True) == snapshot({1: [20, 30], 3: [40]})\n    with pytest.raises(ValidationError, match=r'1\\.1\\s+Input should be greater than or equal to 10'):\n        v.validate_python({'1': [20, 3], 3: [40, 50]}, allow_partial=True)", "target": "def test_cuda_upload_download(self):\n        npMat = (np.random.random((128, 128, 3)) * 255).astype(np.uint8)\n        cuMat = cv.cuda_GpuMat()\n        cuMat.upload(npMat)\n        self.assertTrue(np.allclose(cuMat.download(), npMat))", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|scikit-learn/scikit-learn", "data_split": "train", "example_id": "001354", "source": "def get_xcode_major():\n    ret = check_output([\"xcodebuild\", \"-version\"]).decode('utf-8')\n    m = re.match(r'Xcode\\s+(\\d+)\\..*', ret, flags=re.IGNORECASE)\n    if m:\n        return int(m.group(1))\n    else:\n        raise Exception(\"Failed to parse Xcode version\")", "target": "def _get_guide(*refs, is_developer=False):\n    if len(refs) == 1:\n        ref_desc = f\":ref:`{refs[0]}` section\"\n    elif len(refs) == 2:\n        ref_desc = f\":ref:`{refs[0]}` and :ref:`{refs[1]}` sections\"\n    else:\n        ref_desc = \", \".join(f\":ref:`{ref}`\" for ref in refs[:-1])\n        ref_desc += f\", and :ref:`{refs[-1]}` sections\"\n    guide_name = \"Developer\" if is_developer else \"User\"\n    return f\"**{guide_name} guide.** See the {ref_desc} for further details.\"", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001355", "source": "def test_other_literal():\n    s = SchemaSerializer(core_schema.literal_schema(['a', 1]))\n    assert 'expected_int:{1},expected_str:{\"a\"},expected_py:None' in plain_repr(s)\n    assert s.to_python('a') == 'a'\n    assert s.to_python('a', mode='json') == 'a'\n    assert s.to_python('not in literal') == 'not in literal'\n    assert s.to_json('a') == b'\"a\"'\n    assert s.to_python(1) == 1\n    assert s.to_python(1, mode='json') == 1\n    assert s.to_python(44) == 44\n    assert s.to_json(1) == b'1'", "target": "def test_string_as_int_with_underscores() -> None:\n    v = SchemaValidator(cs.int_schema())\n    assert v.validate_python('1_000_000') == 1_000_000\n    assert v.validate_json('\"1_000_000\"') == 1_000_000\n    for edge_case in ('_1', '1__0', '1_0_', '1_0__0'):\n        with pytest.raises(ValidationError):\n            v.validate_python(edge_case)\n        with pytest.raises(ValidationError):\n            v.validate_json(f'\"{edge_case}\"')", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001356", "source": "def foo(x: torch.Tensor) -> torch.Tensor:\n        return torch.sin(x) + torch.cos(x)", "target": "def foo(x: int, y: int) -> int:\n            return x + y", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "001357", "source": "def scaleMask(self, mask):\n        return np.where((mask==cv.GC_FGD) + (mask==cv.GC_PR_FGD),255,0).astype('uint8')", "target": "def test_internal_error(py_and_json: PyAndJson) -> None:\n    v = py_and_json(\n        cs.arguments_v3_schema(\n            [\n                cs.arguments_v3_parameter(name='a', schema=cs.int_schema(), mode='positional_only'),\n                cs.arguments_v3_parameter(\n                    name='b', schema=cs.no_info_plain_validator_function(double_or_bust), mode='positional_only'\n                ),\n            ]\n        )\n    )\n    assert v.validate_test(ArgsKwargs((1, 2))) == ((1, 4), {})\n    with pytest.raises(RuntimeError, match='bust'):\n        v.validate_test(ArgsKwargs((1, 1)))", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001358", "source": "def test_keyword_only(py_and_json: PyAndJson, input_value) -> None:\n    v = py_and_json(\n        cs.arguments_v3_schema(\n            [\n                cs.arguments_v3_parameter(name='a', schema=cs.int_schema(), mode='keyword_only'),\n                cs.arguments_v3_parameter(\n                    name='b', schema=cs.with_default_schema(cs.bool_schema(), default=True), mode='keyword_only'\n                ),\n            ]\n        )\n    )\n    assert v.validate_test(input_value) == ((), {'a': 1, 'b': True})", "target": "def test_tuple_fail_fast(fail_fast, expected):\n    s = core_schema.tuple_schema(\n        [\n            core_schema.str_schema(),\n            core_schema.int_schema(),\n            core_schema.float_schema(),\n        ],\n        variadic_item_index=None,\n        fail_fast=fail_fast,\n    )\n    v = SchemaValidator(s)\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python(['str', 'not-num', 'again'])\n    assert exc_info.value.errors(include_url=False) == expected", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "001359", "source": "def serialize_deque(value, serializer, info: core_schema.SerializationInfo):\n        items = []\n        for index, item in enumerate(value):\n            try:\n                v = serializer(item, index)\n            except PydanticOmit:\n                pass\n            else:\n                items.append(v)\n        if info.mode_is_json():\n            return items\n        else:\n            return deque(items)", "target": "def execute(cmd, cwd = None):\n    print(\"Executing: %s in %s\" % (cmd, cwd), file=sys.stderr)\n    print('Executing: ' + ' '.join(cmd))\n    retcode = check_call(cmd, cwd = cwd)\n    if retcode != 0:\n        raise Exception(\"Child returned:\", retcode)", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|scikit-learn/scikit-learn", "data_split": "train", "example_id": "001360", "source": "def setup_loading_other_datasets():\n    try:\n        import pandas\n    except ImportError:\n        raise SkipTest(\"Skipping loading_other_datasets.rst, pandas not installed\")\n    run_network_tests = environ.get(\"SKLEARN_SKIP_NETWORK_TESTS\", \"1\") == \"0\"\n    if not run_network_tests:\n        raise SkipTest(\n            \"Skipping loading_other_datasets.rst, tests can be \"\n            \"enabled by setting SKLEARN_SKIP_NETWORK_TESTS=0\"\n        )", "target": "def load_faces():\n    print(\"Loading Olivetti face dataset\")\n    print(\"-----------------------------\")\n    from sklearn.datasets import fetch_olivetti_faces\n    faces = fetch_olivetti_faces(shuffle=True)\n    return faces.data", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001361", "source": "def _work(self):\n        @torch.compile(backend=self.backend(), fullgraph=True)\n        def f(inp, *weights):\n            x = inp\n            for w in weights:\n                x = torch.matmul(w, x).sin().sin()\n            return x\n        f(self.inp, *self.weights)", "target": "def _work(self):\n        @torch.compile(backend=self.backend(), fullgraph=True)\n        def f(x):\n            tmps = [x + i for i in range(16)]\n            tmps = [x + tmp for tmp in tmps]\n            for i in range(len(tmps) - 4):\n                tmps[i] = tmps[i].sin().mul(tmps[i])\n                tmps[i + 1] -= tmps[i]\n                tmps[i + 2] -= tmps[i]\n                tmps[i + 3] -= tmps[i]\n            return sum(tmps)\n        f(self.x)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001362", "source": "def test_non_finite_json_values(py_and_json: PyAndJson, input_value, allow_inf_nan, expected):\n    v = py_and_json({'type': 'decimal', 'allow_inf_nan': allow_inf_nan})\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            v.validate_test(input_value)\n    else:\n        assert v.validate_test(input_value) == expected", "target": "def test_non_finite_json_values(py_and_json: PyAndJson, input_value, allow_inf_nan, expected):\n    v = py_and_json({'type': 'float', 'allow_inf_nan': allow_inf_nan})\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            v.validate_test(input_value)\n    else:\n        assert v.validate_test(input_value) == expected", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "001363", "source": "def test_error_index(py_and_json: PyAndJson):\n    v = py_and_json({'type': 'generator', 'items_schema': {'type': 'int'}})\n    gen = v.validate_test(['wrong'])\n    assert gen.index == 0\n    with pytest.raises(ValidationError) as exc_info:\n        next(gen)\n    assert gen.index == 1\n    assert exc_info.value.title == 'ValidatorIterator'\n    assert str(exc_info.value).startswith('1 validation error for ValidatorIterator\\n')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': (0,),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'wrong',\n        }\n    ]\n    gen = v.validate_test([1, 2, 3, 'wrong', 4])\n    assert gen.index == 0\n    assert next(gen) == 1\n    assert gen.index == 1\n    assert next(gen) == 2\n    assert gen.index == 2\n    assert next(gen) == 3\n    assert gen.index == 3\n    with pytest.raises(ValidationError) as exc_info:\n        next(gen)\n    assert gen.index == 4\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': (3,),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'wrong',\n        }\n    ]\n    assert next(gen) == 4\n    assert gen.index == 5", "target": "def center_crop(self, img):\n        cols = img.shape[1]\n        rows = img.shape[0]\n        y1 = round((rows - self.frame_size) / 2)\n        y2 = round(y1 + self.frame_size)\n        x1 = round((cols - self.frame_size) / 2)\n        x2 = round(x1 + self.frame_size)\n        return img[y1:y2, x1:x2]", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001364", "source": "def test_function_validator_wrapping_args_schema_wrap() -> None:\n    calls: list[Any] = []\n    def func(*args: Any) -> Any:\n        assert len(args) == 2\n        input, handler = args\n        output = handler(input)\n        calls.append((input, output))\n        return output\n    @dataclasses.dataclass\n    class Model:\n        number: int = 1\n    cs = core_schema.dataclass_schema(\n        Model,\n        core_schema.no_info_wrap_validator_function(\n            func,\n            core_schema.dataclass_args_schema(\n                'Model', [core_schema.dataclass_field('number', core_schema.int_schema())]\n            ),\n        ),\n        ['number'],\n    )\n    v = SchemaValidator(cs)\n    instance: Model = v.validate_python({'number': 1})\n    assert instance.number == 1\n    assert calls == [({'number': 1}, ({'number': 1}, None))]\n    v.validate_assignment(instance, 'number', 2)\n    assert instance.number == 2\n    assert calls == [({'number': 1}, ({'number': 1}, None)), ({'number': 2}, ({'number': 2}, None))]", "target": "def unique_list(\n    input_list: list[T] | tuple[T, ...],\n    *,\n    name_factory: Callable[[T], str] = str,\n) -> list[T]:\n    result: list[T] = []\n    result_names: list[str] = []\n    for v in input_list:\n        v_name = name_factory(v)\n        if v_name not in result_names:\n            result_names.append(v_name)\n            result.append(v)\n        else:\n            result[result_names.index(v_name)] = v\n    return result", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|scikit-learn/scikit-learn", "data_split": "train", "example_id": "001365", "source": "def fn():\n        class Model:\n            a: int\n            @classmethod\n            def _validator(cls, v, info):\n                return v\n            @classmethod\n            def _wrap_validator(cls, v, validator, info):\n                return validator(v)\n        field_schema = core_schema.int_schema()\n        if validator == 'field':\n            field_schema = core_schema.with_info_before_validator_function(Model._validator, field_schema)\n            field_schema = core_schema.with_info_wrap_validator_function(Model._wrap_validator, field_schema)\n            field_schema = core_schema.with_info_after_validator_function(Model._validator, field_schema)\n        model_schema = core_schema.model_schema(\n            Model, core_schema.model_fields_schema({'a': core_schema.model_field(field_schema)})\n        )\n        if validator == 'model':\n            model_schema = core_schema.with_info_before_validator_function(Model._validator, model_schema)\n            model_schema = core_schema.with_info_wrap_validator_function(Model._wrap_validator, model_schema)\n            model_schema = core_schema.with_info_after_validator_function(Model._validator, model_schema)\n        Model.__pydantic_validator__ = SchemaValidator(model_schema)\n        return Model", "target": "def setup_link_role(app):\n    app.add_role(\"arxiv\", reference_role, override=True)\n    app.add_role(\"arXiv\", reference_role, override=True)\n    app.add_role(\"doi\", reference_role, override=True)\n    app.add_role(\"DOI\", reference_role, override=True)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "001366", "source": "def update(self, op_code, cur, max=None, message=\"\"):\n        msg = self._cur_line or message\n        if max and cur:\n            percent = int(cur / max * 100)\n            if percent != self._last_percent and percent % self._interval == 0:\n                self._last_percent = percent\n                logger.info(\"Progress: %d%% - %s\", percent, msg)\n        elif msg:\n            logger.info(msg)", "target": "def test_frozenset_from_dict_items(input_value, items_schema, expected):\n    v = SchemaValidator(core_schema.tuple_schema(items_schema=[items_schema], variadic_item_index=0))\n    output = v.validate_python(input_value)\n    assert isinstance(output, tuple)\n    assert output == expected", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001367", "source": "def test_include_generator(any_serializer):\n    assert any_serializer.to_python(as_generator('a', 'b', 'c'), mode='json') == ['a', 'b', 'c']\n    assert any_serializer.to_json(as_generator('a', 'b', 'c')) == b'[\"a\",\"b\",\"c\"]'\n    assert any_serializer.to_python(as_generator(0, 1, 2, 3), include={1, 2}, mode='json') == [1, 2]\n    assert any_serializer.to_python(as_generator('a', 'b', 'c', 'd'), include={1, 2}, mode='json') == ['b', 'c']\n    assert any_serializer.to_json(as_generator('a', 'b', 'c', 'd'), include={1, 2}) == b'[\"b\",\"c\"]'", "target": "def test_custom_error_type(py_and_json: PyAndJson):\n    v = py_and_json(core_schema.custom_error_schema(core_schema.int_schema(), 'recursion_loop'))\n    assert v.validate_test(1) == 1\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_test('X')\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'recursion_loop', 'loc': (), 'msg': 'Recursion error - cyclic reference detected', 'input': 'X'}\n    ]", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001368", "source": "def ser_x(self, v: Any, _) -> str:\n            assert self.x == 1_000 == v\n            return self.x_formatted", "target": "def ser_x(self, v: Any, _) -> str:\n            assert self.x == 1_000\n            return f'{v:_}'", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001369", "source": "def foobar(a, b, c):\n        return a, b, c", "target": "def foobar(*args, **kwargs):\n        return args, kwargs", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "001370", "source": "def main():\n    parser = argparse.ArgumentParser(description=\"Lumos CLI\")\n    subparsers = parser.add_subparsers(dest=\"command\", required=True)\n    parser.add_argument(\n        \"--log-level\", default=\"INFO\", help=\"Log level (DEBUG, INFO, WARNING, ERROR)\"\n    )\n    register_build_commands(subparsers)\n    register_test_commands(subparsers)\n    args = parser.parse_args()\n    setup_logging(getattr(logging, args.log_level.upper(), logging.INFO))\n    logger.debug(\"Parsed args: %s\", args)\n    if hasattr(args, \"func\"):\n        args.func(args)\n    else:\n        parser.print_help()", "target": "def update_field_from_config(config_wrapper: ConfigWrapper, field_name: str, field_info: FieldInfo) -> None:\n    field_title_generator = field_info.field_title_generator or config_wrapper.field_title_generator\n    if field_title_generator is not None:\n        _apply_field_title_generator_to_field_info(field_title_generator, field_name, field_info)\n    if config_wrapper.alias_generator is not None:\n        _apply_alias_generator_to_field_info(config_wrapper.alias_generator, field_name, field_info)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001371", "source": "def relative_typename(self, root: str) -> Optional[str]:\n            if self.type_node is not None:\n                return self.type_node.relative_typename(root)\n            return None", "target": "def relative_typename(self, module: str) -> str:\n        return '_typing.Callable[[{}], {}]'.format(\n            ', '.join(arg.relative_typename(module) for arg in self.arg_types),\n            self.ret_type.relative_typename(module)\n        )", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001372", "source": "def root_validator(\n    *,\n    skip_on_failure: Literal[True],\n    allow_reuse: bool = ...,\n) -> Callable[\n    [_V1RootValidatorFunctionType],\n    _V1RootValidatorFunctionType,\n]: ...", "target": "def root_validator(\n    *,\n    pre: Literal[True],\n    allow_reuse: bool = ...,\n) -> Callable[\n    [_V1RootValidatorFunctionType],\n    _V1RootValidatorFunctionType,\n]: ...", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pytorch/pytorch", "data_split": "train", "example_id": "001373", "source": "def parametrized_wrapper(func):\n                @functools.wraps(func)\n                def wrapped_func(*args, **kwargs):\n                    if self.has_failure:\n                        if ret_type_on_failure is None:\n                            return None\n                        return ret_type_on_failure()\n                    try:\n                        ret_type = func(*args, **kwargs)\n                    except Exception:\n                        self.has_failure = True\n                        warnings.warn(\n                            \"Typing stubs generation has failed.\\n{}\".format(\n                                traceback.format_exc()\n                            )\n                        )\n                        if ret_type_on_failure is None:\n                            return None\n                        return ret_type_on_failure()\n                    return ret_type\n                if self.exceptions_as_warnings:\n                    return wrapped_func\n                else:\n                    return original_func", "target": "def calculate_table_data(results: list[ExperimentResults]) -> dict:\n    table_data = defaultdict(list)\n    aten_perf: Optional[float] = None\n    for experiment_result in results:\n        for key, value in experiment_result.asdict().items():\n            assert key in UNITS, f\"Unknown key {key}\"\n            table_data[key + UNITS[key]].append(value)\n        if experiment_result.name == \"aten\":\n            aten_perf = experiment_result.forward_time\n            table_data[PERF_OVER_ATEN_STR].append(\"NA\")\n        elif aten_perf is not None:\n            perf_over_aten = (\n                (experiment_result.forward_time - aten_perf) / aten_perf * 100\n            )\n            table_data[PERF_OVER_ATEN_STR].append(perf_over_aten)\n        else:\n            table_data[PERF_OVER_ATEN_STR].append(\"NA\")\n    return table_data", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001374", "source": "def name(self):\n        prefix = f\"{self.category()}_{self._name}_{self.backend()}\"\n        return prefix", "target": "def name(self):\n        if self.use_loop:\n            return f\"{self.category()}_loop\"\n        return self.category()", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "001375", "source": "def find_job_id_name(args: Any) -> tuple[str, str]:\n    PYTORCH_REPO = os.environ.get(\"GITHUB_REPOSITORY\", \"pytorch/pytorch\")\n    PYTORCH_GITHUB_API = f\"https://api.github.com/repos/{PYTORCH_REPO}\"\n    GITHUB_TOKEN = os.environ[\"GITHUB_TOKEN\"]\n    REQUEST_HEADERS = {\n        \"Accept\": \"application/vnd.github.v3+json\",\n        \"Authorization\": \"token \" + GITHUB_TOKEN,\n    }\n    url = f\"{PYTORCH_GITHUB_API}/actions/runs/{args.workflow_run_id}/jobs?per_page=100\"\n    jobs = fetch_jobs(url, REQUEST_HEADERS)\n    jobs.sort(key=operator.itemgetter(\"started_at\"), reverse=True)\n    for job in jobs:\n        if job[\"runner_name\"] == args.runner_name:\n            return (job[\"id\"], job[\"name\"])\n    raise RuntimeError(f\"Can't find job id for runner {args.runner_name}\")", "target": "def test_nullable_error():\n    v = SchemaValidator(\n        core_schema.definitions_schema(\n            core_schema.definition_reference_schema('Branch'),\n            [\n                core_schema.typed_dict_schema(\n                    {\n                        'width': core_schema.typed_dict_field(core_schema.int_schema()),\n                        'sub_branch': core_schema.typed_dict_field(\n                            core_schema.with_default_schema(\n                                core_schema.union_schema(\n                                    [core_schema.none_schema(), core_schema.definition_reference_schema('Branch')]\n                                ),\n                                default=None,\n                            )\n                        ),\n                    },\n                    ref='Branch',\n                )\n            ],\n        )\n    )\n    assert v.validate_python({'width': 123, 'sub_branch': {'width': 321}}) == (\n        {'width': 123, 'sub_branch': {'width': 321, 'sub_branch': None}}\n    )\n    with pytest.raises(ValidationError) as exc_info:\n        assert v.validate_python({'width': 123, 'sub_branch': {'width': 'wrong'}})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'none_required',\n            'loc': ('sub_branch', 'none'),\n            'msg': 'Input should be None',\n            'input': {'width': 'wrong'},\n        },\n        {\n            'type': 'int_parsing',\n            'loc': ('sub_branch', 'typed-dict', 'width'),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'wrong',\n        },\n    ]", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001376", "source": "def test_extra_behavior_allow_with_validate_fn_override(\n    config: Union[core_schema.CoreConfig, None],\n    schema_extra_behavior_kw: dict[str, Any],\n):\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            {'f': core_schema.model_field(core_schema.str_schema())}, **schema_extra_behavior_kw\n        ),\n        config=config,\n    )\n    m, model_extra, fields_set = v.validate_python({'f': 'x', 'extra_field': '123'}, extra='allow')\n    assert m == {'f': 'x'}\n    assert model_extra == {'extra_field': '123'}\n    assert fields_set == {'f', 'extra_field'}\n    v.validate_assignment(m, 'f', 'y', extra='allow')\n    assert m == {'f': 'y'}\n    new_m, new_model_extra, new_fields_set = v.validate_assignment({**m, **model_extra}, 'not_f', '123', extra='allow')\n    assert new_m == {'f': 'y'}\n    assert new_model_extra == {'extra_field': '123', 'not_f': '123'}\n    assert new_fields_set == {'not_f'}", "target": "def test_extra_behavior_allow_with_validate_fn_override(\n    config: Union[core_schema.CoreConfig, None],\n    schema_extra_behavior_kw: dict[str, Any],\n):\n    v = SchemaValidator(\n        core_schema.typed_dict_schema(\n            {'f': core_schema.typed_dict_field(core_schema.str_schema())},\n            **schema_extra_behavior_kw,\n            config=config,\n        )\n    )\n    m: dict[str, Any] = v.validate_python({'f': 'x', 'extra_field': '123'}, extra='allow')\n    assert m == {'f': 'x', 'extra_field': '123'}", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "001377", "source": "def test_type_mismatch(self):\n        import_path()\n        with self.assertRaises(cv.error) as context:\n            cv.imread(123)\n        self.assertTrue('str or path-like' in str(context.exception))", "target": "def test_keyword_only(py_and_json: PyAndJson, input_value) -> None:\n    v = py_and_json(\n        cs.arguments_v3_schema(\n            [\n                cs.arguments_v3_parameter(name='a', schema=cs.int_schema(), mode='keyword_only'),\n                cs.arguments_v3_parameter(\n                    name='b', schema=cs.with_default_schema(cs.bool_schema(), default=True), mode='keyword_only'\n                ),\n            ]\n        )\n    )\n    assert v.validate_test(input_value) == ((), {'a': 1, 'b': True})", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|opencv/opencv", "data_split": "train", "example_id": "001378", "source": "def force_create_dir(path: Union[str, Path]) -> Path:\n    remove_dir(path)\n    return ensure_dir_exists(path)", "target": "def string_or_pathlike_(ctype_name: str = \"string\") -> UnionTypeNode:\n        return UnionTypeNode(\n            ctype_name,\n            items=(\n                PrimitiveTypeNode.str_(ctype_name),\n                PathLikeTypeNode(ctype_name)\n            )\n        )", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "001379", "source": "def json_or_python_schema(\n    json_schema: CoreSchema,\n    python_schema: CoreSchema,\n    *,\n    ref: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> JsonOrPythonSchema:\n    return _dict_not_none(\n        type='json-or-python',\n        json_schema=json_schema,\n        python_schema=python_schema,\n        ref=ref,\n        metadata=metadata,\n        serialization=serialization,\n    )", "target": "def add_function(self, name: str, arguments: Sequence[FunctionNode.Arg] = (),\n                     return_type: Optional[FunctionNode.RetType] = None) -> FunctionNode:\n        return self._add_child(FunctionNode, name, arguments=arguments,\n                               return_type=return_type)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001380", "source": "def get_shapes(self) -> tuple[tuple[int, ...], ...]:\n        return (\n            (32768, 256),\n            (32768, 512),\n            (32768, 1024),\n            (32768, 2048),\n            (32768, 4096),\n            (32768, 8192),\n            (32768, 16384),\n            (32768, 32768),\n            (32768, 65536),\n            (16384, 131072),\n            (8192, 262144),\n        )", "target": "def get_shapes(self) -> tuple[tuple[int, ...], ...]:\n        return (\n            (32768, 256),\n            (32768, 512),\n            (32768, 1024),\n            (32768, 2048),\n            (32768, 4096),\n            (32768, 8192),\n            (32768, 16384),\n            (32768, 32768),\n            (32768, 65536),\n        ) + extra_shapes_for_norm", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "001381", "source": "def make_data(self, params):\n        (method,) = params\n        n_samples = 500 if method == \"exact\" else None\n        return _digits_dataset(n_samples=n_samples)", "target": "def make_data(self, params):\n        (representation,) = params\n        if representation == \"sparse\":\n            data = _20newsgroups_highdim_dataset()\n        else:\n            data = _20newsgroups_lowdim_dataset()\n        return data", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001382", "source": "def test_paths_allow_by_name(py_and_json: PyAndJson, input_value):\n    v = py_and_json(\n        {\n            'type': 'model-fields',\n            'fields': {\n                'field_a': {\n                    'validation_alias': [['foo', 'bar'], ['foo']],\n                    'type': 'model-field',\n                    'schema': {'type': 'int'},\n                }\n            },\n        },\n        config=CoreConfig(validate_by_name=True),\n    )\n    assert v.validate_test(input_value) == ({'field_a': 42}, None, {'field_a'})", "target": "def test_paths_allow_by_name(py_and_json: PyAndJson, input_value):\n    v = py_and_json(\n        {\n            'type': 'typed-dict',\n            'fields': {\n                'field_a': {\n                    'validation_alias': [['foo', 'bar'], ['foo']],\n                    'type': 'typed-dict-field',\n                    'schema': {'type': 'int'},\n                }\n            },\n            'config': {'validate_by_name': True},\n        },\n    )\n    assert v.validate_test(input_value) == {'field_a': 42}", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001383", "source": "def test_dict_keys():\n    def fmt(value, _info):\n        return f'<{value}>'\n    s = SchemaSerializer(\n        core_schema.dict_schema(\n            core_schema.int_schema(serialization=core_schema.plain_serializer_function_ser_schema(fmt, info_arg=True))\n        )\n    )\n    assert s.to_python({1: True}) == {'<1>': True}", "target": "def test_p_or_k_optional(py_and_json: PyAndJson, input_value, expected):\n    v = py_and_json(\n        {\n            'type': 'arguments',\n            'arguments_schema': [\n                {\n                    'name': 'a',\n                    'mode': 'positional_or_keyword',\n                    'schema': {'type': 'default', 'schema': {'type': 'int'}, 'default': 1},\n                }\n            ],\n        }\n    )\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)) as exc_info:\n            v.validate_test(input_value)\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        assert v.validate_test(input_value) == expected", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001384", "source": "def test_simple():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'field_a': core_schema.model_field(schema=core_schema.str_schema()),\n                'field_b': core_schema.model_field(schema=core_schema.int_schema()),\n            }\n        )\n    )\n    assert v.validate_python({'field_a': b'abc', 'field_b': 1}) == (\n        {'field_a': 'abc', 'field_b': 1},\n        None,\n        {'field_a', 'field_b'},\n    )", "target": "def test_simple():\n    v = SchemaValidator(\n        core_schema.typed_dict_schema(\n            fields={\n                'field_a': core_schema.typed_dict_field(schema=core_schema.str_schema()),\n                'field_b': core_schema.typed_dict_field(schema=core_schema.int_schema()),\n            }\n        )\n    )\n    assert v.validate_python({'field_a': b'abc', 'field_b': 1}) == {'field_a': 'abc', 'field_b': 1}", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001385", "source": "def _work(self):\n        @torch.compile(backend=self.backend(), fullgraph=True)\n        def f(x):\n            tmps = [x + i for i in range(16)]\n            tmps = [x + tmp for tmp in tmps]\n            for i in range(len(tmps) - 4):\n                tmps[i] = tmps[i].sin().mul(tmps[i])\n                tmps[i + 1] -= tmps[i]\n                tmps[i + 2] -= tmps[i]\n                tmps[i + 3] -= tmps[i]\n            return sum(tmps)\n        f(self.x)", "target": "def _work(self):\n        @torch.compile(fullgraph=True)\n        def f(a, b):\n            xs = b.tolist()\n            for x in xs:\n                torch._check(x >= 0)\n                torch._check(x <= self.N)\n            return a.split(xs)\n        f(self.input, self.splits)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001386", "source": "def test_val_temporal_unit_date(val_temporal_unit, input_value, expected):\n    v = SchemaValidator(\n        cs.date_schema(),\n        config={'val_temporal_unit': val_temporal_unit},\n    )\n    output = v.validate_python(input_value)\n    assert output == expected", "target": "def test_ignored_def():\n    v = SchemaValidator(\n        core_schema.definitions_schema(\n            core_schema.list_schema(core_schema.int_schema()), [core_schema.int_schema(ref='foobar')]\n        )\n    )\n    assert v.validate_python([1, 2, '3']) == [1, 2, 3]\n    r = plain_repr(v)\n    assert r.startswith('SchemaValidator(title=\"list[int]\",')", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001387", "source": "def test_definition_chain():\n    v = SchemaValidator(\n        core_schema.definitions_schema(\n            core_schema.definition_reference_schema('foo'),\n            [core_schema.definition_reference_schema(ref='foo', schema_ref='bar'), core_schema.int_schema(ref='bar')],\n        )\n    )\n    assert v.validate_python('1') == 1", "target": "def test_isinstance_strict():\n    v = SchemaValidator(cs.int_schema(strict=True))\n    assert v.validate_python(123) == 123\n    assert v.isinstance_python(123) is True\n    with pytest.raises(ValidationError, match='Input should be a valid integer'):\n        v.validate_python('123')\n    assert v.isinstance_python('123') is False", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "001388", "source": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "target": "def test_config_no_model():\n    f_kwargs = None\n    def f(input_value, info: core_schema.ValidationInfo):\n        nonlocal f_kwargs\n        f_kwargs = deepcopy_info(info)\n        return input_value + ' Changed'\n    v = SchemaValidator(core_schema.with_info_after_validator_function(f, core_schema.str_schema()))\n    assert v.validate_python(b'abc') == 'abc Changed'\n    assert f_kwargs == {'data': None, 'config': None, 'context': None, 'field_name': None}", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001389", "source": "def get_memory_bytes(self, args, kwargs) -> int:\n        x, target, dloss = args\n        M, N = x.shape\n        return (\n            2 * M * N * x.dtype.itemsize\n            + M * target.dtype.itemsize\n            + M * dloss.dtype.itemsize\n        )", "target": "def get_memory_bytes(self, args, kwargs) -> int:\n        (x,) = args\n        M, N = x.shape\n        return 2 * M * N * x.dtype.itemsize", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001390", "source": "def type_format(self) -> str:\n        return \"\"", "target": "def type_format(self) -> str:\n        if TypeNode.compatible_to_runtime_usage:\n            return \"_typing.Optional[{}]\"\n        return \"{} | None\"", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|pydantic/pydantic", "data_split": "train", "example_id": "001391", "source": "def predict(est, data_test, target_test):\n    if args.no_predict:\n        return\n    tic = time()\n    predicted_test = est.predict(data_test)\n    predicted_proba_test = est.predict_proba(data_test)\n    toc = time()\n    roc_auc = roc_auc_score(target_test, predicted_proba_test[:, 1])\n    acc = accuracy_score(target_test, predicted_test)\n    print(f\"predicted in {toc - tic:.3f}s, ROC AUC: {roc_auc:.4f}, ACC: {acc:.4f}\")", "target": "def test_constraints_schema_validation_error(constraint: str) -> None:\n    with pytest.raises(SchemaError, match=f\"'{constraint}' must be coercible to a date instance\"):\n        SchemaValidator(cs.date_schema(**{constraint: 'bad_value'}))", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "001392", "source": "def test_union_list_bool_int():\n    v = SchemaValidator(\n        core_schema.union_schema(\n            choices=[\n                core_schema.list_schema(items_schema=core_schema.bool_schema()),\n                core_schema.list_schema(items_schema=core_schema.int_schema()),\n            ]\n        )\n    )\n    assert v.validate_python(['true', True, 'no']) == [True, True, False]\n    assert v.validate_python([5, 6, '789']) == [5, 6, 789]\n    assert v.validate_python(['1', '0']) == [1, 0]\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python([3, 'true'])\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'bool_parsing',\n            'loc': ('list[bool]', 0),\n            'msg': 'Input should be a valid boolean, unable to interpret input',\n            'input': 3,\n        },\n        {\n            'type': 'int_parsing',\n            'loc': ('list[int]', 1),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'true',\n        },\n    ]", "target": "def gh_post_pr_comment(\n    org: str, repo: str, pr_num: int, comment: str, dry_run: bool = False\n) -> list[dict[str, Any]]:\n    return _gh_post_comment(\n        f\"{GITHUB_API_URL}/repos/{org}/{repo}/issues/{pr_num}/comments\",\n        comment,\n        dry_run,\n    )", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001393", "source": "def get_tolerance_and_cosine_flag(self, is_training, current_device, name):\n        cosine = self.args.cosine\n        tolerance = 1e-3\n        if self.args.freezing and name in REQUIRE_HIGHER_TOLERANCE_FOR_FREEZING:\n            tolerance = 8 * 1e-2\n        if is_training:\n            from torch._inductor import config as inductor_config\n            if name == \"beit_base_patch16_224\":\n                tolerance = 16 * 1e-2\n            elif name in REQUIRE_EVEN_HIGHER_TOLERANCE or (\n                inductor_config.max_autotune\n                and name in REQUIRE_EVEN_HIGHER_TOLERANCE_MAX_AUTOTUNE\n            ):\n                tolerance = 8 * 1e-2\n            elif name in REQUIRE_HIGHER_TOLERANCE or (\n                self.args.amp and name in REQUIRE_HIGHER_TOLERANCE_AMP\n            ):\n                tolerance = 4 * 1e-2\n            else:\n                tolerance = 1e-2\n        return tolerance, cosine", "target": "def get_tolerance_and_cosine_flag(self, is_training, current_device, name):\n        tolerance = 1e-4\n        cosine = self.args.cosine\n        if self.args.float16 or self.args.amp:\n            if self.args.freezing and (freezing := self._tolerance[\"freezing\"]):\n                higher_fp16 = freezing.get(\"higher_fp16\", None)\n                even_higher = freezing.get(\"even_higher\", None)\n                if higher_fp16 and name in higher_fp16:\n                    return 1e-2, cosine\n                elif even_higher and name in even_higher:\n                    return 8 * 1e-2, cosine\n            if name in self._tolerance[\"higher_fp16\"]:\n                return 1e-2, cosine\n            elif name in self._tolerance[\"even_higher\"]:\n                return 8 * 1e-2, cosine\n            return 1e-3, cosine\n        if self.args.bfloat16:\n            if name in self._tolerance[\"higher_bf16\"]:\n                return 1e-2, cosine\n            elif current_device == \"xpu\" and name in self._tolerance[\"higher_bf16_xpu\"]:\n                return 8 * 1e-2, cosine\n        if is_training and (current_device == \"cuda\" or current_device == \"xpu\"):\n            tolerance = 1e-3\n            if name in self._tolerance[\"cosine\"]:\n                cosine = True\n            elif name in self._tolerance[\"higher\"]:\n                tolerance = 1e-3\n            elif name in self._tolerance[\"even_higher\"]:\n                tolerance = 8 * 1e-2\n        return tolerance, cosine", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001394", "source": "def name(self):\n        prefix = f\"{self.category()}_{self.mode()}\"\n        if self._subclass:\n            prefix += \"_subclass\"\n        else:\n            prefix += \"_nosubclass\"\n        if self.device() == \"cpu\":\n            prefix += \"_cpu\"\n        return prefix", "target": "def name(self):\n        prefix = f\"{self.category()}_{self._name}_{self.backend()}\"\n        return prefix", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "001395", "source": "def make_data(self, params):\n        (method,) = params\n        n_samples = 500 if method == \"exact\" else None\n        return _digits_dataset(n_samples=n_samples)", "target": "def make_data(self, params):\n        data = _synth_classification_dataset(\n            n_samples=10000, n_features=100, n_classes=5\n        )\n        return data", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|opencv/opencv", "data_split": "train", "example_id": "001396", "source": "def generate_experiment_groups(\n    op_names: list[str],\n    shapes: list[tuple[int, int, int]],\n    dtypes: list[torch.dtype],\n    enable_persistent_tma_matmuls: list[bool],\n    cutlass_instantiation_levels: list[str],\n    batch_sizes: list[int],\n) -> list[ExperimentGroupConfig]:\n    groups = []\n    for (\n        op_name,\n        shape,\n        dtype,\n        batch_size,\n    ) in itertools.product(op_names, shapes, dtypes, batch_sizes):\n        group = ExperimentGroupConfig(\n            op_name=op_name,\n            shape=shape,\n            dtype=dtype,\n            batch_size=batch_size,\n        )\n        experiments = generate_experiment_configs(\n            enable_persistent_tma_matmuls, cutlass_instantiation_levels\n        )\n        group.experiments.extend(experiments)\n        groups.append(group)\n    return groups", "target": "def test_garray_type(self):\n            types = [cv.gapi.CV_BOOL   , cv.gapi.CV_INT    , cv.gapi.CV_INT64 , cv.gapi.CV_UINT64,\n                     cv.gapi.CV_DOUBLE , cv.gapi.CV_FLOAT  , cv.gapi.CV_STRING, cv.gapi.CV_POINT ,\n                     cv.gapi.CV_POINT2F, cv.gapi.CV_POINT3F, cv.gapi.CV_SIZE  , cv.gapi.CV_RECT  ,\n                     cv.gapi.CV_SCALAR , cv.gapi.CV_MAT    , cv.gapi.CV_GMAT]\n            for t in types:\n                g_array = cv.GArrayT(t)\n                self.assertEqual(t, g_array.type())", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001397", "source": "def typename(self) -> str:\n        return self.type_node.full_typename", "target": "def typename(self) -> str:\n        return self._export_name", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "train", "example_id": "001398", "source": "def from_markdown_table(data: str) -> TimingResultType:\n    out = data.strip().split(\"\\n\")\n    out = out[2:]\n    res: TimingResultType\n    res = defaultdict(defaultdict)\n    for line in out:\n        model, task, mean, var = (f.strip() for f in line.strip().split(\"|\") if f)\n        res[model][task] = (float(mean), float(var))\n    return res", "target": "def nvprof(cmd, outpath):\n    return system(f\"nvprof -o {outpath} {cmd}\")", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001399", "source": "def children_types(self) -> Tuple[ASTNodeType, ...]:\n        return ()", "target": "def children_types(self) -> Tuple[ASTNodeType, ...]:\n        return (ASTNodeType.Constant, )", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001400", "source": "def test_function_wrap_field_serializer_to_json():\n    class Model(RootModel):\n        def ser_root(self, v: Any, serializer: core_schema.SerializerFunctionWrapHandler, _) -> str:\n            assert self.root == 1_000\n            root = serializer(v)\n            return f'{root:_}'\n    s = SchemaSerializer(\n        core_schema.model_schema(\n            Model,\n            core_schema.int_schema(\n                serialization=core_schema.wrap_serializer_function_ser_schema(\n                    Model.ser_root, is_field_serializer=True, info_arg=True, schema=core_schema.any_schema()\n                )\n            ),\n            root_model=True,\n        )\n    )\n    assert json.loads(s.to_json(Model(1000))) == '1_000'", "target": "def test_function_wrap_field_serializer_to_json():\n    @dataclasses.dataclass\n    class Model:\n        x: int\n        def ser_x(self, v: Any, serializer: core_schema.SerializerFunctionWrapHandler, _) -> str:\n            assert self.x == 1_000\n            x = serializer(v)\n            return f'{x:_}'\n    s = SchemaSerializer(\n        core_schema.model_schema(\n            Model,\n            core_schema.model_fields_schema(\n                {\n                    'x': core_schema.model_field(\n                        core_schema.int_schema(\n                            serialization=core_schema.wrap_serializer_function_ser_schema(\n                                Model.ser_x, is_field_serializer=True, info_arg=True, schema=core_schema.any_schema()\n                            )\n                        )\n                    )\n                }\n            ),\n        )\n    )\n    assert json.loads(s.to_json(Model(x=1000))) == {'x': '1_000'}", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001401", "source": "def eager(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, target, dloss = args\n        loss = F.cross_entropy(x, target, reduction=\"none\")\n        return lambda: torch.autograd.grad(\n            loss, x, grad_outputs=dloss, retain_graph=True\n        )", "target": "def eager(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, w, dy = args\n        y = self.layernorm_ref(x, w)\n        return lambda: torch.autograd.grad(\n            y, [x, w], grad_outputs=dy, retain_graph=True\n        )", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "001402", "source": "def test_missing_error(pydantic_version):\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'field_a': core_schema.model_field(schema=core_schema.str_schema()),\n                'field_b': core_schema.model_field(schema=core_schema.int_schema()),\n            }\n        )\n    )\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python({'field_a': b'abc'})\n    assert (\n        str(exc_info.value)\n        ==\n        + (\n            f'\\n    For further information visit https://errors.pydantic.dev/{pydantic_version}/v/missing'\n            if os.environ.get('PYDANTIC_ERRORS_INCLUDE_URL', '1') != 'false'\n            else ''\n        )\n    )", "target": "def benchmark_dataloader(\n    dataset,\n    batch_size,\n    num_workers,\n    num_epochs=1,\n    max_batches=10,\n    multiprocessing_context=None,\n    logging_freq=10,\n):\n    print(\"\\n--- Benchmarking DataLoader ---\")\n    gc.collect()\n    torch.cuda.empty_cache()\n    model = create_model()\n    memory_before = get_memory_usage()\n    print(f\"Memory before DataLoader creation: {memory_before:.2f} MB\")\n    print_detailed_memory()\n    start = time.perf_counter()\n    dataloader = DataLoader(\n        dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        pin_memory=torch.cuda.is_available(),\n        prefetch_factor=2 if num_workers > 0 else None,\n        multiprocessing_context=multiprocessing_context,\n    )\n    it = iter(dataloader)\n    dataloader_init_time = time.perf_counter() - start\n    memory_after = get_memory_usage()\n    print(f\"Memory after DataLoader creation: {memory_after:.2f} MB\")\n    print(f\"Memory increase: {memory_after - memory_before:.2f} MB\")\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n    model.train()\n    total_batches = 0\n    total_samples = 0\n    total_time = 0\n    total_data_load_time = 0\n    peak_memory = memory_after\n    print(\n        f\"\\nStarting training loop with {num_epochs} epochs (max {max_batches} batches per epoch)\"\n    )\n    for epoch in range(num_epochs):\n        while total_batches < max_batches:\n            batch_start = time.perf_counter()\n            try:\n                inputs, labels = next(it)\n            except StopIteration:\n                break\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            data_load_time = time.perf_counter() - batch_start\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            batch_time = time.perf_counter() - batch_start\n            total_batches += 1\n            total_samples += inputs.size(0)\n            total_data_load_time += data_load_time\n            total_time += batch_time\n            if total_batches % 5 == 0:\n                gc.collect()\n                current_memory = get_memory_usage()\n                if current_memory > peak_memory:\n                    peak_memory = current_memory\n            if total_batches % logging_freq == 0:\n                print(\n                    f\"Epoch {epoch + 1}, Batch {total_batches}, \"\n                    f\"Time: {batch_time:.4f}s, \"\n                    f\"Memory: {current_memory:.2f} MB\"\n                )\n    avg_data_load_time = (\n        total_data_load_time / total_batches if total_batches > 0 else 0\n    )\n    avg_batch_time = total_time / total_batches if total_batches > 0 else 0\n    samples_per_second = total_samples / total_time if total_time > 0 else 0\n    results = {\n        \"dataloader_init_time\": dataloader_init_time,\n        \"num_workers\": num_workers,\n        \"batch_size\": batch_size,\n        \"total_batches\": total_batches,\n        \"avg_batch_time\": avg_batch_time,\n        \"avg_data_load_time\": avg_data_load_time,\n        \"samples_per_second\": samples_per_second,\n        \"peak_memory_mb\": peak_memory,\n        \"memory_increase_mb\": peak_memory - memory_before,\n    }\n    print(\"\\nResults:\")\n    print(f\"  DataLoader init time: {dataloader_init_time:.4f} seconds\")\n    print(f\"  Average data loading time: {avg_data_load_time:.4f} seconds\")\n    print(f\"  Average batch time: {avg_batch_time:.4f} seconds\")\n    print(f\"  Samples per second: {samples_per_second:.2f}\")\n    print(f\"  Peak memory usage: {peak_memory:.2f} MB\")\n    print(f\"  Memory increase: {peak_memory - memory_before:.2f} MB\")\n    del model, optimizer\n    del dataloader\n    gc.collect()\n    torch.cuda.empty_cache()\n    return results", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001403", "source": "def test_simple(self):\n        images = [\n            self.get_sample('stitching/a1.png'),\n            self.get_sample('stitching/a2.png'),\n            self.get_sample('stitching/a3.png')\n        ]\n        orb = cv.ORB_create()\n        features = [cv.detail.computeImageFeatures2(orb, img) for img in images]\n        matcher = cv.detail_BestOf2NearestRangeMatcher(range_width=1)\n        matches = matcher.apply2(features)\n        self.assertNotEqual(matches[1].confidence, 0)\n        self.assertEqual(matches[2].confidence, 0)", "target": "def test_simple(self):\n        images = [\n            self.get_sample('stitching/a1.png'),\n            self.get_sample('stitching/a2.png'),\n            self.get_sample('stitching/a3.png')\n        ]\n        images = [cv.resize(img, [100, 100]) for img in images]\n        finder = cv.detail_GraphCutSeamFinder('COST_COLOR_GRAD')\n        masks = [cv.UMat(255 * np.ones((img.shape[0], img.shape[1]), np.uint8)) for img in images]\n        images_f = [img.astype(np.float32) for img in images]\n        masks_warped = finder.find(images_f, [(0, 0), (75, 0), (150, 0)], masks)\n        self.assertIsNotNone(masks_warped)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001404", "source": "def root_validator(\n    *,\n    skip_on_failure: Literal[True],\n    allow_reuse: bool = ...,\n) -> Callable[\n    [_V1RootValidatorFunctionType],\n    _V1RootValidatorFunctionType,\n]: ...", "target": "def root_validator(\n    *,\n    pre: Literal[False],\n    skip_on_failure: Literal[True],\n    allow_reuse: bool = ...,\n) -> Callable[\n    [_V1RootValidatorFunctionType],\n    _V1RootValidatorFunctionType,\n]: ...", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001405", "source": "def data(self) -> dict[str, Any]:\n        ...", "target": "def data(self) -> dict[str, Any]:\n        return {k: v for ns in self._namespaces for k, v in ns.items()}", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001406", "source": "def ser_x(self, v: Any, _) -> str:\n            assert self.x == 1_000\n            return f'{v:_}'", "target": "def ser_x(self, v: Any, serializer: core_schema.SerializerFunctionWrapHandler, _) -> str:\n            x = serializer(v)\n            assert self.x == 1_000\n            return f'{x:_}'", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001407", "source": "def test_wrong_return_type():\n    s = SchemaSerializer(\n        core_schema.any_schema(\n            serialization=core_schema.plain_serializer_function_ser_schema(\n                repr_function, info_arg=True, return_schema=core_schema.int_schema()\n            )\n        )\n    )\n    with pytest.warns(\n        UserWarning,\n        match=r\"Expected `int` - serialized value may not be as expected \\[input_value='123', input_type=str\\]\",\n    ):\n        assert s.to_python(123) == '123'\n    with pytest.warns(\n        UserWarning,\n        match=r\"Expected `int` - serialized value may not be as expected \\[input_value='123', input_type=str\\]\",\n    ):\n        assert s.to_python(123, mode='json') == '123'\n    with pytest.warns(\n        UserWarning,\n        match=r\"Expected `int` - serialized value may not be as expected \\[input_value='123', input_type=str\\]\",\n    ):\n        assert s.to_json(123) == b'\"123\"'", "target": "def test_alias():\n    s = SchemaSerializer(\n        core_schema.typed_dict_schema(\n            {\n                'cat': core_schema.typed_dict_field(core_schema.int_schema(), serialization_alias='Meow'),\n                'dog': core_schema.typed_dict_field(core_schema.int_schema(), serialization_alias='Woof'),\n                'bird': core_schema.typed_dict_field(core_schema.int_schema()),\n            }\n        )\n    )\n    value = {'cat': 0, 'dog': 1, 'bird': 2}\n    assert s.to_python(value, by_alias=True) == IsStrictDict(Meow=0, Woof=1, bird=2)\n    assert s.to_python(value, exclude={'dog'}, by_alias=True) == IsStrictDict(Meow=0, bird=2)\n    assert s.to_python(value, by_alias=False) == IsStrictDict(cat=0, dog=1, bird=2)\n    assert s.to_python(value, mode='json', by_alias=True) == IsStrictDict(Meow=0, Woof=1, bird=2)\n    assert s.to_python(value, mode='json', include={'cat'}, by_alias=True) == IsStrictDict(Meow=0)\n    assert s.to_python(value, mode='json', by_alias=False) == IsStrictDict(cat=0, dog=1, bird=2)\n    assert json.loads(s.to_json(value, by_alias=True)) == IsStrictDict(Meow=0, Woof=1, bird=2)\n    assert json.loads(s.to_json(value, include={'cat', 'bird'}, by_alias=True)) == IsStrictDict(Meow=0, bird=2)\n    assert json.loads(s.to_json(value, by_alias=False)) == IsStrictDict(cat=0, dog=1, bird=2)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|scikit-learn/scikit-learn", "data_split": "train", "example_id": "001408", "source": "def test_date_key():\n    v = SchemaSerializer(core_schema.dict_schema(core_schema.date_schema(), core_schema.date_schema()))\n    assert v.to_python({date(2022, 12, 2): date(2022, 12, 2)}) == {date(2022, 12, 2): date(2022, 12, 2)}\n    assert v.to_python({date(2022, 12, 2): date(2022, 12, 2)}, mode='json') == {'2022-12-02': '2022-12-02'}\n    assert v.to_json({date(2022, 12, 2): date(2022, 12, 2)}) == b'{\"2022-12-02\":\"2022-12-02\"}'", "target": "def json_urlread(url):\n    try:\n        return json.loads(urlopen(url).read().decode(\"utf8\"))\n    except Exception:\n        print(\"Error reading\", url, file=sys.stderr)\n        raise", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|pytorch/pytorch", "data_split": "train", "example_id": "001409", "source": "def load_faces():\n    print(\"Loading Olivetti face dataset\")\n    print(\"-----------------------------\")\n    from sklearn.datasets import fetch_olivetti_faces\n    faces = fetch_olivetti_faces(shuffle=True)\n    return faces.data", "target": "def benchmark(self):\n        for M, N in self.get_shapes():\n            print(f\"Tensor dimensions: [{M}, {N}]\")\n            torch_dtype = cutlass_torch.dtype(cutlass.BFloat16)\n            x = torch.randn(M, N, device=\"cuda\", dtype=torch_dtype, requires_grad=True)\n            w = torch.randn(N, device=\"cuda\", dtype=torch.float32, requires_grad=True)\n            dy = torch.randn(M, N, device=\"cuda\", dtype=torch_dtype)\n            self.benchmark_single_shape((x, w, dy), setting=f\"shape: [{M}, {N}]\")", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001410", "source": "def compute_speedups(\n    operator, models, example_inputs, repeats, accuracy_checking=False, device=\"cuda\"\n):\n    expected = models[0](*example_inputs)\n    if accuracy_checking:\n        for model in models[1:]:\n            actual = model(*example_inputs)\n            try:\n                same(actual, expected, cos_similarity=True, equal_nan=True)\n            except AssertionError as e:\n                print(e)\n                print(f\"Accuracy check failed: {operator}\")\n                print((expected[0] - actual[0]).abs().max())\n    timings = np.zeros((repeats, len(models)), np.float64)\n    for rep in range(repeats):\n        with maybe_record_function(f\"rep_{rep}\"):\n            for m, model in enumerate(models):\n                with maybe_record_function(f\"model_{m}\"):\n                    if device == \"cuda\":\n                        model(*example_inputs)\n                        timings[rep, m] = benchmarker.benchmark_gpu(\n                            lambda: model(*example_inputs)\n                        )\n                    else:\n                        from torch._inductor.utils import timed\n                        timings[rep, m] = timed(model, example_inputs)\n    return np.median(timings, axis=0)", "target": "def compute_speedups(args, models, example_inputs):\n    expected = models[0](*example_inputs)\n    for model in models[1:]:\n        actual = model(*example_inputs)\n        assert same(actual, expected), expected[0] - actual[0]\n    timings = np.zeros((args.repeat, len(models)), np.float64)\n    for rep in range(args.repeat):\n        for m, model in enumerate(models):\n            timings[rep, m] = timed(model, example_inputs)\n    median = np.median(timings, axis=0)\n    return (median[0] / median[1:]).tolist()", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001411", "source": "def node_type(self) -> ASTNodeType:\n        return ASTNodeType.Namespace", "target": "def node_type(self) -> ASTNodeType:\n        return ASTNodeType.Enumeration", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "001412", "source": "def test_int(input_value, expected):\n    v = SchemaValidator(core_schema.int_schema())\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            v.validate_json(input_value)\n    else:\n        assert v.validate_json(input_value) == expected", "target": "def getCMakeArgs(self):\n        args = TestRunner.getCMakeArgs(self)\n        args = args + [\n            '-DMACOSX_DEPLOYMENT_TARGET=%s' % os.environ['MACOSX_DEPLOYMENT_TARGET']\n        ]\n        return args", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001413", "source": "def resolve_type_nodes(self, root: ASTNode):\n        def has_unresolved_type_node(item) -> bool:\n            return item.type_node is not None and not item.type_node.is_resolved\n        errors = []\n        for overload in self.overloads:\n            for arg in filter(has_unresolved_type_node, overload.arguments):\n                try:\n                    arg.type_node.resolve(root)\n                except TypeResolutionError as e:\n                    errors.append(\n                        'Failed to resolve \"{}\" argument: {}'.format(arg.name, e)\n                    )\n            if overload.return_type is not None and \\\n                    has_unresolved_type_node(overload.return_type):\n                try:\n                    overload.return_type.type_node.resolve(root)\n                except TypeResolutionError as e:\n                    errors.append('Failed to resolve return type: {}'.format(e))\n        if len(errors) > 0:\n            raise TypeResolutionError(\n                'Failed to resolve \"{}\" function against \"{}\". Errors: {}'.format(\n                    self.full_export_name, root.full_export_name,\n                    \", \".join(\"[{}]: {}\".format(i, e) for i, e in enumerate(errors))\n                )\n            )", "target": "def resolve_type_nodes(self, root: ASTNode) -> None:\n        errors = []\n        for child in itertools.chain(self.properties,\n                                     self.functions.values(),\n                                     self.classes.values()):\n            try:\n                try:\n                    child.resolve_type_nodes(self)\n                except TypeResolutionError:\n                    child.resolve_type_nodes(root)\n            except TypeResolutionError as e:\n                errors.append(str(e))\n        if len(errors) > 0:\n            raise TypeResolutionError(\n                'Failed to resolve \"{}\" class against \"{}\". Errors: {}'.format(\n                    self.full_export_name, root.full_export_name, errors\n                )\n            )", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001414", "source": "def ser_x(self, v: Any, _) -> str:\n            assert self.x == 1_000 == v\n            return self.x_formatted", "target": "def ser_x(data: Model, v: Any, info: core_schema.FieldSerializationInfo) -> str:\n        assert data['x'] == 1_000\n        return f'{v:_}-{info.field_name}'", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|pydantic/pydantic", "data_split": "train", "example_id": "001415", "source": "def neg_mean_data_error(X, U, V):\n    return -np.sqrt(((X - U.dot(V)) ** 2).mean())", "target": "def _import_string_logic(dotted_path: str) -> Any:\n    from importlib import import_module\n    components = dotted_path.strip().split(':')\n    if len(components) > 2:\n        raise ImportError(f\"Import strings should have at most one ':'; received {dotted_path!r}\")\n    module_path = components[0]\n    if not module_path:\n        raise ImportError(f'Import strings should have a nonempty module name; received {dotted_path!r}')\n    try:\n        module = import_module(module_path)\n    except ModuleNotFoundError as e:\n        if '.' in module_path:\n            maybe_module_path, maybe_attribute = dotted_path.strip().rsplit('.', 1)\n            try:\n                return _import_string_logic(f'{maybe_module_path}:{maybe_attribute}')\n            except ImportError:\n                pass\n            raise ImportError(f'No module named {module_path!r}') from e\n        raise e\n    if len(components) > 1:\n        attribute = components[1]\n        try:\n            return getattr(module, attribute)\n        except AttributeError as e:\n            raise ImportError(f'cannot import name {attribute!r} from {module_path!r}') from e\n    else:\n        return module", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001416", "source": "def test_encoding(any_serializer, gen_input, kwargs, expected_json):\n    assert to_json(gen_input(), **kwargs) == expected_json\n    if not kwargs:\n        assert any_serializer.to_python(gen_input(), mode='json') == json.loads(expected_json)", "target": "def json_or_python_schema(\n    json_schema: CoreSchema,\n    python_schema: CoreSchema,\n    *,\n    ref: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> JsonOrPythonSchema:\n    return _dict_not_none(\n        type='json-or-python',\n        json_schema=json_schema,\n        python_schema=python_schema,\n        ref=ref,\n        metadata=metadata,\n        serialization=serialization,\n    )", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001417", "source": "async def wrapper_function(*args, **kwargs):\n            return await wrapper(*args, **kwargs)", "target": "def wrapper_function(*args: Any, **kwargs: Any) -> Any:\n            return vd.call(*args, **kwargs)", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001418", "source": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "target": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001419", "source": "def name(self) -> str:\n        prefix = f\"{self.category()}_{self.backend()}\"\n        if self.is_dynamic():\n            prefix += \"_dynamic\"\n        if self.device() == \"cuda\":\n            prefix += \"_gpu\"\n        return prefix", "target": "def name(self):\n        prefix = f\"{self.category()}_{self._name}_{self.backend()}\"\n        return prefix", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001420", "source": "def outMeta(arr_desc0, arr_desc1, arr_desc2):\n        return cv.empty_array_desc(), cv.empty_array_desc(), \\\n               cv.empty_array_desc(), cv.empty_array_desc()", "target": "def outMeta(desc):\n            out_desc = desc.withType(desc.depth, 1)\n            return out_desc, out_desc, out_desc", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001421", "source": "def get_output(self, input_blob):\n        self.net.setBlob(\"\", input_blob)\n        self.net.forward()\n        return self.net.getBlob(self.net.getLayerNames()[-1])", "target": "def get_output(self, input_blob):\n        if self.need_reshape:\n            self.net.blobs[self.in_blob_name].reshape(*input_blob.shape)\n        return self.net.forward_all(**{self.in_blob_name: input_blob})[self.out_blob_name]", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "001422", "source": "def test_include_exclude_schema():\n    s = SchemaSerializer(\n        core_schema.typed_dict_schema(\n            {\n                '0': core_schema.typed_dict_field(core_schema.int_schema(), serialization_exclude=True),\n                '1': core_schema.typed_dict_field(core_schema.int_schema()),\n                '2': core_schema.typed_dict_field(\n                    core_schema.int_schema(), serialization_exclude=True, serialization_exclude_if=lambda x: x < 0\n                ),\n                '3': core_schema.typed_dict_field(\n                    core_schema.int_schema(), serialization_exclude=False, serialization_exclude_if=lambda x: x < 0\n                ),\n            }\n        )\n    )\n    value = {'0': 0, '1': 1, '2': 2, '3': 3}\n    assert s.to_python(value) == {'1': 1, '3': 3}\n    assert s.to_python(value, mode='json') == {'1': 1, '3': 3}\n    assert json.loads(s.to_json(value)) == {'1': 1, '3': 3}\n    value = {'0': 0, '1': 1, '2': 2, '3': -3}\n    assert s.to_python(value) == {'1': 1}\n    assert s.to_python(value, mode='json') == {'1': 1}\n    assert json.loads(s.to_json(value)) == {'1': 1}", "target": "def _benchmark_tile_reduce_single(\n        self,\n        full_size: int,\n        tile_size: int,\n        warmup_iters: int = 5,\n        bench_iters: int = 10,\n    ) -> dict:\n        self._init_device()\n        group_name = dist.group.WORLD.group_name\n        symm_mem.enable_symm_mem_for_group(group_name)\n        dtype = torch.float\n        full_inp = symm_mem.empty(\n            full_size, full_size, dtype=dtype, device=self.device\n        ).fill_(self.rank)\n        full_out = symm_mem.empty(\n            full_size, full_size, dtype=dtype, device=self.device\n        ).fill_(0)\n        slice_ut = slice(0, tile_size)\n        inp_tile = full_inp[slice_ut, slice_ut]\n        out_tile = full_out[slice_ut, slice_ut]\n        root = 0\n        for _ in range(warmup_iters):\n            torch.ops.symm_mem.tile_reduce(inp_tile, out_tile, root, group_name)\n            torch.cuda.synchronize(self.device)\n        times = []\n        dist.barrier()\n        torch.cuda.synchronize(self.device)\n        start_time = time.perf_counter()\n        for _ in range(bench_iters):\n            torch.ops.symm_mem.tile_reduce(inp_tile, out_tile, root, group_name)\n        torch.cuda.synchronize(self.device)\n        end_time = time.perf_counter()\n        times.append((end_time - start_time) / bench_iters)\n        times = torch.tensor(times, dtype=torch.float64)\n        tile_elements = tile_size * tile_size\n        tile_bytes = (\n            tile_elements * dtype.itemsize\n            if hasattr(dtype, \"itemsize\")\n            else tile_elements * 4\n        )\n        results = {\n            \"full_size\": full_size,\n            \"tile_size\": tile_size,\n            \"tile_elements\": tile_elements,\n            \"tile_bytes\": tile_bytes,\n            \"world_size\": self.world_size,\n            \"mean_time_ms\": times.mean().item() * 1000,\n            \"std_time_ms\": times.std().item() * 1000,\n            \"min_time_ms\": times.min().item() * 1000,\n            \"max_time_ms\": times.max().item() * 1000,\n            \"throughput_gb_s\": tile_bytes / (times.mean().item() * 1e9),\n            \"elements_per_sec\": tile_elements / times.mean().item(),\n        }\n        return results", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "001423", "source": "def generate_libtorch_matrix(\n    os: str,\n    release_type: str,\n    arches: Optional[list[str]] = None,\n    libtorch_variants: Optional[list[str]] = None,\n) -> list[dict[str, str]]:\n    if arches is None:\n        arches = [\"cpu\"]\n        if os == \"linux\":\n            arches += CUDA_ARCHES\n            arches += ROCM_ARCHES\n        elif os == \"windows\":\n            windows_cuda_arches = CUDA_ARCHES.copy()\n            windows_cuda_arches.remove(\"12.9\")\n            arches += windows_cuda_arches\n    if libtorch_variants is None:\n        libtorch_variants = [\n            \"shared-with-deps\",\n            \"shared-without-deps\",\n            \"static-with-deps\",\n            \"static-without-deps\",\n        ]\n    ret: list[dict[str, str]] = []\n    for arch_version in arches:\n        for libtorch_variant in libtorch_variants:\n            gpu_arch_type = arch_type(arch_version)\n            gpu_arch_version = \"\" if arch_version == \"cpu\" else arch_version\n            if gpu_arch_type == \"rocm\" and (\"without-deps\" in libtorch_variant):\n                continue\n            ret.append(\n                {\n                    \"gpu_arch_type\": gpu_arch_type,\n                    \"gpu_arch_version\": gpu_arch_version,\n                    \"desired_cuda\": translate_desired_cuda(\n                        gpu_arch_type, gpu_arch_version\n                    ),\n                    \"libtorch_config\": release_type,\n                    \"libtorch_variant\": libtorch_variant,\n                    \"container_image\": (\n                        LIBTORCH_CONTAINER_IMAGES[arch_version].split(\":\")[0]\n                        if os not in (\"windows\", \"windows-arm64\")\n                        else \"\"\n                    ),\n                    \"container_image_tag_prefix\": (\n                        LIBTORCH_CONTAINER_IMAGES[arch_version].split(\":\")[1]\n                        if os not in (\"windows\", \"windows-arm64\")\n                        else \"\"\n                    ),\n                    \"package_type\": \"libtorch\",\n                    \"build_name\": f\"libtorch-{gpu_arch_type}{gpu_arch_version}-{libtorch_variant}-{release_type}\".replace(\n                        \".\", \"_\"\n                    ),\n                }\n            )\n    return ret", "target": "def test_custom_error():\n    v = SchemaValidator(\n        core_schema.tagged_union_schema(\n            discriminator='foo',\n            custom_error_type='snap',\n            custom_error_message='Input should be a foo or bar',\n            choices={\n                'apple': core_schema.typed_dict_schema(\n                    fields={\n                        'foo': core_schema.typed_dict_field(schema=core_schema.str_schema()),\n                        'bar': core_schema.typed_dict_field(schema=core_schema.int_schema()),\n                    }\n                ),\n                'banana': core_schema.typed_dict_schema(\n                    fields={\n                        'foo': core_schema.typed_dict_field(schema=core_schema.str_schema()),\n                        'spam': core_schema.typed_dict_field(\n                            schema=core_schema.list_schema(items_schema=core_schema.int_schema())\n                        ),\n                    }\n                ),\n            },\n        )\n    )\n    assert v.validate_python({'foo': 'apple', 'bar': '123'}) == {'foo': 'apple', 'bar': 123}\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python({'spam': 'apple', 'bar': 'Bar'})\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'snap', 'loc': (), 'msg': 'Input should be a foo or bar', 'input': {'spam': 'apple', 'bar': 'Bar'}}\n    ]\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python({'foo': 'other', 'bar': 'Bar'})\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'snap', 'loc': (), 'msg': 'Input should be a foo or bar', 'input': {'foo': 'other', 'bar': 'Bar'}}\n    ]", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001424", "source": "def get_output(self, input_blob):\n        return super(DnnTfInceptionModel, self).get_output(input_blob)[..., 1:1001]", "target": "def get_output(self, input_blob):\n        self.net.setBlob(\"\", input_blob)\n        self.net.forward()\n        return self.net.getBlob(self.net.getLayerNames()[-1])", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "001425", "source": "def compute_bench(samples_range, features_range):\n    it = 0\n    results = defaultdict(lambda: [])\n    chunk = 100\n    max_it = len(samples_range) * len(features_range)\n    for n_samples in samples_range:\n        for n_features in features_range:\n            it += 1\n            print(\"==============================\")\n            print(\"Iteration %03d of %03d\" % (it, max_it))\n            print(\"==============================\")\n            print()\n            data = nr.randint(-50, 51, (n_samples, n_features))\n            print(\"K-Means\")\n            tstart = time()\n            kmeans = KMeans(init=\"k-means++\", n_clusters=10).fit(data)\n            delta = time() - tstart\n            print(\"Speed: %0.3fs\" % delta)\n            print(\"Inertia: %0.5f\" % kmeans.inertia_)\n            print()\n            results[\"kmeans_speed\"].append(delta)\n            results[\"kmeans_quality\"].append(kmeans.inertia_)\n            print(\"Fast K-Means\")\n            mbkmeans = MiniBatchKMeans(\n                init=\"k-means++\", n_clusters=10, batch_size=chunk\n            )\n            tstart = time()\n            mbkmeans.fit(data)\n            delta = time() - tstart\n            print(\"Speed: %0.3fs\" % delta)\n            print(\"Inertia: %f\" % mbkmeans.inertia_)\n            print()\n            print()\n            results[\"MiniBatchKMeans Speed\"].append(delta)\n            results[\"MiniBatchKMeans Quality\"].append(mbkmeans.inertia_)\n    return results", "target": "def compute_bench(samples_range, features_range):\n    it = 0\n    results = defaultdict(lambda: [])\n    max_it = len(samples_range) * len(features_range)\n    for n_samples in samples_range:\n        for n_features in features_range:\n            it += 1\n            print(\"====================\")\n            print(\"Iteration %03d of %03d\" % (it, max_it))\n            print(\"====================\")\n            dataset_kwargs = {\n                \"n_samples\": n_samples,\n                \"n_features\": n_features,\n                \"n_informative\": n_features // 10,\n                \"effective_rank\": min(n_samples, n_features) / 10,\n                \"bias\": 0.0,\n            }\n            print(\"n_samples: %d\" % n_samples)\n            print(\"n_features: %d\" % n_features)\n            X, y = make_regression(**dataset_kwargs)\n            gc.collect()\n            print(\"benchmarking lars_path (with Gram):\", end=\"\")\n            sys.stdout.flush()\n            tstart = time()\n            G = np.dot(X.T, X)\n            Xy = np.dot(X.T, y)\n            lars_path_gram(Xy=Xy, Gram=G, n_samples=y.size, method=\"lasso\")\n            delta = time() - tstart\n            print(\"%0.3fs\" % delta)\n            results[\"lars_path (with Gram)\"].append(delta)\n            gc.collect()\n            print(\"benchmarking lars_path (without Gram):\", end=\"\")\n            sys.stdout.flush()\n            tstart = time()\n            lars_path(X, y, method=\"lasso\")\n            delta = time() - tstart\n            print(\"%0.3fs\" % delta)\n            results[\"lars_path (without Gram)\"].append(delta)\n            gc.collect()\n            print(\"benchmarking lasso_path (with Gram):\", end=\"\")\n            sys.stdout.flush()\n            tstart = time()\n            lasso_path(X, y, precompute=True)\n            delta = time() - tstart\n            print(\"%0.3fs\" % delta)\n            results[\"lasso_path (with Gram)\"].append(delta)\n            gc.collect()\n            print(\"benchmarking lasso_path (without Gram):\", end=\"\")\n            sys.stdout.flush()\n            tstart = time()\n            lasso_path(X, y, precompute=False)\n            delta = time() - tstart\n            print(\"%0.3fs\" % delta)\n            results[\"lasso_path (without Gram)\"].append(delta)\n    return results", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|scikit-learn/scikit-learn", "data_split": "train", "example_id": "001426", "source": "def test_ot_smoke(self):\n            img_path = self.find_file('cv/face/david2.jpg', [os.environ.get('OPENCV_TEST_DATA_PATH')])\n            in_image = cv.cvtColor(cv.imread(img_path), cv.COLOR_RGB2BGR)\n            in_rects = [ (138, 89, 71, 64) ]\n            in_rects_cls = [ 0 ]\n            g_in = cv.GMat()\n            g_in_rects = cv.GArray.Rect()\n            g_in_rects_cls = cv.GArray.Int()\n            delta = 0.5\n            g_out_rects, g_out_rects_cls, g_track_ids, g_track_sts = \\\n                cv.gapi.ot.track(g_in, g_in_rects, g_in_rects_cls, delta)\n            comp = cv.GComputation(cv.GIn(g_in, g_in_rects, g_in_rects_cls),\n                                   cv.GOut(g_out_rects, g_out_rects_cls,\n                                           g_track_ids, g_track_sts))\n            __, __, __, sts = comp.apply(cv.gin(in_image, in_rects, in_rects_cls),\n                args=cv.gapi.compile_args(cv.gapi.ot.cpu.kernels()))\n            self.assertEqual(cv.gapi.ot.NEW, sts[0])", "target": "def autolabel_auc(rects, ax):\n    for rect in rects:\n        height = rect.get_height()\n        ax.text(\n            rect.get_x() + rect.get_width() / 2.0,\n            1.05 * height,\n            \"%.3f\" % height,\n            ha=\"center\",\n            va=\"bottom\",\n        )", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001427", "source": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "target": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001428", "source": "def is_resolved(self) -> bool:\n        return self.value.is_resolved", "target": "def is_resolved(self) -> bool:\n        return self.positive_branch_type.is_resolved \\\n                and self.negative_branch_type.is_resolved", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001429", "source": "def type_format(self) -> str:\n        if TypeNode.compatible_to_runtime_usage:\n            return \"_typing.Tuple[{}]\"\n        return \"tuple[{}]\"", "target": "def type_format(self) -> str:\n        if TypeNode.compatible_to_runtime_usage:\n            return \"_typing.Optional[{}]\"\n        return \"{} | None\"", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001430", "source": "def test_error_loc():\n    v = SchemaValidator(\n        core_schema.typed_dict_schema(\n            fields={\n                'field_a': core_schema.typed_dict_field(\n                    schema=core_schema.list_schema(items_schema=core_schema.int_schema())\n                )\n            },\n            extras_schema=core_schema.int_schema(),\n            extra_behavior='allow',\n        )\n    )\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_json('{\"field_a\": [1, 2, \"wrong\"]}')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('field_a', 2),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'wrong',\n        }\n    ]", "target": "def check_kwargs(cls, v: Optional[dict[str, Any]]) -> Optional[dict[str, Any]]:\n                if takes_kwargs or v is None:\n                    return v\n                plural = '' if len(v) == 1 else 's'\n                keys = ', '.join(map(repr, v.keys()))\n                raise TypeError(f'unexpected keyword argument{plural}: {keys}')", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001431", "source": "def compiled(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        (x,) = args\n        torch._dynamo.mark_dynamic(x, 0)\n        compiled_softmax = torch.compile(\n            lambda x: F.softmax(x, dim=-1), mode=self.compile_mode, fullgraph=True\n        )\n        return lambda: compiled_softmax(x)", "target": "def compiled(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, w = args\n        torch._dynamo.mark_dynamic(x, 0)\n        compiled_rms_norm = torch.compile(\n            self.rms_norm_ref, mode=self.compile_mode, fullgraph=True\n        )\n        return lambda: compiled_rms_norm(x, w)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001432", "source": "def test_alias(py_and_json: PyAndJson):\n    v = py_and_json(\n        {\n            'type': 'model-fields',\n            'fields': {'field_a': {'validation_alias': 'FieldA', 'type': 'model-field', 'schema': {'type': 'int'}}},\n        }\n    )\n    assert v.validate_test({'FieldA': '123'}) == ({'field_a': 123}, None, {'field_a'})\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'foobar': '123'})\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'field_a': '123'})", "target": "def test_alias(py_and_json: PyAndJson):\n    v = py_and_json(\n        {\n            'type': 'typed-dict',\n            'fields': {\n                'field_a': {'validation_alias': 'FieldA', 'type': 'typed-dict-field', 'schema': {'type': 'int'}}\n            },\n        }\n    )\n    assert v.validate_test({'FieldA': '123'}) == {'field_a': 123}\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'foobar': '123'})\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'field_a': '123'})", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001433", "source": "def test_bytes_mode_set_via_model_config_not_serializer_config():\n    s = SchemaSerializer(\n        core_schema.model_schema(\n            BasicModel,\n            core_schema.model_fields_schema(\n                {\n                    'foo': core_schema.model_field(core_schema.bytes_schema()),\n                }\n            ),\n            config=core_schema.CoreConfig(ser_json_bytes='base64'),\n        )\n    )\n    bm = BasicModel(foo=b'foobar')\n    assert s.to_python(bm) == {'foo': b'foobar'}\n    assert s.to_json(bm) == b'{\"foo\":\"Zm9vYmFy\"}'\n    assert s.to_python(bm, mode='json') == {'foo': 'Zm9vYmFy'}\n    BasicModel.__pydantic_serializer__ = s\n    assert to_json(bm, bytes_mode='utf8') == b'{\"foo\":\"Zm9vYmFy\"}'\n    assert to_json({'foo': b'some bytes'}, bytes_mode='base64') == b'{\"foo\":\"c29tZSBieXRlcw==\"}'\n    assert to_json({'bar': bm}, bytes_mode='base64') == b'{\"bar\":{\"foo\":\"Zm9vYmFy\"}}'", "target": "def test_recursive_function():\n    s = SchemaSerializer(\n        core_schema.definitions_schema(\n            core_schema.definition_reference_schema('my_ref'),\n            [\n                core_schema.typed_dict_schema(\n                    {'root': core_schema.typed_dict_field(core_schema.definition_reference_schema('my_ref'))},\n                    ref='my_ref',\n                    serialization=core_schema.wrap_serializer_function_ser_schema(function=lambda x, _handler: x),\n                )\n            ],\n        )\n    )\n    assert s.to_python({'root': {'root': {}}}) == {'root': {'root': {}}}", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001434", "source": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "target": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "train", "example_id": "001435", "source": "def run_mlp_layer_norm_gelu(device: str = \"cuda\"):\n    dtype_flops_utilization_map = {\n        torch.bfloat16: \"0.8\",\n    }\n    input_shapes = [1024, 4096, 8192, 16384]\n    intermediate_size = 14336\n    results = []\n    for dtype, expected_flops_utilization in dtype_flops_utilization_map.items():\n        flops_utilization = 0\n        for D in input_shapes:\n            mod = SimpleMLP(\n                input_dim=D, hidden_dim=intermediate_size, output_dim=D, dtype=dtype\n            ).to(device)\n            x = torch.randn(D, device=device, dtype=torch.bfloat16)\n            with FlopCounterMode(display=False) as mode:\n                mod(x)\n            flops = mode.get_total_flops()\n            compiled_mod = torch.compile(mod, dynamic=False)\n            for _ in range(WARMUP_ITER):\n                compiled_mod(x)\n            us_per_iter = benchmarker.benchmark(compiled_mod, (x,), {}) * 1000\n            flops_utilization += us_per_iter * flops / 1e9 / A100_40G_BF16_TFLOPS\n        flops_utilization = flops_utilization / len(input_shapes)\n        dtype_str = str(dtype).replace(\"torch.\", \"\")\n        results.append(\n            Experiment(\n                \"mlp_layer_norm_gelu\",\n                \"flops_utilization\",\n                expected_flops_utilization,\n                f\"{flops_utilization:.02f}\",\n                dtype_str,\n                device,\n                get_arch_name(),\n            )\n        )\n    return results", "target": "def description(self):\n        return \"partitioner benchmark 1 input and 100 weights, mix of recompute and non-recompute ops\"", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "001436", "source": "def test_alias_allow_pop(py_and_json: PyAndJson):\n    schema = core_schema.dataclass_schema(\n        BasicDataclass,\n        core_schema.dataclass_args_schema(\n            'BasicDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), validation_alias='FieldA'),\n            ],\n        ),\n        ['a'],\n        config=core_schema.CoreConfig(validate_by_name=True, validate_by_alias=True),\n    )\n    v = py_and_json(schema)\n    assert v.validate_test({'FieldA': 'hello'}) == BasicDataclass(a='hello')\n    assert v.validate_test({'a': 'hello'}) == BasicDataclass(a='hello')\n    assert v.validate_test(\n        {\n            'FieldA': 'hello',\n            'a': 'world',\n        }\n    ) == BasicDataclass(a='hello')\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'foobar': 'hello'})", "target": "def output_json(output_file, headers, row):\n    mapping_headers = {headers[i]: v for i, v in enumerate(row)}\n    record = {\n        \"benchmark\": {\n            \"name\": \"PyTorch gpt-fast benchmark\",\n            \"mode\": \"inference\",\n            \"dtype\": mapping_headers[\"dtype\"],\n            \"extra_info\": {\n                \"device\": mapping_headers[\"device\"],\n                \"arch\": mapping_headers[\"arch\"],\n            },\n        },\n        \"model\": {\n            \"name\": mapping_headers[\"name\"],\n            \"type\": \"OSS model\" if mapping_headers[\"is_model\"] else \"micro-benchmark\",\n            \"origins\": [\"pytorch\"],\n        },\n        \"metric\": {\n            \"name\": mapping_headers[\"metric\"],\n            \"benchmark_values\": [mapping_headers[\"actual\"]],\n            \"target_value\": mapping_headers[\"target\"],\n        },\n    }\n    with open(f\"{os.path.splitext(output_file)[0]}.json\", \"a\") as f:\n        print(json.dumps(record), file=f)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001437", "source": "def benchmark(self):\n        for M, N in self.get_shapes():\n            print(f\"\\n Tensor dimensions: [{M}, {N}]\")\n            torch_dtype = cutlass_torch.dtype(cutlass.BFloat16)\n            x = 0.1 * torch.randn(M, N, device=\"cuda\", dtype=torch_dtype)\n            target = torch.randint(0, N, (M,), device=\"cuda\", dtype=torch.int64)\n            self.benchmark_single_shape((x, target), setting=f\"shape: [{M}, {N}]\")", "target": "def benchmark(self):\n        for M, N in self.get_shapes():\n            print(f\"Tensor dimensions: [{M}, {N}]\")\n            torch_dtype = cutlass_torch.dtype(cutlass.BFloat16)\n            x = torch.randn(M, N, device=\"cuda\", dtype=torch_dtype)\n            w = torch.randn(N, device=\"cuda\", dtype=torch.float32)\n            self.benchmark_single_shape((x, w), setting=f\"shape: [{M}, {N}]\")", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "001438", "source": "def eqDictIntKeyIntValue(self, input: dict[int, int]) -> dict[int, int]:\n        return input", "target": "def test_custom_error(py_and_json: PyAndJson):\n    v = py_and_json(\n        core_schema.custom_error_schema(core_schema.int_schema(), 'foobar', custom_error_message='Hello there')\n    )\n    assert v.validate_test(1) == 1\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_test('foobar')\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'foobar', 'loc': (), 'msg': 'Hello there', 'input': 'foobar'}\n    ]", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "001439", "source": "def test_tz_info_copy():\n    output = SchemaValidator(cs.datetime_schema()).validate_python('2023-02-15T16:23:44.037Z')\n    c = copy.copy(output)\n    assert repr(output.tzinfo) == 'TzInfo(0)'\n    assert repr(c.tzinfo) == 'TzInfo(0)'\n    assert c == output", "target": "def test_parse_vector_rect_not_convertible(self):\n        np.random.seed(1238765)\n        arr = np.random.randint(5, 20, 4 * 3).astype(np.float32).reshape(3, 4)\n        for not_convertible in (((1, 2, 3, 4), (10.5, -20, 30.1, 10)), arr,\n                                [[5, 3, 1, 4], []],\n                                ((float(4), np.uint8(10), int(32), np.int16(55)),)):\n            with self.assertRaises(TypeError, msg=get_no_exception_msg(not_convertible)):\n                _ = cv.utils.dumpVectorOfRect(not_convertible)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "001440", "source": "def get_cmake_cmd(self):\n        cmd = [\n            \"cmake\",\n            \"-DPYTHON_DEFAULT_EXECUTABLE=%s\" % sys.executable,\n               \"-DENABLE_PIC=FALSE\",\n               \"-DCMAKE_BUILD_TYPE=Release\",\n               \"-DCPU_BASELINE=''\",\n               \"-DCMAKE_INSTALL_PREFIX=/usr/local\",\n               \"-DCPU_DISPATCH=''\",\n               \"-DCV_TRACE=OFF\",\n               \"-DBUILD_SHARED_LIBS=OFF\",\n               \"-DWITH_1394=OFF\",\n               \"-DWITH_ADE=OFF\",\n               \"-DWITH_VTK=OFF\",\n               \"-DWITH_EIGEN=OFF\",\n               \"-DWITH_FFMPEG=OFF\",\n               \"-DWITH_GSTREAMER=OFF\",\n               \"-DWITH_GTK=OFF\",\n               \"-DWITH_GTK_2_X=OFF\",\n               \"-DWITH_IPP=OFF\",\n               \"-DWITH_AVIF=OFF\",\n               \"-DWITH_JASPER=OFF\",\n               \"-DWITH_JPEG=OFF\",\n               \"-DWITH_WEBP=OFF\",\n               \"-DWITH_OPENEXR=OFF\",\n               \"-DWITH_OPENJPEG=OFF\",\n               \"-DWITH_OPENGL=OFF\",\n               \"-DWITH_OPENVX=OFF\",\n               \"-DWITH_OPENNI=OFF\",\n               \"-DWITH_OPENNI2=OFF\",\n               \"-DWITH_PNG=OFF\",\n               \"-DWITH_TBB=OFF\",\n               \"-DWITH_TIFF=OFF\",\n               \"-DWITH_V4L=OFF\",\n               \"-DWITH_OPENCL=OFF\",\n               \"-DWITH_OPENCL_SVM=OFF\",\n               \"-DWITH_OPENCLAMDFFT=OFF\",\n               \"-DWITH_OPENCLAMDBLAS=OFF\",\n               \"-DWITH_GPHOTO2=OFF\",\n               \"-DWITH_LAPACK=OFF\",\n               \"-DWITH_ITT=OFF\",\n               \"-DWITH_QUIRC=OFF\",\n               \"-DBUILD_ZLIB=ON\",\n               \"-DBUILD_opencv_apps=OFF\",\n               \"-DBUILD_opencv_calib3d=ON\",\n               \"-DBUILD_opencv_dnn=ON\",\n               \"-DBUILD_opencv_features2d=ON\",\n               \"-DBUILD_opencv_flann=ON\",\n               \"-DBUILD_opencv_gapi=OFF\",\n               \"-DBUILD_opencv_ml=OFF\",\n               \"-DBUILD_opencv_photo=ON\",\n               \"-DBUILD_opencv_imgcodecs=OFF\",\n               \"-DBUILD_opencv_shape=OFF\",\n               \"-DBUILD_opencv_videoio=OFF\",\n               \"-DBUILD_opencv_videostab=OFF\",\n               \"-DBUILD_opencv_highgui=OFF\",\n               \"-DBUILD_opencv_superres=OFF\",\n               \"-DBUILD_opencv_stitching=OFF\",\n               \"-DBUILD_opencv_java=OFF\",\n               \"-DBUILD_opencv_js=ON\",\n               \"-DBUILD_opencv_python2=OFF\",\n               \"-DBUILD_opencv_python3=OFF\",\n               \"-DBUILD_EXAMPLES=ON\",\n               \"-DBUILD_PACKAGE=OFF\",\n               \"-DBUILD_TESTS=ON\",\n               \"-DBUILD_PERF_TESTS=ON\"]\n        if self.options.cmake_option:\n            cmd += self.options.cmake_option\n        if not self.options.cmake_option or all([\"-DCMAKE_TOOLCHAIN_FILE\" not in opt for opt in self.options.cmake_option]):\n            cmd.append(\"-DCMAKE_TOOLCHAIN_FILE='%s'\" % self.get_toolchain_file())\n        if self.options.build_doc:\n            cmd.append(\"-DBUILD_DOCS=ON\")\n        else:\n            cmd.append(\"-DBUILD_DOCS=OFF\")\n        if self.options.threads:\n            cmd.append(\"-DWITH_PTHREADS_PF=ON\")\n        else:\n            cmd.append(\"-DWITH_PTHREADS_PF=OFF\")\n        if self.options.simd:\n            cmd.append(\"-DCV_ENABLE_INTRINSICS=ON\")\n        else:\n            cmd.append(\"-DCV_ENABLE_INTRINSICS=OFF\")\n        if self.options.build_wasm_intrin_test:\n            cmd.append(\"-DBUILD_WASM_INTRIN_TESTS=ON\")\n        else:\n            cmd.append(\"-DBUILD_WASM_INTRIN_TESTS=OFF\")\n        if self.options.webnn:\n            cmd.append(\"-DWITH_WEBNN=ON\")\n        flags = self.get_build_flags()\n        if flags:\n            cmd += [\"-DCMAKE_C_FLAGS='%s'\" % flags,\n                    \"-DCMAKE_CXX_FLAGS='%s'\" % flags]\n        if self.options.extra_modules:\n            cmd.append(\"-DOPENCV_EXTRA_MODULES_PATH='%s'\" % self.options.extra_modules)\n        return cmd", "target": "def test_dataclass_args(py_and_json: PyAndJson, input_value, expected):\n    schema = core_schema.dataclass_args_schema(\n        'MyDataclass',\n        [\n            core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), kw_only=False),\n            core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), kw_only=False),\n        ],\n    )\n    v = py_and_json(schema)\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)) as exc_info:\n            v.validate_test(input_value)\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        assert v.validate_test(input_value) == expected", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "001441", "source": "def test_ser_function_plain():\n    def f(input: Any, info: core_schema.SerializationInfo, /) -> str:\n        return str(info)\n    s = SchemaSerializer(\n        core_schema.any_schema(\n            serialization=core_schema.plain_serializer_function_ser_schema(\n                f, info_arg=True, return_schema=core_schema.str_schema()\n            )\n        )\n    )\n    assert s.to_python(123) == (\n        \"SerializationInfo(include=None, exclude=None, context=None, mode='python', by_alias=False, exclude_unset=False, \"\n        'exclude_defaults=False, exclude_none=False, exclude_computed_fields=False, round_trip=False, serialize_as_any=False)'\n    )", "target": "def test_houghcircles(self):\n        fn = \"samples/data/board.jpg\"\n        src = self.get_sample(fn, 1)\n        img = cv.cvtColor(src, cv.COLOR_BGR2GRAY)\n        img = cv.medianBlur(img, 5)\n        circles = cv.HoughCircles(img, cv.HOUGH_GRADIENT, 1, 10, np.array([]), 100, 30, 1, 30)[0]\n        testCircles = [[38, 181, 17.6],\n        [99.7, 166, 13.12],\n        [142.7, 160, 13.52],\n        [223.6, 110, 8.62],\n        [79.1, 206.7, 8.62],\n        [47.5, 351.6, 11.64],\n        [189.5, 354.4, 11.64],\n        [189.8, 298.9, 10.64],\n        [189.5, 252.4, 14.62],\n        [252.5, 393.4, 15.62],\n        [602.9, 467.5, 11.42],\n        [222, 210.4, 9.12],\n        [263.1, 216.7, 9.12],\n        [359.8, 222.6, 9.12],\n        [518.9, 120.9, 9.12],\n        [413.8, 113.4, 9.12],\n        [489, 127.2, 9.12],\n        [448.4, 121.3, 9.12],\n        [384.6, 128.9, 8.62]]\n        matches_counter = 0\n        for i in range(len(testCircles)):\n            for j in range(len(circles)):\n                tstCircle = circleApproximation(testCircles[i])\n                circle = circleApproximation(circles[j])\n                if convContoursIntersectiponRate(tstCircle, circle) > 0.6:\n                    matches_counter += 1\n        self.assertGreater(float(matches_counter) / len(testCircles), .5)\n        self.assertLess(float(len(circles) - matches_counter) / len(circles), .75)\n        circles_acc = cv.HoughCirclesWithAccumulator(\n            image=img,\n            method=cv.HOUGH_GRADIENT,\n            dp=1,\n            minDist=10,\n            circles=np.array([]),\n            param1=150,\n            param2=45,\n            minRadius=1,\n            maxRadius=30)\n        self.assertEqual(circles_acc.shape, (1, 2, 4))\n        self.assertEqual(circles_acc[0, 0, 3], 66.)\n        self.assertEqual(circles_acc[0, 1, 3], 62.)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001442", "source": "def required_usage_imports(self) -> Generator[str, None, None]:\n        yield \"import cv2.typing\"", "target": "def required_usage_imports(self) -> Generator[str, None, None]:\n        yield \"import cv2.typing\"", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|opencv/opencv", "data_split": "train", "example_id": "001443", "source": "def setup_twenty_newsgroups():\n    cache_path = _pkl_filepath(get_data_home(), CACHE_NAME)\n    if not exists(cache_path):\n        raise SkipTest(\"Skipping dataset loading doctests\")", "target": "def resolve(self, root: ASTNode):\n        if self.is_resolved:\n            return\n        node = _resolve_symbol(root, self.typename)\n        if node is None:\n            raise TypeResolutionError('Failed to resolve \"{}\" exposed as \"{}\"'.format(\n                self.ctype_name, self.typename\n            ))\n        self._ast_node = weakref.proxy(node)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001444", "source": "def quack(self, args, kwargs=None) -> Any:\n        from quack.softmax import softmax\n        assert kwargs is None\n        x, dy = args\n        y = softmax(x)\n        return lambda: torch.autograd.grad(y, x, grad_outputs=dy, retain_graph=True)", "target": "def quack(self, args, kwargs) -> Any:\n        from quack.layernorm import layernorm\n        x, w = args\n        return lambda: layernorm(x, w, eps=1e-6)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001445", "source": "def test_length_ctx():\n    v = SchemaValidator(cs.list_schema(min_length=2, max_length=3))\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python([1])\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'too_short',\n            'loc': (),\n            'msg': 'List should have at least 2 items after validation, not 1',\n            'input': [1],\n            'ctx': {'field_type': 'List', 'min_length': 2, 'actual_length': 1},\n        }\n    ]\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python([1, 2, 3, 4])\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'too_long',\n            'loc': (),\n            'msg': 'List should have at most 3 items after validation, not 4',\n            'input': [1, 2, 3, 4],\n            'ctx': {'field_type': 'List', 'max_length': 3, 'actual_length': 4},\n        }\n    ]", "target": "def test_smart_union_does_nested_dataclass_field_counting() -> None:\n    @dataclass\n    class SubModelA:\n        x: int = 1\n    @dataclass\n    class SubModelB:\n        y: int = 2\n    @dataclass\n    class ModelA:\n        sub: SubModelA\n    @dataclass\n    class ModelB:\n        sub: SubModelB\n    dc_a_schema = core_schema.dataclass_schema(\n        ModelA,\n        core_schema.dataclass_args_schema(\n            'ModelA',\n            [\n                core_schema.dataclass_field(\n                    'sub',\n                    core_schema.with_default_schema(\n                        core_schema.dataclass_schema(\n                            SubModelA,\n                            core_schema.dataclass_args_schema(\n                                'SubModelA',\n                                [\n                                    core_schema.dataclass_field(\n                                        'x', core_schema.with_default_schema(core_schema.int_schema(), default=1)\n                                    )\n                                ],\n                            ),\n                            ['x'],\n                        ),\n                        default=SubModelA(),\n                    ),\n                )\n            ],\n        ),\n        ['sub'],\n    )\n    dc_b_schema = core_schema.dataclass_schema(\n        ModelB,\n        core_schema.dataclass_args_schema(\n            'ModelB',\n            [\n                core_schema.dataclass_field(\n                    'sub',\n                    core_schema.with_default_schema(\n                        core_schema.dataclass_schema(\n                            SubModelB,\n                            core_schema.dataclass_args_schema(\n                                'SubModelB',\n                                [\n                                    core_schema.dataclass_field(\n                                        'y', core_schema.with_default_schema(core_schema.int_schema(), default=2)\n                                    )\n                                ],\n                            ),\n                            ['y'],\n                        ),\n                        default=SubModelB(),\n                    ),\n                )\n            ],\n        ),\n        ['sub'],\n    )\n    for choices in permute_choices([dc_a_schema, dc_b_schema]):\n        validator = SchemaValidator(core_schema.union_schema(choices=choices))\n        assert isinstance(validator.validate_python({'sub': {'x': 1}}), ModelA)\n        assert isinstance(validator.validate_python({'sub': {'y': 3}}), ModelB)\n        assert isinstance(validator.validate_python({'sub': {}}), choices[0]['cls'])", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001446", "source": "def dec(f: Callable[..., Any] | classmethod[Any, Any, Any] | staticmethod[Any, Any]) -> Any:\n        if _decorators.is_instance_method_from_sig(f):\n            raise TypeError('`@root_validator` cannot be applied to instance methods')\n        res = _decorators.ensure_classmethod_based_on_signature(f)\n        dec_info = _decorators.RootValidatorDecoratorInfo(mode=mode)\n        return _decorators.PydanticDescriptorProxy(res, dec_info, shim=wrap)", "target": "def dec(f: ModelSerializer) -> _decorators.PydanticDescriptorProxy[Any]:\n        dec_info = _decorators.ModelSerializerDecoratorInfo(mode=mode, return_type=return_type, when_used=when_used)\n        return _decorators.PydanticDescriptorProxy(f, dec_info)", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001447", "source": "def predict(self, samples):\n        _retval, results, _neigh_resp, _dists = self.model.findNearest(samples, self.k)\n        return results.ravel()", "target": "def predict(self, samples):\n        _ret, resp = self.model.predict(samples)\n        return resp.ravel()", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001448", "source": "def test_custom_serializer():\n    s = SchemaSerializer(core_schema.any_schema(serialization=core_schema.simple_ser_schema('json')))\n    assert s.to_python({1: 2}) == {1: 2}\n    assert s.to_python({1: 2}, mode='json') == {'1': 2}\n    assert s.to_python({1: 2}, mode='json', round_trip=True) == '{\"1\":2}'\n    assert s.to_json({1: 2}) == b'{\"1\":2}'\n    assert s.to_json({1: 2}, round_trip=True) == b'\"{\\\\\"1\\\\\":2}\"'", "target": "def test_custom_serializer():\n    s = SchemaSerializer(core_schema.any_schema(serialization=core_schema.simple_ser_schema('multi-host-url')))\n    multi_host_url = MultiHostUrl('https://ex.com,ex.org/path')\n    assert s.to_python(multi_host_url) == multi_host_url", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001449", "source": "def typename(self) -> Optional[str]:\n            return getattr(self.type_node, \"full_typename\", None)", "target": "def typename(self) -> str:\n        return self.type_node.full_typename", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001450", "source": "def compiled(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, target, dloss = args\n        compiled_cross_entropy = torch.compile(\n            lambda x, target: F.cross_entropy(x, target, reduction=\"none\"),\n            mode=self.compile_mode,\n            fullgraph=True,\n        )\n        loss = compiled_cross_entropy(x, target)\n        return lambda: torch.autograd.grad(\n            loss, x, grad_outputs=dloss, retain_graph=True\n        )", "target": "def compiled(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, w, dy = args\n        compiled_layernorm = torch.compile(\n            self.layernorm_ref, mode=self.compile_mode, fullgraph=True\n        )\n        y = compiled_layernorm(x, w)\n        return lambda: torch.autograd.grad(\n            y, [x, w], grad_outputs=dy, retain_graph=True\n        )", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "001451", "source": "def make_data(self, params):\n        data = _synth_classification_dataset(n_samples=10000, n_features=100)\n        return data", "target": "def make_data(self, params):\n        (representation,) = params\n        if representation == \"dense\":\n            data = _synth_regression_dataset(n_samples=100000, n_features=200)\n        else:\n            data = _synth_regression_sparse_dataset(\n                n_samples=100000, n_features=1000, density=0.01\n            )\n        return data", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "001452", "source": "def compute_bench(samples_range, features_range):\n    it = 0\n    results = defaultdict(lambda: [])\n    max_it = len(samples_range) * len(features_range)\n    for n_samples in samples_range:\n        for n_features in features_range:\n            it += 1\n            print(\"==============================\")\n            print(\"Iteration %03d of %03d\" % (it, max_it))\n            print(\"n_samples %05d; n_features %02d\" % (n_samples, n_features))\n            print(\"==============================\")\n            print()\n            data = nr.randint(-50, 51, (n_samples, n_features))\n            for linkage in (\"single\", \"average\", \"complete\", \"ward\"):\n                print(linkage.capitalize())\n                tstart = time()\n                AgglomerativeClustering(linkage=linkage, n_clusters=10).fit(data)\n                delta = time() - tstart\n                print(\"Speed: %0.3fs\" % delta)\n                print()\n                results[linkage].append(delta)\n    return results", "target": "def compute_bench(samples_range, features_range):\n    it = 0\n    results = defaultdict(lambda: [])\n    max_it = len(samples_range) * len(features_range)\n    for n_samples in samples_range:\n        for n_features in features_range:\n            it += 1\n            print(\"====================\")\n            print(\"Iteration %03d of %03d\" % (it, max_it))\n            print(\"====================\")\n            dataset_kwargs = {\n                \"n_samples\": n_samples,\n                \"n_features\": n_features,\n                \"n_informative\": n_features // 10,\n                \"effective_rank\": min(n_samples, n_features) / 10,\n                \"bias\": 0.0,\n            }\n            print(\"n_samples: %d\" % n_samples)\n            print(\"n_features: %d\" % n_features)\n            X, y = make_regression(**dataset_kwargs)\n            gc.collect()\n            print(\"benchmarking lars_path (with Gram):\", end=\"\")\n            sys.stdout.flush()\n            tstart = time()\n            G = np.dot(X.T, X)\n            Xy = np.dot(X.T, y)\n            lars_path_gram(Xy=Xy, Gram=G, n_samples=y.size, method=\"lasso\")\n            delta = time() - tstart\n            print(\"%0.3fs\" % delta)\n            results[\"lars_path (with Gram)\"].append(delta)\n            gc.collect()\n            print(\"benchmarking lars_path (without Gram):\", end=\"\")\n            sys.stdout.flush()\n            tstart = time()\n            lars_path(X, y, method=\"lasso\")\n            delta = time() - tstart\n            print(\"%0.3fs\" % delta)\n            results[\"lars_path (without Gram)\"].append(delta)\n            gc.collect()\n            print(\"benchmarking lasso_path (with Gram):\", end=\"\")\n            sys.stdout.flush()\n            tstart = time()\n            lasso_path(X, y, precompute=True)\n            delta = time() - tstart\n            print(\"%0.3fs\" % delta)\n            results[\"lasso_path (with Gram)\"].append(delta)\n            gc.collect()\n            print(\"benchmarking lasso_path (without Gram):\", end=\"\")\n            sys.stdout.flush()\n            tstart = time()\n            lasso_path(X, y, precompute=False)\n            delta = time() - tstart\n            print(\"%0.3fs\" % delta)\n            results[\"lasso_path (without Gram)\"].append(delta)\n    return results", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001453", "source": "def validate_b(cls, v: str, info: core_schema.ValidationInfo) -> str:\n            assert v == 'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return 'hello world!'", "target": "def validate_b(cls, v: bytes, info: core_schema.ValidationInfo) -> bytes:\n            assert v == b'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return b'hello world!'", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "train", "example_id": "001454", "source": "def check_lib_statically_linked_libstdc_cxx_abi_symbols(lib: str) -> None:\n    cxx11_statically_linked_symbols = grep_symbols(\n        lib, STATICALLY_LINKED_CXX11_ABI, symbol_type=\"T\"\n    )\n    num_statically_linked_symbols = len(cxx11_statically_linked_symbols)\n    print(f\"num_statically_linked_symbols (T): {num_statically_linked_symbols}\")\n    if num_statically_linked_symbols > 0:\n        raise RuntimeError(\n            f\"Found statically linked libstdc++ symbols (recursive_directory_iterator): {cxx11_statically_linked_symbols[:100]}\"\n        )", "target": "def forward(sequences, hidden):\n        packed = rnn_utils.pack_sequence(sequences, enforce_sorted=False)\n        out, new_hidden = module(packed, hidden)\n        padded, lengths = rnn_utils.pad_packed_sequence(out)\n        return padded, new_hidden", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001455", "source": "def my_function(a, b, c):\n        return a + b + c", "target": "def my_function(a, b, c):\n        return a + b + c", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "001456", "source": "def test_lax_or_strict_custom_ser():\n    s = SchemaSerializer(\n        core_schema.lax_or_strict_schema(\n            core_schema.int_schema(),\n            core_schema.str_schema(),\n            serialization=core_schema.format_ser_schema('^5s', when_used='always'),\n        )\n    )\n    assert s.to_python('abc') == ' abc '\n    assert s.to_python('abc', mode='json') == ' abc '\n    assert s.to_json('abc') == b'\" abc \"'", "target": "def runTracker(self):\n        framesCounter = 0\n        self.selection = True\n        xmin, ymin, xmax, ymax = self.render.getCurrentRect()\n        self.track_window = (xmin, ymin, xmax - xmin, ymax - ymin)\n        while True:\n            framesCounter += 1\n            self.frame = self.render.getNextFrame()\n            hsv = cv.cvtColor(self.frame, cv.COLOR_BGR2HSV)\n            mask = cv.inRange(hsv, np.array((0., 60., 32.)), np.array((180., 255., 255.)))\n            if self.selection:\n                x0, y0, x1, y1 = self.render.getCurrentRect() + 50\n                x0 -= 100\n                y0 -= 100\n                hsv_roi = hsv[y0:y1, x0:x1]\n                mask_roi = mask[y0:y1, x0:x1]\n                hist = cv.calcHist( [hsv_roi], [0], mask_roi, [16], [0, 180] )\n                cv.normalize(hist, hist, 0, 255, cv.NORM_MINMAX)\n                self.hist = hist.reshape(-1)\n                self.selection = False\n            if self.track_window and self.track_window[2] > 0 and self.track_window[3] > 0:\n                self.selection = None\n                prob = cv.calcBackProject([hsv], [0], self.hist, [0, 180], 1)\n                prob &= mask\n                term_crit = ( cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 1 )\n                _track_box, self.track_window = cv.CamShift(prob, self.track_window, term_crit)\n            trackingRect = np.array(self.track_window)\n            trackingRect[2] += trackingRect[0]\n            trackingRect[3] += trackingRect[1]\n            if intersectionRate(self.render.getCurrentRect(), trackingRect) < 0.4:\n                self.errors += 1\n            if framesCounter > self.framesNum:\n                break\n        self.assertLess(float(self.errors) / self.framesNum, 0.4)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "train", "example_id": "001457", "source": "def read_models_from_docs():\n        models = set()\n        for fn in glob.glob(\"../pytorch-image-models/docs/models/*.md\"):\n            with open(fn) as f:\n                while True:\n                    line = f.readline()\n                    if not line:\n                        break\n                    if not line.startswith(\"model = timm.create_model(\"):\n                        continue\n                    model = line.split(\"'\")[1]\n                    models.add(model)\n        return models", "target": "def get_wheels(\n    output_dir: Path,\n    max_depth: Optional[int] = None,\n) -> list[str]:\n    root = Path(output_dir)\n    if not root.exists():\n        return []\n    items = []\n    for dirpath, _, filenames in os.walk(root):\n        depth = Path(dirpath).relative_to(root).parts\n        if max_depth is not None and len(depth) > max_depth:\n            continue\n        for fname in sorted(filenames):\n            if fname.endswith(\".whl\"):\n                pkg = fname.split(\"-\")[0]\n                relpath = str((Path(dirpath) / fname).relative_to(root))\n                items.append({\"pkg\": pkg, \"relpath\": relpath})\n    return items", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001458", "source": "def test_by_alias_and_name_config_interaction(\n    config_by_alias: Union[bool, None],\n    config_by_name: Union[bool, None],\n    runtime_by_alias: Union[bool, None],\n    runtime_by_name: Union[bool, None],\n) -> None:\n    if config_by_alias is False and config_by_name is False and runtime_by_alias is False and runtime_by_name is False:\n        pytest.skip(\"Can't have both by_alias and by_name as effectively False\")\n    core_config = {\n        **({'validate_by_alias': config_by_alias} if config_by_alias is not None else {}),\n        **({'validate_by_name': config_by_name} if config_by_name is not None else {}),\n    }\n    @dataclasses.dataclass\n    class MyDataclass:\n        my_field: int\n    schema = core_schema.dataclass_schema(\n        MyDataclass,\n        core_schema.dataclass_args_schema(\n            'MyDataclass',\n            [\n                core_schema.dataclass_field(\n                    name='my_field', schema=core_schema.int_schema(), validation_alias='my_alias'\n                ),\n            ],\n        ),\n        ['my_field'],\n        config=core_schema.CoreConfig(**core_config),\n    )\n    s = SchemaValidator(schema)\n    alias_allowed = next(x for x in (runtime_by_alias, config_by_alias, True) if x is not None)\n    name_allowed = next(x for x in (runtime_by_name, config_by_name, False) if x is not None)\n    if alias_allowed:\n        assert dataclasses.asdict(\n            s.validate_python({'my_alias': 1}, by_alias=runtime_by_alias, by_name=runtime_by_name)\n        ) == {'my_field': 1}\n    if name_allowed:\n        assert dataclasses.asdict(\n            s.validate_python({'my_field': 1}, by_alias=runtime_by_alias, by_name=runtime_by_name)\n        ) == {'my_field': 1}", "target": "def test_by_alias_and_name_config_interaction(\n    config_by_alias: Union[bool, None],\n    config_by_name: Union[bool, None],\n    runtime_by_alias: Union[bool, None],\n    runtime_by_name: Union[bool, None],\n) -> None:\n    if config_by_alias is False and config_by_name is False and runtime_by_alias is False and runtime_by_name is False:\n        pytest.skip(\"Can't have both by_alias and by_name as effectively False\")\n    core_config = {\n        **({'validate_by_alias': config_by_alias} if config_by_alias is not None else {}),\n        **({'validate_by_name': config_by_name} if config_by_name is not None else {}),\n    }\n    schema = core_schema.typed_dict_schema(\n        fields={\n            'my_field': core_schema.typed_dict_field(schema=core_schema.int_schema(), validation_alias='my_alias'),\n        },\n        config=core_schema.CoreConfig(**core_config),\n    )\n    s = SchemaValidator(schema)\n    alias_allowed = next(x for x in (runtime_by_alias, config_by_alias, True) if x is not None)\n    name_allowed = next(x for x in (runtime_by_name, config_by_name, False) if x is not None)\n    if alias_allowed:\n        assert s.validate_python({'my_alias': 1}, by_alias=runtime_by_alias, by_name=runtime_by_name) == {'my_field': 1}\n    if name_allowed:\n        assert s.validate_python({'my_field': 1}, by_alias=runtime_by_alias, by_name=runtime_by_name) == {'my_field': 1}", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "001459", "source": "def test_constraints_schema_validation_error(constraint: str) -> None:\n    with pytest.raises(SchemaError, match=f\"'{constraint}' must be coercible to an integer\"):\n        SchemaValidator(cs.int_schema(**{constraint: 'bad_value'}))", "target": "def patch_init_py(\n    path: Path, *, version: str, expected_version: Optional[str] = None\n) -> None:\n    if not expected_version:\n        expected_version = read_triton_version()\n    with open(path) as f:\n        orig = f.read()\n    orig = check_and_replace(\n        orig, f\"__version__ = '{expected_version}'\", f'__version__ = \"{version}\"'\n    )\n    with open(path, \"w\") as f:\n        f.write(orig)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001460", "source": "def get_memory_bytes(self, args, kwargs) -> int:\n        x, w, dy = args\n        M, N = x.shape\n        return 3 * M * N * x.dtype.itemsize + 2 * N * w.dtype.itemsize", "target": "def get_memory_bytes(self, args, kwargs) -> int:\n        x, w = args\n        M, N = x.shape\n        return 2 * M * N * x.dtype.itemsize + N * w.dtype.itemsize", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001461", "source": "def ser_x(self, v: Any, _) -> str:\n            assert self.x == 1_000\n            return f'{v:_}'", "target": "def ser_x(data: Model, v: Any, serializer: core_schema.SerializerFunctionWrapHandler) -> str:\n        assert data['x'] == 1_000\n        x = serializer(v)\n        return f'{x:_}'", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|opencv/opencv", "data_split": "train", "example_id": "001462", "source": "def process(self, frameworks, data_fetcher):\n        sorted_imgs_names = sorted(self.img_classes.keys())\n        correct_answers = [0] * len(frameworks)\n        samples_handled = 0\n        blobs_l1_diff = [0] * len(frameworks)\n        blobs_l1_diff_count = [0] * len(frameworks)\n        blobs_l_inf_diff = [sys.float_info.min] * len(frameworks)\n        inference_time = [0.0] * len(frameworks)\n        for x in xrange(0, len(sorted_imgs_names), self.batch_size):\n            sublist = sorted_imgs_names[x:x + self.batch_size]\n            batch = data_fetcher.get_batch(sublist)\n            samples_handled += len(sublist)\n            frameworks_out = []\n            fw_accuracy = []\n            for i in range(len(frameworks)):\n                start = time.time()\n                out = frameworks[i].get_output(batch)\n                end = time.time()\n                correct_answers[i] += get_correct_answers(sublist, self.img_classes, out)\n                fw_accuracy.append(100 * correct_answers[i] / float(samples_handled))\n                frameworks_out.append(out)\n                inference_time[i] += end - start\n                print(samples_handled, 'Accuracy for', frameworks[i].get_name() + ':', fw_accuracy[i], file=self.log)\n                print(\"Inference time, ms \", \\\n                    frameworks[i].get_name(), inference_time[i] / samples_handled * 1000, file=self.log)\n            for i in range(1, len(frameworks)):\n                log_str = frameworks[0].get_name() + \" vs \" + frameworks[i].get_name() + ':'\n                diff = np.abs(frameworks_out[0] - frameworks_out[i])\n                l1_diff = np.sum(diff) / diff.size\n                print(samples_handled, \"L1 difference\", log_str, l1_diff, file=self.log)\n                blobs_l1_diff[i] += l1_diff\n                blobs_l1_diff_count[i] += 1\n                if np.max(diff) > blobs_l_inf_diff[i]:\n                    blobs_l_inf_diff[i] = np.max(diff)\n                print(samples_handled, \"L_INF difference\", log_str, blobs_l_inf_diff[i], file=self.log)\n            self.log.flush()\n        for i in range(1, len(blobs_l1_diff)):\n            log_str = frameworks[0].get_name() + \" vs \" + frameworks[i].get_name() + ':'\n            print('Final l1 diff', log_str, blobs_l1_diff[i] / blobs_l1_diff_count[i], file=self.log)", "target": "def make_sysroot(self):\n        cfg_file = str(self.cpath / 'sysroot.config.py')\n        with open(cfg_file, 'r') as f:\n            cfg = f.read()\n        exec(compile(cfg, cfg_file, 'exec'))\n        log.info('DLDT sysroot preparation completed')", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001463", "source": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "target": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001464", "source": "def test_pandas():\n    v = SchemaSerializer(core_schema.timedelta_schema())\n    d = pandas.Timestamp('2023-01-01T02:00:00Z') - pandas.Timestamp('2023-01-01T00:00:00Z')\n    assert v.to_python(d) == d\n    assert v.to_python(d, mode='json') == 'PT2H'\n    assert v.to_json(d) == b'\"PT2H\"'", "target": "def decimal_places_validator(x: Any, decimal_places: Any) -> Any:\n    try:\n        decimal_places_, _ = _extract_decimal_digits_info(x)\n        if decimal_places_ > decimal_places:\n            normalized_decimal_places, _ = _extract_decimal_digits_info(x.normalize())\n            if normalized_decimal_places > decimal_places:\n                raise PydanticKnownError(\n                    'decimal_max_places',\n                    {'decimal_places': decimal_places},\n                )\n        return x\n    except TypeError:\n        raise TypeError(f\"Unable to apply constraint 'decimal_places' to supplied value {x}\")", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "train", "example_id": "001465", "source": "def setup_caches(self, max_batch_size, max_seq_length):\n        if (\n            self.max_seq_length >= max_seq_length\n            and self.max_batch_size >= max_batch_size\n        ):\n            return\n        head_dim = self.config.dim // self.config.n_head\n        max_seq_length = find_multiple(max_seq_length, 8)\n        self.max_seq_length = max_seq_length\n        self.max_batch_size = max_batch_size\n        for b in self.layers:\n            b.attention.kv_cache = KVCache(\n                max_batch_size, max_seq_length, self.config.n_local_heads, head_dim\n            )\n        self.freqs_cis = precompute_freqs_cis(\n            self.config.block_size,\n            self.config.dim // self.config.n_head,\n            self.config.rope_base,\n        )\n        self.causal_mask = torch.tril(\n            torch.ones(self.max_seq_length, self.max_seq_length, dtype=torch.bool)\n        )", "target": "def simple_backward(output, grad_output, **kwargs):\n    return output.backward(grad_output, **kwargs)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|scikit-learn/scikit-learn", "data_split": "train", "example_id": "001466", "source": "def setUp(self):\n        cv.setRNGSeed(10)\n        self.image_cache = {}", "target": "def make_estimator(self, params):\n        representation, precompute = params\n        estimator = Lasso(precompute=precompute, alpha=0.001, random_state=0)\n        return estimator", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "train", "example_id": "001467", "source": "def get_fcn_resnet(device: torch.device) -> GetterReturnType:\n    N = 8\n    criterion = torch.nn.MSELoss()\n    model = models.fcn_resnet50(pretrained=False, pretrained_backbone=False)\n    if has_functorch:\n        from functorch.experimental import replace_all_batch_norm_modules_\n        replace_all_batch_norm_modules_(model)\n        model.eval()\n    model.to(device)\n    params, names = extract_weights(model)\n    inputs = torch.rand([N, 3, 480, 480], device=device)\n    labels = torch.rand([N, 21, 480, 480], device=device)\n    def forward(*new_params: Tensor) -> Tensor:\n        load_weights(model, names, new_params)\n        out = model(inputs)[\"out\"]\n        loss = criterion(out, labels)\n        return loss\n    return forward, params", "target": "def main():\n    result_path = sys.argv[1]\n    Benchmark().enable_compile_time_instruction_count().collect_all().append_results(\n        result_path\n    )", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|scikit-learn/scikit-learn", "data_split": "train", "example_id": "001468", "source": "def test_compute_diff(self) -> None:\n        diff = self.repo.diff(\"HEAD\")\n        sha = _shasum(diff)\n        self.assertEqual(len(sha), 64)", "target": "def make_estimator(self, params):\n        representation, precompute = params\n        estimator = ElasticNet(precompute=precompute, alpha=0.001, random_state=0)\n        return estimator", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001469", "source": "def test_positional_or_keyword(py_and_json: PyAndJson, input_value, expected) -> None:\n    v = py_and_json(\n        cs.arguments_v3_schema(\n            [\n                cs.arguments_v3_parameter(name='a', schema=cs.int_schema(), mode='positional_or_keyword'),\n                cs.arguments_v3_parameter(\n                    name='b',\n                    schema=cs.with_default_schema(cs.bool_schema(), default=True),\n                    mode='positional_or_keyword',\n                ),\n            ]\n        )\n    )\n    assert v.validate_test(input_value) == expected", "target": "def test_positional_or_keyword(py_and_json: PyAndJson, input_value, expected):\n    v = py_and_json(\n        {\n            'type': 'arguments',\n            'arguments_schema': [\n                {'name': 'a', 'mode': 'positional_or_keyword', 'schema': {'type': 'int'}},\n                {'name': 'b', 'schema': {'type': 'str'}},\n                {'name': 'c', 'mode': 'keyword_only', 'schema': {'type': 'bool'}},\n            ],\n        }\n    )\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)) as exc_info:\n            v.validate_test(input_value)\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        assert v.validate_test(input_value) == expected", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "001470", "source": "def should_init_forbid_extra(self, fields: list[PydanticModelField], config: ModelConfigData) -> bool:\n        if not (config.validate_by_name or config.populate_by_name):\n            if self.is_dynamic_alias_present(fields, bool(config.has_alias_generator)):\n                return False\n        if config.forbid_extra:\n            return True\n        return self.plugin_config.init_forbid_extra", "target": "def test_letter_recog(self):\n        eps = 0.01\n        models = [RTrees, KNearest, Boost, SVM, MLP]\n        models = dict( [(cls.__name__.lower(), cls) for cls in models] )\n        testErrors = {RTrees: (98.930000, 92.390000), KNearest: (94.960000, 92.010000),\n         Boost: (85.970000, 74.920000), SVM: (99.780000, 95.680000), MLP: (90.060000, 87.410000)}\n        for model in models:\n            Model = models[model]\n            classifier = Model()\n            samples, responses = load_base(self.repoPath + '/samples/data/letter-recognition.data')\n            train_n = int(len(samples)*classifier.train_ratio)\n            classifier.train(samples[:train_n], responses[:train_n])\n            train_rate = np.mean(classifier.predict(samples[:train_n]) == responses[:train_n].astype(int))\n            test_rate  = np.mean(classifier.predict(samples[train_n:]) == responses[train_n:].astype(int))\n            self.assertLess(train_rate - testErrors[Model][0], eps)\n            self.assertLess(test_rate - testErrors[Model][1], eps)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|opencv/opencv", "data_split": "train", "example_id": "001471", "source": "def build_parser(specs: dict[str, TargetSpec]) -> argparse.ArgumentParser:\n    parser = argparse.ArgumentParser(prog=\"app\", formatter_class=RichHelp)\n    register_targets(\n        parser=parser,\n        target_specs=specs,\n        common_args=common_args,\n    )\n    return parser", "target": "def verify(self, mask, exp):\n        maxDiffRatio = 0.02\n        expArea = np.count_nonzero(exp)\n        nonIntersectArea = np.count_nonzero(mask != exp)\n        curRatio = float(nonIntersectArea) / expArea\n        return curRatio < maxDiffRatio", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "001472", "source": "def register_test_commands(subparsers: argparse._SubParsersAction) -> None:\n    build_parser = subparsers.add_parser(\n        \"test\",\n        help=\"test related commands\",\n        formatter_class=RichHelp,\n    )\n    build_subparsers = build_parser.add_subparsers(dest=\"test_command\", required=True)\n    overview = \"\\n\".join(\n        f\"  {name:12} {spec.get('help', '')}\" for name, spec in _TARGETS.items()\n    )\n    external_parser = build_subparsers.add_parser(\n        \"external\",\n        help=\"Test external targets\",\n        description=\"Test third-party targets.\\n\\nAvailable targets:\\n\" + overview,\n        formatter_class=RichHelp,\n    )\n    register_targets(external_parser, _TARGETS, common_args=common_args)", "target": "def omit(v, info):\n        if v == 'omit':\n            raise PydanticOmit\n        elif v == 'error':\n            raise ValueError('error')\n        else:\n            return v", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001473", "source": "def f2(input_value, info):\n        info.context['f2'] = input_value\n        return input_value + f'| context: {info.context}'", "target": "def f2(input_value, info):\n        info.context['f2'] = input_value\n        return input_value + f'| context: {info.context}'", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "001474", "source": "def invalid_schema(ref: str | None = None, metadata: dict[str, Any] | None = None) -> InvalidSchema:\n    return _dict_not_none(type='invalid', ref=ref, metadata=metadata)", "target": "def test_is_iterable(self) -> None:\n        from collections.abc import Iterator\n        iter_ = PeekableIterator(\"\")\n        self.assertTrue(isinstance(iter_, Iterator))", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "001475", "source": "def test_chain_many():\n    validator = SchemaValidator(\n        cs.chain_schema(\n            steps=[\n                core_schema.with_info_plain_validator_function(lambda v, info: f'{v}-1'),\n                core_schema.with_info_plain_validator_function(lambda v, info: f'{v}-2'),\n                core_schema.with_info_plain_validator_function(lambda v, info: f'{v}-3'),\n                core_schema.with_info_plain_validator_function(lambda v, info: f'{v}-4'),\n            ]\n        )\n    )\n    assert validator.validate_python('input') == 'input-1-2-3-4'", "target": "def run(arr):\n            squares = [val**2 for val in arr]\n            return sum(arr) / len(arr), squares", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "001476", "source": "def fit(est, data_train, target_train, libname, **fit_params):\n    print(f\"Fitting a {libname} model...\")\n    tic = time()\n    est.fit(data_train, target_train, **fit_params)\n    toc = time()\n    print(f\"fitted in {toc - tic:.3f}s\")", "target": "def fit(est, data_train, target_train, libname):\n    print(f\"Fitting a {libname} model...\")\n    tic = time()\n    est.fit(data_train, target_train)\n    toc = time()\n    print(f\"fitted in {toc - tic:.3f}s\")", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "001477", "source": "def load_digits(self, fn):\n        digits_img = self.get_sample(fn, 0)\n        digits = split2d(digits_img, (SZ, SZ))\n        labels = np.repeat(np.arange(CLASS_N), len(digits)/CLASS_N)\n        return digits, labels", "target": "def with_config(*, config: ConfigDict) -> Callable[[_TypeT], _TypeT]: ...", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001478", "source": "def test_custom_timezone_utc_repr():\n    output = SchemaValidator(cs.datetime_schema()).validate_python('2022-06-08T12:13:14Z')\n    assert output == datetime(2022, 6, 8, 12, 13, 14, tzinfo=timezone(timedelta(0)))\n    assert output.tzinfo.utcoffset(output) == timedelta(0)\n    assert output.tzinfo.dst(output) is None\n    assert output.tzinfo.tzname(output) == 'UTC'\n    assert str(output.tzinfo) == 'UTC'\n    assert repr(output.tzinfo) == 'TzInfo(0)'", "target": "def unique_list(\n    input_list: list[T] | tuple[T, ...],\n    *,\n    name_factory: Callable[[T], str] = str,\n) -> list[T]:\n    result: list[T] = []\n    result_names: list[str] = []\n    for v in input_list:\n        v_name = name_factory(v)\n        if v_name not in result_names:\n            result_names.append(v_name)\n            result.append(v)\n        else:\n            result[result_names.index(v_name)] = v\n    return result", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "001479", "source": "def test_custom_op_size(self):\n            sz = (100, 150, 3)\n            in_mat = np.full(sz, 45, dtype=np.uint8)\n            expected = (100, 150)\n            g_in = cv.GMat()\n            g_sz = GSize.on(g_in)\n            comp = cv.GComputation(cv.GIn(g_in), cv.GOut(g_sz))\n            pkg = cv.gapi.kernels(GSizeImpl)\n            actual = comp.apply(cv.gin(in_mat), args=cv.gapi.compile_args(pkg))\n            self.assertEqual(0.0, cv.norm(expected, actual, cv.NORM_INF))", "target": "def test_dict_value(py_and_json: PyAndJson):\n    v = py_and_json({'type': 'dict', 'keys_schema': {'type': 'int'}, 'values_schema': {'type': 'timedelta'}})\n    assert v.validate_test({2: 'P2DT1H', 4: 'P2DT2H'}) == {2: timedelta(days=2, hours=1), 4: timedelta(days=2, hours=2)}\n    with pytest.raises(ValidationError, match=re.escape('[type=time_delta_parsing')):\n        v.validate_test({4: 'errordata'})", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001480", "source": "def test_function_plain_field_serializer_to_python():\n    class Model(RootModel):\n        def ser_root(self, v: Any, _) -> str:\n            assert self.root == 1_000\n            return f'{v:_}'\n    s = SchemaSerializer(\n        core_schema.model_schema(\n            Model,\n            core_schema.int_schema(\n                serialization=core_schema.plain_serializer_function_ser_schema(\n                    Model.ser_root, is_field_serializer=True, info_arg=True\n                )\n            ),\n            root_model=True,\n        )\n    )\n    assert s.to_python(Model(1000)) == '1_000'", "target": "def test_function_plain_field_serializer_to_python():\n    @dataclasses.dataclass\n    class Model:\n        x: int\n        def ser_x(self, v: Any, _) -> str:\n            assert self.x == 1_000\n            return f'{v:_}'\n    s = SchemaSerializer(\n        core_schema.model_schema(\n            Model,\n            core_schema.model_fields_schema(\n                {\n                    'x': core_schema.model_field(\n                        core_schema.int_schema(\n                            serialization=core_schema.plain_serializer_function_ser_schema(\n                                Model.ser_x, is_field_serializer=True, info_arg=True\n                            )\n                        )\n                    )\n                }\n            ),\n        )\n    )\n    assert s.to_python(Model(x=1000)) == {'x': '1_000'}", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001481", "source": "def f(input_value: Any, *args: Any) -> Any:\n        if mode == 'wrap':\n            handler, _ = args\n            calls.append({'value': input_value})\n            return handler(input_value)\n        else:\n            calls.append({'value': input_value})\n            return input_value", "target": "def load_file(\n    path: str | Path,\n    *,\n    content_type: str | None = None,\n    encoding: str = 'utf8',\n    proto: Protocol | None = None,\n    allow_pickle: bool = False,\n    json_loads: Callable[[str], Any] = json.loads,\n) -> Any:\n    warnings.warn('`load_file` is deprecated.', category=PydanticDeprecatedSince20, stacklevel=2)\n    path = Path(path)\n    b = path.read_bytes()\n    if content_type is None:\n        if path.suffix in ('.js', '.json'):\n            proto = Protocol.json\n        elif path.suffix == '.pkl':\n            proto = Protocol.pickle\n    return load_str_bytes(\n        b, proto=proto, content_type=content_type, encoding=encoding, allow_pickle=allow_pickle, json_loads=json_loads\n    )", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|opencv/opencv", "data_split": "train", "example_id": "001482", "source": "def test_regression_16040(self):\n        obj_points = np.array([[0, 0, 0], [0, 1, 0], [1, 1, 0], [1, 0, 0]], dtype=np.float32)\n        img_points = np.array(\n            [[700, 400], [700, 600], [900, 600], [900, 400]], dtype=np.float32\n        )\n        cameraMatrix = np.array(\n            [[712.0634, 0, 800], [0, 712.540, 500], [0, 0, 1]], dtype=np.float32\n        )\n        distCoeffs = np.array([[0, 0, 0, 0]], dtype=np.float32)\n        r = np.array([], dtype=np.float32)\n        x, r, t, e = cv.solvePnPGeneric(\n            obj_points, img_points, cameraMatrix, distCoeffs, reprojectionError=r\n        )", "target": "def int_(cls, ctype_name: Optional[str] = None,\n             required_modules: Tuple[str, ...] = ()):\n        if ctype_name is None:\n            ctype_name = \"int\"\n        return PrimitiveTypeNode(ctype_name, typename=\"int\", required_modules=required_modules)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001483", "source": "def outMeta(desc1, desc2, depth):\n            return desc1", "target": "def outMeta(mat_desc, scalar_desc, dtype):\n            return mat_desc", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "001484", "source": "def test_simple(self):\n        finder = cv.ORB.create()\n        img1 = self.get_sample('stitching/a1.png')\n        img2 = self.get_sample('stitching/a2.png')\n        img_feat1 = cv.detail.computeImageFeatures2(finder, img1)\n        img_feat2 = cv.detail.computeImageFeatures2(finder, img2)\n        matcher = cv.detail.BestOf2NearestMatcher_create()\n        matches_info = matcher.apply(img_feat1, img_feat2)\n        self.assertIsNotNone(matches_info.matches)\n        self.assertIsNotNone(matches_info.inliers_mask)", "target": "def field_serializer(\n    *fields: str,\n    mode: Literal['plain', 'wrap'] = 'plain',\n    return_type: Any = PydanticUndefined,\n    when_used: WhenUsed = 'always',\n    check_fields: bool | None = None,\n) -> (\n    Callable[[_FieldWrapSerializerT], _FieldWrapSerializerT]\n    | Callable[[_FieldPlainSerializerT], _FieldPlainSerializerT]\n):\n    def dec(f: FieldSerializer) -> _decorators.PydanticDescriptorProxy[Any]:\n        dec_info = _decorators.FieldSerializerDecoratorInfo(\n            fields=fields,\n            mode=mode,\n            return_type=return_type,\n            when_used=when_used,\n            check_fields=check_fields,\n        )\n        return _decorators.PydanticDescriptorProxy(f, dec_info)\n    return dec", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001485", "source": "def hessian_fwdrev(model, inp, strict=None):\n        return functional.hessian(\n            model,\n            inp,\n            strict=False,\n            vectorize=True,\n            outer_jacobian_strategy=\"forward-mode\",\n        )", "target": "def hessian_fwdrev(model, inp, v=None, strict=None):\n        argnums = tuple(range(len(inp)))\n        return ft.jacfwd(ft.jacrev(model, argnums=argnums), argnums=argnums)(*inp)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001486", "source": "def apply_discriminator(\n    schema: core_schema.CoreSchema,\n    discriminator: str | Discriminator,\n    definitions: dict[str, core_schema.CoreSchema] | None = None,\n) -> core_schema.CoreSchema:\n    from ..types import Discriminator\n    if isinstance(discriminator, Discriminator):\n        if isinstance(discriminator.discriminator, str):\n            discriminator = discriminator.discriminator\n        else:\n            return discriminator._convert_schema(schema)\n    return _ApplyInferredDiscriminator(discriminator, definitions or {}).apply(schema)", "target": "def format_ser_schema(formatting_string: str, *, when_used: WhenUsed = 'json-unless-none') -> FormatSerSchema:\n    if when_used == 'json-unless-none':\n        when_used = None\n    return _dict_not_none(type='format', formatting_string=formatting_string, when_used=when_used)", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|pydantic/pydantic", "data_split": "train", "example_id": "001487", "source": "def plot_batch_times(all_times, n_features, all_batch_sizes, data):\n    plt.figure()\n    plot_results(all_batch_sizes, all_times[\"pca\"], label=\"PCA\")\n    plot_results(all_batch_sizes, all_times[\"ipca\"], label=\"IncrementalPCA\")\n    plt.legend(loc=\"lower left\")\n    plt.suptitle(\n        \"Algorithm runtime vs. batch_size for n_components %i\\n                  LFW,\"\n        \" size %i x %i\" % (n_features, data.shape[0], data.shape[1])\n    )\n    plt.xlabel(\"Batch size\")\n    plt.ylabel(\"Time (seconds)\")", "target": "def test_basic_schema_serializer():\n    s = SchemaSerializer(core_schema.dict_schema())\n    s = pickle.loads(pickle.dumps(s))\n    assert s.to_python({'a': 1, b'b': 2, 33: 3}) == {'a': 1, b'b': 2, 33: 3}\n    assert s.to_python({'a': 1, b'b': 2, 33: 3, True: 4}, mode='json') == {'a': 1, 'b': 2, '33': 3, 'true': 4}\n    assert s.to_json({'a': 1, b'b': 2, 33: 3, True: 4}) == b'{\"a\":1,\"b\":2,\"33\":3,\"true\":4}'\n    assert s.to_python({(1, 2): 3}) == {(1, 2): 3}\n    assert s.to_python({(1, 2): 3}, mode='json') == {'1,2': 3}\n    assert s.to_json({(1, 2): 3}) == b'{\"1,2\":3}'", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001488", "source": "def test_alias_validate_by_name(py_and_json: PyAndJson, input_value, expected):\n    v = py_and_json(\n        cs.arguments_v3_schema(\n            [\n                cs.arguments_v3_parameter(name='a', schema=cs.int_schema(), alias='Foo', mode='positional_or_keyword'),\n            ],\n            validate_by_name=True,\n        )\n    )\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            v.validate_test(input_value)\n    else:\n        assert v.validate_test(input_value) == expected", "target": "def test_alias_validate_by_name(py_and_json: PyAndJson, input_value, expected):\n    v = py_and_json(\n        {\n            'type': 'arguments',\n            'arguments_schema': [\n                {'name': 'a', 'mode': 'positional_or_keyword', 'schema': {'type': 'int'}, 'alias': 'Foo'}\n            ],\n            'validate_by_name': True,\n        }\n    )\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            v.validate_test(input_value)\n    else:\n        assert v.validate_test(input_value) == expected", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001489", "source": "def test_infer_serialize():\n    class MyEnum(Enum):\n        complex_ = complex(1, 2)\n    v = SchemaSerializer(core_schema.enum_schema(MyEnum, list(MyEnum.__members__.values())))\n    assert v.to_json(MyEnum.complex_) == b'\"1+2j\"'", "target": "def check_tz_aware(v: object) -> bool:\n                    assert isinstance(v, datetime.datetime)\n                    return v.tzinfo is not None", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "001490", "source": "def test_constrained_bytes(py_and_json: PyAndJson, opts: dict[str, Any], input, expected):\n    v = py_and_json({'type': 'bytes', **opts})\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            v.validate_test(input)\n        assert v.isinstance_test(input) is False\n    else:\n        assert v.validate_test(input) == expected\n        assert v.isinstance_test(input) is True", "target": "def get_normal_nd_mat():\n        shape = (2, 2, 1, 2)\n        cn = 4\n        image = np.zeros(shape + (cn,), np.float64)\n        image[:] = (0.888, 0.111, 0.666, 0.444)\n        return image", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001491", "source": "def f(value, serializer):\n        return f'result={serializer(len(value))} repr={serializer!r}'", "target": "def f(value, serializer, _info):\n        return f'result={serializer(len(value))} repr={serializer!r}'", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|opencv/opencv", "data_split": "train", "example_id": "001492", "source": "def make_gen_reg_scorers(caller):\n    caller.test_scorer = r2_score\n    caller.train_scorer = r2_score", "target": "def l2norm(x, name):\n    with tf.variable_scope(name):\n        layer = dnnLayer(name)\n        w = tf.Variable(layer.blobs[0].flatten(), dtype=dtype, name='mul')\n        return tf.nn.l2_normalize(x, 3, epsilon=1e-10) * w", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001493", "source": "def relative_typename(self, full_node_name: str) -> str:\n        return self.type_node.relative_typename(full_node_name)", "target": "def relative_typename(self, module: str) -> str:\n        return self.full_typename", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001494", "source": "def liger(self, args, kwargs=None) -> Any:\n        from liger_kernel.transformers.softmax import LigerSoftmax\n        assert kwargs is None\n        x, dy = args\n        softmax = LigerSoftmax().to(\"cuda\")\n        y = softmax(x)\n        return lambda: torch.autograd.grad(y, x, grad_outputs=dy, retain_graph=True)", "target": "def liger(self, args, kwargs) -> Any:\n        from liger_kernel.ops.layer_norm import layer_norm_backward\n        x, w, dy = args\n        eps = 1e-6\n        mean, rstd = self.compute_mean_rstd(x, eps)\n        M, N = x.shape\n        return lambda: layer_norm_backward(dy, x, w, None, mean, rstd)[0:2]", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001495", "source": "def ser_root(self, v: Any, serializer: core_schema.SerializerFunctionWrapHandler, _) -> str:\n            root = serializer(v)\n            assert self.root == 1_000\n            return f'{root:_}'", "target": "def ser_root(self, v: Any, _) -> str:\n            assert self.root == 1_000\n            return f'{v:_}'", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001496", "source": "def forward(self, x):\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        x = self.dropout1(x)\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.dropout2(x)\n        x = self.fc2(x)\n        output = F.log_softmax(x, dim=1)\n        return output", "target": "def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = F.max_pool2d(x, 2)\n        x = torch.flatten(x, 1)\n        output = self.fc1(x)\n        return output", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001497", "source": "def test_date_past_future_today():\n    v = SchemaValidator(core_schema.date_schema(now_op='past'))\n    today = datetime.now(timezone.utc).date()\n    assert v.isinstance_python(today) is False\n    assert v.isinstance_python(today - timedelta(days=1)) is True\n    assert v.isinstance_python(today + timedelta(days=1)) is False\n    v = SchemaValidator(core_schema.date_schema(now_op='future'))\n    assert v.isinstance_python(today) is False\n    assert v.isinstance_python(today - timedelta(days=1)) is False\n    assert v.isinstance_python(today + timedelta(days=1)) is True", "target": "def area(self) -> bytes:\n            a = self.width * self.height\n            return b'%d' % a", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001498", "source": "def pick_grad(self, name, is_training):\n        if is_training:\n            return torch.enable_grad()\n        else:\n            return torch.no_grad()", "target": "def pick_grad(self, name, is_training):\n        if is_training or name in (\"maml\",):\n            return torch.enable_grad()\n        else:\n            return torch.no_grad()", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "001499", "source": "def gh_query_issues_by_labels(\n    org: str, repo: str, labels: list[str], state: str = \"open\"\n) -> list[dict[str, Any]]:\n    url = f\"{GITHUB_API_URL}/repos/{org}/{repo}/issues\"\n    return gh_fetch_json(\n        url, method=\"GET\", params={\"labels\": \",\".join(labels), \"state\": state}\n    )", "target": "def test_union_tuple_var_len(input_value, expected):\n    v = SchemaValidator(\n        core_schema.union_schema(\n            choices=[\n                core_schema.tuple_schema(items_schema=[core_schema.int_schema()], variadic_item_index=0, strict=True),\n                core_schema.tuple_schema(items_schema=[core_schema.str_schema()], variadic_item_index=0, strict=True),\n            ]\n        )\n    )\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)) as exc_info:\n            v.validate_python(input_value)\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        assert v.validate_python(input_value) == expected", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "train", "example_id": "001500", "source": "def test_env_bool_uses_str2bool_when_set(self):\n        def fake_str2bool(s: str) -> bool:\n            return s.lower() in {\"1\", \"true\", \"yes\", \"on\", \"y\"}\n        with (\n            patch.dict(os.environ, {\"FLAG\": \"yEs\"}, clear=True),\n            patch.object(m, \"str2bool\", fake_str2bool),\n        ):\n            self.assertTrue(m.env_bool(\"FLAG\", default=False))", "target": "def replace_linear_weight_only_int8_per_channel(module):\n    for name, child in module.named_children():\n        if isinstance(child, nn.Linear):\n            setattr(\n                module,\n                name,\n                WeightOnlyInt8Linear(child.in_features, child.out_features),\n            )\n        else:\n            replace_linear_weight_only_int8_per_channel(child)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001501", "source": "def validate_b(\n            cls, v: bytes, nxt: core_schema.ValidatorFunctionWrapHandler, info: core_schema.ValidationInfo\n        ) -> str:\n            assert v == b'hello'\n            v = nxt(v)\n            assert v == 'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return 'hello world!'", "target": "def validate_b(cls, v: str, info: core_schema.ValidationInfo) -> str:\n            assert v == 'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return 'hello world!'", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pytorch/pytorch", "data_split": "train", "example_id": "001502", "source": "def test_geometry(self):\n        npt = 100\n        np.random.seed(244)\n        a = np.random.randn(npt,2).astype('float32')*50 + 150\n        be = cv.fitEllipse(a)\n        br = cv.minAreaRect(a)\n        mc, mr = cv.minEnclosingCircle(a)\n        be0 = ((150.2511749267578, 150.77322387695312), (158.024658203125, 197.57696533203125), 37.57804489135742)\n        br0 = ((161.2974090576172, 154.41793823242188), (207.7177734375, 199.2301483154297), 80.83544921875)\n        mc0, mr0 = (160.41790771484375, 144.55152893066406), 136.713500977\n        self.check_close_boxes(be, be0, 5, 15)\n        self.check_close_boxes(br, br0, 5, 15)\n        self.check_close_pairs(mc, mc0, 5)\n        self.assertLessEqual(abs(mr - mr0), 5)", "target": "def list_without(in_list: list[str], without: list[str]) -> list[str]:\n    return [item for item in in_list if item not in without]", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001503", "source": "def _wrap_validator(cls, v, validator, info):\n                return validator(v)", "target": "def _wrap_validator(cls, v, validator, info):\n                return validator(v)", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|pydantic/pydantic", "data_split": "train", "example_id": "001504", "source": "def make_estimator(self, params):\n        (representation,) = params\n        n_estimators = 100 if Benchmark.data_size == \"large\" else 10\n        estimator = GradientBoostingClassifier(\n            n_estimators=n_estimators,\n            max_features=\"log2\",\n            subsample=0.5,\n            random_state=0,\n        )\n        return estimator", "target": "def test_function_before():\n    s = SchemaSerializer(\n        core_schema.with_info_before_validator_function(lambda v, info: v + 1, core_schema.int_schema())\n    )\n    assert plain_repr(s) == 'SchemaSerializer(serializer=Int(IntSerializer),definitions=[])'", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001505", "source": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "target": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pytorch/pytorch", "data_split": "train", "example_id": "001506", "source": "def test_parse_to_int64_not_convertible(self):\n        min_int64, max_int64 = get_limits(ctypes.c_longlong)\n        for not_convertible in (1.2, np.float32(4), float(3), np.double(45), 's', 'str',\n                                np.array([1, 2]), (1,), [1, 2], min_int64 - 1, max_int64 + 1,\n                                complex(1, 1), complex(imag=2), complex(1.1), np.bool_(True),\n                                True, False, np.float32(2.3), np.array([3, ], dtype=int),\n                                np.array([-2, ], dtype=np.int32), np.array([11, ], dtype=np.uint8)):\n            with self.assertRaises((TypeError, OverflowError, ValueError),\n                                   msg=get_no_exception_msg(not_convertible)):\n                _ = cv.utils.dumpInt64(not_convertible)", "target": "def compute_rstd(self, x, eps):\n        return torch.rsqrt(torch.mean(x.float().square(), dim=-1, keepdim=True) + eps)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "001507", "source": "def f(input_value, _info):\n        return input_value + ' Changed'", "target": "def cfgPrePostProcessing(self, pp_callback):\n            ppp = PrePostProcessor(self.model)\n            pp_callback(ppp)\n            self.model = ppp.build()", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001508", "source": "def test_by_alias_and_name_config_interaction(\n    config_by_alias: bool | None,\n    config_by_name: bool | None,\n    runtime_by_alias: bool | None,\n    runtime_by_name: bool | None,\n) -> None:\n    if config_by_alias is False and config_by_name is False and runtime_by_alias is False and runtime_by_name is False:\n        pytest.skip(\"Can't have both by_alias and by_name as effectively False\")\n    schema = cs.arguments_v3_schema(\n        arguments=[\n            cs.arguments_v3_parameter(name='my_field', schema=cs.int_schema(), alias='my_alias'),\n        ],\n        **({'validate_by_alias': config_by_alias} if config_by_alias is not None else {}),\n        **({'validate_by_name': config_by_name} if config_by_name is not None else {}),\n    )\n    s = SchemaValidator(schema)\n    alias_allowed = next(x for x in (runtime_by_alias, config_by_alias, True) if x is not None)\n    name_allowed = next(x for x in (runtime_by_name, config_by_name, False) if x is not None)\n    if alias_allowed:\n        assert s.validate_python(\n            ArgsKwargs((), {'my_alias': 1}), by_alias=runtime_by_alias, by_name=runtime_by_name\n        ) == (\n            (),\n            {'my_field': 1},\n        )\n    if name_allowed:\n        assert s.validate_python(\n            ArgsKwargs((), {'my_field': 1}), by_alias=runtime_by_alias, by_name=runtime_by_name\n        ) == (\n            (),\n            {'my_field': 1},\n        )", "target": "def test_by_alias_and_name_config_interaction(\n    config_by_alias: Union[bool, None],\n    config_by_name: Union[bool, None],\n    runtime_by_alias: Union[bool, None],\n    runtime_by_name: Union[bool, None],\n) -> None:\n    if config_by_alias is False and config_by_name is False and runtime_by_alias is False and runtime_by_name is False:\n        pytest.skip(\"Can't have both by_alias and by_name as effectively False\")\n    core_config = {\n        **({'validate_by_alias': config_by_alias} if config_by_alias is not None else {}),\n        **({'validate_by_name': config_by_name} if config_by_name is not None else {}),\n    }\n    @dataclasses.dataclass\n    class MyDataclass:\n        my_field: int\n    schema = core_schema.dataclass_schema(\n        MyDataclass,\n        core_schema.dataclass_args_schema(\n            'MyDataclass',\n            [\n                core_schema.dataclass_field(\n                    name='my_field', schema=core_schema.int_schema(), validation_alias='my_alias'\n                ),\n            ],\n        ),\n        ['my_field'],\n        config=core_schema.CoreConfig(**core_config),\n    )\n    s = SchemaValidator(schema)\n    alias_allowed = next(x for x in (runtime_by_alias, config_by_alias, True) if x is not None)\n    name_allowed = next(x for x in (runtime_by_name, config_by_name, False) if x is not None)\n    if alias_allowed:\n        assert dataclasses.asdict(\n            s.validate_python({'my_alias': 1}, by_alias=runtime_by_alias, by_name=runtime_by_name)\n        ) == {'my_field': 1}\n    if name_allowed:\n        assert dataclasses.asdict(\n            s.validate_python({'my_field': 1}, by_alias=runtime_by_alias, by_name=runtime_by_name)\n        ) == {'my_field': 1}", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "001509", "source": "def test_aliases_debug():\n    v = SchemaValidator(\n        core_schema.typed_dict_schema(\n            fields={\n                'field_a': core_schema.typed_dict_field(\n                    validation_alias=[['foo', 'bar', 'bat'], ['foo', 3]], schema=core_schema.int_schema()\n                )\n            }\n        )\n    )\n    print(repr(v))\n    assert repr(v).startswith('SchemaValidator(title=\"typed-dict\", validator=TypedDict(')\n    assert 'PathChoices(' in repr(v)", "target": "def quack(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, target = args\n        from quack.cross_entropy import _cross_entropy\n        return lambda: _cross_entropy(x, target)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001510", "source": "def replace_tag(filename) -> None:\n    with open(filename) as f:\n        lines = f.readlines()\n    for i, line in enumerate(lines):\n        if line.startswith(\"Tag:\"):\n            lines[i] = line.replace(\"-linux_\", \"-manylinux_2_28_\")\n            print(f\"Updated tag from {line} to {lines[i]}\")\n            break\n    with open(filename, \"w\") as f:\n        f.writelines(lines)", "target": "def replace_tag(filename):\n    with open(filename) as f:\n        lines = f.read().split(\"\\\\n\")\n    for i, line in enumerate(lines):\n        if not line.startswith(\"Tag: \"):\n            continue\n        lines[i] = line.replace(\"-linux_\", \"-manylinux2014_\")\n        print(f\"Updated tag from {line} to {lines[i]}\")\n    with open(filename, \"w\") as f:\n        f.write(\"\\\\n\".join(lines))", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|pytorch/pytorch", "data_split": "train", "example_id": "001511", "source": "def _20newsgroups_lowdim_dataset(n_components=100, ngrams=(1, 1), dtype=np.float32):\n    newsgroups = fetch_20newsgroups()\n    vectorizer = TfidfVectorizer(ngram_range=ngrams)\n    X = vectorizer.fit_transform(newsgroups.data)\n    X = X.astype(dtype, copy=False)\n    svd = TruncatedSVD(n_components=n_components)\n    X = svd.fit_transform(X)\n    y = newsgroups.target\n    X, X_val, y, y_val = train_test_split(X, y, test_size=0.1, random_state=0)\n    return X, X_val, y, y_val", "target": "def _gh_post_comment(\n    url: str, comment: str, dry_run: bool = False\n) -> list[dict[str, Any]]:\n    if dry_run:\n        print(comment)\n        return []\n    return gh_fetch_json_list(url, data={\"body\": comment})", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|scikit-learn/scikit-learn", "data_split": "train", "example_id": "001512", "source": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "target": "def show_with_diff(image, reference, title):\n    plt.figure(figsize=(5, 3.3))\n    plt.subplot(1, 2, 1)\n    plt.title(\"Image\")\n    plt.imshow(image, vmin=0, vmax=1, cmap=plt.cm.gray, interpolation=\"nearest\")\n    plt.xticks(())\n    plt.yticks(())\n    plt.subplot(1, 2, 2)\n    difference = image - reference\n    plt.title(\"Difference (norm: %.2f)\" % np.sqrt(np.sum(difference**2)))\n    plt.imshow(\n        difference, vmin=-0.5, vmax=0.5, cmap=plt.cm.PuOr, interpolation=\"nearest\"\n    )\n    plt.xticks(())\n    plt.yticks(())\n    plt.suptitle(title, size=16)\n    plt.subplots_adjust(0.02, 0.02, 0.98, 0.79, 0.02, 0.2)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|opencv/opencv", "data_split": "train", "example_id": "001513", "source": "def build(self):\n        self.cmake_path = 'cmake'\n        build_config = 'Release' if not self.config.build_debug else 'Debug'\n        cmd = [self.cmake_path, '-G', 'Visual Studio 16 2019', '-A', 'x64']\n        cmake_vars = dict(\n            CMAKE_BUILD_TYPE=build_config,\n            TREAT_WARNING_AS_ERROR='OFF',\n            ENABLE_SAMPLES='OFF',\n            ENABLE_TESTS='OFF',\n            BUILD_TESTS='OFF',\n            ENABLE_OPENCV='OFF',\n            ENABLE_GNA='OFF',\n            ENABLE_SPEECH_DEMO='OFF',\n            NGRAPH_DOC_BUILD_ENABLE='OFF',\n            NGRAPH_UNIT_TEST_ENABLE='OFF',\n            NGRAPH_UNIT_TEST_OPENVINO_ENABLE='OFF',\n            NGRAPH_TEST_UTIL_ENABLE='OFF',\n            NGRAPH_ONNX_IMPORT_ENABLE='OFF',\n            CMAKE_INSTALL_PREFIX=str(self.build_dir / 'install'),\n            OUTPUT_ROOT=str(self.build_dir),\n        )\n        self.build_config_file = str(self.cpath / 'build.config.py')\n        if os.path.exists(str(self.build_config_file)):\n            with open(self.build_config_file, 'r') as f:\n                cfg = f.read()\n            exec(compile(cfg, str(self.build_config_file), 'exec'))\n            log.info('DLDT processed build configuration script')\n        cmd += [ '-D%s=%s' % (k, v) for (k, v) in cmake_vars.items() if v is not None]\n        if self.config.cmake_option_dldt:\n            cmd += self.config.cmake_option_dldt\n        cmd.append(str(self.srcdir))\n        build_dir = self.build_dir\n        try:\n            execute(cmd, cwd=build_dir)\n            cmd = [self.cmake_path, '--build', '.', '--config', build_config,\n                    '--',\n                    '/v:n', '/consoleloggerparameters:NoSummary',\n            ]\n            execute(cmd, cwd=build_dir)\n            cmd = [self.cmake_path, '-DBUILD_TYPE=' + build_config, '-P', 'cmake_install.cmake']\n            execute(cmd, cwd=build_dir / 'ngraph')\n        except:\n            raise\n        log.info('DLDT build completed')", "target": "def generate_vector(shape, dtype):\n    if np.issubdtype(dtype, np.integer):\n        return np.random.randint(0, 100, shape).astype(dtype)\n    else:\n        return np.random.normal(10., 12.5, shape).astype(dtype)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001514", "source": "def test_aliases_path_negative(input_value, expected):\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={'field_a': core_schema.model_field(validation_alias=['foo', -2], schema=core_schema.int_schema())}\n        ),\n        config=CoreConfig(loc_by_alias=False),\n    )\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=expected.message):\n            val = v.validate_python(input_value)\n            print(f'UNEXPECTED OUTPUT: {val!r}')\n    else:\n        output = v.validate_python(input_value)\n        assert output == expected", "target": "def test_recursive_function():\n    s = SchemaSerializer(\n        core_schema.definitions_schema(\n            core_schema.definition_reference_schema('my_ref'),\n            [\n                core_schema.typed_dict_schema(\n                    {'root': core_schema.typed_dict_field(core_schema.definition_reference_schema('my_ref'))},\n                    ref='my_ref',\n                    serialization=core_schema.wrap_serializer_function_ser_schema(function=lambda x, _handler: x),\n                )\n            ],\n        )\n    )\n    assert s.to_python({'root': {'root': {}}}) == {'root': {'root': {}}}", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001515", "source": "def test_to_json_list_of_lists(benchmark):\n    data = [[i + j for j in range(10)] for i in range(1000)]\n    benchmark(to_json, data)", "target": "def repr_function(value, _info):\n        if value == 'unexpected':\n            raise PydanticSerializationUnexpectedValue()\n        return f'func: {value!r}'", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "train", "example_id": "001516", "source": "def parse_arguments():\n    from argparse import ArgumentParser\n    parser = ArgumentParser(\"AARCH64 wheels python CD\")\n    parser.add_argument(\"--debug\", action=\"store_true\")\n    parser.add_argument(\"--build-only\", action=\"store_true\")\n    parser.add_argument(\"--test-only\", type=str)\n    parser.add_argument(\"--enable-mkldnn\", action=\"store_true\")\n    parser.add_argument(\"--enable-cuda\", action=\"store_true\")\n    return parser.parse_args()", "target": "def register_experiment(name: Optional[str] = None):\n    def decorator(func):\n        key = name or func.__name__\n        all_experiments[key] = func\n        return func\n    return decorator", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001517", "source": "def liger(self, args, kwargs=None) -> Any:\n        from liger_kernel.transformers.softmax import LigerSoftmax\n        assert kwargs is None\n        x, dy = args\n        softmax = LigerSoftmax().to(\"cuda\")\n        y = softmax(x)\n        return lambda: torch.autograd.grad(y, x, grad_outputs=dy, retain_graph=True)", "target": "def liger(self, args, kwargs) -> Any:\n        from liger_kernel.transformers.layer_norm import LigerLayerNorm\n        x, w = args\n        M, N = x.shape\n        liger_layernorm = LigerLayerNorm(hidden_size=N, eps=1e-6).cuda()\n        liger_layernorm.weight.data.copy_(w)\n        liger_layernorm.bias.data.copy_(\n            torch.zeros(N, device=\"cuda\", dtype=torch.float32)\n        )\n        return lambda: liger_layernorm(x)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "001518", "source": "def describe_sizes(**sizes):\n    return \"s{}-l{}-i{}-h{}-b{}\".format(\n        sizes[\"seqLength\"],\n        sizes[\"numLayers\"],\n        sizes[\"inputSize\"],\n        sizes[\"hiddenSize\"],\n        sizes[\"miniBatch\"],\n    )", "target": "def test_only_validate_by_name(py_and_json) -> None:\n    v = py_and_json(\n        cs.arguments_v3_schema(\n            [\n                cs.arguments_v3_parameter(\n                    name='a', schema=cs.str_schema(), alias='FieldA', mode='positional_or_keyword'\n                ),\n            ],\n            validate_by_name=True,\n            validate_by_alias=False,\n        )\n    )\n    assert v.validate_test(ArgsKwargs((), {'a': 'hello'})) == ((), {'a': 'hello'})\n    assert v.validate_test({'a': 'hello'}) == (('hello',), {})\n    with pytest.raises(ValidationError, match=r'a\\n +Missing required argument \\[type=missing_argument,'):\n        assert v.validate_test(ArgsKwargs((), {'FieldA': 'hello'}))\n    with pytest.raises(ValidationError, match=r'a\\n +Missing required argument \\[type=missing_argument,'):\n        assert v.validate_test({'FieldA': 'hello'})", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|opencv/opencv", "data_split": "train", "example_id": "001519", "source": "def required_definition_imports(self) -> Generator[str, None, None]:\n        yield \"import typing as _typing\"\n        yield from super().required_definition_imports", "target": "def test_grabcut(self):\n        img = self.get_sample('cv/shared/airplane.png')\n        mask_prob = self.get_sample(\"cv/grabcut/mask_probpy.png\", 0)\n        exp_mask1 = self.get_sample(\"cv/grabcut/exp_mask1py.png\", 0)\n        exp_mask2 = self.get_sample(\"cv/grabcut/exp_mask2py.png\", 0)\n        if img is None:\n            self.assertTrue(False, 'Missing test data')\n        rect = (24, 126, 459, 168)\n        mask = np.zeros(img.shape[:2], dtype = np.uint8)\n        bgdModel = np.zeros((1,65),np.float64)\n        fgdModel = np.zeros((1,65),np.float64)\n        cv.grabCut(img, mask, rect, bgdModel, fgdModel, 0, cv.GC_INIT_WITH_RECT)\n        cv.grabCut(img, mask, rect, bgdModel, fgdModel, 2, cv.GC_EVAL)\n        if mask_prob is None:\n            mask_prob = mask.copy()\n            cv.imwrite(self.extraTestDataPath + '/cv/grabcut/mask_probpy.png', mask_prob)\n        if exp_mask1 is None:\n            exp_mask1 = self.scaleMask(mask)\n            cv.imwrite(self.extraTestDataPath + '/cv/grabcut/exp_mask1py.png', exp_mask1)\n        self.assertEqual(self.verify(self.scaleMask(mask), exp_mask1), True)\n        mask = mask_prob\n        bgdModel = np.zeros((1,65),np.float64)\n        fgdModel = np.zeros((1,65),np.float64)\n        cv.grabCut(img, mask, rect, bgdModel, fgdModel, 0, cv.GC_INIT_WITH_MASK)\n        cv.grabCut(img, mask, rect, bgdModel, fgdModel, 1, cv.GC_EVAL)\n        if exp_mask2 is None:\n            exp_mask2 = self.scaleMask(mask)\n            cv.imwrite(self.extraTestDataPath + '/cv/grabcut/exp_mask2py.png', exp_mask2)\n        self.assertEqual(self.verify(self.scaleMask(mask), exp_mask2), True)", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "001520", "source": "def predict_proba(self, X):\n        df = self.decision_function(X)\n        calibrated_df = (df - self.df_min_) / (self.df_max_ - self.df_min_)\n        proba_pos_class = np.clip(calibrated_df, 0, 1)\n        proba_neg_class = 1 - proba_pos_class\n        proba = np.c_[proba_neg_class, proba_pos_class]\n        return proba", "target": "def predict_proba(self, X):\n        df = self.decision_function(X)\n        calibrated_df = (df - self.df_min_) / (self.df_max_ - self.df_min_)\n        proba_pos_class = np.clip(calibrated_df, 0, 1)\n        proba_neg_class = 1 - proba_pos_class\n        proba = np.c_[proba_neg_class, proba_pos_class]\n        return proba", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|opencv/opencv", "data_split": "train", "example_id": "001521", "source": "def get_class_names(path):\n    class_names = []\n    with open(path) as f:\n        for row in f:\n            class_names.append(row[:-1])\n    return class_names", "target": "def test_parse_to_rect_convertible(self):\n        Rect = namedtuple('Rect', ('x', 'y', 'w', 'h'))\n        try_to_convert = partial(self._try_to_convert, cv.utils.dumpRect)\n        for convertible in ((1, 2, 4, 5), [5, 3, 10, 20], np.array([10, 20, 23, 10]),\n                            Rect(10, 30, 40, 55), tuple(np.array([40, 20, 24, 20])),\n                            list(np.array([20, 40, 30, 35]))):\n            expected = 'rect: (x={}, y={}, w={}, h={})'.format(*convertible)\n            actual = try_to_convert(convertible)\n            self.assertEqual(expected, actual,\n                             msg=get_conversion_error_msg(convertible, expected, actual))", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|scikit-learn/scikit-learn", "data_split": "train", "example_id": "001522", "source": "def make_estimator(self, params):\n        algorithm, dimension, n_jobs = params\n        estimator = KNeighborsClassifier(algorithm=algorithm, n_jobs=n_jobs)\n        return estimator", "target": "def get_estimator_and_data():\n    if args.problem == \"classification\":\n        X, y = make_classification(\n            args.n_samples_max * 2,\n            n_features=args.n_features,\n            n_classes=args.n_classes,\n            n_clusters_per_class=1,\n            n_informative=args.n_classes,\n            random_state=0,\n        )\n        return X, y, HistGradientBoostingClassifier\n    elif args.problem == \"regression\":\n        X, y = make_regression(\n            args.n_samples_max * 2, n_features=args.n_features, random_state=0\n        )\n        return X, y, HistGradientBoostingRegressor", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|opencv/opencv", "data_split": "train", "example_id": "001523", "source": "def test_texture_flow(self):\n        img = self.get_sample('samples/data/chessboard.png')\n        gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n        h, w = img.shape[:2]\n        eigen = cv.cornerEigenValsAndVecs(gray, 5, 3)\n        eigen = eigen.reshape(h, w, 3, 2)\n        flow = eigen[:,:,2]\n        d = 300\n        eps = d / 30\n        points =  np.dstack( np.mgrid[d/2:w:d, d/2:h:d] ).reshape(-1, 2)\n        textureVectors = []\n        for x, y in np.int32(points):\n            textureVectors.append(np.int32(flow[y, x]*d))\n        for i in range(len(textureVectors)):\n            self.assertTrue(cv.norm(textureVectors[i], cv.NORM_L2) < eps\n            or abs(cv.norm(textureVectors[i], cv.NORM_L2) - d) < eps)", "target": "def test_age_gender_infer_roi_list(self):\n            if not cv.dnn.DNN_TARGET_CPU in cv.dnn.getAvailableTargets(cv.dnn.DNN_BACKEND_INFERENCE_ENGINE):\n                return\n            root_path    = '/omz_intel_models/intel/age-gender-recognition-retail-0013/FP32/age-gender-recognition-retail-0013'\n            model_path   = self.find_file(root_path + '.xml',   [os.environ.get('OPENCV_DNN_TEST_DATA_PATH')], required=False)\n            weights_path = self.find_file(root_path + '.bin',   [os.environ.get('OPENCV_DNN_TEST_DATA_PATH')], required=False)\n            device_id    = 'CPU'\n            rois = [(10, 15, 62, 62), (23, 50, 62, 62), (14, 100, 62, 62), (80, 50, 62, 62)]\n            img_path = self.find_file('cv/face/david2.jpg', [os.environ.get('OPENCV_TEST_DATA_PATH')])\n            img = cv.imread(img_path)\n            dnn_age_list    = []\n            dnn_gender_list = []\n            for roi in rois:\n                age, gender = self.infer_reference_network(model_path,\n                                                           weights_path,\n                                                           self.make_roi(img, roi))\n                dnn_age_list.append(age)\n                dnn_gender_list.append(gender)\n            g_in   = cv.GMat()\n            g_rois = cv.GArrayT(cv.gapi.CV_RECT)\n            inputs = cv.GInferInputs()\n            inputs.setInput('data', g_in)\n            outputs  = cv.gapi.infer(\"net\", g_rois, inputs)\n            age_g    = outputs.at(\"age_conv3\")\n            gender_g = outputs.at(\"prob\")\n            comp = cv.GComputation(cv.GIn(g_in, g_rois), cv.GOut(age_g, gender_g))\n            pp = cv.gapi.ie.params(\"net\", model_path, weights_path, device_id)\n            gapi_age_list, gapi_gender_list = comp.apply(cv.gin(img, rois),\n                                                         args=cv.gapi.compile_args(cv.gapi.networks(pp)))\n            for gapi_age, gapi_gender, dnn_age, dnn_gender in zip(gapi_age_list,\n                                                                  gapi_gender_list,\n                                                                  dnn_age_list,\n                                                                  dnn_gender_list):\n                self.assertEqual(0.0, cv.norm(dnn_gender, gapi_gender, cv.NORM_INF))\n                self.assertEqual(0.0, cv.norm(dnn_age, gapi_age, cv.NORM_INF))", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "001524", "source": "def shortName(self):\n        name = self.getName()\n        fixture = self.getFixture()\n        return '::'.join(filter(None, [name, fixture]))", "target": "def f(input_value, info):\n        _, count = input_value.split('-')\n        return f'f-{int(count) + 1}'", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001525", "source": "def constrain(self: _Pipeline[_InT, _NewOutLe], constraint: annotated_types.Le) -> _Pipeline[_InT, _NewOutLe]: ...", "target": "def constrain(\n        self: _Pipeline[_InT, _NewOutInterval], constraint: annotated_types.Interval\n    ) -> _Pipeline[_InT, _NewOutInterval]: ...", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "001526", "source": "def test_parse_to_float_convertible(self):\n        try_to_convert = partial(self._try_to_convert, cv.utils.dumpFloat)\n        min_float, max_float = get_limits(ctypes.c_float)\n        for convertible in (2, -13, 1.24, np.float32(32.45), float(32), np.double(12.23),\n                            np.float32(-12.3), np.float64(3.22), min_float,\n                            max_float, np.inf, -np.inf, float('Inf'), -float('Inf'),\n                            np.double(np.inf), np.double(-np.inf), np.double(float('Inf')),\n                            np.double(-float('Inf'))):\n            expected = 'Float: {0:.2f}'.format(convertible).lower()\n            actual = try_to_convert(convertible)\n            self.assertEqual(expected, actual,\n                             msg=get_conversion_error_msg(convertible, expected, actual))\n        for nan in (float('NaN'), np.nan, np.float32(np.nan), np.double(np.nan),\n                    np.double(float('NaN'))):\n            actual = try_to_convert(nan)\n            self.assertIn('nan', actual, msg=\"Can't convert nan of type {} to float. \"\n                          \"Actual: {}\".format(type(nan).__name__, actual))\n        min_double, max_double = get_limits(ctypes.c_double)\n        for inf in (min_float * 10, max_float * 10, min_double, max_double):\n            expected = 'float: {}inf'.format('-' if inf < 0 else '')\n            actual = try_to_convert(inf)\n            self.assertEqual(expected, actual,\n                             msg=get_conversion_error_msg(inf, expected, actual))", "target": "def append_args(value, info):\n        return f'{value} info={info}'", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001527", "source": "def resolve_ref_schema(self, maybe_ref_json_schema: JsonSchemaValue) -> JsonSchemaValue:\n        if '$ref' not in maybe_ref_json_schema:\n            return maybe_ref_json_schema\n        ref = maybe_ref_json_schema['$ref']\n        json_schema = self.generate_json_schema.get_schema_from_definitions(ref)\n        if json_schema is None:\n            raise LookupError(\n                f'Could not find a ref for {ref}.'\n                ' Maybe you tried to call resolve_ref_schema from within a recursive model?'\n            )\n        return json_schema", "target": "def resolve_ref_schema(self, maybe_ref_schema: core_schema.CoreSchema) -> core_schema.CoreSchema:\n        if maybe_ref_schema['type'] == 'definition-ref':\n            ref = maybe_ref_schema['schema_ref']\n            definition = self._generate_schema.defs.get_schema_from_ref(ref)\n            if definition is None:\n                raise LookupError(\n                    f'Could not find a ref for {ref}.'\n                    ' Maybe you tried to call resolve_ref_schema from within a recursive model?'\n                )\n            return definition\n        elif maybe_ref_schema['type'] == 'definitions':\n            return self.resolve_ref_schema(maybe_ref_schema['schema'])\n        return maybe_ref_schema", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "001528", "source": "def input_type_str(self) -> str:\n        return f'{self.input_type.__name__}' if hasattr(self.input_type, '__name__') else f'{self.input_type}'", "target": "def pip_install(package):\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "001529", "source": "def make_data(self, params):\n        representation, solver, n_jobs = params\n        if Benchmark.data_size == \"large\":\n            if representation == \"sparse\":\n                data = _20newsgroups_highdim_dataset(n_samples=10000)\n            else:\n                data = _20newsgroups_lowdim_dataset(n_components=1e3)\n        else:\n            if representation == \"sparse\":\n                data = _20newsgroups_highdim_dataset(n_samples=2500)\n            else:\n                data = _20newsgroups_lowdim_dataset()\n        return data", "target": "def make_data(self, params):\n        (representation,) = params\n        if representation == \"dense\":\n            data = _synth_regression_dataset(n_samples=100000, n_features=200)\n        else:\n            data = _synth_regression_sparse_dataset(\n                n_samples=100000, n_features=1000, density=0.01\n            )\n        return data", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pytorch/pytorch", "data_split": "train", "example_id": "001530", "source": "def clear(self):\n        self.mdict = {}\n        self.tdict = {}\n        self.mwhere = {}\n        self.twhere = {}\n        self.empty_stubs_cnt = 0\n        self.r1 = re.compile(\"\\s*public\\s+(?:static\\s+)?(\\w+)\\(([^)]*)\\)\")\n        self.r2 = re.compile(\"\\s*(?:(?:public|static|final)\\s+){1,3}\\S+\\s+(\\w+)\\(([^)]*)\\)\")\n        self.r3 = re.compile('\\s*fail\\(\"Not yet implemented\"\\);')", "target": "def contiguousChannelsLast3d(self, x: Tensor) -> Tensor:\n        return x.contiguous(memory_format=torch.channels_last_3d)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001531", "source": "def test_json_int():\n    s = SchemaSerializer(core_schema.json_schema(core_schema.int_schema()))\n    assert s.to_python(1) == 1\n    assert s.to_python(1, round_trip=True) == '1'\n    assert s.to_python(1, mode='json') == 1\n    assert s.to_python(1, mode='json', round_trip=True) == '1'\n    assert s.to_json(1) == b'1'\n    assert s.to_json(1, round_trip=True) == b'\"1\"'", "target": "def test_from_attributes():\n    v = SchemaValidator(\n        core_schema.tagged_union_schema(\n            discriminator='foobar',\n            choices={\n                'apple': core_schema.model_fields_schema(\n                    fields={\n                        'a': core_schema.model_field(schema=core_schema.str_schema()),\n                        'b': core_schema.model_field(schema=core_schema.int_schema()),\n                    }\n                ),\n                'banana': core_schema.model_fields_schema(\n                    fields={\n                        'c': core_schema.model_field(schema=core_schema.str_schema()),\n                        'd': core_schema.model_field(schema=core_schema.int_schema()),\n                    }\n                ),\n            },\n        ),\n        config=CoreConfig(from_attributes=True),\n    )\n    assert v.validate_python({'foobar': 'apple', 'a': 'apple', 'b': '13'}) == (\n        {'a': 'apple', 'b': 13},\n        None,\n        {'a', 'b'},\n    )\n    assert v.validate_python(Cls(foobar='apple', a='apple', b='13')) == ({'a': 'apple', 'b': 13}, None, {'a', 'b'})\n    assert v.validate_python({'foobar': 'banana', 'c': 'banana', 'd': '31'}) == (\n        {'c': 'banana', 'd': 31},\n        None,\n        {'c', 'd'},\n    )\n    assert v.validate_python(Cls(foobar='banana', c='banana', d='31')) == ({'c': 'banana', 'd': 31}, None, {'c', 'd'})", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001532", "source": "def f(value, serializer, _info):\n        return f'result={serializer(len(value))} repr={serializer!r}'", "target": "def f(value, handler, _info):\n        return handler(value)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001533", "source": "def forward(self, x):\n        return torch.mm(x, self.weight)", "target": "def forward(self, x):\n                total = sum(t.item() for t in x)\n                return total // 2", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "001534", "source": "def test_only_validate_by_name(py_and_json) -> None:\n    schema = core_schema.dataclass_schema(\n        BasicDataclass,\n        core_schema.dataclass_args_schema(\n            'BasicDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), validation_alias='FieldA'),\n            ],\n        ),\n        ['a'],\n        config=core_schema.CoreConfig(validate_by_name=True, validate_by_alias=False),\n    )\n    v = py_and_json(schema)\n    assert v.validate_test({'a': 'hello'}) == BasicDataclass(a='hello')\n    with pytest.raises(ValidationError, match=r'a\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'FieldA': 'hello'})", "target": "def type_format(self) -> str:\n        if TypeNode.compatible_to_runtime_usage:\n            return \"_typing.Optional[{}]\"\n        return \"{} | None\"", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001535", "source": "def ser_root(self, v: Any, _) -> str:\n            assert self.root == 1_000\n            return f'{v:_}'", "target": "def ser_root(self, v: Any, serializer: core_schema.SerializerFunctionWrapHandler, _) -> str:\n            assert self.root == 1_000\n            root = serializer(v)\n            return f'{root:_}'", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|opencv/opencv", "data_split": "train", "example_id": "001536", "source": "def test_fitline(self):\n        noise = 5\n        n = 200\n        r = 5 / 100.0\n        outn = int(n*r)\n        p0, p1 = (90, 80), (w-90, h-80)\n        line_points = sample_line(p0, p1, n-outn, noise)\n        outliers = np.random.rand(outn, 2) * (w, h)\n        points = np.vstack([line_points, outliers])\n        lines = []\n        for name in dist_func_names:\n            func = getattr(cv, name)\n            vx, vy, cx, cy = cv.fitLine(np.float32(points), func, 0, 0.01, 0.01)\n            line = [float(vx), float(vy), float(cx), float(cy)]\n            lines.append(line)\n        eps = 0.05\n        refVec =  (np.float32(p1) - p0) / cv.norm(np.float32(p1) - p0)\n        for i in range(len(lines)):\n            self.assertLessEqual(cv.norm(refVec - lines[i][0:2], cv.NORM_L2), eps)", "target": "def test_imread_to_buffer(self):\n        path = self.extraTestDataPath + '/cv/shared/lena.png'\n        ref = cv.imread(path)\n        img = np.zeros_like(ref)\n        cv.imread(path, img)\n        self.assertEqual(cv.norm(ref, img, cv.NORM_INF), 0.0)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001537", "source": "def f(value, serializer, _info):\n        return f'result={serializer(len(value))} repr={serializer!r}'", "target": "def f(value, serializer, _info):\n        return f'result={serializer(value)}'", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "001538", "source": "def test_url_subclass(base_class):\n    class MyUrl(base_class):\n        def some_method(self):\n            return self.path + '-success'\n    m = MyUrl('http://ex.com/path')\n    assert m.some_method() == '/path-success'", "target": "def output_json(output_file, headers, row):\n    mapping_headers = {headers[i]: v for i, v in enumerate(row)}\n    record = {\n        \"benchmark\": {\n            \"name\": \"PyTorch gpt-fast benchmark\",\n            \"mode\": \"inference\",\n            \"dtype\": mapping_headers[\"dtype\"],\n            \"extra_info\": {\n                \"device\": mapping_headers[\"device\"],\n                \"arch\": mapping_headers[\"arch\"],\n            },\n        },\n        \"model\": {\n            \"name\": mapping_headers[\"name\"],\n            \"type\": \"OSS model\" if mapping_headers[\"is_model\"] else \"micro-benchmark\",\n            \"origins\": [\"pytorch\"],\n        },\n        \"metric\": {\n            \"name\": mapping_headers[\"metric\"],\n            \"benchmark_values\": [mapping_headers[\"actual\"]],\n            \"target_value\": mapping_headers[\"target\"],\n        },\n    }\n    with open(f\"{os.path.splitext(output_file)[0]}.json\", \"a\") as f:\n        print(json.dumps(record), file=f)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "001539", "source": "def fetch_jobs(url: str, headers: dict[str, str]) -> list[dict[str, str]]:\n    response, links = fetch_url(url, headers=headers, reader=parse_json_and_links)\n    jobs = response[\"jobs\"]\n    assert type(jobs) is list\n    while \"next\" in links.keys():\n        response, links = fetch_url(\n            links[\"next\"][\"url\"], headers=headers, reader=parse_json_and_links\n        )\n        jobs.extend(response[\"jobs\"])\n    return jobs", "target": "def test_any():\n    url = Url('https://ex.com')\n    multi_host_url = MultiHostUrl('https://ex.com,ex.org/path')\n    s = SchemaSerializer(core_schema.any_schema())\n    assert s.to_python(url) == url\n    assert type(s.to_python(url)) == Url\n    assert s.to_python(multi_host_url) == multi_host_url\n    assert type(s.to_python(multi_host_url)) == MultiHostUrl\n    assert s.to_python(url, mode='json') == 'https://ex.com/'\n    assert s.to_python(multi_host_url, mode='json') == 'https://ex.com,ex.org/path'\n    assert s.to_json(url) == b'\"https://ex.com/\"'\n    assert s.to_json(multi_host_url) == b'\"https://ex.com,ex.org/path\"'\n    assert s.to_python({url: 1, multi_host_url: 2}) == {url: 1, multi_host_url: 2}\n    assert s.to_python({url: 1, multi_host_url: 2}, mode='json') == {\n        'https://ex.com/': 1,\n        'https://ex.com,ex.org/path': 2,\n    }\n    assert s.to_json({url: 1, multi_host_url: 2}) == b'{\"https://ex.com/\":1,\"https://ex.com,ex.org/path\":2}'", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|scikit-learn/scikit-learn", "data_split": "train", "example_id": "001540", "source": "def update(cur_mode):\n            cur_str_mode = str_modes[0]\n            sz = 10\n            iters = 1\n            opers = cur_mode.split('/')\n            if len(opers) > 1:\n                sz = sz - 10\n                op = opers[sz > 0]\n                sz = abs(sz)\n            else:\n                op = opers[0]\n            sz = sz*2+1\n            str_name = 'MORPH_' + cur_str_mode.upper()\n            oper_name = 'MORPH_' + op.upper()\n            st = cv.getStructuringElement(getattr(cv, str_name), (sz, sz))\n            return cv.morphologyEx(img, getattr(cv, oper_name), st, iterations=iters)", "target": "def _generate_center_coordinates(l_x):\n    X, Y = np.mgrid[:l_x, :l_x].astype(np.float64)\n    center = l_x / 2.0\n    X += 0.5 - center\n    Y += 0.5 - center\n    return X, Y", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001541", "source": "def test_union_serializes_list_of_model_subclass_from_definition() -> None:\n    class BaseModel:\n        __slots__ = '__dict__', '__pydantic_fields_set__', '__pydantic_extra__', '__pydantic_private__'\n        def __init__(self, **kwargs: Any):\n            for key, value in kwargs.items():\n                setattr(self, key, value)\n    class User(BaseModel):\n        name: str\n    class DBUser(User):\n        password: str\n        __pydantic_serializer__: ClassVar[SchemaSerializer]\n    DBUser.__pydantic_serializer__ = SchemaSerializer(\n        core_schema.model_schema(\n            DBUser,\n            core_schema.model_fields_schema(\n                {\n                    'name': core_schema.model_field(core_schema.str_schema()),\n                    'password': core_schema.model_field(core_schema.str_schema()),\n                }\n            ),\n        )\n    )\n    class Item(BaseModel):\n        price: float\n    s = SchemaSerializer(\n        core_schema.definitions_schema(\n            core_schema.union_schema(\n                [\n                    core_schema.list_schema(core_schema.definition_reference_schema('User'), strict=False),\n                    core_schema.list_schema(core_schema.definition_reference_schema('Item'), strict=False),\n                ]\n            ),\n            [\n                core_schema.model_schema(\n                    User,\n                    core_schema.model_fields_schema({'name': core_schema.model_field(core_schema.str_schema())}),\n                    ref='User',\n                ),\n                core_schema.model_schema(\n                    Item,\n                    core_schema.model_fields_schema({'price': core_schema.model_field(core_schema.float_schema())}),\n                    ref='Item',\n                ),\n            ],\n        )\n    )\n    assert s.to_python([DBUser(name='John', password='secret')]) == [{'name': 'John'}]", "target": "def update(self, config: ModelConfigData | None) -> None:\n        if config is None:\n            return\n        for k, v in config.get_values_dict().items():\n            setattr(self, k, v)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001542", "source": "def f(input: Any, info: core_schema.SerializationInfo, /) -> str:\n        return str(info)", "target": "def f(\n        input: Any, serialize: core_schema.SerializerFunctionWrapHandler, info: core_schema.SerializationInfo, /\n    ) -> str:\n        return f'{serialize} {info}'", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001543", "source": "def func(*args: Any) -> Any:\n        calls.append(args)\n        return args[0]", "target": "def func(*args: Any) -> Any:\n        assert len(args) == 2\n        input, handler = args\n        output = handler(input)\n        calls.append((input, output))\n        return output", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001544", "source": "def type_format(self) -> str:\n        return \"_typing.Sequence[{}]\"", "target": "def type_format(self) -> str:\n        if TypeNode.compatible_to_runtime_usage:\n            return \"_typing.Optional[{}]\"\n        return \"{} | None\"", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001545", "source": "def test_dict():\n    v = SchemaValidator(core_schema.dict_schema(core_schema.int_schema(), core_schema.date_schema()))\n    assert v.validate_strings({'1': '2017-01-01', '2': '2017-01-02'}) == {1: date(2017, 1, 1), 2: date(2017, 1, 2)}\n    assert v.validate_strings({'1': '2017-01-01', '2': '2017-01-02'}, strict=True) == {\n        1: date(2017, 1, 1),\n        2: date(2017, 1, 2),\n    }", "target": "def test_dict(py_and_json: PyAndJson):\n    v = py_and_json({'type': 'dict', 'keys_schema': {'type': 'time'}, 'values_schema': {'type': 'int'}})\n    assert v.validate_test({'12:01:01': 2, '12:01:02': 4}) == {time(12, 1, 1): 2, time(12, 1, 2): 4}", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001546", "source": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "target": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001547", "source": "def test_slots_mixed():\n    @dataclasses.dataclass(slots=True)\n    class Model:\n        x: int\n        y: dataclasses.InitVar[str]\n        z: ClassVar[str] = 'z-classvar'\n    @dataclasses.dataclass\n    class SubModel(Model):\n        x2: int\n        y2: dataclasses.InitVar[str]\n        z2: ClassVar[str] = 'z2-classvar'\n    schema = core_schema.dataclass_schema(\n        SubModel,\n        core_schema.dataclass_args_schema(\n            'SubModel',\n            [\n                core_schema.dataclass_field(name='x', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(name='y', init_only=True, schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='x2', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(name='y2', init_only=True, schema=core_schema.str_schema()),\n            ],\n        ),\n        ['x', 'x2'],\n        slots=True,\n    )\n    dc = SubModel(x=1, y='a', x2=2, y2='b')\n    assert dataclasses.asdict(dc) == {'x': 1, 'x2': 2}\n    s = SchemaSerializer(schema)\n    assert s.to_python(dc) == {'x': 1, 'x2': 2}\n    assert s.to_json(dc) == b'{\"x\":1,\"x2\":2}'", "target": "def test_slots_mixed():\n    @dataclasses.dataclass(slots=True)\n    class Model:\n        x: int\n        y: dataclasses.InitVar[str]\n        z: ClassVar[str] = 'z-classvar'\n    @dataclasses.dataclass\n    class SubModel(Model):\n        x2: int\n        y2: dataclasses.InitVar[str]\n        z2: ClassVar[str] = 'z2-classvar'\n    schema = core_schema.dataclass_schema(\n        SubModel,\n        core_schema.dataclass_args_schema(\n            'SubModel',\n            [\n                core_schema.dataclass_field(name='x', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(name='y', init_only=True, schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='x2', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(name='y2', init_only=True, schema=core_schema.str_schema()),\n            ],\n        ),\n        ['x'],\n        slots=True,\n    )\n    v = SchemaValidator(schema)\n    dc = v.validate_python({'x': 1, 'y': 'a', 'x2': 2, 'y2': 'b'})\n    assert dc.x == 1\n    assert dc.x2 == 2\n    assert dataclasses.asdict(dc) == {'x': 1, 'x2': 2}", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "001548", "source": "def gh_update_pr_state(org: str, repo: str, pr_num: int, state: str = \"open\") -> None:\n    url = f\"{GITHUB_API_URL}/repos/{org}/{repo}/pulls/{pr_num}\"\n    try:\n        gh_fetch_url(url, method=\"PATCH\", data={\"state\": state})\n    except HTTPError as err:\n        if err.code == 422 and state == \"open\":\n            warnings.warn(\n                f\"Failed to open {pr_num} because its head branch has been deleted: {err}\"\n            )\n        else:\n            raise", "target": "def datetime_tz_naive(self: _Pipeline[_InT, datetime.datetime]) -> _Pipeline[_InT, datetime.datetime]:\n        return self.constrain(annotated_types.Timezone(None))", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "001549", "source": "def get_shapes(self) -> tuple[tuple[int, ...], ...]:\n        return (\n            (32768, 256),\n            (32768, 512),\n            (32768, 1024),\n            (32768, 2048),\n            (32768, 4096),\n            (32768, 8192),\n            (32768, 16384),\n            (32768, 32768),\n            (32768, 65536),\n        ) + extra_shapes_for_norm", "target": "def test_no_args_constructor(self):\n        tz = TzInfo()\n        self.assertEqual(tz.utcoffset(None), timedelta(0))\n        self.assertEqual(str(tz), 'UTC')", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001550", "source": "def gen():\n    yield 1\n    yield 2\n    yield 3", "target": "def gen(error: bool):\n        yield 1\n        yield 2\n        if error:\n            raise RuntimeError('error')\n        yield 3", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001551", "source": "def to_var(\n        self,\n        current_info: TypeInfo,\n        api: SemanticAnalyzerPluginInterface,\n        use_alias: bool,\n        force_typevars_invariant: bool = False,\n    ) -> Var:\n        if use_alias and self.alias is not None:\n            name = self.alias\n        else:\n            name = self.name\n        return Var(name, self.expand_type(current_info, api, force_typevars_invariant))", "target": "def chain_schema(\n    steps: list[CoreSchema],\n    *,\n    ref: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> ChainSchema:\n    return _dict_not_none(type='chain', steps=steps, ref=ref, metadata=metadata, serialization=serialization)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001552", "source": "def test_alias(py_and_json: PyAndJson, input_value, expected) -> None:\n    v = py_and_json(\n        cs.arguments_v3_schema(\n            [\n                cs.arguments_v3_parameter(name='a', schema=cs.int_schema(), alias='Foo', mode='positional_or_keyword'),\n            ]\n        )\n    )\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            v.validate_test(input_value)\n    else:\n        assert v.validate_test(input_value) == expected", "target": "def test_alias(py_and_json: PyAndJson):\n    v = py_and_json(\n        {\n            'type': 'model-fields',\n            'fields': {'field_a': {'validation_alias': 'FieldA', 'type': 'model-field', 'schema': {'type': 'int'}}},\n        }\n    )\n    assert v.validate_test({'FieldA': '123'}) == ({'field_a': 123}, None, {'field_a'})\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'foobar': '123'})\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'field_a': '123'})", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001553", "source": "def collect_dataclass_fields(\n    cls: type[StandardDataclass],\n    *,\n    config_wrapper: ConfigWrapper,\n    ns_resolver: NsResolver | None = None,\n    typevars_map: dict[Any, Any] | None = None,\n) -> dict[str, FieldInfo]:\n    FieldInfo_ = import_cached_field_info()\n    fields: dict[str, FieldInfo] = {}\n    ns_resolver = ns_resolver or NsResolver()\n    dataclass_fields = cls.__dataclass_fields__\n    for base in reversed(cls.__mro__):\n        if not dataclasses.is_dataclass(base):\n            continue\n        with ns_resolver.push(base):\n            for ann_name, dataclass_field in dataclass_fields.items():\n                base_anns = _typing_extra.safe_get_annotations(base)\n                if ann_name not in base_anns:\n                    continue\n                globalns, localns = ns_resolver.types_namespace\n                ann_type, evaluated = _typing_extra.try_eval_type(dataclass_field.type, globalns, localns)\n                if _typing_extra.is_classvar_annotation(ann_type):\n                    continue\n                if (\n                    not dataclass_field.init\n                    and dataclass_field.default is dataclasses.MISSING\n                    and dataclass_field.default_factory is dataclasses.MISSING\n                ):\n                    continue\n                if isinstance(dataclass_field.default, FieldInfo_):\n                    if dataclass_field.default.init_var:\n                        if dataclass_field.default.init is False:\n                            raise PydanticUserError(\n                                f'Dataclass field {ann_name} has init=False and init_var=True, but these are mutually exclusive.',\n                                code='clashing-init-and-init-var',\n                            )\n                        continue\n                    field_info = FieldInfo_.from_annotated_attribute(\n                        ann_type, dataclass_field.default, _source=AnnotationSource.DATACLASS\n                    )\n                    field_info._original_assignment = dataclass_field.default\n                else:\n                    field_info = FieldInfo_.from_annotated_attribute(\n                        ann_type, dataclass_field, _source=AnnotationSource.DATACLASS\n                    )\n                    field_info._original_assignment = dataclass_field\n                if not evaluated:\n                    field_info._complete = False\n                    field_info._original_annotation = ann_type\n                fields[ann_name] = field_info\n                update_field_from_config(config_wrapper, ann_name, field_info)\n                if field_info.default is not PydanticUndefined and isinstance(\n                    getattr(cls, ann_name, field_info), FieldInfo_\n                ):\n                    setattr(cls, ann_name, field_info.default)\n    if typevars_map:\n        for field in fields.values():\n            field.apply_typevars_map(typevars_map)\n    if config_wrapper.use_attribute_docstrings:\n        _update_fields_from_docstrings(\n            cls,\n            fields,\n            use_inspect=not hasattr(cls, '__is_pydantic_dataclass__'),\n        )\n    return fields", "target": "def test_basic_schema_validator():\n    v = SchemaValidator(\n        {'type': 'dict', 'strict': True, 'keys_schema': {'type': 'int'}, 'values_schema': {'type': 'int'}}\n    )\n    v = pickle.loads(pickle.dumps(v))\n    assert v.validate_python({'1': 2, '3': 4}) == {1: 2, 3: 4}\n    assert v.validate_python({}) == {}\n    with pytest.raises(ValidationError, match=re.escape('[type=dict_type, input_value=[], input_type=list]')):\n        v.validate_python([])", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001554", "source": "def f(input_value: Any, info: core_schema.ValidationInfo) -> Any:\n        assert info.field_name == 'x'\n        assert info.data == {}\n        assert repr(info) == \"ValidationInfo(config=None, context=None, data={}, field_name='x')\"\n        assert str(info) == \"ValidationInfo(config=None, context=None, data={}, field_name='x')\"\n        assert isinstance(input_value, bytes)\n        return f'input: {input_value.decode()}'", "target": "def test_validate_assignment(pydantic_version) -> None:\n    @dataclass\n    class Model:\n        x: list['Model']\n    schema = core_schema.definitions_schema(\n        core_schema.definition_reference_schema('model'),\n        [\n            core_schema.dataclass_schema(\n                Model,\n                core_schema.dataclass_args_schema(\n                    'Model',\n                    [\n                        core_schema.dataclass_field(\n                            name='x',\n                            schema=core_schema.list_schema(core_schema.definition_reference_schema('model')),\n                            kw_only=False,\n                        )\n                    ],\n                ),\n                ['x'],\n                ref='model',\n                config=core_schema.CoreConfig(revalidate_instances='always'),\n            )\n        ],\n    )\n    v = SchemaValidator(schema)\n    data = [Model(x=[Model(x=[])])]\n    instance = Model(x=[])\n    v.validate_assignment(instance, 'x', data)\n    assert instance.x == data\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_assignment(instance, 'x', [Model(x=[Model(x=[Model(x=[123])])])])\n    assert exc_info.value.errors() == [\n        {\n            'type': 'dataclass_type',\n            'loc': ('x', 0, 'x', 0, 'x', 0, 'x', 0),\n            'msg': 'Input should be a dictionary or an instance of Model',\n            'input': 123,\n            'ctx': {'class_name': 'Model'},\n            'url': f'https://errors.pydantic.dev/{pydantic_version}/v/dataclass_type',\n        }\n    ]", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001555", "source": "def typename(self) -> str:\n        return \"None\"", "target": "def typename(self) -> str:\n        if self._export_name is not None:\n            return self._export_name\n        return self.ctype_name", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "001556", "source": "def f(input_value, info):\n        return input_value + f'| context: {info.context}'", "target": "def projectChessboard(squaresX, squaresY, squareSize, imageSize, cameraMatrix, rvec, tvec):\n    img = np.ones(imageSize, np.uint8) * 255\n    distCoeffs = np.zeros((5, 1), np.float64)\n    for y in range(squaresY):\n        startY = y * squareSize\n        for x in range(squaresX):\n            if (y % 2 != x % 2):\n                continue\n            startX = x * squareSize\n            squareCorners = np.array([[startX - squaresX*squareSize/2,\n                                       startY - squaresY*squareSize/2,\n                                       0]], np.float32)\n            squareCorners = np.stack((squareCorners[0],\n                                      squareCorners[0] + [squareSize, 0, 0],\n                                      squareCorners[0] + [squareSize, squareSize, 0],\n                                      squareCorners[0] + [0, squareSize, 0]))\n            projectedCorners, _ = cv.projectPoints(squareCorners, rvec, tvec, cameraMatrix, distCoeffs)\n            projectedCorners = projectedCorners.astype(np.int64)\n            projectedCorners = projectedCorners.reshape(1, 4, 2)\n            img = cv.fillPoly(img, [projectedCorners], 0)\n    return img", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001557", "source": "def typename(self) -> Optional[str]:\n            return getattr(self.type_node, \"full_typename\", None)", "target": "def typename(self) -> str:\n        return self._export_name", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|opencv/opencv", "data_split": "train", "example_id": "001558", "source": "def load_data(dtype=np.float32, order=\"C\", random_state=13):\n    print(\"Loading dataset...\")\n    data = fetch_covtype(\n        download_if_missing=True, shuffle=True, random_state=random_state\n    )\n    X = check_array(data[\"data\"], dtype=dtype, order=order)\n    y = (data[\"target\"] != 1).astype(int)\n    print(\"Creating train-test split...\")\n    n_train = 522911\n    X_train = X[:n_train]\n    y_train = y[:n_train]\n    X_test = X[n_train:]\n    y_test = y[n_train:]\n    mean = X_train.mean(axis=0)\n    std = X_train.std(axis=0)\n    mean[10:] = 0.0\n    std[10:] = 1.0\n    X_train = (X_train - mean) / std\n    X_test = (X_test - mean) / std\n    return X_train, X_test, y_train, y_test", "target": "def norm_hamming(x, y=None):\n    def norm(vec):\n        return sum(bin(i).count('1') for i in vec.flatten())\n    return norm(x) if y is None else norm(np.bitwise_xor(x, y))", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "001559", "source": "def get_artifacts_urls(results, suites):\n    urls = {}\n    for r in results:\n        if (\n            r[\"workflowName\"] in (\"inductor\", \"inductor-periodic\")\n            and \"test\" in r[\"jobName\"]\n            and \"build\" not in r[\"jobName\"]\n            and \"runner-determinator\" not in r[\"jobName\"]\n            and \"unit-test\" not in r[\"jobName\"]\n        ):\n            *_, test_str = parse_job_name(r[\"jobName\"])\n            suite, shard_id, num_shards, machine, *_ = parse_test_str(test_str)\n            workflowId = r[\"workflowId\"]\n            id = r[\"id\"]\n            runAttempt = r[\"runAttempt\"]\n            if suite in suites:\n                artifact_filename = f\"test-reports-test-{suite}-{shard_id}-{num_shards}-{machine}_{id}.zip\"\n                s3_url = f\"{S3_BASE_URL}/{repo}/{workflowId}/{runAttempt}/artifact/{artifact_filename}\"\n                urls[(suite, int(shard_id))] = s3_url\n                print(f\"{suite} {shard_id}, {num_shards}: {s3_url}\")\n    return urls", "target": "def test_filter_int():\n    s = SchemaSerializer(\n        core_schema.dict_schema(\n            core_schema.any_schema(), serialization=core_schema.filter_dict_schema(include={1, 3, 5}, exclude={5, 6})\n        )\n    )\n    assert s.to_python({0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7}) == {1: 1, 3: 3}", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|pydantic/pydantic", "data_split": "train", "example_id": "001560", "source": "def bench(clfs):\n    for (\n        name,\n        clf,\n        iter_range,\n        train_losses,\n        train_scores,\n        test_scores,\n        durations,\n    ) in clfs:\n        print(\"training %s\" % name)\n        clf_type = type(clf)\n        clf_params = clf.get_params()\n        for n_iter in iter_range:\n            gc.collect()\n            train_loss, train_score, test_score, duration = bench_one(\n                name, clf_type, clf_params, n_iter\n            )\n            train_losses.append(train_loss)\n            train_scores.append(train_score)\n            test_scores.append(test_score)\n            durations.append(duration)\n            print(\"classifier: %s\" % name)\n            print(\"train_loss: %.8f\" % train_loss)\n            print(\"train_score: %.8f\" % train_score)\n            print(\"test_score: %.8f\" % test_score)\n            print(\"time for fit: %.8f seconds\" % duration)\n            print(\"\")\n        print(\"\")\n    return clfs", "target": "def test_no_args_constructor(self):\n        tz = TzInfo()\n        self.assertEqual(tz.utcoffset(None), timedelta(0))\n        self.assertEqual(str(tz), 'UTC')", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001561", "source": "def node_type(self) -> ASTNodeType:\n        return ASTNodeType.Function", "target": "def node_type(self) -> ASTNodeType:\n        return ASTNodeType.Constant", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001562", "source": "def ser_x(data: Model, v: Any, serializer: core_schema.SerializerFunctionWrapHandler, _) -> str:\n        x = serializer(v)\n        assert data['x'] == 1_000\n        return f'{x:_}'", "target": "def ser_x(\n        data: Model,\n        v: Any,\n        serializer: core_schema.SerializerFunctionWrapHandler,\n        info: core_schema.FieldSerializationInfo,\n    ) -> str:\n        assert data['x'] == 1_000\n        x = serializer(v)\n        return f'{x:_}-{info.field_name}'", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "001563", "source": "def fn():\n        class Model:\n            a: int\n            @classmethod\n            def _validator(cls, v, info):\n                return v\n            @classmethod\n            def _wrap_validator(cls, v, validator, info):\n                return validator(v)\n        field_schema = core_schema.int_schema()\n        if validator == 'field':\n            field_schema = core_schema.with_info_before_validator_function(Model._validator, field_schema)\n            field_schema = core_schema.with_info_wrap_validator_function(Model._wrap_validator, field_schema)\n            field_schema = core_schema.with_info_after_validator_function(Model._validator, field_schema)\n        model_schema = core_schema.model_schema(\n            Model, core_schema.model_fields_schema({'a': core_schema.model_field(field_schema)})\n        )\n        if validator == 'model':\n            model_schema = core_schema.with_info_before_validator_function(Model._validator, model_schema)\n            model_schema = core_schema.with_info_wrap_validator_function(Model._wrap_validator, model_schema)\n            model_schema = core_schema.with_info_after_validator_function(Model._validator, model_schema)\n        Model.__pydantic_validator__ = SchemaValidator(model_schema)\n        return Model", "target": "def merge_dependencies_lists(deps_lists):\n    result = []\n    for d_list in deps_lists:\n        for i in range(len(d_list)):\n            if d_list[i] not in result:\n                if i == 0:\n                    result.append(d_list[i])\n                else:\n                    index = result.index(d_list[i-1])\n                    result = result[:index + 1] + [d_list[i]] + result[index + 1:]\n    return result", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001564", "source": "def test_non_finite_json_values(py_and_json: PyAndJson, input_value, allow_inf_nan, expected):\n    v = py_and_json({'type': 'float', 'allow_inf_nan': allow_inf_nan})\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            v.validate_test(input_value)\n    else:\n        assert v.validate_test(input_value) == expected", "target": "def test_only_allow_alias(py_and_json) -> None:\n    schema = core_schema.dataclass_schema(\n        BasicDataclass,\n        core_schema.dataclass_args_schema(\n            'BasicDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), validation_alias='FieldA'),\n            ],\n        ),\n        ['a'],\n        config=core_schema.CoreConfig(validate_by_name=False, validate_by_alias=True),\n    )\n    v = py_and_json(schema)\n    assert v.validate_test({'FieldA': 'hello'}) == BasicDataclass(a='hello')\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'a': 'hello'})", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "001565", "source": "def _check_func(\n    func: Callable[[Any], bool], predicate_err: str | Callable[[], str], s: cs.CoreSchema | None\n) -> cs.CoreSchema:\n    def handler(v: Any) -> Any:\n        if func(v):\n            return v\n        raise ValueError(f'Expected {predicate_err if isinstance(predicate_err, str) else predicate_err()}')\n    if s is None:\n        return cs.no_info_plain_validator_function(handler)\n    else:\n        return cs.no_info_after_validator_function(handler, s)", "target": "def _install_test_dependencies(self):\n        logger.info(\"generate test.txt from requirements/test.in with local torch whls\")\n        preprocess_test_in()\n        copy(\"requirements/test.txt\", \"snapshot_constraint.txt\")\n        run_command(\n            f\"{sys.executable} -m uv pip compile requirements/test.in \"\n            \"-o test.txt \"\n            \"--index-strategy unsafe-best-match \"\n            \"--constraint snapshot_constraint.txt \"\n            \"--torch-backend cu128\"\n        )\n        pip_install_packages(requirements=\"test.txt\", prefer_uv=True)\n        logger.info(\"Done. installed requirements for test dependencies\")", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|scikit-learn/scikit-learn", "data_split": "train", "example_id": "001566", "source": "def test_literal_py_and_json(py_and_json: PyAndJson, kwarg_expected, input_value, expected):\n    v = py_and_json({'type': 'literal', 'expected': kwarg_expected})\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)) as exc_info:\n            v.validate_test(input_value)\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        assert v.validate_test(input_value) == expected", "target": "def make_data(self, params):\n        representation, algorithm, init = params\n        if representation == \"sparse\":\n            data = _20newsgroups_highdim_dataset(n_samples=8000)\n        else:\n            data = _blobs_dataset(n_clusters=20)\n        return data", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "001567", "source": "def make_data(self, params):\n        representation, precompute = params\n        if representation == \"dense\":\n            data = _synth_regression_dataset(n_samples=1000000, n_features=100)\n        else:\n            data = _synth_regression_sparse_dataset(\n                n_samples=50000, n_features=5000, density=0.01\n            )\n        return data", "target": "def make_data(self, params):\n        representation, n_jobs = params\n        if representation == \"sparse\":\n            data = _20newsgroups_highdim_dataset()\n        else:\n            data = _20newsgroups_lowdim_dataset()\n        return data", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001568", "source": "def constrain(self: _Pipeline[_InT, _NewOutGt], constraint: annotated_types.Gt) -> _Pipeline[_InT, _NewOutGt]: ...", "target": "def constrain(\n        self: _Pipeline[_InT, _NewOutLen], constraint: annotated_types.Len\n    ) -> _Pipeline[_InT, _NewOutLen]: ...", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001569", "source": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "target": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "001570", "source": "def compute_bench(alpha, n_samples, n_features, precompute):\n    lasso_results = []\n    lars_lasso_results = []\n    it = 0\n    for ns in n_samples:\n        for nf in n_features:\n            it += 1\n            print(\"==================\")\n            print(\"Iteration %s of %s\" % (it, max(len(n_samples), len(n_features))))\n            print(\"==================\")\n            n_informative = nf // 10\n            X, Y, coef_ = make_regression(\n                n_samples=ns,\n                n_features=nf,\n                n_informative=n_informative,\n                noise=0.1,\n                coef=True,\n            )\n            X /= np.sqrt(np.sum(X**2, axis=0))\n            gc.collect()\n            print(\"- benchmarking Lasso\")\n            clf = Lasso(alpha=alpha, fit_intercept=False, precompute=precompute)\n            tstart = time()\n            clf.fit(X, Y)\n            lasso_results.append(time() - tstart)\n            gc.collect()\n            print(\"- benchmarking LassoLars\")\n            clf = LassoLars(alpha=alpha, fit_intercept=False, precompute=precompute)\n            tstart = time()\n            clf.fit(X, Y)\n            lars_lasso_results.append(time() - tstart)\n    return lasso_results, lars_lasso_results", "target": "def compute_bench(samples_range, features_range):\n    it = 0\n    results = dict()\n    lars = np.empty((len(features_range), len(samples_range)))\n    lars_gram = lars.copy()\n    omp = lars.copy()\n    omp_gram = lars.copy()\n    max_it = len(samples_range) * len(features_range)\n    for i_s, n_samples in enumerate(samples_range):\n        for i_f, n_features in enumerate(features_range):\n            it += 1\n            n_informative = n_features // 10\n            print(\"====================\")\n            print(\"Iteration %03d of %03d\" % (it, max_it))\n            print(\"====================\")\n            dataset_kwargs = {\n                \"n_samples\": 1,\n                \"n_components\": n_features,\n                \"n_features\": n_samples,\n                \"n_nonzero_coefs\": n_informative,\n                \"random_state\": 0,\n            }\n            print(\"n_samples: %d\" % n_samples)\n            print(\"n_features: %d\" % n_features)\n            y, X, _ = make_sparse_coded_signal(**dataset_kwargs)\n            X = np.asfortranarray(X.T)\n            gc.collect()\n            print(\"benchmarking lars_path (with Gram):\", end=\"\")\n            sys.stdout.flush()\n            tstart = time()\n            G = np.dot(X.T, X)\n            Xy = np.dot(X.T, y)\n            lars_path_gram(Xy=Xy, Gram=G, n_samples=y.size, max_iter=n_informative)\n            delta = time() - tstart\n            print(\"%0.3fs\" % delta)\n            lars_gram[i_f, i_s] = delta\n            gc.collect()\n            print(\"benchmarking lars_path (without Gram):\", end=\"\")\n            sys.stdout.flush()\n            tstart = time()\n            lars_path(X, y, Gram=None, max_iter=n_informative)\n            delta = time() - tstart\n            print(\"%0.3fs\" % delta)\n            lars[i_f, i_s] = delta\n            gc.collect()\n            print(\"benchmarking orthogonal_mp (with Gram):\", end=\"\")\n            sys.stdout.flush()\n            tstart = time()\n            orthogonal_mp(X, y, precompute=True, n_nonzero_coefs=n_informative)\n            delta = time() - tstart\n            print(\"%0.3fs\" % delta)\n            omp_gram[i_f, i_s] = delta\n            gc.collect()\n            print(\"benchmarking orthogonal_mp (without Gram):\", end=\"\")\n            sys.stdout.flush()\n            tstart = time()\n            orthogonal_mp(X, y, precompute=False, n_nonzero_coefs=n_informative)\n            delta = time() - tstart\n            print(\"%0.3fs\" % delta)\n            omp[i_f, i_s] = delta\n    results[\"time(LARS) / time(OMP)\\n (w/ Gram)\"] = lars_gram / omp_gram\n    results[\"time(LARS) / time(OMP)\\n (w/o Gram)\"] = lars / omp\n    return results", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pytorch/pytorch", "data_split": "train", "example_id": "001571", "source": "def __load_extra_py_code_for_module(base, name, enable_debug_print=False):\n    module_name = \"{}.{}\".format(__name__, name)\n    export_module_name = \"{}.{}\".format(base, name)\n    native_module = sys.modules.pop(module_name, None)\n    try:\n        py_module = importlib.import_module(module_name)\n    except ImportError as err:\n        if enable_debug_print:\n            print(\"Can't load Python code for module:\", module_name,\n                  \". Reason:\", err)\n        return False\n    if base in sys.modules and not hasattr(sys.modules[base], name):\n        setattr(sys.modules[base], name, py_module)\n    sys.modules[export_module_name] = py_module\n    if native_module:\n        setattr(py_module, \"_native\", native_module)\n        for k, v in filter(lambda kv: not hasattr(py_module, kv[0]),\n                           native_module.__dict__.items()):\n            if enable_debug_print: print('    symbol({}): {} = {}'.format(name, k, v))\n            setattr(py_module, k, v)\n    return True", "target": "def compute_layernorm_stats(self, input):\n        mu = input.mean(-1, keepdim=True)\n        sigma = input.std(-1, keepdim=True, unbiased=False)\n        return mu, sigma", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "001572", "source": "def smoke_test_modules():\n    cwd = os.getcwd()\n    for module in MODULES:\n        if module[\"repo\"]:\n            if not os.path.exists(f\"{cwd}/{module['repo_name']}\"):\n                print(f\"Path does not exist: {cwd}/{module['repo_name']}\")\n                try:\n                    subprocess.check_output(\n                        f\"git clone --depth 1 {module['repo']}\",\n                        stderr=subprocess.STDOUT,\n                        shell=True,\n                    )\n                except subprocess.CalledProcessError as exc:\n                    raise RuntimeError(\n                        f\"Cloning {module['repo']} FAIL: {exc.returncode} Output: {exc.output}\"\n                    ) from exc\n            try:\n                smoke_test_command = f\"python3 {module['smoke_test']}\"\n                if target_os == \"windows\":\n                    smoke_test_command = f\"python {module['smoke_test']}\"\n                output = subprocess.check_output(\n                    smoke_test_command,\n                    stderr=subprocess.STDOUT,\n                    shell=True,\n                    universal_newlines=True,\n                )\n            except subprocess.CalledProcessError as exc:\n                raise RuntimeError(\n                    f\"Module {module['name']} FAIL: {exc.returncode} Output: {exc.output}\"\n                ) from exc\n            else:\n                print(f\"Output: \\n{output}\\n\")", "target": "def ser_x(\n        data: Model,\n        v: Any,\n        serializer: core_schema.SerializerFunctionWrapHandler,\n        info: core_schema.FieldSerializationInfo,\n    ) -> str:\n        assert data['x'] == 1_000\n        x = serializer(v)\n        return f'{x:_}-{info.field_name}'", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "001573", "source": "def bench_one(\n    name, X, W0, H0, X_shape, clf_type, clf_params, init, n_components, random_state\n):\n    W = W0.copy()\n    H = H0.copy()\n    clf = clf_type(**clf_params)\n    st = time()\n    W = clf.fit_transform(X, W=W, H=H)\n    end = time()\n    H = clf.components_\n    this_loss = _beta_divergence(X, W, H, 2.0, True)\n    duration = end - st\n    return this_loss, duration", "target": "def bench_one(name, clf_type, clf_params, n_iter):\n    clf = clf_type(**clf_params)\n    try:\n        clf.set_params(max_iter=n_iter, random_state=42)\n    except Exception:\n        clf.set_params(n_iter=n_iter, random_state=42)\n    st = time.time()\n    clf.fit(X, y)\n    end = time.time()\n    try:\n        C = 1.0 / clf.alpha / n_samples\n    except Exception:\n        C = clf.C\n    try:\n        intercept = clf.intercept_\n    except Exception:\n        intercept = 0.0\n    train_loss = get_loss(clf.coef_, intercept, X, y, C)\n    train_score = clf.score(X, y)\n    test_score = clf.score(X_test, y_test)\n    duration = end - st\n    return train_loss, train_score, test_score, duration", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "train", "example_id": "001574", "source": "def forward(self, x):\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        x = self.dropout1(x)\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.dropout2(x)\n        x = self.fc2(x)\n        output = F.log_softmax(x, dim=1)\n        return output", "target": "def revert(self, ref: str) -> None:\n        self._run_git(\"revert\", \"--no-edit\", ref)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001575", "source": "def benchmark(self):\n        for M, N in self.get_shapes():\n            print(f\"Tensor dimensions: [{M}, {N}]\")\n            torch_dtype = cutlass_torch.dtype(cutlass.BFloat16)\n            x = 0.1 * torch.randn(\n                M, N, device=\"cuda\", dtype=torch_dtype, requires_grad=True\n            )\n            dy = torch.randn(M, N, device=\"cuda\", dtype=torch_dtype)\n            self.benchmark_single_shape((x, dy), setting=f\"shape: [{M}, {N}]\")", "target": "def benchmark(self):\n        for M, N in self.get_shapes():\n            print(f\"Tensor dimensions: [{M}, {N}]\")\n            torch_dtype = cutlass_torch.dtype(cutlass.BFloat16)\n            x = torch.randn(M, N, device=\"cuda\", dtype=torch_dtype)\n            w = torch.randn(N, device=\"cuda\", dtype=torch.float32)\n            self.benchmark_single_shape((x, w), setting=f\"shape: [{M}, {N}]\")", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "train", "example_id": "001576", "source": "def with_nnc():\n    torch._C._jit_override_can_fuse_on_cpu(True)\n    torch._C._jit_override_can_fuse_on_gpu(True)\n    torch._C._jit_set_texpr_fuser_enabled(True)\n    torch._C._jit_set_nvfuser_enabled(False)\n    torch._C._jit_set_profiling_executor(True)\n    torch._C._jit_set_profiling_mode(True)", "target": "def _shasum(value: str) -> str:\n    import hashlib\n    m = hashlib.sha256()\n    m.update(value.encode(\"utf-8\"))\n    return m.hexdigest()", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001577", "source": "def ser_root(self, v: Any, _) -> str:\n            assert self.root == 1_000\n            return f'{v:_}'", "target": "def ser_root(self, v: Any, serializer: core_schema.SerializerFunctionWrapHandler, _) -> str:\n            assert self.root == 1_000\n            root = serializer(v)\n            return f'{root:_}'", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001578", "source": "def test_allow_extra_invalid():\n    with pytest.raises(SchemaError, match='extras_schema can only be used if extra_behavior=allow'):\n        SchemaValidator(\n            schema=core_schema.model_fields_schema(\n                fields={}, extras_schema=core_schema.int_schema(), extra_behavior='ignore'\n            )\n        )\n    with pytest.raises(SchemaError, match='extras_keys_schema can only be used if extra_behavior=allow'):\n        SchemaValidator(\n            schema=core_schema.model_fields_schema(\n                fields={}, extras_keys_schema=core_schema.int_schema(), extra_behavior='ignore'\n            )\n        )", "target": "def test_alias_extra_forbid(py_and_json: PyAndJson):\n    v = py_and_json(\n        {\n            'type': 'typed-dict',\n            'extra_behavior': 'forbid',\n            'fields': {\n                'field_a': {'type': 'typed-dict-field', 'validation_alias': 'FieldA', 'schema': {'type': 'int'}}\n            },\n        }\n    )\n    assert v.validate_test({'FieldA': 1}) == {'field_a': 1}", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001579", "source": "def required_definition_imports(self) -> Generator[str, None, None]:\n        yield \"import typing as _typing\"\n        yield from super().required_definition_imports", "target": "def required_definition_imports(self) -> Generator[str, None, None]:\n        yield \"import typing as _typing\"\n        yield from super().required_definition_imports", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001580", "source": "def get_shapes(self) -> tuple[tuple[int, ...], ...]:\n        return (\n            (32768, 256),\n            (32768, 512),\n            (32768, 1024),\n            (32768, 2048),\n            (32768, 4096),\n            (32768, 8192),\n            (32768, 16384),\n        ) + extra_shapes_for_norm", "target": "def get_shapes(self) -> tuple[tuple[int, ...], ...]:\n        return (\n            (32768, 256),\n            (32768, 512),\n            (32768, 1024),\n            (32768, 2048),\n            (32768, 4096),\n            (32768, 8192),\n            (32768, 16384),\n            (32768, 32768),\n            (32768, 65536),\n        ) + extra_shapes_for_norm", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "001581", "source": "def get_call_frequency(self, op):\n        assert str(op) in self.operator_db, (\n            f\"Could not find {op}, must provide overload\"\n        )\n        count = 0\n        for counter in self.operator_db[str(op)].values():\n            count += counter\n        return count", "target": "def test_forbid_extra():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={'field_a': core_schema.model_field(schema=core_schema.str_schema())}, extra_behavior='forbid'\n        )\n    )\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python({'field_a': 'abc', 'field_b': 1})\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'extra_forbidden', 'loc': ('field_b',), 'msg': 'Extra inputs are not permitted', 'input': 1}\n    ]", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001582", "source": "def liger(self, args, kwargs=None) -> Any:\n        from liger_kernel.transformers.softmax import LigerSoftmax\n        assert kwargs is None\n        (x,) = args\n        softmax = LigerSoftmax().to(\"cuda\")\n        return lambda: softmax(x)", "target": "def liger(self, args, kwargs) -> Any:\n        from liger_kernel.transformers.rms_norm import LigerRMSNorm\n        x, w = args\n        M, N = x.shape\n        liger_rmsnorm = LigerRMSNorm(hidden_size=N, eps=1e-6).cuda()\n        liger_rmsnorm.weight.data.copy_(w)\n        return lambda: liger_rmsnorm(x)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001583", "source": "def test_serialization_alias():\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bytes_schema(), serialization_alias='BAR'),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    s = SchemaSerializer(schema)\n    assert s.to_python(Foo(a='hello', b=b'more'), by_alias=True) == IsStrictDict(a='hello', BAR=b'more')\n    assert s.to_python(Foo(a='hello', b=b'more'), mode='json', by_alias=True) == IsStrictDict(a='hello', BAR='more')\n    j = s.to_json(Foo(a='hello', b=b'more'), by_alias=True)\n    if on_pypy:\n        assert json.loads(j) == {'a': 'hello', 'BAR': 'more'}\n    else:\n        assert j == b'{\"a\":\"hello\",\"BAR\":\"more\"}'", "target": "def test_repeat_after():\n    with pytest.raises(SchemaError, match='SchemaError: Duplicate ref: `foobar`'):\n        SchemaSerializer(\n            core_schema.definitions_schema(\n                core_schema.tuple_positional_schema(\n                    [\n                        core_schema.definitions_schema(\n                            core_schema.definition_reference_schema('foobar'), [core_schema.int_schema(ref='foobar')]\n                        ),\n                        core_schema.definition_reference_schema('foobar'),\n                    ]\n                ),\n                [core_schema.int_schema(ref='foobar')],\n            )\n        )", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "001584", "source": "def test_length_constraints_omit(input_value, expected):\n    v = SchemaValidator(\n        core_schema.tuple_schema(\n            items_schema=[core_schema.with_default_schema(schema=core_schema.int_schema(), on_error='omit')],\n            variadic_item_index=0,\n            max_length=4,\n        )\n    )\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            v.validate_python(input_value)\n    else:\n        assert v.validate_python(input_value) == expected", "target": "def run(value, state):\n            state.counter += value\n            return state.counter", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "001585", "source": "def f(value, serializer, _info):\n        return f'result={serializer(value)}'", "target": "def get_output(self, input_blob):\n        tensor = torch.FloatTensor(input_blob)\n        out = self.net.forward(tensor).numpy()\n        return out", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001586", "source": "def test_by_alias_and_name_config_interaction(config, runtime, expected) -> None:\n    class Model:\n        def __init__(self, my_field: int) -> None:\n            self.my_field = my_field\n    schema = core_schema.model_schema(\n        Model,\n        core_schema.model_fields_schema(\n            {\n                'my_field': core_schema.model_field(core_schema.int_schema(), serialization_alias='my_alias'),\n            }\n        ),\n        config=core_schema.CoreConfig(serialize_by_alias=config or False),\n    )\n    s = SchemaSerializer(schema)\n    assert s.to_python(Model(1), by_alias=runtime) == expected", "target": "def test_by_alias_and_name_config_interaction(\n    config_by_alias: Union[bool, None],\n    config_by_name: Union[bool, None],\n    runtime_by_alias: Union[bool, None],\n    runtime_by_name: Union[bool, None],\n) -> None:\n    if config_by_alias is False and config_by_name is False and runtime_by_alias is False and runtime_by_name is False:\n        pytest.skip(\"Can't have both by_alias and by_name as effectively False\")\n    class Model:\n        def __init__(self, my_field: int) -> None:\n            self.my_field = my_field\n    core_config = {\n        **({'validate_by_alias': config_by_alias} if config_by_alias is not None else {}),\n        **({'validate_by_name': config_by_name} if config_by_name is not None else {}),\n    }\n    schema = core_schema.model_schema(\n        Model,\n        core_schema.model_fields_schema(\n            {\n                'my_field': core_schema.model_field(core_schema.int_schema(), validation_alias='my_alias'),\n            }\n        ),\n        config=core_schema.CoreConfig(**core_config),\n    )\n    s = SchemaValidator(schema)\n    alias_allowed = next(x for x in (runtime_by_alias, config_by_alias, True) if x is not None)\n    name_allowed = next(x for x in (runtime_by_name, config_by_name, False) if x is not None)\n    if alias_allowed:\n        assert s.validate_python({'my_alias': 1}, by_alias=runtime_by_alias, by_name=runtime_by_name).my_field == 1\n    if name_allowed:\n        assert s.validate_python({'my_field': 1}, by_alias=runtime_by_alias, by_name=runtime_by_name).my_field == 1", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001587", "source": "def row_as_markdown(cols: list[str]) -> str:\n        return f'| {\" | \".join(cols)} |'", "target": "def to_argument(\n        self,\n        current_info: TypeInfo,\n        typed: bool,\n        model_strict: bool,\n        force_optional: bool,\n        use_alias: bool,\n        api: SemanticAnalyzerPluginInterface,\n        force_typevars_invariant: bool,\n        is_root_model_root: bool,\n    ) -> Argument:\n        variable = self.to_var(current_info, api, use_alias, force_typevars_invariant)\n        strict = model_strict if self.strict is None else self.strict\n        if typed or strict:\n            type_annotation = self.expand_type(current_info, api, include_root_type=True)\n        else:\n            type_annotation = AnyType(TypeOfAny.explicit)\n        return Argument(\n            variable=variable,\n            type_annotation=type_annotation,\n            initializer=None,\n            kind=ARG_OPT\n            if is_root_model_root\n            else (ARG_NAMED_OPT if force_optional or self.has_default else ARG_NAMED),\n        )", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "001588", "source": "def test_type_mismatch(self):\n        import_path()\n        with self.assertRaises(cv.error) as context:\n            cv.imread(123)\n        self.assertTrue('str or path-like' in str(context.exception))", "target": "def _infer_discriminator_values_for_choice(\n        self, choice: core_schema.CoreSchema, source_name: str | None\n    ) -> list[str | int]:\n        if choice['type'] == 'definitions':\n            return self._infer_discriminator_values_for_choice(choice['schema'], source_name=source_name)\n        elif _core_utils.is_function_with_inner_schema(choice):\n            return self._infer_discriminator_values_for_choice(choice['schema'], source_name=source_name)\n        elif choice['type'] == 'lax-or-strict':\n            return sorted(\n                set(\n                    self._infer_discriminator_values_for_choice(choice['lax_schema'], source_name=None)\n                    + self._infer_discriminator_values_for_choice(choice['strict_schema'], source_name=None)\n                )\n            )\n        elif choice['type'] == 'tagged-union':\n            values: list[str | int] = []\n            subchoices = [x for x in choice['choices'].values() if not isinstance(x, (str, int))]\n            for subchoice in subchoices:\n                subchoice_values = self._infer_discriminator_values_for_choice(subchoice, source_name=None)\n                values.extend(subchoice_values)\n            return values\n        elif choice['type'] == 'union':\n            values = []\n            for subchoice in choice['choices']:\n                subchoice_schema = subchoice[0] if isinstance(subchoice, tuple) else subchoice\n                subchoice_values = self._infer_discriminator_values_for_choice(subchoice_schema, source_name=None)\n                values.extend(subchoice_values)\n            return values\n        elif choice['type'] == 'nullable':\n            self._should_be_nullable = True\n            return self._infer_discriminator_values_for_choice(choice['schema'], source_name=None)\n        elif choice['type'] == 'model':\n            return self._infer_discriminator_values_for_choice(choice['schema'], source_name=choice['cls'].__name__)\n        elif choice['type'] == 'dataclass':\n            return self._infer_discriminator_values_for_choice(choice['schema'], source_name=choice['cls'].__name__)\n        elif choice['type'] == 'model-fields':\n            return self._infer_discriminator_values_for_model_choice(choice, source_name=source_name)\n        elif choice['type'] == 'dataclass-args':\n            return self._infer_discriminator_values_for_dataclass_choice(choice, source_name=source_name)\n        elif choice['type'] == 'typed-dict':\n            return self._infer_discriminator_values_for_typed_dict_choice(choice, source_name=source_name)\n        elif choice['type'] == 'definition-ref':\n            schema_ref = choice['schema_ref']\n            if schema_ref not in self.definitions:\n                raise MissingDefinitionForUnionRef(schema_ref)\n            return self._infer_discriminator_values_for_choice(self.definitions[schema_ref], source_name=source_name)\n        else:\n            err_str = f'The core schema type {choice[\"type\"]!r} is not a valid discriminated union variant.'\n            if choice['type'] == 'list':\n                err_str += (\n                    ' If you are making use of a list of union types, make sure the discriminator is applied to the '\n                    'union type and not the list (e.g. `list[Annotated[<T> | <U>, Field(discriminator=...)]]`).'\n                )\n            raise TypeError(err_str)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001589", "source": "def type_format(self) -> str:\n        if TypeNode.compatible_to_runtime_usage:\n            return \"_typing.Tuple[{}]\"\n        return \"tuple[{}]\"", "target": "def type_format(self) -> str:\n        return \"_typing.Type[{}]\"", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001590", "source": "def my_function(input_value, info):\n        return input_value + 'x'", "target": "def my_function(input_value, info):\n        return input_value + 'x'", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001591", "source": "def required_usage_imports(self) -> Generator[str, None, None]:\n        yield from ()", "target": "def required_usage_imports(self) -> Generator[str, None, None]:\n        yield \"import os\"", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001592", "source": "def test_constraints_schema_validation() -> None:\n    val = SchemaValidator(cs.date_schema(gt='2020-01-01'))\n    with pytest.raises(ValidationError):\n        val.validate_python('2019-01-01')", "target": "def test_constraints_schema_validation() -> None:\n    val = SchemaValidator(cs.datetime_schema(gt='2020-01-01T00:00:00'))\n    with pytest.raises(ValidationError):\n        val.validate_python('2019-01-01T00:00:00')", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001593", "source": "def test_tz_constraint_too_high():\n    with pytest.raises(SchemaError, match='OverflowError: Python int too large.*'):\n        SchemaValidator(core_schema.datetime_schema(tz_constraint=2**64))", "target": "def test_tz_constraint_too_high():\n    with pytest.raises(SchemaError, match='OverflowError: Python int too large.*'):\n        SchemaValidator(core_schema.time_schema(tz_constraint=2**64))", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "001594", "source": "def test_duplicate_parameter_name() -> None:\n    with pytest.raises(SchemaError, match=\"Duplicate parameter 'test'\"):\n        SchemaValidator(\n            schema=cs.arguments_v3_schema(\n                [\n                    cs.arguments_v3_parameter(name='test', schema=cs.int_schema()),\n                    cs.arguments_v3_parameter(name='a', schema=cs.int_schema()),\n                    cs.arguments_v3_parameter(name='test', schema=cs.int_schema()),\n                ]\n            )\n        )", "target": "def fix_type_usage(type_node: TypeNode) -> None:\n            if isinstance(type_node, AggregatedTypeNode):\n                for item in type_node.items:\n                    fix_type_usage(item)\n            if isinstance(type_node, ASTNodeTypeNode):\n                if type_node._typename in USED_TYPES:\n                    type_node._typename = f\"cuda_{type_node._typename}\"", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "001595", "source": "def test_benchmark_tile_reduce_various_sizes(self) -> None:\n        tile_sizes = [512, 1024, 2048, 4096, 8192, 16384]\n        full_size = tile_sizes[-1]\n        warmup_iters = 5\n        bench_iters = 20\n        results = []\n        for tile_size in tile_sizes:\n            try:\n                result = self._benchmark_tile_reduce_single(\n                    full_size, tile_size, warmup_iters, bench_iters\n                )\n                results.append(result)\n                if self.rank == 0:\n                    print(\n                        f\"Matrix Size: {full_size}x{full_size}, Tile Size: {tile_size}x{tile_size}\"\n                    )\n                    print(\n                        f\"  Mean Time: {result['mean_time_ms']:.3f}  {result['std_time_ms']:.3f} ms\"\n                    )\n                    print(f\"  Throughput: {result['throughput_gb_s']:.2f} GB/s\")\n                    print(f\"  Bytes: {result['tile_bytes']:.0f}\")\n                    print()\n            except Exception as e:\n                if self.rank == 0:\n                    print(f\"Failed to benchmark matrix size {full_size}: {e}\")\n        if self.rank == 0 and results:\n            print(\"=== BENCHMARK SUMMARY ===\")\n            print(\n                f\"{'Matrix Size':<12} {'Tile Size':<10} {'Time (ms)':<12} {'Throughput (GB/s)':<18} {'Bytes':<15}\"\n            )\n            print(\"-\" * 70)\n            for result in results:\n                print(\n                    f\"{result['full_size']}x{result['full_size']:<7} \"\n                    f\"{result['tile_size']}x{result['tile_size']:<5} \"\n                    f\"{result['mean_time_ms']:<12.3f} \"\n                    f\"{result['throughput_gb_s']:<18.2f} \"\n                    f\"{result['tile_bytes']:<15.0f}\"\n                )", "target": "def set_dataclass_fields(\n    cls: type[StandardDataclass],\n    config_wrapper: _config.ConfigWrapper,\n    ns_resolver: NsResolver | None = None,\n) -> None:\n    typevars_map = get_standard_typevars_map(cls)\n    fields = collect_dataclass_fields(\n        cls, ns_resolver=ns_resolver, typevars_map=typevars_map, config_wrapper=config_wrapper\n    )\n    cls.__pydantic_fields__ = fields", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001596", "source": "def relative_typename(self, root: str) -> Optional[str]:\n            return self.type_node.relative_typename(root)", "target": "def relative_typename(self, module: str) -> str:\n        return self.full_typename", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001597", "source": "def test_model_a(self, schema_validator: SchemaValidator):\n        m_a = schema_validator.validate_python({'a': 1, 'b': 'hello'})\n        assert isinstance(m_a, self.ModelA)\n        assert m_a.a == 1\n        assert m_a.b == 'hello'", "target": "def test_model_a(self, schema_validator: SchemaValidator):\n        m = schema_validator.validate_python({'a': 1, 'b': 'hello'})\n        assert isinstance(m, self.ModelA)\n        assert m.a == 1\n        assert m.b == 'hello'\n        assert not hasattr(m, 'c')", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "001598", "source": "def get_output(self, input_blob):\n        self.net.setInput(input_blob, self.in_blob_name)\n        return self.net.forward(self.out_blob_name)", "target": "def test_model_b(model_serializer: SchemaSerializer, input_value):\n    assert model_serializer.to_python(input_value) == {'c': b'bite', 'd': '2.35'}\n    assert model_serializer.to_python(input_value, mode='json') == {'c': 'bite', 'd': '2.35'}\n    assert model_serializer.to_json(input_value) == b'{\"c\":\"bite\",\"d\":\"2.35\"}'", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "001599", "source": "def intersectionRate(s1, s2):\n    x1, y1, x2, y2 = s1\n    s1 = np.array([[x1, y1], [x2,y1], [x2, y2], [x1, y2]])\n    area, _intersection = cv.intersectConvexConvex(s1, np.array(s2))\n    return 2 * area / (cv.contourArea(s1) + cv.contourArea(np.array(s2)))", "target": "def test_serialize_as_any_with_field_serializer_root_model() -> None:\n    schema = core_schema.model_schema(\n        type('Test', (), {}),\n        core_schema.int_schema(\n            serialization=core_schema.plain_serializer_function_ser_schema(\n                lambda model, v: v * 2, is_field_serializer=True\n            )\n        ),\n        root_model=True,\n    )\n    v = SchemaValidator(schema).validate_python(123)\n    cls = type(v)\n    s = SchemaSerializer(schema)\n    cls.__pydantic_serializer__ = s\n    assert s.to_python(v, serialize_as_any=False) == 246\n    assert s.to_python(v, serialize_as_any=True) == 246\n    assert s.to_json(v, serialize_as_any=False) == b'246'\n    assert s.to_json(v, serialize_as_any=True) == b'246'", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "001600", "source": "def _infer_discriminator_values_for_model_choice(\n        self, choice: core_schema.ModelFieldsSchema, source_name: str | None = None\n    ) -> list[str | int]:\n        source = 'ModelFields' if source_name is None else f'Model {source_name!r}'\n        field = choice['fields'].get(self.discriminator)\n        if field is None:\n            raise PydanticUserError(\n                f'{source} needs a discriminator field for key {self.discriminator!r}', code='discriminator-no-field'\n            )\n        return self._infer_discriminator_values_for_field(field, source)", "target": "def test_array_with_custom_type(self):\n            @cv.gapi.op('custom.op', in_types=[cv.GArray.Any, cv.GArray.Any], out_types=[cv.GArray.Any])\n            class GConcat:\n                @staticmethod\n                def outMeta(arr_desc0, arr_desc1):\n                    return cv.empty_array_desc()\n            @cv.gapi.kernel(GConcat)\n            class GConcatImpl:\n                @staticmethod\n                def run(arr0, arr1):\n                    return arr0 + arr1\n            g_arr0 = cv.GArray.Any()\n            g_arr1 = cv.GArray.Any()\n            g_out  = GConcat.on(g_arr0, g_arr1)\n            comp = cv.GComputation(cv.GIn(g_arr0, g_arr1), cv.GOut(g_out))\n            arr0 = ((2, 2), 2.0)\n            arr1 = (3,    'str')\n            out = comp.apply(cv.gin(arr0, arr1),\n                             args=cv.gapi.compile_args(cv.gapi.kernels(GConcatImpl)))\n            self.assertEqual(arr0 + arr1, out)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001601", "source": "def forward(self, x):\n        return torch.mm(x, self.weight)", "target": "def forward(self, x, y):\n        return (x + y,)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001602", "source": "def test_empty_string_field_name(py_and_json: PyAndJson):\n    v = py_and_json({'type': 'model-fields', 'fields': {'': {'type': 'model-field', 'schema': {'type': 'int'}}}})\n    assert v.validate_test({'': 123}) == ({'': 123}, None, {''})", "target": "def test_empty_string_field_name(py_and_json: PyAndJson):\n    v = py_and_json({'type': 'typed-dict', 'fields': {'': {'type': 'typed-dict-field', 'schema': {'type': 'int'}}}})\n    assert v.validate_test({'': 123}) == {'': 123}", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001603", "source": "def ser_x(self, v: Any, _) -> str:\n            assert self.x == 1_000\n            return f'{v:_}'", "target": "def ser_x(self, v: Any, serializer: core_schema.SerializerFunctionWrapHandler, _) -> str:\n            assert self.x == 1_000\n            x = serializer(v)\n            return f'{x:_}'", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "001604", "source": "def test_url_pickle(value):\n    pickled = pickle.dumps(value)\n    unpickled = pickle.loads(pickled)\n    assert value == unpickled", "target": "def foo(x: int, y: int) -> int:\n            return x + y", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "001605", "source": "def test_utcoffset(self):\n        dummy = self.DT\n        for h in [0, 1.5, 12]:\n            offset = h * HOUR\n            self.assertEqual(timedelta(seconds=offset), TzInfo(offset).utcoffset(dummy))\n            self.assertEqual(timedelta(seconds=-offset), TzInfo(-offset).utcoffset(dummy))\n        self.assertEqual(self.EST.utcoffset(''), timedelta(hours=-5))\n        self.assertEqual(self.EST.utcoffset(5), timedelta(hours=-5))", "target": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001606", "source": "def get_output(self, input_blob):\n        tensor = torch.FloatTensor(input_blob)\n        out = self.net.forward(tensor).numpy()\n        return out", "target": "def get_output(self, input_blob):\n        self.net.setInput(input_blob, self.in_blob_name)\n        return self.net.forward(self.out_blob_name)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001607", "source": "def test_default_factory(py_and_json: PyAndJson, input_value, expected) -> None:\n    v = py_and_json(\n        cs.arguments_v3_schema(\n            [\n                cs.arguments_v3_parameter(name='a', schema=cs.int_schema(), mode='positional_or_keyword'),\n                cs.arguments_v3_parameter(\n                    name='b',\n                    schema=cs.with_default_schema(schema=cs.int_schema(), default_factory=lambda: 42),\n                    mode='positional_or_keyword',\n                ),\n            ]\n        )\n    )\n    assert v.validate_test(input_value) == expected", "target": "def test_default_factory(py_and_json: PyAndJson, input_value, expected):\n    v = py_and_json(\n        {\n            'type': 'arguments',\n            'arguments_schema': [\n                {'name': 'a', 'mode': 'positional_or_keyword', 'schema': {'type': 'int'}},\n                {\n                    'name': 'b',\n                    'mode': 'positional_or_keyword',\n                    'schema': {'type': 'default', 'schema': {'type': 'int'}, 'default_factory': lambda: 42},\n                },\n            ],\n        }\n    )\n    assert v.validate_test(input_value) == expected", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pytorch/pytorch", "data_split": "train", "example_id": "001608", "source": "def typename(self) -> str:\n        return self.alias_export_name", "target": "def _run_torchbench_from_args(\n    cmd_args: argparse.Namespace,\n    model: str,\n    args: list[str],\n) -> tuple[list[float], list[float]]:\n    cold_compile_time: list[float] = []\n    warm_compile_time: list[float] = []\n    for _ in range(cmd_args.repeat):\n        with fresh_cache():\n            env = os.environ.copy()\n            with tempfile.NamedTemporaryFile(suffix=\".csv\") as file:\n                args.append(\"--output=\" + file.name)\n                logger.info(f\"Performing cold-start run for {model}\")\n                subprocess.check_call(args, timeout=TIMEOUT, env=env)\n                cold_compile_time.append(get_compile_time(file))\n            args.pop()\n            with tempfile.NamedTemporaryFile(suffix=\".csv\") as file:\n                args.append(\"--output=\" + file.name)\n                logger.info(f\"Performing warm-start run for {model}\")\n                subprocess.check_call(args, timeout=TIMEOUT, env=env)\n                warm_compile_time.append(get_compile_time(file))\n    return cold_compile_time, warm_compile_time", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001609", "source": "def test_model_custom_init():\n    calls = []\n    class Model:\n        def __init__(self, **kwargs):\n            calls.append(repr(kwargs))\n            if 'a' in kwargs:\n                kwargs['a'] *= 2\n            self.__pydantic_validator__.validate_python(kwargs, self_instance=self)\n            self.c = self.a + 2\n    v = SchemaValidator(\n        core_schema.model_schema(\n            Model,\n            core_schema.model_fields_schema(\n                {\n                    'a': core_schema.model_field(core_schema.with_default_schema(core_schema.int_schema(), default=1)),\n                    'b': core_schema.model_field(core_schema.int_schema()),\n                }\n            ),\n            custom_init=True,\n        )\n    )\n    Model.__pydantic_validator__ = v\n    m = v.validate_python({'b': 2})\n    assert m.a == 1\n    assert m.b == 2\n    assert m.c == 3\n    assert m.__pydantic_fields_set__ == {'b'}\n    assert calls == [\"{'b': 2}\"]\n    m2 = v.validate_python({'a': 5, 'b': 3})\n    assert m2.a == 10\n    assert m2.b == 3\n    assert m2.c == 12\n    assert m2.__pydantic_fields_set__ == {'a', 'b'}\n    assert calls == [\"{'b': 2}\", \"{'a': 5, 'b': 3}\"]\n    m3 = v.validate_json('{\"a\":10, \"b\": 4}')\n    assert m3.a == 20\n    assert m3.b == 4\n    assert m3.c == 22\n    assert m3.__pydantic_fields_set__ == {'a', 'b'}\n    assert calls == [\"{'b': 2}\", \"{'a': 5, 'b': 3}\", \"{'a': 10, 'b': 4}\"]", "target": "def test_filter_args(params):\n    s = SchemaSerializer(core_schema.list_schema())\n    include, exclude, expected = params['include'], params['exclude'], params['expected']\n    value = ['0', '1', '2', '3']\n    assert s.to_python(value, include=include, exclude=exclude) == expected\n    assert s.to_python(value, mode='json', include=include, exclude=exclude) == expected\n    assert json.loads(s.to_json(value, include=include, exclude=exclude)) == expected", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "001610", "source": "def liger(self, args, kwargs=None) -> Any:\n        from liger_kernel.transformers.softmax import LigerSoftmax\n        assert kwargs is None\n        x, dy = args\n        softmax = LigerSoftmax().to(\"cuda\")\n        y = softmax(x)\n        return lambda: torch.autograd.grad(y, x, grad_outputs=dy, retain_graph=True)", "target": "def test_alias(py_and_json: PyAndJson, input_value, expected) -> None:\n    v = py_and_json(\n        cs.arguments_v3_schema(\n            [\n                cs.arguments_v3_parameter(name='a', schema=cs.int_schema(), alias='Foo', mode='positional_or_keyword'),\n            ]\n        )\n    )\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            v.validate_test(input_value)\n    else:\n        assert v.validate_test(input_value) == expected", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "001611", "source": "def test_parse_to_float_not_convertible_extra(self):\n        for not_convertible in (np.bool_(False), True, False, np.array([123, ], dtype=int),\n                                np.array([1., ]), np.array([False]),\n                                np.array([True])):\n            with self.assertRaises((TypeError, OverflowError),\n                                   msg=get_no_exception_msg(not_convertible)):\n                _ = cv.utils.dumpFloat(not_convertible)", "target": "def test_negative_json(input_value, expected):\n    v = SchemaValidator(cs.int_schema(lt=0))\n    json_input = json.dumps(input_value)\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            v.validate_json(json_input)\n    else:\n        output = v.validate_json(json_input)\n        assert output == expected\n        assert isinstance(output, int)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001612", "source": "def children_types(self) -> Tuple[ASTNodeType, ...]:\n        return (ASTNodeType.Constant, )", "target": "def children_types(self) -> Tuple[ASTNodeType, ...]:\n        return ()", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "001613", "source": "def process_landmarks(r_x, r_y, r_w, r_h, landmarks):\n    lmrks = landmarks[0]\n    raw_x = lmrks[::2] * r_w + r_x\n    raw_y = lmrks[1::2] * r_h + r_y\n    return np.array([[int(x), int(y)] for x, y in zip(raw_x, raw_y)])", "target": "def type_var_default_factory() -> None:\n                raise RuntimeError(\n                    'Generic defaultdict cannot be used without a concrete value type or an'\n                    ' explicit default factory, ' + instructions\n                )", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "001614", "source": "def test_date():\n    v = SchemaSerializer(core_schema.date_schema())\n    assert v.to_python(date(2022, 12, 2)) == date(2022, 12, 2)\n    assert v.to_python(date(2022, 12, 2), mode='json') == '2022-12-02'\n    assert v.to_json(date(2022, 12, 2)) == b'\"2022-12-02\"'", "target": "def types_separator(self) -> str:\n        if TypeNode.compatible_to_runtime_usage:\n            return \", \"\n        return \" | \"", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001615", "source": "def test_on_error_bad_default(self):\n        with pytest.raises(SchemaError, match=\"'on_error = default' requires a `default` or `default_factory`\"):\n            SchemaValidator(\n                schema=core_schema.model_fields_schema(\n                    fields={\n                        'x': core_schema.model_field(\n                            schema=core_schema.with_default_schema(schema=core_schema.str_schema(), on_error='default')\n                        )\n                    }\n                )\n            )", "target": "def test_on_error_bad_default(self):\n        with pytest.raises(SchemaError, match=\"'on_error = default' requires a `default` or `default_factory`\"):\n            SchemaValidator(\n                schema=core_schema.typed_dict_schema(\n                    fields={\n                        'x': core_schema.typed_dict_field(\n                            schema=core_schema.with_default_schema(schema=core_schema.str_schema(), on_error='default')\n                        )\n                    }\n                )\n            )", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "001616", "source": "def has_unresolved_type_node(item) -> bool:\n            return item.type_node is not None and not item.type_node.is_resolved", "target": "def test_by_alias_and_name_config_interaction(config, runtime, expected) -> None:\n    @dataclasses.dataclass\n    class Foo:\n        my_field: str\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(\n                    name='my_field', schema=core_schema.str_schema(), serialization_alias='my_alias'\n                ),\n            ],\n        ),\n        ['my_field'],\n        config=core_schema.CoreConfig(serialize_by_alias=config or False),\n    )\n    s = SchemaSerializer(schema)\n    assert s.to_python(Foo(my_field='hello'), by_alias=runtime) == expected", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001617", "source": "def predict(self, samples):\n        new_samples = self.unroll_samples(samples)\n        _ret, resp = self.model.predict(new_samples)\n        return resp.ravel().reshape(-1, self.class_n).argmax(1)", "target": "def predict(self, samples):\n        _ret, resp = self.model.predict(samples)\n        return resp.argmax(-1)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001618", "source": "def test_union():\n    v = SchemaValidator(cs.union_schema(choices=[cs.str_schema(strict=True), cs.bytes_schema(strict=True)]))\n    assert v.validate_python('oh, a string') == 'oh, a string'\n    assert v.validate_python(b'oh, bytes') == b'oh, bytes'", "target": "def test_union():\n    v = SchemaValidator(core_schema.union_schema(choices=[core_schema.str_schema(), core_schema.time_schema()]))\n    assert v.validate_python('12:01:02') == '12:01:02'\n    assert v.validate_python(time(12, 1, 2)) == time(12, 1, 2)\n    v = SchemaValidator(core_schema.union_schema(choices=[core_schema.time_schema(), core_schema.str_schema()]))\n    assert v.validate_python('12:01:02') == '12:01:02'\n    assert v.validate_python(time(12, 1, 2)) == time(12, 1, 2)", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "001619", "source": "def prepare_dir(d, clean=False):\n    d = str(d)\n    d = os.path.abspath(d)\n    log.info(\"Preparing directory: '%s' (clean: %r)\", d, clean)\n    if os.path.exists(d):\n        if not os.path.isdir(d):\n            raise Fail(\"Not a directory: %s\" % d)\n        if clean:\n            for item in os.listdir(d):\n                rm_one(os.path.join(d, item))\n    else:\n        os.makedirs(d)\n    return Path(d)", "target": "def test_case_labels():\n    v = SchemaValidator(\n        core_schema.union_schema(\n            choices=[core_schema.none_schema(), ({'type': 'int'}, 'my_label'), core_schema.str_schema()]\n        )\n    )\n    assert v.validate_python(None) is None\n    assert v.validate_python(1) == 1\n    with pytest.raises(ValidationError, match=r'3 validation errors for union\\[none,my_label,str]') as exc_info:\n        v.validate_python(1.5)\n    assert exc_info.value.errors(include_url=False) == [\n        {'input': 1.5, 'loc': ('none',), 'msg': 'Input should be None', 'type': 'none_required'},\n        {\n            'input': 1.5,\n            'loc': ('my_label',),\n            'msg': 'Input should be a valid integer, got a number with a fractional part',\n            'type': 'int_from_float',\n        },\n        {'input': 1.5, 'loc': ('str',), 'msg': 'Input should be a valid string', 'type': 'string_type'},\n    ]", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001620", "source": "def area(self) -> int:\n            return self.width * self.height", "target": "def area(self, area: float) -> None:\n            self.side = area**0.5", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "train", "example_id": "001621", "source": "def local_image_exists(\n    image_name: str, client: Optional[docker.DockerClient] = None\n) -> bool:\n    if not image_name:\n        return False\n    client = client or _get_client()\n    try:\n        client.images.get(image_name)\n        return True\n    except (NotFound, APIError) as e:\n        logger.error(\n            \"Error when checking Docker image '%s': %s\",\n            image_name,\n            e.explanation if hasattr(e, \"explanation\") else str(e),\n        )\n        return False", "target": "def str3Concat(self, input: str) -> str:\n        return input + input + input", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|opencv/opencv", "data_split": "train", "example_id": "001622", "source": "def checkout(self, branch: str) -> None:\n        self._run_git(\"checkout\", branch)", "target": "def relative_typename(self, root: str) -> Optional[str]:\n            if self.type_node is not None:\n                return self.type_node.relative_typename(root)\n            return None", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "001623", "source": "def test_complete_core_json(benchmark):\n    v = SchemaValidator(schema())\n    json_data = json.dumps(input_data_lax(), default=default_json_encoder)\n    benchmark(v.validate_json, json_data)", "target": "def test_ghstack_branches_in_sync(self) -> None:\n        head_ref = \"gh/SS-JIA/206/head\"\n        self._skip_if_ref_does_not_exist(head_ref)\n        self.assertTrue(are_ghstack_branches_in_sync(self.repo, head_ref))", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|scikit-learn/scikit-learn", "data_split": "train", "example_id": "001624", "source": "def test_bounding_rect(self):\n            sz = 1280\n            fscale = 256\n            def sample_value(fscale):\n                return np.random.uniform(0, 255 * fscale) / fscale\n            points = np.array([(sample_value(fscale), sample_value(fscale)) for _ in range(1280)], np.float32)\n            expected = cv.boundingRect(points)\n            g_in  = cv.GMat()\n            g_out = cv.gapi.boundingRect(g_in)\n            comp = cv.GComputation(cv.GIn(g_in), cv.GOut(g_out))\n            for pkg_name, pkg in pkgs:\n                actual = comp.apply(cv.gin(points), args=cv.gapi.compile_args(pkg))\n                self.assertEqual(0.0, cv.norm(expected, actual, cv.NORM_INF),\n                                 'Failed on ' + pkg_name + ' backend')", "target": "def plot_species_distribution(\n    species=(\"bradypus_variegatus_0\", \"microryzomys_minutus_0\"),\n):\n    if len(species) > 2:\n        print(\n            \"Note: when more than two species are provided,\"\n            \" only the first two will be used\"\n        )\n    t0 = time()\n    data = fetch_species_distributions()\n    xgrid, ygrid = construct_grids(data)\n    X, Y = np.meshgrid(xgrid, ygrid[::-1])\n    BV_bunch = create_species_bunch(\n        species[0], data.train, data.test, data.coverages, xgrid, ygrid\n    )\n    MM_bunch = create_species_bunch(\n        species[1], data.train, data.test, data.coverages, xgrid, ygrid\n    )\n    np.random.seed(13)\n    background_points = np.c_[\n        np.random.randint(low=0, high=data.Ny, size=10000),\n        np.random.randint(low=0, high=data.Nx, size=10000),\n    ].T\n    land_reference = data.coverages[6]\n    for i, species in enumerate([BV_bunch, MM_bunch]):\n        print(\"_\" * 80)\n        print(\"Modeling distribution of species '%s'\" % species.name)\n        mean = species.cov_train.mean(axis=0)\n        std = species.cov_train.std(axis=0)\n        train_cover_std = (species.cov_train - mean) / std\n        print(\" - fit OneClassSVM ... \", end=\"\")\n        clf = svm.OneClassSVM(nu=0.1, kernel=\"rbf\", gamma=0.5)\n        clf.fit(train_cover_std)\n        print(\"done.\")\n        plt.subplot(1, 2, i + 1)\n        if basemap:\n            print(\" - plot coastlines using basemap\")\n            m = Basemap(\n                projection=\"cyl\",\n                llcrnrlat=Y.min(),\n                urcrnrlat=Y.max(),\n                llcrnrlon=X.min(),\n                urcrnrlon=X.max(),\n                resolution=\"c\",\n            )\n            m.drawcoastlines()\n            m.drawcountries()\n        else:\n            print(\" - plot coastlines from coverage\")\n            plt.contour(\n                X, Y, land_reference, levels=[-9998], colors=\"k\", linestyles=\"solid\"\n            )\n            plt.xticks([])\n            plt.yticks([])\n        print(\" - predict species distribution\")\n        Z = np.ones((data.Ny, data.Nx), dtype=np.float64)\n        idx = (land_reference > -9999).nonzero()\n        coverages_land = data.coverages[:, idx[0], idx[1]].T\n        pred = clf.decision_function((coverages_land - mean) / std)\n        Z *= pred.min()\n        Z[idx[0], idx[1]] = pred\n        levels = np.linspace(Z.min(), Z.max(), 25)\n        Z[land_reference == -9999] = -9999\n        plt.contourf(X, Y, Z, levels=levels, cmap=plt.cm.Reds)\n        plt.colorbar(format=\"%.2f\")\n        plt.scatter(\n            species.pts_train[\"dd long\"],\n            species.pts_train[\"dd lat\"],\n            s=2**2,\n            c=\"black\",\n            marker=\"^\",\n            label=\"train\",\n        )\n        plt.scatter(\n            species.pts_test[\"dd long\"],\n            species.pts_test[\"dd lat\"],\n            s=2**2,\n            c=\"black\",\n            marker=\"x\",\n            label=\"test\",\n        )\n        plt.legend()\n        plt.title(species.name)\n        plt.axis(\"equal\")\n        pred_background = Z[background_points[0], background_points[1]]\n        pred_test = clf.decision_function((species.cov_test - mean) / std)\n        scores = np.r_[pred_test, pred_background]\n        y = np.r_[np.ones(pred_test.shape), np.zeros(pred_background.shape)]\n        fpr, tpr, thresholds = metrics.roc_curve(y, scores)\n        roc_auc = metrics.auc(fpr, tpr)\n        plt.text(-35, -70, \"AUC: %.3f\" % roc_auc, ha=\"right\")\n        print(\"\\n Area under the ROC curve : %f\" % roc_auc)\n    print(\"\\ntime elapsed: %.2fs\" % (time() - t0))", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "001625", "source": "def test_force_create_dir_clears_existing(self):\n        d = self.tmp_path / \"fresh\"\n        (d / \"inner\").mkdir(parents=True)\n        (d / \"inner\" / \"f.txt\").write_text(\"x\")\n        force_create_dir(d)\n        self.assertTrue(d.exists())\n        self.assertEqual(list(d.iterdir()), [])", "target": "def test_exact_check(self, schema_validator: SchemaValidator):\n        m_b = schema_validator.validate_python({'c': 2, 'd': 'again'})\n        assert isinstance(m_b, self.ModelB)\n        m_b2 = schema_validator.validate_python(m_b)\n        assert m_b2 is m_b", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001626", "source": "def test_dict(py_and_json: PyAndJson):\n    v = py_and_json({'type': 'dict', 'keys_schema': {'type': 'date'}, 'values_schema': {'type': 'int'}})\n    assert v.validate_test({'2000-01-01': 2, '2000-01-02': 4}) == {date(2000, 1, 1): 2, date(2000, 1, 2): 4}", "target": "def test_dict(py_and_json: PyAndJson):\n    v = py_and_json({'type': 'dict', 'keys_schema': {'type': 'int'}, 'values_schema': {'type': 'int'}})\n    assert v.validate_test({'1': 2, '3': 4}) == {1: 2, 3: 4}\n    v = py_and_json({'type': 'dict', 'strict': True, 'keys_schema': {'type': 'int'}, 'values_schema': {'type': 'int'}})\n    assert v.validate_test({'1': 2, '3': 4}) == {1: 2, 3: 4}\n    assert v.validate_test({}) == {}\n    with pytest.raises(ValidationError, match=re.escape('[type=dict_type, input_value=[], input_type=list]')):\n        v.validate_test([])", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001627", "source": "def func(x, info):\n        calls.append(str(info))\n        return x * 2", "target": "def func(*args: Any) -> Any:\n        calls.append(args)\n        return args[0]", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "001628", "source": "def value(self) -> Any:\n            return self.get_value()", "target": "def main():\n    result_path = sys.argv[1]\n    all = [\n        Benchmark(training=False, subclass=False),\n        Benchmark(training=True, subclass=False),\n        Benchmark(training=False, subclass=True),\n        Benchmark(training=True, subclass=True),\n    ]\n    for benchmark in all:\n        benchmark.enable_compile_time_instruction_count().collect_all().append_results(\n            result_path\n        )", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "001629", "source": "def regurgitate(depth, use_pyyaml_formatter=False):\n    data = yaml.safe_load(sys.stdin)\n    if use_pyyaml_formatter:\n        output = yaml.dump(data, sort_keys=True)\n        sys.stdout.write(output)\n    else:\n        miniyaml.render(sys.stdout, data, depth)", "target": "def test_function_no_info():\n    def f(input_value):\n        return input_value + ' Changed'\n    v = SchemaValidator(core_schema.no_info_after_validator_function(f, core_schema.str_schema()))\n    assert v.validate_python('input value') == 'input value Changed'", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001630", "source": "def relative_typename(self, root: str) -> Optional[str]:\n            return self.type_node.relative_typename(root)", "target": "def relative_typename(self, full_node_name: str) -> str:\n        return self.type_node.relative_typename(full_node_name)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "001631", "source": "def test_simple(self) -> None:\n        @retries_decorator()\n        def foo(x: int, y: int) -> int:\n            return x + y\n        self.assertEqual(foo(3, 4), 7)", "target": "def import_cached_field_info() -> type['FieldInfo']:\n    from pydantic.fields import FieldInfo\n    return FieldInfo", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001632", "source": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "target": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|opencv/opencv", "data_split": "train", "example_id": "001633", "source": "def test_noop_when_empty_path(self):\n        start = Path.cwd()\n        with working_directory(\"\"):\n            self.assertEqual(Path.cwd(), start)\n        self.assertEqual(Path.cwd(), start)", "target": "def cvt_yuv_to_nv12(self, yuv, y_out, uv_out):\n            chs = cv.split(yuv, [y_out, None, None])\n            uv = cv.merge([chs[1], chs[2]])\n            uv_out = cv.resize(uv, (uv.shape[0] // 2, uv.shape[1] // 2), dst=uv_out)\n            return y_out, uv_out", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001634", "source": "def _norm(self, x):\n        return x * torch.rsqrt(torch.mean(x * x, dim=-1, keepdim=True) + self.eps)", "target": "def _norm(self, x):\n        return x * torch.rsqrt(torch.mean(x * x, dim=-1, keepdim=True) + self.eps)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001635", "source": "def parse_args() -> Any:\n    from argparse import ArgumentParser\n    parser = ArgumentParser(\"Check PR labels\")\n    parser.add_argument(\"pr_num\", type=int)\n    parser.add_argument(\n        \"--exit-non-zero\",\n        action=\"store_true\",\n        help=\"Return a non-zero exit code if the PR does not have the required labels\",\n    )\n    return parser.parse_args()", "target": "def parse_args() -> Any:\n    from argparse import ArgumentParser\n    parser = ArgumentParser(\"Export PR labels\")\n    parser.add_argument(\"org\", type=str)\n    parser.add_argument(\"repo\", type=str)\n    return parser.parse_args()", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001636", "source": "def test_wrap_on_error(self, py_and_json: PyAndJson):\n        def wrap_function(input_value, validator, info):\n            try:\n                return validator(input_value)\n            except ValidationError:\n                if isinstance(input_value, list):\n                    return str(len(input_value))\n                else:\n                    return repr(input_value)\n        v = py_and_json(\n            {\n                'type': 'model-fields',\n                'fields': {\n                    'x': {\n                        'type': 'model-field',\n                        'schema': {\n                            'type': 'default',\n                            'on_error': 'raise',\n                            'schema': {\n                                'type': 'function-wrap',\n                                'function': {'type': 'with-info', 'function': wrap_function},\n                                'schema': {'type': 'str'},\n                            },\n                        },\n                    }\n                },\n            }\n        )\n        assert v.validate_test({'x': 'foo'}) == ({'x': 'foo'}, None, {'x'})\n        assert v.validate_test({'x': ['foo']}) == ({'x': '1'}, None, {'x'})\n        assert v.validate_test({'x': ['foo', 'bar']}) == ({'x': '2'}, None, {'x'})\n        assert v.validate_test({'x': {'a': 'b'}}) == ({'x': \"{'a': 'b'}\"}, None, {'x'})", "target": "def test_wrap_on_error(self, py_and_json: PyAndJson):\n        def wrap_function(input_value, validator, info):\n            try:\n                return validator(input_value)\n            except ValidationError:\n                if isinstance(input_value, list):\n                    return str(len(input_value))\n                else:\n                    return repr(input_value)\n        v = py_and_json(\n            {\n                'type': 'typed-dict',\n                'fields': {\n                    'x': {\n                        'type': 'typed-dict-field',\n                        'schema': {\n                            'type': 'default',\n                            'on_error': 'raise',\n                            'schema': {\n                                'type': 'function-wrap',\n                                'function': {'type': 'with-info', 'function': wrap_function},\n                                'schema': {'type': 'str'},\n                            },\n                        },\n                    }\n                },\n            }\n        )\n        assert v.validate_test({'x': 'foo'}) == {'x': 'foo'}\n        assert v.validate_test({'x': ['foo']}) == {'x': '1'}\n        assert v.validate_test({'x': ['foo', 'bar']}) == {'x': '2'}\n        assert v.validate_test({'x': {'a': 'b'}}) == {'x': \"{'a': 'b'}\"}", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001637", "source": "def test_dataclass_serialization_json(benchmark):\n    s = SchemaSerializer(dataclass_schema)\n    dc = Foo(a='hello', b=b'more', c=123, d=1.23)\n    assert s.to_python(dc) == {'a': 'hello', 'b': b'more', 'c': 123, 'd': 1.23}\n    benchmark(s.to_json, dc)", "target": "def test_from_attributes():\n    v = SchemaValidator(\n        core_schema.tagged_union_schema(\n            discriminator='foobar',\n            choices={\n                'apple': core_schema.model_fields_schema(\n                    fields={\n                        'a': core_schema.model_field(schema=core_schema.str_schema()),\n                        'b': core_schema.model_field(schema=core_schema.int_schema()),\n                    }\n                ),\n                'banana': core_schema.model_fields_schema(\n                    fields={\n                        'c': core_schema.model_field(schema=core_schema.str_schema()),\n                        'd': core_schema.model_field(schema=core_schema.int_schema()),\n                    }\n                ),\n            },\n        ),\n        config=CoreConfig(from_attributes=True),\n    )\n    assert v.validate_python({'foobar': 'apple', 'a': 'apple', 'b': '13'}) == (\n        {'a': 'apple', 'b': 13},\n        None,\n        {'a', 'b'},\n    )\n    assert v.validate_python(Cls(foobar='apple', a='apple', b='13')) == ({'a': 'apple', 'b': 13}, None, {'a', 'b'})\n    assert v.validate_python({'foobar': 'banana', 'c': 'banana', 'd': '31'}) == (\n        {'c': 'banana', 'd': 31},\n        None,\n        {'c', 'd'},\n    )\n    assert v.validate_python(Cls(foobar='banana', c='banana', d='31')) == ({'c': 'banana', 'd': 31}, None, {'c', 'd'})", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001638", "source": "def required_definition_imports(self) -> Generator[str, None, None]:\n        yield from self.positive_branch_type.required_usage_imports\n        yield from self.negative_branch_type.required_usage_imports\n        yield from self._condition_required_imports", "target": "def required_definition_imports(self) -> Generator[str, None, None]:\n        yield \"import typing as _typing\"\n        yield from super().required_definition_imports", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|opencv/opencv", "data_split": "train", "example_id": "001639", "source": "def test_20968(self):\n        pixel = np.uint8([[[40, 50, 200]]])\n        _ = cv.cvtColor(pixel, cv.COLOR_RGB2BGR)", "target": "def weight(self) -> int:\n        return 1 + sum(base.weight for base in self.bases)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001640", "source": "def _prepare_once(self):\n        self.x = torch.randn(4, 4, requires_grad=True)", "target": "def _prepare_once(self):\n        self.m = self.ModuleClass()\n        torch.set_float32_matmul_precision(\"high\")\n        self.input = torch.ones(10, device=self.device())", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001641", "source": "def is_resolved(self) -> bool:\n        return self.value.is_resolved", "target": "def is_resolved(self) -> bool:\n        return all(item.is_resolved for item in self.items)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "001642", "source": "def ser_x(self, v: Any, _) -> str:\n            assert self.x == 1_000\n            return f'{v:_}'", "target": "def relative_typename(self, module: str) -> str:\n        assert self._ast_node is not None or self._module_name is not None, \\\n            \"'{}' exported as '{}' is not resolved yet\".format(self.ctype_name,\n                                                               self.typename)\n        if self._module_name is None:\n            type_module = self._ast_node.parent\n            while type_module.node_type is not ASTNodeType.Namespace:\n                type_module = type_module.parent\n            module_name = type_module.full_export_name\n        else:\n            module_name = self._module_name\n        if module_name != module:\n            return self.full_typename\n        return self.full_typename[len(module_name) + 1:]", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001643", "source": "def test_exclude_default():\n    v = SchemaSerializer(\n        core_schema.typed_dict_schema(\n            {\n                'foo': core_schema.typed_dict_field(core_schema.nullable_schema(core_schema.int_schema())),\n                'bar': core_schema.typed_dict_field(\n                    core_schema.with_default_schema(core_schema.bytes_schema(), default=b'[default]')\n                ),\n            }\n        )\n    )\n    assert v.to_python({'foo': 1, 'bar': b'x'}) == {'foo': 1, 'bar': b'x'}\n    assert v.to_python({'foo': 1, 'bar': b'[default]'}) == {'foo': 1, 'bar': b'[default]'}\n    assert v.to_python({'foo': 1, 'bar': b'[default]'}, exclude_defaults=True) == {'foo': 1}\n    assert v.to_python({'foo': 1, 'bar': b'[default]'}, mode='json') == {'foo': 1, 'bar': '[default]'}\n    assert v.to_python({'foo': 1, 'bar': b'[default]'}, exclude_defaults=True, mode='json') == {'foo': 1}\n    assert v.to_json({'foo': 1, 'bar': b'[default]'}) == b'{\"foo\":1,\"bar\":\"[default]\"}'\n    assert v.to_json({'foo': 1, 'bar': b'[default]'}, exclude_defaults=True) == b'{\"foo\":1}'", "target": "def test_validate_scientific_notation_from_json(input_value, expected):\n    v = SchemaValidator(cs.float_schema())\n    assert v.validate_json(input_value) == expected", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001644", "source": "def is_resolved(self) -> bool:\n        return True", "target": "def is_resolved(self) -> bool:\n        return self.positive_branch_type.is_resolved \\\n                and self.negative_branch_type.is_resolved", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001645", "source": "def test_dict():\n    v = SchemaValidator(\n        core_schema.dict_schema(keys_schema=core_schema.int_schema(), values_schema=core_schema.int_schema())\n    )\n    assert v.validate_json('{\"1\": 2, \"3\": 4}') == {1: 2, 3: 4}\n    assert json.loads('{\"1\": 1, \"1\": 2}') == {'1': 2}\n    assert v.validate_json('{\"1\": 1, \"1\": 2}') == {1: 2}", "target": "def test_dict(py_and_json: PyAndJson):\n    v = py_and_json({'type': 'dict', 'keys_schema': {'type': 'time'}, 'values_schema': {'type': 'int'}})\n    assert v.validate_test({'12:01:01': 2, '12:01:02': 4}) == {time(12, 1, 1): 2, time(12, 1, 2): 4}", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001646", "source": "def huge_graph(x):\n    for _ in range(N):\n        x = x.sin()\n    return x", "target": "def huge_graph():\n    def fn(x):\n        for _ in range(N):\n            x = x.sin()\n        return x\n    return torch.fx.symbolic_trace(fn)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "001647", "source": "def test_bool():\n    v = SchemaValidator(core_schema.bool_schema())\n    assert v.validate_strings('true') is True\n    assert v.validate_strings('true', strict=True) is True\n    assert v.validate_strings('false') is False", "target": "def test_mat_construct_4d(self):\n            data = np.random.random([5, 10, 10, 3])\n            mat_data0 = cv.Mat(data)\n            assert isinstance(mat_data0, cv.Mat)\n            assert isinstance(mat_data0, np.ndarray)\n            self.assertEqual(mat_data0.wrap_channels, False)\n            res0 = cv.utils.dumpInputArray(mat_data0)\n            self.assertEqual(res0, \"InputArray: empty()=false kind=0x00010000 flags=0x01010000 total(-1)=1500 dims(-1)=4 size(-1)=[5 10 10 3] type(-1)=CV_64FC1\")\n            mat_data1 = cv.Mat(data, wrap_channels=True)\n            assert isinstance(mat_data1, cv.Mat)\n            assert isinstance(mat_data1, np.ndarray)\n            self.assertEqual(mat_data1.wrap_channels, True)\n            res1 = cv.utils.dumpInputArray(mat_data1)\n            self.assertEqual(res1, \"InputArray: empty()=false kind=0x00010000 flags=0x01010000 total(-1)=500 dims(-1)=3 size(-1)=[5 10 10] type(-1)=CV_64FC3\")\n            mat_data2 = cv.Mat(mat_data1)\n            assert isinstance(mat_data2, cv.Mat)\n            assert isinstance(mat_data2, np.ndarray)\n            self.assertEqual(mat_data2.wrap_channels, True)\n            res2 = cv.utils.dumpInputArray(mat_data2)\n            self.assertEqual(res2, \"InputArray: empty()=false kind=0x00010000 flags=0x01010000 total(-1)=500 dims(-1)=3 size(-1)=[5 10 10] type(-1)=CV_64FC3\")", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "001648", "source": "def compute_bench(samples_range, features_range, n_iter=3, rank=50):\n    it = 0\n    results = defaultdict(lambda: [])\n    max_it = len(samples_range) * len(features_range)\n    for n_samples in samples_range:\n        for n_features in features_range:\n            it += 1\n            print(\"====================\")\n            print(\"Iteration %03d of %03d\" % (it, max_it))\n            print(\"====================\")\n            X = make_low_rank_matrix(\n                n_samples, n_features, effective_rank=rank, tail_strength=0.2\n            )\n            gc.collect()\n            print(\"benchmarking scipy svd: \")\n            tstart = time()\n            svd(X, full_matrices=False)\n            results[\"scipy svd\"].append(time() - tstart)\n            gc.collect()\n            print(\"benchmarking scikit-learn randomized_svd: n_iter=0\")\n            tstart = time()\n            randomized_svd(X, rank, n_iter=0)\n            results[\"scikit-learn randomized_svd (n_iter=0)\"].append(time() - tstart)\n            gc.collect()\n            print(\"benchmarking scikit-learn randomized_svd: n_iter=%d \" % n_iter)\n            tstart = time()\n            randomized_svd(X, rank, n_iter=n_iter)\n            results[\"scikit-learn randomized_svd (n_iter=%d)\" % n_iter].append(\n                time() - tstart\n            )\n    return results", "target": "def compute_bench(samples_range, features_range):\n    it = 0\n    results = defaultdict(lambda: [])\n    max_it = len(samples_range) * len(features_range)\n    for n_samples in samples_range:\n        for n_features in features_range:\n            it += 1\n            print(\"==============================\")\n            print(\"Iteration %03d of %03d\" % (it, max_it))\n            print(\"n_samples %05d; n_features %02d\" % (n_samples, n_features))\n            print(\"==============================\")\n            print()\n            data = nr.randint(-50, 51, (n_samples, n_features))\n            for linkage in (\"single\", \"average\", \"complete\", \"ward\"):\n                print(linkage.capitalize())\n                tstart = time()\n                AgglomerativeClustering(linkage=linkage, n_clusters=10).fit(data)\n                delta = time() - tstart\n                print(\"Speed: %0.3fs\" % delta)\n                print()\n                results[linkage].append(delta)\n    return results", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001649", "source": "def types_separator(self) -> str:\n        return \", \"", "target": "def types_separator(self) -> str:\n        return \", \"", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001650", "source": "def dynamic_rnn(\n        sequences: list[Tensor],\n        hiddens: tuple[Tensor, Tensor],\n        w_ih: Tensor,\n        w_hh: Tensor,\n        b_ih: Tensor,\n        b_hh: Tensor,\n    ) -> tuple[list[Tensor], tuple[list[Tensor], list[Tensor]]]:\n        hx, cx = hiddens\n        hxs = hx.unbind(1)\n        cxs = cx.unbind(1)\n        outputs = []\n        hx_outs = []\n        cx_outs = []\n        for batch in range(len(sequences)):\n            output = []\n            hy, cy = hxs[batch], cxs[batch]\n            inputs = sequences[batch].unbind(0)\n            for seq_idx in range(len(inputs)):\n                hy, cy = cell(\n                    inputs[seq_idx].unsqueeze(0), (hy, cy), w_ih, w_hh, b_ih, b_hh\n                )\n                output += [hy]\n            outputs += [torch.stack(output)]\n            hx_outs += [hy.unsqueeze(0)]\n            cx_outs += [cy.unsqueeze(0)]\n        return outputs, (hx_outs, cx_outs)", "target": "def dynamic_rnn(\n        input: Tensor,\n        hidden: tuple[Tensor, Tensor],\n        w_ih: Tensor,\n        w_hh: Tensor,\n        b_ih: Tensor,\n        b_hh: Tensor,\n    ) -> tuple[Tensor, tuple[Tensor, Tensor]]:\n        hx, cx = hidden\n        outputs = []\n        inpSize = input.size()\n        inpSize = input.size()\n        inputs = torch.mm(input.view(-1, inpSize[2]), w_ih.t()) + b_ih\n        inputs = inputs.view(inpSize[0], inpSize[1], -1).unbind(0)\n        hy, cy = hx[0], cx[0]\n        for seq_idx in range(len(inputs)):\n            hy, cy = premul_cell(inputs[seq_idx], (hy, cy), w_hh, b_hh)\n            outputs += [hy]\n        return torch.stack(outputs), (hy.unsqueeze(0), cy.unsqueeze(0))", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "001651", "source": "def test_slots_mixed():\n    @dataclasses.dataclass(slots=True)\n    class Model:\n        x: int\n        y: dataclasses.InitVar[str]\n        z: ClassVar[str] = 'z-classvar'\n    @dataclasses.dataclass\n    class SubModel(Model):\n        x2: int\n        y2: dataclasses.InitVar[str]\n        z2: ClassVar[str] = 'z2-classvar'\n    schema = core_schema.dataclass_schema(\n        SubModel,\n        core_schema.dataclass_args_schema(\n            'SubModel',\n            [\n                core_schema.dataclass_field(name='x', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(name='y', init_only=True, schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='x2', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(name='y2', init_only=True, schema=core_schema.str_schema()),\n            ],\n        ),\n        ['x'],\n        slots=True,\n    )\n    v = SchemaValidator(schema)\n    dc = v.validate_python({'x': 1, 'y': 'a', 'x2': 2, 'y2': 'b'})\n    assert dc.x == 1\n    assert dc.x2 == 2\n    assert dataclasses.asdict(dc) == {'x': 1, 'x2': 2}", "target": "def forward(*new_params: Tensor) -> Tensor:\n        load_weights(model, names, new_params)\n        out = model(inputs)\n        loss = criterion(out, labels)\n        weight_dict = criterion.weight_dict\n        final_loss = cast(\n            Tensor,\n            sum(loss[k] * weight_dict[k] for k in loss.keys() if k in weight_dict),\n        )\n        return final_loss", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|scikit-learn/scikit-learn", "data_split": "train", "example_id": "001652", "source": "def gen(error: bool):\n        yield 1\n        yield 2\n        if error:\n            raise RuntimeError('my error')\n        yield 3", "target": "def min_arg(col):\n    col_split = pl.col(col).str.split(\" \")\n    return pl.arg_sort_by(\n        col_split.list.get(0).cast(pl.Float64),\n        col_split.list.get(2).cast(pl.Float64),\n    ).first()", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001653", "source": "def get_shapes(self) -> tuple[tuple[int, ...], ...]:\n        return (\n            (32768, 256),\n            (32768, 512),\n            (32768, 1024),\n            (32768, 2048),\n            (32768, 4096),\n            (32768, 8192),\n            (32768, 16384),\n            (32768, 32768),\n            (32768, 65536),\n            (16384, 131072),\n            (8192, 262144),\n        )", "target": "def get_shapes(self) -> tuple[tuple[int, ...], ...]:\n        return (\n            (32768, 256),\n            (32768, 512),\n            (32768, 1024),\n            (32768, 2048),\n            (32768, 4096),\n            (32768, 8192),\n            (32768, 16384),\n            (32768, 32768),\n            (32768, 65536),\n        ) + extra_shapes_for_norm", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001654", "source": "def permute_choices(choices: list[core_schema.CoreSchema]) -> list[list[core_schema.CoreSchema]]:\n    return [list(p) for p in permutations(choices)]", "target": "def area(self) -> bytes:\n            a = self.width * self.height\n            return b'%d' % a", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "train", "example_id": "001655", "source": "def main():\n    duration = int(sys.argv[1])\n    scribe.open_source_signpost(\n        subsystem=\"pr_time_benchmarks\",\n        name=\"duration\",\n        parameters=json.dumps(duration),\n    )", "target": "def test_dataclass_fields_read_env_at_instantiation(self):\n        @dataclass\n        class Cfg:\n            flag: bool = m.env_bool_field(\"FLAG\", default=False)\n            out: Path = m.env_path_field(\"OUT\", default=\"ab\", resolve=True)\n            name: str = m.env_str_field(\"NAME\", default=\"anon\")\n        with patch.dict(\n            os.environ, {\"FLAG\": \"true\", \"OUT\": \"outdir\", \"NAME\": \"alice\"}, clear=True\n        ):\n            cfg1 = Cfg()\n            self.assertTrue(cfg1.flag)\n            self.assertIsInstance(cfg1.out, Path)\n            self.assertTrue(cfg1.out.is_absolute())\n            self.assertEqual(cfg1.name, \"alice\")\n            cfg1.name = \"bob\"\n            self.assertEqual(cfg1.name, \"bob\")\n        with patch.dict(os.environ, {\"FLAG\": \"false\", \"NAME\": \"\"}, clear=True):\n            cfg2 = Cfg()\n            self.assertFalse(cfg2.flag)\n            self.assertTrue(\"ab\" in str(cfg2.out))\n            self.assertIsInstance(cfg2.out, Path)\n            self.assertTrue(cfg2.out.is_absolute())\n            self.assertEqual(cfg2.name, \"anon\")", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "001656", "source": "def get_arch_name() -> str:\n    if torch.cuda.is_available():\n        return torch.cuda.get_device_name()\n    else:\n        return platform.machine()", "target": "def test_extra_custom_serializer():\n    @dataclasses.dataclass\n    class Model:\n        pass\n    schema = core_schema.dataclass_schema(\n        Model,\n        core_schema.dataclass_args_schema(\n            'Model',\n            [],\n            extra_behavior='allow',\n        ),\n        [],\n    )\n    s = SchemaSerializer(schema)\n    v = SchemaValidator(schema)\n    m = v.validate_python({'extra': 'extra'})\n    assert s.to_python(m) == {'extra': 'extra bam!'}", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001657", "source": "def area(self) -> bytes:\n            a = self.width * self.height\n            return b'%d' % a", "target": "def area(self) -> int:\n            return self.width * self.height", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001658", "source": "def required_definition_imports(self) -> Generator[str, None, None]:\n        return self.value.required_usage_imports", "target": "def required_definition_imports(self) -> Generator[str, None, None]:\n        yield \"import typing as _typing\"\n        yield from super().required_definition_imports", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001659", "source": "def liger(self, args, kwargs=None) -> Any:\n        from liger_kernel.transformers.softmax import LigerSoftmax\n        assert kwargs is None\n        (x,) = args\n        softmax = LigerSoftmax().to(\"cuda\")\n        return lambda: softmax(x)", "target": "def liger(self, args, kwargs=None) -> Any:\n        from liger_kernel.transformers.softmax import LigerSoftmax\n        assert kwargs is None\n        x, dy = args\n        softmax = LigerSoftmax().to(\"cuda\")\n        y = softmax(x)\n        return lambda: torch.autograd.grad(y, x, grad_outputs=dy, retain_graph=True)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|opencv/opencv", "data_split": "train", "example_id": "001660", "source": "def get_transformer(device: torch.device) -> GetterReturnType:\n    N = 64\n    seq_length = 128\n    ntoken = 50\n    model = models.TransformerModel(\n        ntoken=ntoken, ninp=720, nhead=12, nhid=2048, nlayers=2\n    )\n    model.to(device)\n    if has_functorch:\n        model.eval()\n    criterion = nn.NLLLoss()\n    params, names = extract_weights(model)\n    data = torch.rand(N, seq_length + 1, device=device).mul(ntoken).long()\n    inputs = data.narrow(1, 0, seq_length)\n    targets = data.narrow(1, 1, seq_length)\n    def forward(*new_params: Tensor) -> Tensor:\n        load_weights(model, names, new_params)\n        out = model(inputs)\n        loss = criterion(\n            out.reshape(N * seq_length, ntoken), targets.reshape(N * seq_length)\n        )\n        return loss\n    return forward, params", "target": "def refine_cuda_module(root: NamespaceNode) -> None:\n    def fix_cudaoptflow_enums_names() -> None:\n        for class_name in (\"NvidiaOpticalFlow_1_0\", \"NvidiaOpticalFlow_2_0\"):\n            if class_name not in cuda_root.classes:\n                continue\n            opt_flow_class = cuda_root.classes[class_name]\n            _trim_class_name_from_argument_types(\n                for_each_function_overload(opt_flow_class), class_name\n            )\n    def fix_namespace_usage_scope(cuda_ns: NamespaceNode) -> None:\n        USED_TYPES = (\"GpuMat\", \"Stream\")\n        def fix_type_usage(type_node: TypeNode) -> None:\n            if isinstance(type_node, AggregatedTypeNode):\n                for item in type_node.items:\n                    fix_type_usage(item)\n            if isinstance(type_node, ASTNodeTypeNode):\n                if type_node._typename in USED_TYPES:\n                    type_node._typename = f\"cuda_{type_node._typename}\"\n        for overload in for_each_function_overload(cuda_ns):\n            if overload.return_type is not None:\n                fix_type_usage(overload.return_type.type_node)\n            for type_node in [arg.type_node for arg in overload.arguments\n                              if arg.type_node is not None]:\n                fix_type_usage(type_node)\n    if \"cuda\" not in root.namespaces:\n        return\n    cuda_root = root.namespaces[\"cuda\"]\n    fix_cudaoptflow_enums_names()\n    for ns in [ns for ns_name, ns in root.namespaces.items()\n               if ns_name.startswith(\"cuda\")]:\n        fix_namespace_usage_scope(ns)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "001661", "source": "def test_force_create_dir_clears_existing(self):\n        d = self.tmp_path / \"fresh\"\n        (d / \"inner\").mkdir(parents=True)\n        (d / \"inner\" / \"f.txt\").write_text(\"x\")\n        force_create_dir(d)\n        self.assertTrue(d.exists())\n        self.assertEqual(list(d.iterdir()), [])", "target": "def test_json_error():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'field_a': core_schema.model_field(\n                    schema=core_schema.list_schema(items_schema=core_schema.int_schema())\n                )\n            }\n        )\n    )\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_json('{\"field_a\": [123, \"wrong\"]}')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('field_a', 1),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'wrong',\n        }\n    ]", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001662", "source": "def model_serializer(f: _ModelPlainSerializerT, /) -> _ModelPlainSerializerT: ...", "target": "def model_serializer(\n    *,\n    mode: Literal['plain'] = ...,\n    when_used: WhenUsed = 'always',\n    return_type: Any = ...,\n) -> Callable[[_ModelPlainSerializerT], _ModelPlainSerializerT]: ...", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001663", "source": "def predict(self, samples):\n        _retval, results, _neigh_resp, _dists = self.model.findNearest(samples, k = 10)\n        return results.ravel()", "target": "def predict(self, samples):\n        new_samples = self.unroll_samples(samples)\n        _ret, resp = self.model.predict(new_samples)\n        return resp.ravel().reshape(-1, self.class_n).argmax(1)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001664", "source": "def to_options(self) -> dict[str, Any]:\n        return {\n            \"max_autotune\": self.max_autotune,\n            \"coordinate_descent_tuning\": self.coordinate_descent_tuning,\n            \"max_autotune_gemm_backends\": self.max_autotune_gemm_backends,\n        }", "target": "def to_options(self) -> dict[str, Any]:\n        return {\n            **super().to_options(),\n            \"triton.enable_persistent_tma_matmul\": self.enable_persistent_tma_matmul,\n        }", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "001665", "source": "def compute_bench(alpha, n_samples, n_features, precompute):\n    lasso_results = []\n    lars_lasso_results = []\n    it = 0\n    for ns in n_samples:\n        for nf in n_features:\n            it += 1\n            print(\"==================\")\n            print(\"Iteration %s of %s\" % (it, max(len(n_samples), len(n_features))))\n            print(\"==================\")\n            n_informative = nf // 10\n            X, Y, coef_ = make_regression(\n                n_samples=ns,\n                n_features=nf,\n                n_informative=n_informative,\n                noise=0.1,\n                coef=True,\n            )\n            X /= np.sqrt(np.sum(X**2, axis=0))\n            gc.collect()\n            print(\"- benchmarking Lasso\")\n            clf = Lasso(alpha=alpha, fit_intercept=False, precompute=precompute)\n            tstart = time()\n            clf.fit(X, Y)\n            lasso_results.append(time() - tstart)\n            gc.collect()\n            print(\"- benchmarking LassoLars\")\n            clf = LassoLars(alpha=alpha, fit_intercept=False, precompute=precompute)\n            tstart = time()\n            clf.fit(X, Y)\n            lars_lasso_results.append(time() - tstart)\n    return lasso_results, lars_lasso_results", "target": "def compute_bench(samples_range, features_range):\n    it = 0\n    results = defaultdict(lambda: [])\n    max_it = len(samples_range) * len(features_range)\n    for n_samples in samples_range:\n        for n_features in features_range:\n            it += 1\n            print(\"==============================\")\n            print(\"Iteration %03d of %03d\" % (it, max_it))\n            print(\"n_samples %05d; n_features %02d\" % (n_samples, n_features))\n            print(\"==============================\")\n            print()\n            data = nr.randint(-50, 51, (n_samples, n_features))\n            for linkage in (\"single\", \"average\", \"complete\", \"ward\"):\n                print(linkage.capitalize())\n                tstart = time()\n                AgglomerativeClustering(linkage=linkage, n_clusters=10).fit(data)\n                delta = time() - tstart\n                print(\"Speed: %0.3fs\" % delta)\n                print()\n                results[linkage].append(delta)\n    return results", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001666", "source": "def ser_x(data: Model, v: Any, serializer: core_schema.SerializerFunctionWrapHandler, _) -> str:\n        x = serializer(v)\n        assert data['x'] == 1_000\n        return f'{x:_}'", "target": "def ser_x(data: Model, v: Any, info: core_schema.FieldSerializationInfo) -> str:\n        assert data['x'] == 1_000\n        return f'{v:_}-{info.field_name}'", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|scikit-learn/scikit-learn", "data_split": "train", "example_id": "001667", "source": "def get_transformer(device: torch.device) -> GetterReturnType:\n    N = 64\n    seq_length = 128\n    ntoken = 50\n    model = models.TransformerModel(\n        ntoken=ntoken, ninp=720, nhead=12, nhid=2048, nlayers=2\n    )\n    model.to(device)\n    if has_functorch:\n        model.eval()\n    criterion = nn.NLLLoss()\n    params, names = extract_weights(model)\n    data = torch.rand(N, seq_length + 1, device=device).mul(ntoken).long()\n    inputs = data.narrow(1, 0, seq_length)\n    targets = data.narrow(1, 1, seq_length)\n    def forward(*new_params: Tensor) -> Tensor:\n        load_weights(model, names, new_params)\n        out = model(inputs)\n        loss = criterion(\n            out.reshape(N * seq_length, ntoken), targets.reshape(N * seq_length)\n        )\n        return loss\n    return forward, params", "target": "def predict(self, X):\n        check_is_fitted(self)\n        predictions = [self.classes_[0]] * len(X)\n        return predictions", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "001668", "source": "def test_bytes():\n    s = SchemaValidator(core_schema.bytes_schema())\n    assert s.validate_json('\"foobar\"') == b'foobar'\n    with pytest.raises(ValidationError, match=r'Input should be a valid bytes \\[type=bytes_type,'):\n        s.validate_json('false')\n    with pytest.raises(ValidationError, match=r'Input should be a valid bytes \\[type=bytes_type,'):\n        s.validate_json('123')", "target": "def _run_torchbench_model(\n    cmd_args: argparse.Namespace,\n    results: list[RunResult],\n    model: str,\n) -> None:\n    cur_file = os.path.abspath(__file__)\n    torchbench_file = os.path.join(\n        os.path.dirname(cur_file), BENCHMARK_FILE[cmd_args.benchmark]\n    )\n    assert os.path.exists(torchbench_file), (\n        f\"Torchbench does not exist at {torchbench_file}\"\n    )\n    dynamic = cmd_args.dynamic\n    dynamic_args = [\"--dynamic-shapes\", \"--dynamic-batch-only\"] if dynamic else []\n    args = (\n        [\n            sys.executable,\n            torchbench_file,\n            f\"--only={model}\",\n            \"--repeat=1\",\n            \"--performance\",\n            \"--backend=inductor\",\n            f\"--device={cmd_args.device}\",\n        ]\n        + MODE_ARGS_DICT[cmd_args.mode]\n        + dynamic_args\n    )\n    logger.info(f\"Command: {args}\")\n    try:\n        cold_compile_t, warm_compile_t = _run_torchbench_from_args(\n            cmd_args, model, args\n        )\n        speedup_pct = (1 - (sum(warm_compile_t) / sum(cold_compile_t))) * 100\n        results.append(\n            RunResult(\n                model=model,\n                mode=cmd_args.mode,\n                benchmark=cmd_args.benchmark,\n                dynamic=dynamic,\n                device=cmd_args.device,\n                cold_compile_s=cold_compile_t,\n                warm_compile_s=warm_compile_t,\n                speedup_pct=speedup_pct,\n            )\n        )\n    except Exception:\n        logger.info(\"fail\", exc_info=True)\n        return None", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "001669", "source": "def get_pytorch_root() -> Path:\n    return Path(\n        subprocess.check_output([\"git\", \"rev-parse\", \"--show-toplevel\"])\n        .decode(\"ascii\")\n        .strip()\n    )", "target": "def test_list_py_or_json(py_and_json: PyAndJson, input_value, expected):\n    v = py_and_json({'type': 'list', 'items_schema': {'type': 'int'}})\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=expected.message):\n            v.validate_test(input_value)\n    else:\n        assert v.validate_test(input_value) == expected", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "001670", "source": "def test_mapping_error():\n    class BadMapping(Mapping):\n        def __getitem__(self, key):\n            raise None\n        def __iter__(self):\n            raise RuntimeError('intentional error')\n        def __len__(self):\n            return 1\n    v = SchemaValidator(cs.dict_schema(keys_schema=cs.int_schema(), values_schema=cs.int_schema()))\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python(BadMapping())\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'mapping_type',\n            'loc': (),\n            'msg': 'Input should be a valid mapping, error: RuntimeError: intentional error',\n            'input': HasRepr(IsStr(regex='.+BadMapping object at.+')),\n            'ctx': {'error': 'RuntimeError: intentional error'},\n        }\n    ]", "target": "def run(arr):\n                    GSumImpl.last_result = sum(arr)\n                    return GSumImpl.last_result", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001671", "source": "def forward_and_backward_pass(self, mod, inputs, collect_outputs=True):\n        cloned_inputs = clone_inputs(inputs)\n        self.optimizer_zero_grad(mod)\n        with self.autocast(**self.autocast_arg):\n            pred = mod(*cloned_inputs)\n            if isinstance(pred, tuple):\n                pred = pred[0]\n            loss = self.compute_loss(pred)\n        self.grad_scaler.scale(loss).backward()\n        self.optimizer_step()\n        if collect_outputs:\n            return collect_results(mod, None, loss, cloned_inputs)\n        return None", "target": "def forward_and_backward_pass(self, mod, inputs, collect_outputs=True):\n        cloned_inputs = clone_inputs(inputs)\n        self.optimizer_zero_grad(mod)\n        with self.autocast(**self.autocast_arg):\n            if isinstance(cloned_inputs, dict):\n                pred = mod(**cloned_inputs)\n            else:\n                pred = mod(*cloned_inputs)\n            loss = self.compute_loss(pred)\n        self.grad_scaler.scale(loss).backward()\n        self.optimizer_step()\n        if collect_outputs:\n            return collect_results(mod, None, loss, cloned_inputs)\n        return None", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001672", "source": "def test_infer_serialize():\n    class MyEnum(Enum):\n        complex_ = complex(1, 2)\n    v = SchemaSerializer(core_schema.enum_schema(MyEnum, list(MyEnum.__members__.values())))\n    assert v.to_json(MyEnum.complex_) == b'\"1+2j\"'", "target": "def test_error_index(py_and_json: PyAndJson):\n    v = py_and_json({'type': 'generator', 'items_schema': {'type': 'int'}})\n    gen = v.validate_test(['wrong'])\n    assert gen.index == 0\n    with pytest.raises(ValidationError) as exc_info:\n        next(gen)\n    assert gen.index == 1\n    assert exc_info.value.title == 'ValidatorIterator'\n    assert str(exc_info.value).startswith('1 validation error for ValidatorIterator\\n')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': (0,),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'wrong',\n        }\n    ]\n    gen = v.validate_test([1, 2, 3, 'wrong', 4])\n    assert gen.index == 0\n    assert next(gen) == 1\n    assert gen.index == 1\n    assert next(gen) == 2\n    assert gen.index == 2\n    assert next(gen) == 3\n    assert gen.index == 3\n    with pytest.raises(ValidationError) as exc_info:\n        next(gen)\n    assert gen.index == 4\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': (3,),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'wrong',\n        }\n    ]\n    assert next(gen) == 4\n    assert gen.index == 5", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001673", "source": "def test_by_alias_and_name_config_interaction(\n    config_by_alias: Union[bool, None],\n    config_by_name: Union[bool, None],\n    runtime_by_alias: Union[bool, None],\n    runtime_by_name: Union[bool, None],\n) -> None:\n    if config_by_alias is False and config_by_name is False and runtime_by_alias is False and runtime_by_name is False:\n        pytest.skip(\"Can't have both by_alias and by_name as effectively False\")\n    schema = core_schema.arguments_schema(\n        arguments=[\n            core_schema.arguments_parameter(name='my_field', schema=core_schema.int_schema(), alias='my_alias'),\n        ],\n        **({'validate_by_alias': config_by_alias} if config_by_alias is not None else {}),\n        **({'validate_by_name': config_by_name} if config_by_name is not None else {}),\n    )\n    s = SchemaValidator(schema)\n    alias_allowed = next(x for x in (runtime_by_alias, config_by_alias, True) if x is not None)\n    name_allowed = next(x for x in (runtime_by_name, config_by_name, False) if x is not None)\n    if alias_allowed:\n        assert s.validate_python(\n            ArgsKwargs((), {'my_alias': 1}), by_alias=runtime_by_alias, by_name=runtime_by_name\n        ) == (\n            (),\n            {'my_field': 1},\n        )\n    if name_allowed:\n        assert s.validate_python(\n            ArgsKwargs((), {'my_field': 1}), by_alias=runtime_by_alias, by_name=runtime_by_name\n        ) == (\n            (),\n            {'my_field': 1},\n        )", "target": "def test_by_alias_and_name_config_interaction(\n    config_by_alias: Union[bool, None],\n    config_by_name: Union[bool, None],\n    runtime_by_alias: Union[bool, None],\n    runtime_by_name: Union[bool, None],\n) -> None:\n    if config_by_alias is False and config_by_name is False and runtime_by_alias is False and runtime_by_name is False:\n        pytest.skip(\"Can't have both by_alias and by_name as effectively False\")\n    class Model:\n        def __init__(self, my_field: int) -> None:\n            self.my_field = my_field\n    core_config = {\n        **({'validate_by_alias': config_by_alias} if config_by_alias is not None else {}),\n        **({'validate_by_name': config_by_name} if config_by_name is not None else {}),\n    }\n    schema = core_schema.model_schema(\n        Model,\n        core_schema.model_fields_schema(\n            {\n                'my_field': core_schema.model_field(core_schema.int_schema(), validation_alias='my_alias'),\n            }\n        ),\n        config=core_schema.CoreConfig(**core_config),\n    )\n    s = SchemaValidator(schema)\n    alias_allowed = next(x for x in (runtime_by_alias, config_by_alias, True) if x is not None)\n    name_allowed = next(x for x in (runtime_by_name, config_by_name, False) if x is not None)\n    if alias_allowed:\n        assert s.validate_python({'my_alias': 1}, by_alias=runtime_by_alias, by_name=runtime_by_name).my_field == 1\n    if name_allowed:\n        assert s.validate_python({'my_field': 1}, by_alias=runtime_by_alias, by_name=runtime_by_name).my_field == 1", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001674", "source": "def test_length_ctx():\n    v = SchemaValidator(cs.bytes_schema(min_length=2, max_length=3))\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python(b'1')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'bytes_too_short',\n            'loc': (),\n            'msg': 'Data should have at least 2 bytes',\n            'input': b'1',\n            'ctx': {'min_length': 2},\n        }\n    ]\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python(b'1234')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'bytes_too_long',\n            'loc': (),\n            'msg': 'Data should have at most 3 bytes',\n            'input': b'1234',\n            'ctx': {'max_length': 3},\n        }\n    ]", "target": "def test_length_ctx():\n    v = SchemaValidator(cs.list_schema(min_length=2, max_length=3))\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python([1])\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'too_short',\n            'loc': (),\n            'msg': 'List should have at least 2 items after validation, not 1',\n            'input': [1],\n            'ctx': {'field_type': 'List', 'min_length': 2, 'actual_length': 1},\n        }\n    ]\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python([1, 2, 3, 4])\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'too_long',\n            'loc': (),\n            'msg': 'List should have at most 3 items after validation, not 4',\n            'input': [1, 2, 3, 4],\n            'ctx': {'field_type': 'List', 'max_length': 3, 'actual_length': 4},\n        }\n    ]", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001675", "source": "def compiled(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, target, dloss = args\n        compiled_cross_entropy = torch.compile(\n            lambda x, target: F.cross_entropy(x, target, reduction=\"none\"),\n            mode=self.compile_mode,\n            fullgraph=True,\n        )\n        loss = compiled_cross_entropy(x, target)\n        return lambda: torch.autograd.grad(\n            loss, x, grad_outputs=dloss, retain_graph=True\n        )", "target": "def compiled(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, w, dy = args\n        y = torch.compile(self.rms_norm_ref, mode=self.compile_mode, fullgraph=True)(\n            x, w\n        )\n        return lambda: torch.autograd.grad(\n            y, [x, w], grad_outputs=dy, retain_graph=True\n        )", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001676", "source": "def test_model_b_preferred(self, schema_validator: SchemaValidator):\n        m = schema_validator.validate_python({'a': 1, 'b': 'hello', 'c': 2.0})\n        assert isinstance(m, self.ModelB)\n        assert m.a == 1\n        assert m.b == 'hello'\n        assert m.c == 2.0", "target": "def test_to_json_list_of_lists(benchmark):\n    data = [[i + j for j in range(10)] for i in range(1000)]\n    benchmark(to_json, data)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "001677", "source": "def ser_x(\n        data: Model,\n        v: Any,\n        serializer: core_schema.SerializerFunctionWrapHandler,\n        info: core_schema.FieldSerializationInfo,\n    ) -> str:\n        assert data['x'] == 1_000\n        x = serializer(v)\n        return f'{x:_}-{info.field_name}'", "target": "def init_stacked_lstm(num_layers, layer, first_layer_args, other_layer_args):\n    layers = [layer(*first_layer_args)] + [\n        layer(*other_layer_args) for _ in range(num_layers - 1)\n    ]\n    return nn.ModuleList(layers)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001678", "source": "def foobar(a, b, c):\n        return a, b, c", "target": "def foobar(a: int, b: int, *, c: int):\n        return a, b, c", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001679", "source": "def volume(self) -> int:\n            return self.area * self.height", "target": "def volume(self) -> None:\n            return None", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001680", "source": "def dynamic_rnn(\n        input: Tensor,\n        hidden: tuple[Tensor, Tensor],\n        w_ih: Tensor,\n        w_hh: Tensor,\n        b_ih: Tensor,\n        b_hh: Tensor,\n    ) -> tuple[Tensor, tuple[Tensor, Tensor]]:\n        hx, cx = hidden\n        outputs = []\n        inputs = input.unbind(0)\n        hy, cy = hx[0], cx[0]\n        for seq_idx in range(len(inputs)):\n            hy, cy = cell(inputs[seq_idx], (hy, cy), w_ih, w_hh, b_ih, b_hh)\n            outputs += [hy]\n        return torch.stack(outputs), (hy.unsqueeze(0), cy.unsqueeze(0))", "target": "def dynamic_rnn(\n        input: Tensor, hidden: tuple[Tensor, Tensor], params: list[Tensor]\n    ) -> tuple[Tensor, tuple[Tensor, Tensor]]:\n        params_stride = 4\n        hx, cx = hidden\n        hy, cy = hidden\n        inputs, outputs = input.unbind(0), []\n        for layer in range(hx.size(0)):\n            hy = hx[layer]\n            cy = cx[layer]\n            base_idx = layer * params_stride\n            w_ih = params[base_idx]\n            w_hh = params[base_idx + 1]\n            b_ih = params[base_idx + 2]\n            b_hh = params[base_idx + 3]\n            for seq_idx in range(len(inputs)):\n                hy, cy = cell(inputs[seq_idx], (hy, cy), w_ih, w_hh, b_ih, b_hh)\n                outputs += [hy]\n            inputs, outputs = outputs, []\n        return torch.stack(inputs), (hy.unsqueeze(0), cy.unsqueeze(0))", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001681", "source": "def is_resolved(self) -> bool:\n        return self.positive_branch_type.is_resolved \\\n                and self.negative_branch_type.is_resolved", "target": "def is_resolved(self) -> bool:\n        return all(item.is_resolved for item in self.items)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001682", "source": "def name(self):\n        prefix = f\"{self.category()}_{self.mode()}\"\n        if self._subclass:\n            prefix += \"_subclass\"\n        else:\n            prefix += \"_nosubclass\"\n        if self.device() == \"cpu\":\n            prefix += \"_cpu\"\n        return prefix", "target": "def name(self):\n        prefix = f\"{self.category()}_{self.backend()}\"\n        if self._requires_grad:\n            prefix += \"_requires_grad\"\n        if self._inference_mode:\n            prefix += \"_inference_mode\"\n        if self._backward:\n            prefix += \"_backward\"\n        if self.is_dynamic():\n            prefix += \"_dynamic\"\n        return prefix", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001683", "source": "def test_extra_behavior_allow(\n    config: Union[core_schema.CoreConfig, None],\n    schema_extra_behavior_kw: dict[str, Any],\n    validate_fn_extra_kw: Union[ExtraBehavior, None],\n):\n    @dataclasses.dataclass\n    class MyModel:\n        f: str\n    v = SchemaValidator(\n        core_schema.dataclass_schema(\n            MyModel,\n            core_schema.dataclass_args_schema(\n                'MyModel', [core_schema.dataclass_field('f', core_schema.str_schema())], **schema_extra_behavior_kw\n            ),\n            ['f'],\n            config=config,\n        )\n    )\n    m: MyModel = v.validate_python({'f': 'x', 'extra_field': '123'}, extra=validate_fn_extra_kw)\n    assert m.f == 'x'\n    assert getattr(m, 'extra_field') == '123'\n    v.validate_assignment(m, 'f', 'y', extra=validate_fn_extra_kw)\n    assert m.f == 'y'\n    v.validate_assignment(m, 'not_f', '123', extra=validate_fn_extra_kw)\n    assert getattr(m, 'not_f') == '123'", "target": "def test_extra_behavior_allow(\n    config: Union[core_schema.CoreConfig, None],\n    schema_extra_behavior_kw: dict[str, Any],\n    extras_schema_kw: dict[str, Any],\n    expected_extra_value: Any,\n):\n    v = SchemaValidator(\n        core_schema.typed_dict_schema(\n            {'f': core_schema.typed_dict_field(core_schema.str_schema())},\n            **schema_extra_behavior_kw,\n            **extras_schema_kw,\n            config=config,\n        )\n    )\n    m: dict[str, Any] = v.validate_python({'f': 'x', 'extra_field': '123'})\n    assert m == {'f': 'x', 'extra_field': expected_extra_value}", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001684", "source": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "target": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001685", "source": "def multiple_of(self: _Pipeline[_InT, _NewOutDiv], multiple_of: _NewOutDiv) -> _Pipeline[_InT, _NewOutDiv]: ...", "target": "def multiple_of(self: _Pipeline[_InT, Any], multiple_of: Any) -> _Pipeline[_InT, Any]:\n        return self.constrain(annotated_types.MultipleOf(multiple_of))", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|pydantic/pydantic", "data_split": "train", "example_id": "001686", "source": "def make_gen_classif_scorers(caller):\n    caller.train_scorer = balanced_accuracy_score\n    caller.test_scorer = balanced_accuracy_score", "target": "def basic_model_serializer_fixture():\n    return SchemaSerializer(\n        core_schema.model_schema(\n            BasicModel,\n            core_schema.model_fields_schema(\n                {\n                    'a': core_schema.model_field(core_schema.int_schema()),\n                    'b': core_schema.model_field(core_schema.int_schema()),\n                    'c': core_schema.model_field(core_schema.int_schema()),\n                    'd': core_schema.model_field(core_schema.int_schema()),\n                    'e': core_schema.model_field(core_schema.int_schema()),\n                    'f': core_schema.model_field(core_schema.int_schema()),\n                    'g': core_schema.model_field(core_schema.int_schema()),\n                    'h': core_schema.model_field(core_schema.int_schema()),\n                }\n            ),\n        )\n    )", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001687", "source": "def use_larger_multiplier_for_smaller_tensor(self, name):\n        return name in REQUIRE_LARGER_MULTIPLIER_FOR_SMALLER_TENSOR", "target": "def use_larger_multiplier_for_smaller_tensor(self, name):\n        return name in self._require_larger_multiplier_for_smaller_tensor", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001688", "source": "def test_filter(benchmark):\n    v = SchemaSerializer(core_schema.list_schema(core_schema.any_schema()))\n    assert v.to_python(['a', 'b', 'c', 'd', 'e'], include={-1, -2}) == ['d', 'e']\n    @benchmark\n    def t():\n        v.to_python(['a', 'b', 'c', 'd', 'e'], include={-1, -2})", "target": "def test_ignored_def():\n    v = SchemaValidator(\n        core_schema.definitions_schema(\n            core_schema.list_schema(core_schema.int_schema()), [core_schema.int_schema(ref='foobar')]\n        )\n    )\n    assert v.validate_python([1, 2, '3']) == [1, 2, 3]\n    r = plain_repr(v)\n    assert r.startswith('SchemaValidator(title=\"list[int]\",')", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|pydantic/pydantic", "data_split": "train", "example_id": "001689", "source": "def generate_api_toc_html(kind=\"html\"):\n        soup = context[\"pst_generate_toc_html\"](kind=\"soup\")\n        try:\n            soup.ul.unwrap()\n            soup.li.unwrap()\n            soup.a.decompose()\n            lis = soup.ul.select(\"li.toc-h2\")\n            main_li = lis[0]\n            meth_list = main_li.ul\n            if meth_list is not None:\n                meth_list[\"class\"].append(\"visible\")\n                for meth in meth_list.find_all(\"li\", {\"class\": \"toc-h3\"}):\n                    target = meth.a.code.span\n                    target.string = target.string.split(\".\", 1)[1]\n            return str(soup) if kind == \"html\" else soup\n        except Exception as e:\n            logger.warning(\n                f\"Failed to generate API pagetoc for {pagename}: {e}; falling back\"\n            )\n            return context[\"pst_generate_toc_html\"](kind=kind)", "target": "def test_aliases_debug():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'field_a': core_schema.model_field(\n                    validation_alias=[['foo', 'bar', 'bat'], ['foo', 3]], schema=core_schema.int_schema()\n                )\n            }\n        )\n    )\n    print(repr(v))\n    assert repr(v).startswith('SchemaValidator(title=\"model-fields\", validator=ModelFields(')\n    assert 'PathChoices(' in repr(v)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001690", "source": "def types_separator(self) -> str:\n        return \", \"", "target": "def types_separator(self) -> str:\n        return \", \"", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|scikit-learn/scikit-learn", "data_split": "train", "example_id": "001691", "source": "def branches_containing_ref(\n        self, ref: str, *, include_remote: bool = True\n    ) -> list[str]:\n        rc = (\n            self._run_git(\"branch\", \"--remote\", \"--contains\", ref)\n            if include_remote\n            else self._run_git(\"branch\", \"--contains\", ref)\n        )\n        return [x.strip() for x in rc.split(\"\\n\") if x.strip()] if len(rc) > 0 else []", "target": "def pytest_collection_modifyitems(config, items):\n    skip_doctests = False\n    if np_base_version < parse_version(\"2\"):\n        reason = \"Due to NEP 51 numpy scalar repr has changed in numpy 2\"\n        skip_doctests = True\n    if sp_version < parse_version(\"1.14\"):\n        reason = \"Scipy sparse matrix repr has changed in scipy 1.14\"\n        skip_doctests = True\n    for item in items:\n        if isinstance(item, DoctestItem):\n            item.dtest.globs = {}\n    if skip_doctests:\n        skip_marker = pytest.mark.skip(reason=reason)\n        for item in items:\n            if isinstance(item, DoctestItem):\n                item.add_marker(skip_marker)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|opencv/opencv", "data_split": "train", "example_id": "001692", "source": "def test_get_path_resolves(self):\n        rel_str = \"sub/f.txt\"\n        p = get_path(str(self.tmp_path / rel_str), resolve=True)\n        self.assertTrue(p.is_absolute())\n        self.assertTrue(str(p).endswith(rel_str))", "target": "def namespaces(self) -> Dict[str, \"NamespaceNode\"]:\n        return self._children[ASTNodeType.Namespace]", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "001693", "source": "def json_schema(\n    schema: CoreSchema | None = None,\n    *,\n    ref: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> JsonSchema:\n    return _dict_not_none(type='json', schema=schema, ref=ref, metadata=metadata, serialization=serialization)", "target": "def _merge_pytest_caches(\n    pytest_cache_dir_to_merge_from: Path, pytest_cache_dir_to_merge_into: Path\n) -> None:\n    static_files_to_copy = [\n        \".gitignore\",\n        \"CACHEDIR.TAG\",\n        \"README.md\",\n    ]\n    for static_file in static_files_to_copy:\n        source_file = pytest_cache_dir_to_merge_from / static_file\n        if not source_file.is_file():\n            continue\n        dest_file = pytest_cache_dir_to_merge_into / static_file\n        if not dest_file.exists():\n            copy_file(source_file, dest_file)\n    _merge_lastfailed_files(\n        pytest_cache_dir_to_merge_from, pytest_cache_dir_to_merge_into\n    )\n    _merge_additional_failures_files(\n        pytest_cache_dir_to_merge_from, pytest_cache_dir_to_merge_into\n    )", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001694", "source": "def is_valid_field_name(name: str) -> bool:\n    return not name.startswith('_')", "target": "def test_keyword_args(py_and_json: PyAndJson, input_value, expected):\n    v = py_and_json(\n        {\n            'type': 'arguments',\n            'arguments_schema': [\n                {'name': 'a', 'mode': 'keyword_only', 'schema': {'type': 'int'}},\n                {'name': 'b', 'mode': 'keyword_only', 'schema': {'type': 'str'}},\n                {'name': 'c', 'mode': 'keyword_only', 'schema': {'type': 'bool'}},\n            ],\n        }\n    )\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)) as exc_info:\n            v.validate_test(input_value)\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        assert v.validate_test(input_value) == expected", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "001695", "source": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "target": "def missing_sentinel_schema(\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> MissingSentinelSchema:\n    return _dict_not_none(\n        type='missing-sentinel',\n        metadata=metadata,\n        serialization=serialization,\n    )", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|opencv/opencv", "data_split": "train", "example_id": "001696", "source": "def check_parallelism(tests: Any, title: str, shard_id: int = 0, num_shards: int = 0):\n    parallelism = int(tests.get(\"parallelism\", \"0\"))\n    is_parallel = parallelism and parallelism > 1\n    if not is_parallel:\n        return False\n    if shard_id > num_shards:\n        raise RuntimeError(\n            f\"Test {title} expects {num_shards} shards, but invalid {shard_id} is provided\"\n        )\n    if num_shards != parallelism:\n        raise RuntimeError(\n            f\"Test {title} expects {parallelism} shards, but invalid {num_shards} is provided\"\n        )\n    return True", "target": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "001697", "source": "def test_function_only_json():\n    def double(value, _):\n        return value * 2\n    s = SchemaSerializer(\n        core_schema.any_schema(\n            serialization=core_schema.plain_serializer_function_ser_schema(double, info_arg=True, when_used='json')\n        )\n    )\n    assert s.to_python(4) == 4\n    assert s.to_python(4, mode='foobar') == 4\n    assert s.to_python(4, mode='json') == 8\n    assert s.to_json(4) == b'8'", "target": "def set_output(name: str, val: Any) -> None:\n    print(f\"Setting output {name}={val}\")\n    if os.getenv(\"GITHUB_OUTPUT\"):\n        with open(str(os.getenv(\"GITHUB_OUTPUT\")), \"a\") as env:\n            print(f\"{name}={val}\", file=env)\n    else:\n        print(f\"::set-output name={name}::{val}\")", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "001698", "source": "def make_estimator(self, params):\n        representation, solver, n_jobs = params\n        penalty = \"l2\" if solver == \"lbfgs\" else \"l1\"\n        estimator = LogisticRegression(\n            solver=solver,\n            penalty=penalty,\n            tol=0.01,\n            n_jobs=n_jobs,\n            random_state=0,\n        )\n        return estimator", "target": "def make_estimator(self, params):\n        (representation,) = params\n        n_estimators = 100 if Benchmark.data_size == \"large\" else 10\n        estimator = GradientBoostingClassifier(\n            n_estimators=n_estimators,\n            max_features=\"log2\",\n            subsample=0.5,\n            random_state=0,\n        )\n        return estimator", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001699", "source": "def test_extra_behavior_ignore(\n    config: Union[core_schema.CoreConfig, None],\n    schema_extra_behavior_kw: dict[str, Any],\n    validate_fn_extra_kw: Union[ExtraBehavior, None],\n):\n    @dataclasses.dataclass\n    class MyModel:\n        f: str\n    v = SchemaValidator(\n        core_schema.dataclass_schema(\n            MyModel,\n            core_schema.dataclass_args_schema(\n                'MyModel', [core_schema.dataclass_field('f', core_schema.str_schema())], **schema_extra_behavior_kw\n            ),\n            ['f'],\n        ),\n        config=config,\n    )\n    m: MyModel = v.validate_python({'f': 'x', 'extra_field': 123}, extra=validate_fn_extra_kw)\n    assert m.f == 'x'\n    assert not hasattr(m, 'extra_field')\n    v.validate_assignment(m, 'f', 'y', extra=validate_fn_extra_kw)\n    assert m.f == 'y'\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_assignment(m, 'not_f', 'xyz', extra=validate_fn_extra_kw)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'no_such_attribute',\n            'loc': ('not_f',),\n            'msg': \"Object has no attribute 'not_f'\",\n            'input': 'xyz',\n            'ctx': {'attribute': 'not_f'},\n        }\n    ]\n    assert not hasattr(m, 'not_f')", "target": "def test_include_exclude_schema():\n    s = SchemaSerializer(\n        core_schema.typed_dict_schema(\n            {\n                '0': core_schema.typed_dict_field(core_schema.int_schema(), serialization_exclude=True),\n                '1': core_schema.typed_dict_field(core_schema.int_schema()),\n                '2': core_schema.typed_dict_field(\n                    core_schema.int_schema(), serialization_exclude=True, serialization_exclude_if=lambda x: x < 0\n                ),\n                '3': core_schema.typed_dict_field(\n                    core_schema.int_schema(), serialization_exclude=False, serialization_exclude_if=lambda x: x < 0\n                ),\n            }\n        )\n    )\n    value = {'0': 0, '1': 1, '2': 2, '3': 3}\n    assert s.to_python(value) == {'1': 1, '3': 3}\n    assert s.to_python(value, mode='json') == {'1': 1, '3': 3}\n    assert json.loads(s.to_json(value)) == {'1': 1, '3': 3}\n    value = {'0': 0, '1': 1, '2': 2, '3': -3}\n    assert s.to_python(value) == {'1': 1}\n    assert s.to_python(value, mode='json') == {'1': 1}\n    assert json.loads(s.to_json(value)) == {'1': 1}", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|opencv/opencv", "data_split": "train", "example_id": "001700", "source": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "target": "def test_goodFeaturesToTrack(self):\n        arr = self.get_sample('samples/data/lena.jpg', 0)\n        original = arr.copy()\n        threshes = [ x / 100. for x in range(1,10) ]\n        numPoints = 20000\n        results = dict([(t, cv.goodFeaturesToTrack(arr, numPoints, t, 2, useHarrisDetector=True)) for t in threshes])\n        self.assertTrue(arr.tobytes() == original.tobytes())\n        for i in range(1):\n            results2 = dict([(t, cv.goodFeaturesToTrack(arr, numPoints, t, 2, useHarrisDetector=True)) for t in threshes])\n            for t in threshes:\n                self.assertTrue(len(results2[t]) == len(results[t]))\n                for i in range(len(results[t])):\n                    self.assertTrue(cv.norm(results[t][i][0] - results2[t][i][0]) == 0)\n        for t0,t1 in zip(threshes, threshes[1:]):\n            r0 = results[t0]\n            r1 = results[t1]\n            self.assertTrue(len(r0) > len(r1))\n            for i in range(len(r1)):\n                self.assertTrue(cv.norm(r1[i][0] - r0[i][0])==0)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|opencv/opencv", "data_split": "train", "example_id": "001701", "source": "def get_memory_bytes(self, args, kwargs) -> int:\n        x, dy = args\n        M, N = x.shape\n        return 3 * M * N * x.dtype.itemsize", "target": "def test_parse_to_float_not_convertible_extra(self):\n        for not_convertible in (np.bool_(False), True, False, np.array([123, ], dtype=int),\n                                np.array([1., ]), np.array([False]),\n                                np.array([True])):\n            with self.assertRaises((TypeError, OverflowError),\n                                   msg=get_no_exception_msg(not_convertible)):\n                _ = cv.utils.dumpFloat(not_convertible)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|opencv/opencv", "data_split": "train", "example_id": "001702", "source": "def value_type(self) -> str:\n        return self._value_type", "target": "def enumerations(self) -> Dict[str, EnumerationNode]:\n        return self._children[ASTNodeType.Enumeration]", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|scikit-learn/scikit-learn", "data_split": "train", "example_id": "001703", "source": "def forward(self, x, lengths):\n        for module in self.seq_module:\n            x = module(x)\n            mask = torch.BoolTensor(x.size()).fill_(0)\n            if x.is_cuda:\n                mask = mask.cuda()\n            for i, length in enumerate(lengths):\n                length = length.item()\n                if (mask[i].size(2) - length) > 0:\n                    mask[i].narrow(2, length, mask[i].size(2) - length).fill_(1)\n            x = x.masked_fill(mask, 0)\n        return x, lengths", "target": "def bench_sample(sampling, n_population, n_samples):\n    gc.collect()\n    t_start = datetime.now()\n    sampling(n_population, n_samples)\n    delta = datetime.now() - t_start\n    time = compute_time(t_start, delta)\n    return time", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "001704", "source": "def test_smart_union_json_string_types_str_first(schema: core_schema.CoreSchema, input_value: str):\n    validator = SchemaValidator(core_schema.union_schema([core_schema.str_schema(), schema]))\n    assert validator.validate_json(f'\"{input_value}\"') == input_value\n    assert validator.validate_python(input_value) == input_value", "target": "def first_matching_pkg(pattern: str) -> str:\n    matches = sorted(glob.glob(pattern))\n    if not matches:\n        raise FileNotFoundError(f\"No wheel matching: {pattern}\")\n    return matches[0]", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001705", "source": "def test_constraints_schema_validation() -> None:\n    val = SchemaValidator(cs.decimal_schema(gt='1'))\n    with pytest.raises(ValidationError):\n        val.validate_python('0')", "target": "def test_constraints_schema_validation() -> None:\n    val = SchemaValidator(core_schema.timedelta_schema(gt=3))\n    with pytest.raises(ValidationError):\n        val.validate_python(1)", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|pydantic/pydantic", "data_split": "train", "example_id": "001706", "source": "def _blobs_dataset(n_samples=500000, n_features=3, n_clusters=100, dtype=np.float32):\n    X, _ = make_blobs(\n        n_samples=n_samples, n_features=n_features, centers=n_clusters, random_state=0\n    )\n    X = X.astype(dtype, copy=False)\n    X, X_val = train_test_split(X, test_size=0.1, random_state=0)\n    return X, X_val, None, None", "target": "def test_simple():\n    v = SchemaValidator(core_schema.str_schema())\n    assert v.validate_python(b'abc') == 'abc'\n    assert v.isinstance_python(b'abc') is True\n    assert v.validate_python(b'abc', self_instance='foobar') == 'abc'\n    assert v.isinstance_python(b'abc', self_instance='foobar') is True\n    assert v.validate_json('\"abc\"') == 'abc'\n    assert v.validate_json('\"abc\"', self_instance='foobar') == 'abc'", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|pydantic/pydantic", "data_split": "train", "example_id": "001707", "source": "def setup_compose():\n    try:\n        import pandas\n    except ImportError:\n        raise SkipTest(\"Skipping compose.rst, pandas not installed\")", "target": "def f(input_value, validator, info):\n        return validator(input_value=input_value) + ' Changed'", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "001708", "source": "def test_time_strict(input_value, expected):\n    v = SchemaValidator(core_schema.time_schema(strict=True))\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            v.validate_python(input_value)\n    else:\n        output = v.validate_python(input_value)\n        assert output == expected", "target": "def get_limits(dtype):\n    if not is_numeric(dtype):\n        return None, None\n    if np.issubdtype(dtype, np.integer):\n        info = np.iinfo(dtype)\n    else:\n        info = np.finfo(dtype)\n    return info.min, info.max", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001709", "source": "def test_alias_extra(py_and_json: PyAndJson):\n    v = py_and_json(\n        {\n            'type': 'model-fields',\n            'extra_behavior': 'allow',\n            'fields': {\n                'field_a': {\n                    'validation_alias': [['FieldA'], ['foo', 2]],\n                    'type': 'model-field',\n                    'schema': {'type': 'int'},\n                }\n            },\n        },\n        {'loc_by_alias': False},\n    )\n    assert v.validate_test({'FieldA': 1}) == ({'field_a': 1}, {}, {'field_a'})\n    assert v.validate_test({'foo': [1, 2, 3]}) == ({'field_a': 3}, {}, {'field_a'})\n    with pytest.raises(ValidationError) as exc_info:\n        assert v.validate_test({'FieldA': '...'}) == ({'field_a': 1}, {}, {'field_a'})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('field_a',),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': '...',\n        }\n    ]", "target": "def test_alias_extra(py_and_json: PyAndJson):\n    v = py_and_json(\n        {\n            'type': 'typed-dict',\n            'extra_behavior': 'allow',\n            'fields': {\n                'field_a': {\n                    'validation_alias': [['FieldA'], ['foo', 2]],\n                    'type': 'typed-dict-field',\n                    'schema': {'type': 'int'},\n                }\n            },\n            'config': {'loc_by_alias': False},\n        }\n    )\n    assert v.validate_test({'FieldA': 1}) == {'field_a': 1}\n    assert v.validate_test({'foo': [1, 2, 3]}) == {'field_a': 3}\n    with pytest.raises(ValidationError) as exc_info:\n        assert v.validate_test({'FieldA': '...'}) == {'field_a': 1}\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('field_a',),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': '...',\n        }\n    ]", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001710", "source": "def ser_x(self, v: Any, serializer: core_schema.SerializerFunctionWrapHandler, _) -> str:\n            x = serializer(v)\n            assert self.x == 1_000\n            return f'{x:_}'", "target": "def test_validate_assignment(pydantic_version) -> None:\n    @dataclass\n    class Model:\n        x: list['Model']\n    schema = core_schema.definitions_schema(\n        core_schema.definition_reference_schema('model'),\n        [\n            core_schema.dataclass_schema(\n                Model,\n                core_schema.dataclass_args_schema(\n                    'Model',\n                    [\n                        core_schema.dataclass_field(\n                            name='x',\n                            schema=core_schema.list_schema(core_schema.definition_reference_schema('model')),\n                            kw_only=False,\n                        )\n                    ],\n                ),\n                ['x'],\n                ref='model',\n                config=core_schema.CoreConfig(revalidate_instances='always'),\n            )\n        ],\n    )\n    v = SchemaValidator(schema)\n    data = [Model(x=[Model(x=[])])]\n    instance = Model(x=[])\n    v.validate_assignment(instance, 'x', data)\n    assert instance.x == data\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_assignment(instance, 'x', [Model(x=[Model(x=[Model(x=[123])])])])\n    assert exc_info.value.errors() == [\n        {\n            'type': 'dataclass_type',\n            'loc': ('x', 0, 'x', 0, 'x', 0, 'x', 0),\n            'msg': 'Input should be a dictionary or an instance of Model',\n            'input': 123,\n            'ctx': {'class_name': 'Model'},\n            'url': f'https://errors.pydantic.dev/{pydantic_version}/v/dataclass_type',\n        }\n    ]", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001711", "source": "def resolve(self, root: ASTNode):\n        try:\n            self.positive_branch_type.resolve(root)\n            self.negative_branch_type.resolve(root)\n        except TypeResolutionError as e:\n            raise TypeResolutionError(\n                'Failed to resolve alias \"{}\" exposed as \"{}\"'.format(\n                    self.ctype_name, self.typename\n                )\n            ) from e", "target": "def resolve(self, root: ASTNode):\n        if self.is_resolved:\n            return\n        node = _resolve_symbol(root, self.typename)\n        if node is None:\n            raise TypeResolutionError('Failed to resolve \"{}\" exposed as \"{}\"'.format(\n                self.ctype_name, self.typename\n            ))\n        self._ast_node = weakref.proxy(node)", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|pydantic/pydantic", "data_split": "train", "example_id": "001712", "source": "def make_estimator(self, params):\n        algorithm, dimension, n_jobs = params\n        estimator = KNeighborsClassifier(algorithm=algorithm, n_jobs=n_jobs)\n        return estimator", "target": "def _import_execute(source: str, *, custom_module_name: str | None = None):\n        module_name = custom_module_name or request.node.name\n        module_path = tmp_work_path / f'{module_name}.py'\n        module_path.write_text(source)\n        spec = importlib.util.spec_from_file_location('__main__', str(module_path))\n        module = importlib.util.module_from_spec(spec)\n        try:\n            spec.loader.exec_module(module)\n        except KeyboardInterrupt:\n            print('KeyboardInterrupt')\n        else:\n            return module", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001713", "source": "def required_usage_imports(self) -> Generator[str, None, None]:\n        yield \"import typing as _typing\"", "target": "def required_usage_imports(self) -> Generator[str, None, None]:\n        yield \"import cv2.typing\"", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001714", "source": "def test_alias_extra_by_name(py_and_json: PyAndJson):\n    v = py_and_json(\n        {\n            'type': 'typed-dict',\n            'extra_behavior': 'allow',\n            'fields': {\n                'field_a': {'validation_alias': 'FieldA', 'type': 'typed-dict-field', 'schema': {'type': 'int'}}\n            },\n            'config': {'validate_by_name': True},\n        },\n    )\n    assert v.validate_test({'FieldA': 1}) == {'field_a': 1}\n    assert v.validate_test({'field_a': 1}) == {'field_a': 1}", "target": "def test_dc_smart_union_with_defaults() -> None:\n    @dataclass\n    class ModelA:\n        a: int = 0\n    @dataclass\n    class ModelB:\n        b: int = 0\n    dc_a_schema = core_schema.dataclass_schema(\n        ModelA,\n        core_schema.dataclass_args_schema(\n            'ModelA',\n            [\n                core_schema.dataclass_field(\n                    'a', core_schema.with_default_schema(schema=core_schema.int_schema(), default=0)\n                )\n            ],\n        ),\n        ['a'],\n    )\n    dc_b_schema = core_schema.dataclass_schema(\n        ModelB,\n        core_schema.dataclass_args_schema(\n            'ModelB',\n            [\n                core_schema.dataclass_field(\n                    'b', core_schema.with_default_schema(schema=core_schema.int_schema(), default=0)\n                )\n            ],\n        ),\n        ['b'],\n    )\n    for choices in permute_choices([dc_a_schema, dc_b_schema]):\n        validator = SchemaValidator(core_schema.union_schema(choices=choices))\n        assert isinstance(validator.validate_python({'a': 1}), ModelA)\n        assert isinstance(validator.validate_python({'b': 1}), ModelB)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001715", "source": "def test_dataclass():\n    schema = core_schema.dataclass_schema(\n        FooDataclass,\n        core_schema.dataclass_args_schema(\n            'FooDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 'hello', 'b': True})\n    assert dataclasses.is_dataclass(foo)\n    assert foo.a == 'hello'\n    assert foo.b is True\n    assert dataclasses.asdict(v.validate_python(FooDataclass(a='hello', b=True))) == {'a': 'hello', 'b': True}\n    with pytest.raises(ValidationError, match='Input should be an instance of FooDataclass') as exc_info:\n        v.validate_python({'a': 'hello', 'b': True}, strict=True)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'dataclass_exact_type',\n            'loc': (),\n            'msg': 'Input should be an instance of FooDataclass',\n            'input': {'a': 'hello', 'b': True},\n            'ctx': {'class_name': 'FooDataclass'},\n        }\n    ]", "target": "def test_validate_scientific_notation_from_json(input_value, expected):\n    v = SchemaValidator(cs.float_schema())\n    assert v.validate_json(input_value) == expected", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001716", "source": "def pattern_either_validator(input_value: Any, /) -> re.Pattern[Any]:\n    if isinstance(input_value, re.Pattern):\n        return input_value\n    elif isinstance(input_value, (str, bytes)):\n        return compile_pattern(input_value)\n    else:\n        raise PydanticCustomError('pattern_type', 'Input should be a valid pattern')", "target": "def test_non_finite_constrained_float_values(input_value, allow_inf_nan, expected):\n    v = SchemaValidator(cs.float_schema(allow_inf_nan=allow_inf_nan, gt=0))\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            v.validate_python(input_value)\n    else:\n        assert v.validate_python(input_value) == expected", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "001717", "source": "def run_mixtral_8x7b_autoquant(device: str = \"cuda\"):\n    model = GPTModelConfig(\n        \"Mixtral-8x7B-v0.1\",\n        MixtralMoE,\n        \"autoquant\",\n        None,\n        175,\n        1130,\n        133,\n    )\n    token_per_sec, memory_bandwidth, compilation_time = run_experiment(\n        model, device=device\n    )\n    return [\n        Experiment(\n            model.name,\n            \"token_per_sec\",\n            model.token_per_sec,\n            f\"{token_per_sec:.02f}\",\n            model.mode,\n            device,\n            get_arch_name(),\n            True,\n        ),\n        Experiment(\n            model.name,\n            \"memory_bandwidth(GB/s)\",\n            model.memory_bandwidth,\n            f\"{memory_bandwidth:.02f}\",\n            model.mode,\n            device,\n            get_arch_name(),\n            True,\n        ),\n        Experiment(\n            model.name,\n            \"compilation_time(s)\",\n            model.compilation_time,\n            f\"{compilation_time:.02f}\",\n            model.mode,\n            device,\n            get_arch_name(),\n            True,\n        ),\n    ]", "target": "def _union_orderings_key(typevar_values: Any) -> Any:\n    if isinstance(typevar_values, tuple):\n        return tuple(_union_orderings_key(value) for value in typevar_values)\n    elif typing_objects.is_union(typing_extensions.get_origin(typevar_values)):\n        return get_args(typevar_values)\n    else:\n        return ()", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001718", "source": "def test_missing_error(pydantic_version):\n    v = SchemaValidator(\n        core_schema.typed_dict_schema(\n            fields={\n                'field_a': core_schema.typed_dict_field(schema=core_schema.str_schema()),\n                'field_b': core_schema.typed_dict_field(schema=core_schema.int_schema()),\n            }\n        )\n    )\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python({'field_a': b'abc'})\n    assert str(exc_info.value) == (\n        '1 validation error for typed-dict\\n'\n        'field_b\\n'\n        \"  Field required [type=missing, input_value={'field_a': b'abc'}, input_type=dict]\"\n        + (\n            f'\\n    For further information visit https://errors.pydantic.dev/{pydantic_version}/v/missing'\n            if os.environ.get('PYDANTIC_ERRORS_INCLUDE_URL', '1') != 'false'\n            else ''\n        )\n    )", "target": "def test_generator_int():\n    s = SchemaSerializer(core_schema.generator_schema(core_schema.int_schema()))\n    assert list(s.to_python(iter([1, 2, 3]))) == [1, 2, 3]\n    assert list(s.to_python(gen_ok(1, 2, 3))) == [1, 2, 3]\n    assert s.to_python(iter([1, 2, 3]), mode='json') == [1, 2, 3]\n    assert s.to_json(iter([1, 2, 3])) == b'[1,2,3]'\n    assert s.to_json(gen_ok(1, 2, 3)) == b'[1,2,3]'\n    with pytest.raises(ValueError, match='oops'):\n        list(s.to_python(gen_error(1, 2)))\n    with pytest.raises(ValueError, match='oops'):\n        s.to_json(gen_error(1, 2))\n    with pytest.warns(\n        UserWarning,\n        match=r\"Expected `int` - serialized value may not be as expected \\[input_value='a', input_type=str\\]\",\n    ):\n        s.to_json(gen_ok(1, 'a'))\n    gen = s.to_python(gen_ok(1, 'a'))\n    assert next(gen) == 1\n    with pytest.warns(\n        UserWarning,\n        match=r\"Expected `int` - serialized value may not be as expected \\[input_value='a', input_type=str\\]\",\n    ):\n        assert next(gen) == 'a'\n    with pytest.warns(\n        UserWarning,\n        match=r'Expected `generator` - serialized value may not be as expected \\[input_value=\\(1, 2, 3\\), input_type=tuple\\]',\n    ):\n        s.to_python((1, 2, 3))", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "001719", "source": "def make_optional_arg(*arg_names: str) -> Callable[[NamespaceNode, SymbolName], None]:\n    def _make_optional_arg(root_node: NamespaceNode,\n                           function_symbol_name: SymbolName) -> None:\n        function = find_function_node(root_node, function_symbol_name)\n        for arg_name in arg_names:\n            found_overload_with_arg = False\n            for overload in function.overloads:\n                arg_idx = _find_argument_index(overload.arguments, arg_name)\n                if arg_idx is None:\n                    continue\n                if isinstance(overload.arguments[arg_idx].type_node, OptionalTypeNode):\n                    continue\n                overload.arguments[arg_idx].type_node = OptionalTypeNode(\n                    cast(TypeNode, overload.arguments[arg_idx].type_node)\n                )\n                found_overload_with_arg = True\n            if not found_overload_with_arg:\n                raise RuntimeError(\n                    f\"Failed to find argument with name: '{arg_name}'\"\n                    f\" in '{function_symbol_name.name}' overloads\"\n                )\n    return _make_optional_arg", "target": "def map_generic_model_arguments(cls: type[BaseModel], args: tuple[Any, ...]) -> dict[TypeVar, Any]:\n    parameters = cls.__pydantic_generic_metadata__['parameters']\n    expected_len = len(parameters)\n    typevars_map: dict[TypeVar, Any] = {}\n    _missing = object()\n    for parameter, argument in zip_longest(parameters, args, fillvalue=_missing):\n        if parameter is _missing:\n            raise TypeError(f'Too many arguments for {cls}; actual {len(args)}, expected {expected_len}')\n        if argument is _missing:\n            param = cast(TypeVar, parameter)\n            try:\n                has_default = param.has_default()\n            except AttributeError:\n                has_default = False\n            if has_default:\n                typevars_map[param] = replace_types(param.__default__, typevars_map)\n            else:\n                expected_len -= sum(hasattr(p, 'has_default') and p.has_default() for p in parameters)\n                raise TypeError(f'Too few arguments for {cls}; actual {len(args)}, expected at least {expected_len}')\n        else:\n            param = cast(TypeVar, parameter)\n            typevars_map[param] = argument\n    return typevars_map", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "001720", "source": "def serialize_sequence_via_list(\n    v: Any, handler: core_schema.SerializerFunctionWrapHandler, info: core_schema.SerializationInfo\n) -> Any:\n    items: list[Any] = []\n    mapped_origin = SEQUENCE_ORIGIN_MAP.get(type(v), None)\n    if mapped_origin is None:\n        return v\n    for index, item in enumerate(v):\n        try:\n            v = handler(item, index)\n        except PydanticOmit:\n            pass\n        else:\n            items.append(v)\n    if info.mode_is_json():\n        return items\n    else:\n        return mapped_origin(items)", "target": "def map_to_device(e, device):\n    if isinstance(e, torch.Tensor):\n        return e.to(device)\n    elif isinstance(e, torch.device):\n        return device\n    elif isinstance(e, str):\n        if e == \"cuda\" or e == \"cpu\":\n            return device.type\n    else:\n        return e", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001721", "source": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "target": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001722", "source": "def dec(f: Callable[..., Any] | classmethod[Any, Any, Any] | staticmethod[Any, Any]) -> Any:\n        if _decorators.is_instance_method_from_sig(f):\n            raise TypeError('`@root_validator` cannot be applied to instance methods')\n        res = _decorators.ensure_classmethod_based_on_signature(f)\n        dec_info = _decorators.RootValidatorDecoratorInfo(mode=mode)\n        return _decorators.PydanticDescriptorProxy(res, dec_info, shim=wrap)", "target": "def test_uuid_wrap_json():\n    schema = core_schema.no_info_wrap_validator_function(lambda v, handler: handler(v), core_schema.uuid_schema())\n    v = SchemaValidator(schema)\n    assert v.validate_python(UUID('a6cc5730-2261-11ee-9c43-2eb5a363657c'), strict=True) == UUID(\n        'a6cc5730-2261-11ee-9c43-2eb5a363657c'\n    )\n    assert v.validate_json('\"a6cc5730-2261-11ee-9c43-2eb5a363657c\"', strict=True) == UUID(\n        'a6cc5730-2261-11ee-9c43-2eb5a363657c'\n    )", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "001723", "source": "def test_mean(self):\n            img_path = self.find_file('cv/face/david2.jpg', [os.environ.get('OPENCV_TEST_DATA_PATH')])\n            in_mat = cv.imread(img_path)\n            expected = cv.mean(in_mat)\n            g_in = cv.GMat()\n            g_out = cv.gapi.mean(g_in)\n            comp = cv.GComputation(g_in, g_out)\n            for pkg_name, pkg in pkgs:\n                actual = comp.apply(cv.gin(in_mat), args=cv.gapi.compile_args(pkg))\n                self.assertEqual(0.0, cv.norm(expected, actual, cv.NORM_INF),\n                                 'Failed on ' + pkg_name + ' backend')", "target": "def f(value, serializer, _info):\n        return f'result={serializer(len(value))} repr={serializer!r}'", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001724", "source": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "target": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "001725", "source": "def execute(cmd, cwd = None):\n    print(\"Executing: %s in %s\" % (cmd, cwd), file=sys.stderr)\n    print('Executing: ' + ' '.join(cmd))\n    retcode = check_call(cmd, cwd = cwd)\n    if retcode != 0:\n        raise Exception(\"Child returned:\", retcode)", "target": "def test_core_schema_import_missing():\n    with pytest.raises(AttributeError, match=\"module 'pydantic_core' has no attribute 'foobar'\"):\n        core_schema.foobar", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001726", "source": "def test_typed_dict(extra_behavior_kw: dict[str, Any]):\n    v = SchemaSerializer(\n        core_schema.typed_dict_schema(\n            {\n                'foo': core_schema.typed_dict_field(core_schema.int_schema()),\n                'bar': core_schema.typed_dict_field(core_schema.bytes_schema()),\n            },\n            **extra_behavior_kw,\n        )\n    )\n    assert v.to_python({'foo': 1, 'bar': b'more'}) == IsStrictDict(foo=1, bar=b'more')\n    assert v.to_python({'bar': b'more', 'foo': 1}) == IsStrictDict(bar=b'more', foo=1)\n    assert v.to_python({'foo': 1, 'bar': b'more', 'c': 3}) == IsStrictDict(foo=1, bar=b'more')\n    assert v.to_python({'bar': b'more', 'foo': 1, 'c': 3}, mode='json') == IsStrictDict(bar='more', foo=1)\n    assert v.to_json({'bar': b'more', 'foo': 1, 'c': 3}) == b'{\"bar\":\"more\",\"foo\":1}'", "target": "def test_typed_dict():\n    v = SchemaValidator(\n        core_schema.typed_dict_schema(\n            fields={\n                'field_a': core_schema.typed_dict_field(schema=core_schema.str_schema()),\n                'field_b': core_schema.typed_dict_field(schema=core_schema.int_schema()),\n            }\n        )\n    )\n    input_str = '{\"field_a\": \"abc\", \"field_b\": 1}'\n    assert v.validate_json(input_str) == {'field_a': 'abc', 'field_b': 1}\n    input_str = '{\"field_a\": \"a\", \"field_a\": \"b\", \"field_b\": 1}'\n    assert v.validate_json(input_str) == {'field_a': 'b', 'field_b': 1}\n    assert v.validate_json(input_str) == {'field_a': 'b', 'field_b': 1}", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001727", "source": "def get_shapes(self) -> tuple[tuple[int, ...], ...]:\n        return (\n            (32768, 256),\n            (32768, 512),\n            (32768, 1024),\n            (32768, 2048),\n            (32768, 4096),\n            (32768, 8192),\n            (32768, 16384),\n            (32768, 32768),\n            (32768, 65536),\n            (16384, 131072),\n            (8192, 262144),\n        )", "target": "def get_shapes(self) -> tuple[tuple[int, ...], ...]:\n        return (\n            (32768, 256),\n            (32768, 512),\n            (32768, 1024),\n            (32768, 2048),\n            (32768, 4096),\n            (32768, 8192),\n            (32768, 16384),\n            (32768, 32768),\n            (32768, 65536),\n            (16384, 131072),\n            (8192, 262144),\n        )", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001728", "source": "def test_union_decimal_simple(py_and_json: PyAndJson):\n    v = py_and_json({'type': 'union', 'choices': [{'type': 'decimal'}, {'type': 'list'}]})\n    assert v.validate_test('5') == 5\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_test('xxx')\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'decimal_parsing', 'loc': ('decimal',), 'msg': 'Input should be a valid decimal', 'input': 'xxx'},\n        {\n            'type': 'list_type',\n            'loc': ('list[any]',),\n            'msg': IsStr(regex='Input should be a valid (list|array)'),\n            'input': 'xxx',\n        },\n    ]", "target": "def error_untyped_fields(api: SemanticAnalyzerPluginInterface, context: Context) -> None:\n    api.fail('Untyped fields disallowed', context, code=ERROR_UNTYPED)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "001729", "source": "def tupleIntSumReturnTuple(\n        self, input: tuple[int, int, int]\n    ) -> tuple[tuple[int, int, int], int]:\n        sum = 0\n        for x in input:\n            sum += x\n        return (input, sum)", "target": "def test_dataclass_post_init():\n    @dataclasses.dataclass\n    class Foo:\n        a: str\n        b: bool\n        def __post_init__(self):\n            self.a = self.a.upper()\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n            ],\n        ),\n        ['a', 'b'],\n        post_init=True,\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 'hello', 'b': True})\n    assert foo.a == 'HELLO'\n    assert foo.b is True", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001730", "source": "def jacfwd(model, inp, strict=None):\n        return functional.jacobian(\n            model, inp, strict=False, vectorize=True, strategy=\"forward-mode\"\n        )", "target": "def jacfwd(model, inp, v=None, strict=None):\n        argnums = tuple(range(len(inp)))\n        return ft.jacfwd(model, argnums)(*inp)", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|opencv/opencv", "data_split": "train", "example_id": "001731", "source": "def build_test(self):\n        execute([\"make\", \"-j\", str(multiprocessing.cpu_count()), \"opencv_js_test\"])", "target": "def test_vector_fast_return(self):\n        expected_shape = (5, 4)\n        rects = cv.utils.generateVectorOfRect(expected_shape[0])\n        self.assertTrue(isinstance(rects, np.ndarray),\n                        \"Vector of rectangles should be returned as numpy array. Got: {}\".format(type(rects)))\n        self.assertEqual(rects.dtype, np.int32, \"Vector of rectangles has wrong elements type\")\n        self.assertEqual(rects.shape, expected_shape, \"Vector of rectangles has wrong shape\")\n        empty_rects = cv.utils.generateVectorOfRect(0)\n        self.assertTrue(isinstance(empty_rects, tuple),\n                        \"Empty vector should be returned as empty tuple. Got: {}\".format(type(empty_rects)))\n        self.assertEqual(len(empty_rects), 0, \"Vector of size 0 should be returned as tuple of length 0\")\n        expected_shape = (10,)\n        ints = cv.utils.generateVectorOfInt(expected_shape[0])\n        self.assertTrue(isinstance(ints, np.ndarray),\n                        \"Vector of integers should be returned as numpy array. Got: {}\".format(type(ints)))\n        self.assertEqual(ints.dtype, np.int32, \"Vector of integers has wrong elements type\")\n        self.assertEqual(ints.shape, expected_shape, \"Vector of integers has wrong shape.\")", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "001732", "source": "def datetime_tz_naive(self: _Pipeline[_InT, datetime.datetime]) -> _Pipeline[_InT, datetime.datetime]:\n        return self.constrain(annotated_types.Timezone(None))", "target": "def test_fitline(self):\n        noise = 5\n        n = 200\n        r = 5 / 100.0\n        outn = int(n*r)\n        p0, p1 = (90, 80), (w-90, h-80)\n        line_points = sample_line(p0, p1, n-outn, noise)\n        outliers = np.random.rand(outn, 2) * (w, h)\n        points = np.vstack([line_points, outliers])\n        lines = []\n        for name in dist_func_names:\n            func = getattr(cv, name)\n            vx, vy, cx, cy = cv.fitLine(np.float32(points), func, 0, 0.01, 0.01)\n            line = [float(vx), float(vy), float(cx), float(cy)]\n            lines.append(line)\n        eps = 0.05\n        refVec =  (np.float32(p1) - p0) / cv.norm(np.float32(p1) - p0)\n        for i in range(len(lines)):\n            self.assertLessEqual(cv.norm(refVec - lines[i][0:2], cv.NORM_L2), eps)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001733", "source": "def eager(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, w, dy = args\n        y = self.rms_norm_ref(x, w)\n        return lambda: torch.autograd.grad(\n            y, [x, w], grad_outputs=dy, retain_graph=True\n        )", "target": "def eager(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, w, dy = args\n        y = self.layernorm_ref(x, w)\n        return lambda: torch.autograd.grad(\n            y, [x, w], grad_outputs=dy, retain_graph=True\n        )", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001734", "source": "def func(x, info):\n        calls.append(str(info))\n        return x * 2", "target": "def func(*args: Any) -> Any:\n        calls.append(args)\n        return args[0]", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "001735", "source": "def make_data(self, params):\n        representation, precompute = params\n        if representation == \"dense\":\n            data = _synth_regression_dataset(n_samples=1000000, n_features=100)\n        else:\n            data = _synth_regression_sparse_dataset(\n                n_samples=50000, n_features=5000, density=0.01\n            )\n        return data", "target": "def make_data(self, params):\n        representation, n_jobs = params\n        if representation == \"sparse\":\n            data = _20newsgroups_highdim_dataset()\n        else:\n            data = _20newsgroups_lowdim_dataset()\n        return data", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|scikit-learn/scikit-learn", "data_split": "train", "example_id": "001736", "source": "def test_geometry(self):\n        npt = 100\n        np.random.seed(244)\n        a = np.random.randn(npt,2).astype('float32')*50 + 150\n        be = cv.fitEllipse(a)\n        br = cv.minAreaRect(a)\n        mc, mr = cv.minEnclosingCircle(a)\n        be0 = ((150.2511749267578, 150.77322387695312), (158.024658203125, 197.57696533203125), 37.57804489135742)\n        br0 = ((161.2974090576172, 154.41793823242188), (207.7177734375, 199.2301483154297), 80.83544921875)\n        mc0, mr0 = (160.41790771484375, 144.55152893066406), 136.713500977\n        self.check_close_boxes(be, be0, 5, 15)\n        self.check_close_boxes(br, br0, 5, 15)\n        self.check_close_pairs(mc, mc0, 5)\n        self.assertLessEqual(abs(mr - mr0), 5)", "target": "def plot_batch_times(all_times, n_features, all_batch_sizes, data):\n    plt.figure()\n    plot_results(all_batch_sizes, all_times[\"pca\"], label=\"PCA\")\n    plot_results(all_batch_sizes, all_times[\"ipca\"], label=\"IncrementalPCA\")\n    plt.legend(loc=\"lower left\")\n    plt.suptitle(\n        \"Algorithm runtime vs. batch_size for n_components %i\\n                  LFW,\"\n        \" size %i x %i\" % (n_features, data.shape[0], data.shape[1])\n    )\n    plt.xlabel(\"Batch size\")\n    plt.ylabel(\"Time (seconds)\")", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001737", "source": "def constrain(self: _Pipeline[_InT, _NewOutGt], constraint: annotated_types.Gt) -> _Pipeline[_InT, _NewOutGt]: ...", "target": "def constrain(\n        self: _Pipeline[_InT, _NewOutInterval], constraint: annotated_types.Interval\n    ) -> _Pipeline[_InT, _NewOutInterval]: ...", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001738", "source": "def eager(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, target, dloss = args\n        loss = F.cross_entropy(x, target, reduction=\"none\")\n        return lambda: torch.autograd.grad(\n            loss, x, grad_outputs=dloss, retain_graph=True\n        )", "target": "def eager(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        (x,) = args\n        return lambda: F.softmax(x, dim=-1)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001739", "source": "def _work(self):\n        @torch.compile(fullgraph=True)\n        def f(a):\n            xs = a.tolist()\n            y = 0\n            if self.use_loop:\n                for i in xs:\n                    y += i\n            else:\n                y = sum(xs)\n            return torch.tensor(y)\n        f(self.splits)", "target": "def _work(self):\n        @torch.compile(fullgraph=True)\n        def f(a, b):\n            xs = b.tolist()\n            for x in xs:\n                torch._check(x >= 0)\n                torch._check(x <= self.N)\n            return a.split(xs)\n        f(self.input, self.splits)", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001740", "source": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "target": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001741", "source": "def test_validate_max_digits_and_decimal_places_edge_case() -> None:\n    v = SchemaValidator(cs.decimal_schema(max_digits=34, decimal_places=18))\n    assert v.validate_python(Decimal('9999999999999999.999999999999999999')) == Decimal(\n        '9999999999999999.999999999999999999'\n    )", "target": "def test_constraints_schema_validation_error(constraint: str) -> None:\n    with pytest.raises(SchemaError, match=f\"'{constraint}' must be coercible to a datetime instance\"):\n        SchemaValidator(cs.datetime_schema(**{constraint: 'bad_value'}))", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "001742", "source": "def test_computed_field_exclude_none():\n    @dataclasses.dataclass\n    class Model:\n        width: int\n        height: int\n        @property\n        def area(self) -> int:\n            return self.width * self.height\n        @property\n        def volume(self) -> None:\n            return None\n    s = SchemaSerializer(\n        core_schema.model_schema(\n            Model,\n            core_schema.model_fields_schema(\n                {\n                    'width': core_schema.model_field(core_schema.int_schema()),\n                    'height': core_schema.model_field(core_schema.int_schema()),\n                },\n                computed_fields=[\n                    core_schema.computed_field('area', core_schema.int_schema(), alias='Area'),\n                    core_schema.computed_field('volume', core_schema.int_schema()),\n                ],\n            ),\n        )\n    )\n    assert s.to_python(Model(3, 4), exclude_none=False, by_alias=True) == {\n        'width': 3,\n        'height': 4,\n        'Area': 12,\n        'volume': None,\n    }\n    assert s.to_python(Model(3, 4), exclude_none=True, by_alias=True) == {'width': 3, 'height': 4, 'Area': 12}\n    assert s.to_python(Model(3, 4), mode='json', exclude_none=False, by_alias=True) == {\n        'width': 3,\n        'height': 4,\n        'Area': 12,\n        'volume': None,\n    }\n    assert s.to_python(Model(3, 4), mode='json', exclude_none=True, by_alias=True) == {\n        'width': 3,\n        'height': 4,\n        'Area': 12,\n    }\n    assert (\n        s.to_json(Model(3, 4), exclude_none=False, by_alias=True) == b'{\"width\":3,\"height\":4,\"Area\":12,\"volume\":null}'\n    )\n    assert s.to_json(Model(3, 4), exclude_none=True, by_alias=True) == b'{\"width\":3,\"height\":4,\"Area\":12}'", "target": "def dump(self, exe):\n        res = ApkInfo()\n        output = self.run([\"dump\", \"xmltree\", exe, \"AndroidManifest.xml\"], silent=True)\n        if not output:\n            raise Err(\"Can not dump manifest from %s\", exe)\n        tags = re.split(r\"[ ]+E: \", output)\n        manifest_tag = [t for t in tags if t.startswith(\"manifest \")]\n        if not manifest_tag:\n            raise Err(\"Can not read package name from: %s\", exe)\n        res.pkg_name = re.search(r\"^[ ]+A: package=\\\"(?P<pkg>.*?)\\\" \\(Raw: \\\"(?P=pkg)\\\"\\)\\r?$\", manifest_tag[0], flags=re.MULTILINE).group(\"pkg\")\n        instrumentation_tag = [t for t in tags if t.startswith(\"instrumentation \")]\n        if not instrumentation_tag:\n            raise Err(\"Can not find instrumentation details in: %s\", exe)\n        res.pkg_runner = re.search(r\"^[ ]+A: android:name\\(0x[0-9a-f]{8}\\)=\\\"(?P<runner>.*?)\\\" \\(Raw: \\\"(?P=runner)\\\"\\)\\r?$\", instrumentation_tag[0], flags=re.MULTILINE).group(\"runner\")\n        res.pkg_target = re.search(r\"^[ ]+A: android:targetPackage\\(0x[0-9a-f]{8}\\)=\\\"(?P<pkg>.*?)\\\" \\(Raw: \\\"(?P=pkg)\\\"\\)\\r?$\", instrumentation_tag[0], flags=re.MULTILINE).group(\"pkg\")\n        if not res.pkg_name or not res.pkg_runner or not res.pkg_target:\n            raise Err(\"Can not find instrumentation details in: %s\", exe)\n        return res", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "001743", "source": "def print_outlier_ratio(y):\n    uniq, cnt = np.unique(y, return_counts=True)\n    print(\"----- Target count values: \")\n    for u, c in zip(uniq, cnt):\n        print(\"------ %s -> %d occurrences\" % (str(u), c))\n    print(\"----- Outlier ratio: %.5f\" % (np.min(cnt) / len(y)))", "target": "def print_outlier_ratio(y):\n    uniq, cnt = np.unique(y, return_counts=True)\n    print(\"----- Target count values: \")\n    for u, c in zip(uniq, cnt):\n        print(\"------ %s -> %d occurrences\" % (str(u), c))\n    print(\"----- Outlier ratio: %.5f\" % (np.min(cnt) / len(y)))", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001744", "source": "def missing_sentinel_schema(\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> MissingSentinelSchema:\n    return _dict_not_none(\n        type='missing-sentinel',\n        metadata=metadata,\n        serialization=serialization,\n    )", "target": "def f(input_value, validator, info):\n        return validator(input_value, ('4',)) + 2", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001745", "source": "def _prepare_once(self):\n        self.x = torch.randn(4, 4, requires_grad=True)", "target": "def _prepare_once(self) -> None:\n        self.a = torch.ones(10, 10, device=self.device())\n        self.b = torch.torch.ones(10, 10, device=self.device())", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "001746", "source": "def add_foo_args(p: argparse.ArgumentParser) -> None:\n    p.add_argument(\"--x\", type=int, required=True, help=\"x value\")", "target": "def test_function_plain_field_serializer_to_json():\n    class Model(TypedDict):\n        x: int\n    def ser_x(data: Model, v: Any, info: core_schema.FieldSerializationInfo) -> str:\n        assert data['x'] == 1_000\n        return f'{v:_}-{info.field_name}'\n    s = SchemaSerializer(\n        core_schema.typed_dict_schema(\n            {\n                'x': core_schema.typed_dict_field(\n                    core_schema.int_schema(\n                        serialization=core_schema.plain_serializer_function_ser_schema(\n                            ser_x, is_field_serializer=True, info_arg=True\n                        )\n                    )\n                )\n            }\n        )\n    )\n    assert json.loads(s.to_json(Model(x=1000))) == {'x': '1_000-x'}", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "001747", "source": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "target": "def test_bad_default_factory(default_factory, error_message):\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'x': core_schema.model_field(\n                    schema=core_schema.with_default_schema(\n                        schema=core_schema.str_schema(), default_factory=default_factory\n                    )\n                )\n            }\n        )\n    )\n    with pytest.raises(TypeError, match=re.escape(error_message)):\n        v.validate_python({})", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001748", "source": "def benchmark(self):\n        for M, N in self.get_shapes():\n            print(f\"Tensor dimensions: [{M}, {N}]\")\n            torch_dtype = cutlass_torch.dtype(cutlass.BFloat16)\n            x = 0.1 * torch.randn(\n                M, N, device=\"cuda\", dtype=torch_dtype, requires_grad=True\n            )\n            target = torch.randint(0, N, (M,), device=\"cuda\", dtype=torch.int64)\n            dloss = torch.randn(M, device=\"cuda\", dtype=torch.float32)\n            self.benchmark_single_shape(\n                (x, target, dloss), setting=f\"shape: [{M}, {N}]\"\n            )", "target": "def benchmark(self):\n        for M, N in self.get_shapes():\n            print(f\"Tensor dimensions: [{M}, {N}]\")\n            torch_dtype = cutlass_torch.dtype(cutlass.BFloat16)\n            x = torch.randn(M, N, device=\"cuda\", dtype=torch_dtype)\n            w = torch.randn(N, device=\"cuda\", dtype=torch.float32)\n            self.benchmark_single_shape((x, w), setting=f\"shape: [{M}, {N}]\")", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|opencv/opencv", "data_split": "train", "example_id": "001749", "source": "def test_age_gender_infer_planar(self):\n            skip_if_openvino_not_available()\n            root_path  = '/omz_intel_models/intel/age-gender-recognition-retail-0013/FP32/age-gender-recognition-retail-0013'\n            model_path = self.find_file(root_path + '.xml',   [os.environ.get('OPENCV_DNN_TEST_DATA_PATH')], required=False)\n            bin_path   = self.find_file(root_path + '.bin',   [os.environ.get('OPENCV_DNN_TEST_DATA_PATH')], required=False)\n            device_id  = 'CPU'\n            img_path = self.find_file('cv/face/david2.jpg', [os.environ.get('OPENCV_TEST_DATA_PATH')])\n            img = cv.imread(img_path)\n            planar_img = np.transpose(img, (2, 0, 1))\n            planar_img = np.expand_dims(planar_img, 0)\n            def preproc(ppp):\n                ppp.input().tensor().set_element_type(Type.u8)                            \\\n                                    .set_spatial_static_shape(img.shape[0], img.shape[1])\n                ppp.input().preprocess().resize(ResizeAlgorithm.RESIZE_LINEAR)\n            ref = AgeGenderOV(model_path, bin_path, device_id)\n            ref.cfgPrePostProcessing(preproc)\n            ov_age, ov_gender = ref.apply(planar_img)\n            comp = AgeGenderGAPI(model_path, bin_path, device_id)\n            comp.pp.cfgResize(cv.INTER_LINEAR)\n            gapi_age, gapi_gender = comp.apply(planar_img)\n            self.assertEqual(0.0, cv.norm(ov_gender, gapi_gender, cv.NORM_INF))\n            self.assertEqual(0.0, cv.norm(ov_age, gapi_age, cv.NORM_INF))", "target": "def is_numeric(dtype):\n    return np.issubdtype(dtype, np.integer) or np.issubdtype(dtype, np.floating)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "001750", "source": "def nvprof_output_filename(rnns, **params):\n    rnn_tag = \"-\".join(rnns)\n    size_tag = describe_sizes(**params)\n    date_tag = datetime.datetime.now().strftime(\"%m%d%y-%H%M\")\n    return f\"{OUTPUT_DIR}prof_{rnn_tag}_{size_tag}_{date_tag}.nvvp\"", "target": "def test_copy(self):\n        for tz in self.ACDT, self.EST:\n            tz_copy = copy.copy(tz)\n            self.assertEqual(tz_copy, tz)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001751", "source": "def test_core_model_py(benchmark, basic_model_serializer):\n    m = BasicModel(a=1, b=2, c=3, d=4, e=5, f=6, g=7, h=8)\n    assert basic_model_serializer.to_python(m) == {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8}\n    benchmark(basic_model_serializer.to_python, m)", "target": "def test_extra():\n    class MyModel:\n        __slots__ = '__dict__', '__pydantic_fields_set__', '__pydantic_extra__', '__pydantic_private__'\n        field_a: str\n        field_b: int\n    schema = core_schema.model_schema(\n        MyModel,\n        core_schema.model_fields_schema(\n            {\n                'field_a': core_schema.model_field(core_schema.bytes_schema()),\n                'field_b': core_schema.model_field(core_schema.int_schema()),\n            },\n            extra_behavior='allow',\n        ),\n        extra_behavior='allow',\n    )\n    v = SchemaValidator(schema)\n    m = v.validate_python({'field_a': b'test', 'field_b': 12, 'field_c': 'extra'})\n    assert isinstance(m, MyModel)\n    assert m.__dict__ == {'field_a': b'test', 'field_b': 12}\n    assert m.__pydantic_extra__ == {'field_c': 'extra'}\n    assert m.__pydantic_fields_set__ == {'field_a', 'field_b', 'field_c'}\n    s = SchemaSerializer(schema)\n    assert 'mode:ModelExtra' in plain_repr(s)\n    assert 'has_extra:true' in plain_repr(s)\n    assert s.to_python(m) == {'field_a': b'test', 'field_b': 12, 'field_c': 'extra'}\n    assert s.to_python(m, mode='json') == {'field_a': 'test', 'field_b': 12, 'field_c': 'extra'}\n    assert s.to_json(m) == b'{\"field_a\":\"test\",\"field_b\":12,\"field_c\":\"extra\"}'\n    m = v.validate_python({'field_a': b'test', 'field_b': 12, 'field_c': None, 'field_d': [1, 2, 3]})\n    assert isinstance(m, MyModel)\n    assert m.__dict__ == {'field_a': b'test', 'field_b': 12}\n    assert m.__pydantic_extra__ == {'field_c': None, 'field_d': [1, 2, 3]}\n    assert m.__pydantic_fields_set__ == {'field_a', 'field_b', 'field_c', 'field_d'}\n    assert s.to_python(m) == {'field_a': b'test', 'field_b': 12, 'field_c': None, 'field_d': [1, 2, 3]}\n    assert s.to_json(m) == b'{\"field_a\":\"test\",\"field_b\":12,\"field_c\":null,\"field_d\":[1,2,3]}'\n    assert s.to_python(m, exclude_none=True) == {'field_a': b'test', 'field_b': 12, 'field_d': [1, 2, 3]}\n    assert s.to_json(m, exclude_none=True) == b'{\"field_a\":\"test\",\"field_b\":12,\"field_d\":[1,2,3]}'\n    assert s.to_python(m, exclude={'field_c'}) == {'field_a': b'test', 'field_b': 12, 'field_d': [1, 2, 3]}\n    assert s.to_json(m, exclude={'field_c'}) == b'{\"field_a\":\"test\",\"field_b\":12,\"field_d\":[1,2,3]}'\n    assert s.to_python(m, exclude={'field_d': [0]}) == {\n        'field_a': b'test',\n        'field_b': 12,\n        'field_c': None,\n        'field_d': [2, 3],\n    }\n    assert s.to_json(m, exclude={'field_d': [0]}) == b'{\"field_a\":\"test\",\"field_b\":12,\"field_c\":null,\"field_d\":[2,3]}'", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001752", "source": "def validate_b(cls, v: str, info: core_schema.ValidationInfo) -> str:\n            assert v == 'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return 'hello world!'", "target": "def validate_b(\n            cls, v: bytes, nxt: core_schema.ValidatorFunctionWrapHandler, info: core_schema.ValidationInfo\n        ) -> bytes:\n            assert v == b'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return nxt(b'hello world!')", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001753", "source": "def _early_cache_key(cls: type[BaseModel], typevar_values: Any) -> GenericTypesCacheKey:\n    return cls, typevar_values, _union_orderings_key(typevar_values)", "target": "def test_union_cycle(strict: bool):\n    s = SchemaValidator(\n        core_schema.definitions_schema(\n            core_schema.definition_reference_schema('root-schema'),\n            [\n                core_schema.union_schema(\n                    [\n                        core_schema.typed_dict_schema(\n                            {\n                                'foobar': core_schema.typed_dict_field(\n                                    core_schema.list_schema(core_schema.definition_reference_schema('root-schema'))\n                                )\n                            },\n                            strict=strict,\n                        )\n                    ],\n                    auto_collapse=False,\n                    ref='root-schema',\n                )\n            ],\n        )\n    )\n    data = {'foobar': []}\n    data['foobar'].append(data)\n    with pytest.raises(ValidationError) as exc_info:\n        s.validate_python(data)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'recursion_loop',\n            'loc': ('typed-dict', 'foobar', 0),\n            'msg': 'Recursion error - cyclic reference detected',\n            'input': {'foobar': [{'foobar': IsList(length=1)}]},\n        }\n    ]", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001754", "source": "def general_wrap_validator_function(*args, **kwargs):\n    warnings.warn(\n        '`general_wrap_validator_function` is deprecated, use `with_info_wrap_validator_function` instead.',\n        DeprecationWarning,\n    )\n    return with_info_wrap_validator_function(*args, **kwargs)", "target": "def replace_types(type_: Any, type_map: Mapping[TypeVar, Any] | None) -> Any:\n    if not type_map:\n        return type_\n    type_args = get_args(type_)\n    origin_type = get_origin(type_)\n    if typing_objects.is_annotated(origin_type):\n        annotated_type, *annotations = type_args\n        annotated_type = replace_types(annotated_type, type_map)\n        return Annotated[(annotated_type, *annotations)]\n    if type_args:\n        resolved_type_args = tuple(replace_types(arg, type_map) for arg in type_args)\n        if all_identical(type_args, resolved_type_args):\n            return type_\n        if (\n            origin_type is not None\n            and isinstance(type_, _typing_extra.typing_base)\n            and not isinstance(origin_type, _typing_extra.typing_base)\n            and getattr(type_, '_name', None) is not None\n        ):\n            origin_type = getattr(typing, type_._name)\n        assert origin_type is not None\n        if is_union_origin(origin_type):\n            if any(typing_objects.is_any(arg) for arg in resolved_type_args):\n                resolved_type_args = (Any,)\n            resolved_type_args = tuple(\n                arg\n                for arg in resolved_type_args\n                if not (typing_objects.is_noreturn(arg) or typing_objects.is_never(arg))\n            )\n        if sys.version_info >= (3, 10) and origin_type is types.UnionType:\n            return reduce(operator.or_, resolved_type_args)\n        return origin_type[resolved_type_args[0] if len(resolved_type_args) == 1 else resolved_type_args]\n    if not origin_type and is_model_class(type_):\n        parameters = type_.__pydantic_generic_metadata__['parameters']\n        if not parameters:\n            return type_\n        resolved_type_args = tuple(replace_types(t, type_map) for t in parameters)\n        if all_identical(parameters, resolved_type_args):\n            return type_\n        return type_[resolved_type_args]\n    if isinstance(type_, list):\n        resolved_list = [replace_types(element, type_map) for element in type_]\n        if all_identical(type_, resolved_list):\n            return type_\n        return resolved_list\n    return type_map.get(type_, type_)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "001755", "source": "def test_is_instance_invalid(input_cls):\n    with pytest.raises(SchemaError, match=\"SchemaError: 'cls' must be valid as the first argument to 'isinstance'\"):\n        SchemaValidator(cs.is_instance_schema(cls=input_cls))", "target": "def skip_accuracy_checks_large_models_dashboard(self):\n        if self.args.dashboard or self.args.accuracy:\n            return self._accuracy[\"skip\"][\"large_models\"]\n        return set()", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001756", "source": "def liger(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        from liger_kernel.transformers.cross_entropy import LigerCrossEntropyLoss\n        x, target, dloss = args\n        cross_entropy = LigerCrossEntropyLoss(reduction=\"none\")\n        loss = cross_entropy(x, target)\n        return lambda: torch.autograd.grad(\n            loss, x, grad_outputs=dloss, retain_graph=True\n        )", "target": "def liger(self, args, kwargs) -> Any:\n        from liger_kernel.transformers.rms_norm import LigerRMSNorm\n        x, w = args\n        M, N = x.shape\n        liger_rmsnorm = LigerRMSNorm(hidden_size=N, eps=1e-6).cuda()\n        liger_rmsnorm.weight.data.copy_(w)\n        return lambda: liger_rmsnorm(x)", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001757", "source": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "target": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001758", "source": "def benchmark(self):\n        for M, N in self.get_shapes():\n            print(f\"\\n Tensor dimensions: [{M}, {N}]\")\n            torch_dtype = cutlass_torch.dtype(cutlass.BFloat16)\n            x = 0.1 * torch.randn(M, N, device=\"cuda\", dtype=torch_dtype)\n            target = torch.randint(0, N, (M,), device=\"cuda\", dtype=torch.int64)\n            self.benchmark_single_shape((x, target), setting=f\"shape: [{M}, {N}]\")", "target": "def benchmark(self):\n        for M, N in self.get_shapes():\n            print(f\"Tensor dimensions: [{M}, {N}]\")\n            torch_dtype = cutlass_torch.dtype(cutlass.BFloat16)\n            x = 0.1 * torch.randn(M, N, device=\"cuda\", dtype=torch_dtype)\n            self.benchmark_single_shape((x,), setting=f\"shape: [{M}, {N}]\")", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001759", "source": "def test_schema_serializer_containing_config():\n    s = SchemaSerializer(core_schema.timedelta_schema(), config={'ser_json_timedelta': 'float'})\n    s = pickle.loads(pickle.dumps(s))\n    assert s.to_python(timedelta(seconds=4, microseconds=500_000)) == timedelta(seconds=4, microseconds=500_000)\n    assert s.to_python(timedelta(seconds=4, microseconds=500_000), mode='json') == 4.5\n    assert s.to_json(timedelta(seconds=4, microseconds=500_000)) == b'4.5'", "target": "def test_non_partial_typed_dict():\n    v = SchemaValidator(\n        core_schema.typed_dict_schema(\n            {\n                'a': core_schema.typed_dict_field(core_schema.int_schema(gt=10)),\n                'b': core_schema.typed_dict_field(core_schema.int_schema(gt=10), required=True),\n                'c': core_schema.typed_dict_field(core_schema.int_schema(gt=10)),\n            },\n            total=False,\n        )\n    )\n    assert v.validate_python({'a': 11, 'b': '12', 'c': 13}) == snapshot({'a': 11, 'b': 12, 'c': 13})\n    with pytest.raises(ValidationError, match='Input should be greater than 10'):\n        v.validate_python({'a': 11, 'b': '12', 'c': 1})\n    assert v.validate_python({'a': 11, 'b': '12', 'c': 1}, allow_partial=True) == snapshot({'a': 11, 'b': 12})\n    with pytest.raises(ValidationError, match=r'b\\s+Field required'):\n        v.validate_python({'a': 11, 'c': 12}, allow_partial=True)\n    with pytest.raises(ValidationError, match=r'b\\s+Input should be greater than 10'):\n        v.validate_python({'a': 11, 'c': 12, 'b': 1}, allow_partial=True)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|opencv/opencv", "data_split": "train", "example_id": "001760", "source": "def create_branch_and_checkout(self, branch: str) -> None:\n        self._run_git(\"checkout\", \"-b\", branch)", "target": "def process_landmarks(r_x, r_y, r_w, r_h, landmarks):\n    lmrks = landmarks[0]\n    raw_x = lmrks[::2] * r_w + r_x\n    raw_y = lmrks[1::2] * r_h + r_y\n    return np.array([[int(x), int(y)] for x, y in zip(raw_x, raw_y)])", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|scikit-learn/scikit-learn", "data_split": "train", "example_id": "001761", "source": "def test_callable_cases(input_value, expected):\n    v = SchemaValidator(cs.callable_schema())\n    assert v.isinstance_python(input_value) == expected", "target": "def benchmark(estimator, data):\n    gc.collect()\n    print(\"Benching %s\" % estimator)\n    t0 = time()\n    estimator.fit(data)\n    training_time = time() - t0\n    data_t = estimator.transform(data)\n    data_r = estimator.inverse_transform(data_t)\n    reconstruction_error = np.mean(np.abs(data - data_r))\n    return {\"time\": training_time, \"error\": reconstruction_error}", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001762", "source": "def test_on_error_default_factory(self, py_and_json: PyAndJson):\n        v = py_and_json(\n            {\n                'type': 'model-fields',\n                'fields': {\n                    'x': {\n                        'type': 'model-field',\n                        'schema': {\n                            'type': 'default',\n                            'schema': {'type': 'str'},\n                            'on_error': 'default',\n                            'default_factory': lambda: 'pika',\n                        },\n                    }\n                },\n            }\n        )\n        assert v.validate_test({'x': 'foo'}) == ({'x': 'foo'}, None, {'x'})\n        assert v.validate_test({'x': ['foo']}) == ({'x': 'pika'}, None, {'x'})", "target": "def test_on_error_default_factory(self, py_and_json: PyAndJson):\n        v = py_and_json(\n            {\n                'type': 'typed-dict',\n                'fields': {\n                    'x': {\n                        'type': 'typed-dict-field',\n                        'schema': {\n                            'type': 'default',\n                            'schema': {'type': 'str'},\n                            'on_error': 'default',\n                            'default_factory': lambda: 'pika',\n                        },\n                    }\n                },\n            }\n        )\n        assert v.validate_test({'x': 'foo'}) == {'x': 'foo'}\n        assert v.validate_test({'x': ['foo']}) == {'x': 'pika'}", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001763", "source": "def ser_x(self, v: Any, serializer: core_schema.SerializerFunctionWrapHandler, _) -> str:\n            assert self.x == 1_000\n            x = serializer(v)\n            return f'{x:_}'", "target": "def ser_x(data: Model, v: Any, _) -> str:\n        assert data['x'] == 1_000\n        return f'{v:_}'", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001764", "source": "def constrain(self: _Pipeline[_InT, _NewOutGt], constraint: annotated_types.Gt) -> _Pipeline[_InT, _NewOutGt]: ...", "target": "def constrain(self: _Pipeline[_InT, _OutT], constraint: _In) -> _Pipeline[_InT, _OutT]: ...", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001765", "source": "def ser_x(self, v: Any, _) -> str:\n            assert self.x == 1_000\n            return f'{v:_}'", "target": "def ser_x(self, v: Any, _) -> str:\n            assert self.x == 1_000\n            return f'{v:_}'", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001766", "source": "def test_function_positional_tuple():\n    def f(prefix, value, _info):\n        return f'{prefix}{value}'\n    s = SchemaSerializer(\n        {\n            'type': 'tuple',\n            'items_schema': [\n                core_schema.any_schema(\n                    serialization=core_schema.plain_serializer_function_ser_schema(partial(f, 'a'), info_arg=True)\n                ),\n                core_schema.any_schema(\n                    serialization=core_schema.plain_serializer_function_ser_schema(partial(f, 'b'), info_arg=True)\n                ),\n                core_schema.any_schema(\n                    serialization=core_schema.plain_serializer_function_ser_schema(partial(f, 'extra'), info_arg=True)\n                ),\n            ],\n            'variadic_item_index': 2,\n        }\n    )\n    assert s.to_python((1,)) == ('a1',)\n    assert s.to_python((1, 2)) == ('a1', 'b2')\n    assert s.to_python((1, 2, 3)) == ('a1', 'b2', 'extra3')\n    assert s.to_python((1,), mode='json') == ['a1']\n    assert s.to_python((1, 2), mode='json') == ['a1', 'b2']\n    assert s.to_python((1, 2, 3), mode='json') == ['a1', 'b2', 'extra3']\n    assert s.to_json((1,)) == b'[\"a1\"]'\n    assert s.to_json((1, 2)) == b'[\"a1\",\"b2\"]'\n    assert s.to_json((1, 2, 3)) == b'[\"a1\",\"b2\",\"extra3\"]'", "target": "def test_validate_assignment():\n    def f(input_value):\n        input_value.more = 'foobar'\n        return input_value\n    class Model:\n        __slots__ = '__dict__', '__pydantic_fields_set__', '__pydantic_extra__', '__pydantic_private__'\n        field_a: str\n        def __init__(self):\n            self.__pydantic_extra__ = None\n    v = SchemaValidator(\n        core_schema.no_info_after_validator_function(\n            f,\n            core_schema.model_schema(\n                Model, core_schema.model_fields_schema({'field_a': core_schema.model_field(core_schema.str_schema())})\n            ),\n        )\n    )\n    m = v.validate_python({'field_a': 'test'})\n    assert isinstance(m, Model)\n    assert m.field_a == 'test'\n    assert m.__pydantic_fields_set__ == {'field_a'}\n    assert m.__dict__ == {'field_a': 'test', 'more': 'foobar'}\n    assert m.__pydantic_extra__ is None\n    m2 = Model()\n    m2.field_a = 'test'\n    assert v.validate_assignment(m2, 'field_a', b'abc') is m2\n    assert m2.__dict__ == {'field_a': 'abc', 'more': 'foobar'}\n    assert not hasattr(m2, '__pydantic_fields_set__')", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001767", "source": "def gen(error: bool):\n        yield 1\n        yield 2\n        if error:\n            raise RuntimeError('error')\n        yield 3", "target": "def gen(error: bool):\n        yield 1\n        yield 2\n        if error:\n            raise RuntimeError('my error')\n        yield 3", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001768", "source": "def get_output(self, input_blob):\n        self.net.setBlob(\"\", input_blob)\n        self.net.forward()\n        return self.net.getBlob(self.net.getLayerNames()[-1])", "target": "def get_output(self, input_blob):\n        self.net.setInput(input_blob, self.in_blob_name)\n        return self.net.forward(self.out_blob_name)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001769", "source": "def type_var_default_factory() -> None:\n                raise RuntimeError(\n                    'Generic defaultdict cannot be used without a concrete value type or an'\n                    ' explicit default factory, ' + instructions\n                )", "target": "def expand_typevar_from_subtype(self, sub_type: TypeInfo, api: SemanticAnalyzerPluginInterface) -> None:\n        if self.type is not None:\n            with state.strict_optional_set(api.options.strict_optional):\n                self.type = map_type_from_supertype(self.type, sub_type, self.info)", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|pydantic/pydantic", "data_split": "train", "example_id": "001770", "source": "def run(self):\n        lst = nodes.bullet_list()\n        for i in [\"cluster\", \"regressor\", \"classifier\", \"transformer\"]:\n            item = self.make_paragraph_for_estimator_type(i)\n            if item is not None:\n                lst += item\n        return [lst]", "target": "def test_float_no_remainder():\n    v = SchemaValidator(core_schema.int_schema())\n    assert v.validate_json('123.0') == 123", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001771", "source": "def test_naive():\n    v = SchemaValidator(core_schema.datetime_schema(tz_constraint='naive'))\n    value = datetime.now()\n    assert value is v.validate_python(value)\n    assert v.validate_python('2022-06-08T12:13:14') == datetime(2022, 6, 8, 12, 13, 14)\n    value = datetime.now(tz=timezone.utc)\n    with pytest.raises(ValidationError, match=r'Input should not have timezone info \\[type=timezone_naive,'):\n        v.validate_python(value)\n    with pytest.raises(ValidationError, match=r'Input should not have timezone info \\[type=timezone_naive,'):\n        v.validate_python('2022-06-08T12:13:14Z')", "target": "def test_naive():\n    v = SchemaValidator(core_schema.time_schema(tz_constraint='naive'))\n    value = time(12, 13, 15)\n    assert value is v.validate_python(value)\n    assert v.validate_python('12:13:14') == time(12, 13, 14)\n    value = time(12, 13, 15, tzinfo=timezone.utc)\n    with pytest.raises(ValidationError, match=r'Input should not have timezone info'):\n        v.validate_python(value)\n    with pytest.raises(ValidationError, match=r'Input should not have timezone info'):\n        v.validate_python('12:13:14Z')", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001772", "source": "def test_uuid_version(input_value, version, expected):\n    schema = core_schema.uuid_schema()\n    if version is not None:\n        schema = core_schema.uuid_schema(version=version)\n    v = SchemaValidator(schema)\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            v.validate_python(input_value)\n    else:\n        output = v.validate_python(input_value)\n        assert output == expected\n        assert isinstance(output, UUID)", "target": "def test_only_allow_alias(py_and_json) -> None:\n    schema = core_schema.dataclass_schema(\n        BasicDataclass,\n        core_schema.dataclass_args_schema(\n            'BasicDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), validation_alias='FieldA'),\n            ],\n        ),\n        ['a'],\n        config=core_schema.CoreConfig(validate_by_name=False, validate_by_alias=True),\n    )\n    v = py_and_json(schema)\n    assert v.validate_test({'FieldA': 'hello'}) == BasicDataclass(a='hello')\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'a': 'hello'})", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001773", "source": "def getBuildCommand(self, arch, target):\n        buildcmd = [\n            \"xcodebuild\",\n            \"XROS_DEPLOYMENT_TARGET=\" + os.environ['XROS_DEPLOYMENT_TARGET'],\n            \"ARCHS=%s\" % arch,\n            \"-sdk\", target.lower(),\n            \"-configuration\", \"Debug\" if self.debug else \"Release\",\n            \"-parallelizeTargets\",\n            \"-jobs\", str(multiprocessing.cpu_count())\n        ]\n        return buildcmd", "target": "def getBuildCommand(self, arch, target):\n        buildcmd = [\n            \"xcodebuild\",\n            \"MACOSX_DEPLOYMENT_TARGET=\" + os.environ['MACOSX_DEPLOYMENT_TARGET'],\n            \"ARCHS=%s\" % arch,\n            \"-sdk\", \"macosx\" if target == \"Catalyst\" else target.lower(),\n            \"-configuration\", \"Debug\" if self.debug else \"Release\",\n            \"-parallelizeTargets\",\n            \"-jobs\", str(multiprocessing.cpu_count())\n        ]\n        if target == \"Catalyst\":\n            buildcmd.append(\"-destination 'platform=macOS,arch=%s,variant=Mac Catalyst'\" % arch)\n            buildcmd.append(\"-UseModernBuildSystem=YES\")\n            buildcmd.append(\"SKIP_INSTALL=NO\")\n            buildcmd.append(\"BUILD_LIBRARY_FOR_DISTRIBUTION=YES\")\n            buildcmd.append(\"TARGETED_DEVICE_FAMILY=\\\"1,2\\\"\")\n            buildcmd.append(\"SDKROOT=iphoneos\")\n            buildcmd.append(\"SUPPORTS_MAC_CATALYST=YES\")\n        return buildcmd", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001774", "source": "def test_function_wrap_str():\n    def f(input_value, validator, info):\n        return plain_repr(validator)\n    v = SchemaValidator(core_schema.with_info_wrap_validator_function(f, core_schema.str_schema()))\n    assert (\n        v.validate_python('input value')\n        == 'ValidatorCallable(Str(StrValidator{strict:false,coerce_numbers_to_str:false}))'\n    )", "target": "def test_positional_or_keyword_validation_error(py_and_json: PyAndJson, input_value, err_loc) -> None:\n    v = py_and_json(\n        cs.arguments_v3_schema(\n            [\n                cs.arguments_v3_parameter(name='a', schema=cs.int_schema(), mode='positional_or_keyword'),\n            ]\n        )\n    )\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_test(input_value)\n    error = exc_info.value.errors()[0]\n    assert error['type'] == 'int_parsing'\n    assert error['loc'] == err_loc", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001775", "source": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "target": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001776", "source": "def full_typename(self) -> str:\n        return self.typename", "target": "def full_typename(self) -> str:\n        return \"cv2.typing.\" + self.typename", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001777", "source": "def test_extra_behavior_allow(\n    config: Union[core_schema.CoreConfig, None],\n    schema_extra_behavior_kw: dict[str, Any],\n    extras_schema_kw: dict[str, Any],\n    expected_extra_value: Any,\n):\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            {'f': core_schema.model_field(core_schema.str_schema())}, **schema_extra_behavior_kw, **extras_schema_kw\n        ),\n        config=config,\n    )\n    m, model_extra, fields_set = v.validate_python({'f': 'x', 'extra_field': '123'})\n    assert m == {'f': 'x'}\n    assert model_extra == {'extra_field': expected_extra_value}\n    assert fields_set == {'f', 'extra_field'}\n    v.validate_assignment(m, 'f', 'y')\n    assert m == {'f': 'y'}\n    new_m, new_model_extra, new_fields_set = v.validate_assignment({**m, **model_extra}, 'not_f', '123')\n    assert new_m == {'f': 'y'}\n    assert new_model_extra == {'extra_field': expected_extra_value, 'not_f': expected_extra_value}\n    assert new_fields_set == {'not_f'}", "target": "def test_extra_behavior_allow(\n    config: Union[core_schema.CoreConfig, None],\n    schema_extra_behavior_kw: dict[str, Any],\n    extras_schema_kw: dict[str, Any],\n    expected_extra_value: Any,\n):\n    v = SchemaValidator(\n        core_schema.typed_dict_schema(\n            {'f': core_schema.typed_dict_field(core_schema.str_schema())},\n            **schema_extra_behavior_kw,\n            **extras_schema_kw,\n            config=config,\n        )\n    )\n    m: dict[str, Any] = v.validate_python({'f': 'x', 'extra_field': '123'})\n    assert m == {'f': 'x', 'extra_field': expected_extra_value}", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "001778", "source": "def test_long_python():\n    v = SchemaValidator(cs.int_schema())\n    s = v.validate_python('1' * 4_300)\n    assert s == int('1' * 4_300)\n    s = v.validate_python('-' + '1' * 400)\n    assert s == -int('1' * 400)\n    with pytest.raises(ValidationError, match='Input should be a valid integer'):\n        v.validate_python('nan')", "target": "def projectCharucoBoard(board, cameraMatrix, yaw, pitch, distance, imageSize, markerBorder):\n    rvec, tvec = getSyntheticRT(yaw, pitch, distance)\n    img = np.ones(imageSize, np.uint8) * 255\n    for indexMarker in range(len(board.getIds())):\n        img = projectMarker(img, board, indexMarker, cameraMatrix, rvec, tvec, markerBorder)\n    chessboard = projectChessboard(board.getChessboardSize()[0], board.getChessboardSize()[1],\n                                   board.getSquareLength(), imageSize, cameraMatrix, rvec, tvec)\n    chessboard = (chessboard != 0).astype(np.uint8)\n    img = img * chessboard\n    return img, rvec, tvec", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001779", "source": "def gen():\n    yield 1\n    yield 2\n    yield 3", "target": "def gen(error: bool):\n        yield 1\n        yield 2\n        if error:\n            raise RuntimeError('my error')\n        yield 3", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001780", "source": "def test_include_exclude_args(params):\n    s = SchemaSerializer(\n        core_schema.model_schema(\n            BasicModel,\n            core_schema.model_fields_schema(\n                {\n                    'a': core_schema.model_field(core_schema.int_schema()),\n                    'b': core_schema.model_field(core_schema.int_schema()),\n                    'c': core_schema.model_field(core_schema.int_schema()),\n                    'd': core_schema.model_field(core_schema.int_schema()),\n                }\n            ),\n        )\n    )\n    include, exclude, expected = params['include'], params['exclude'], IsStrictDict(params['expected'])\n    value = BasicModel(a=0, b=1, c=2, d=3)\n    assert s.to_python(value, include=include, exclude=exclude) == expected\n    assert s.to_python(value, mode='json', include=include, exclude=exclude) == expected\n    assert json.loads(s.to_json(value, include=include, exclude=exclude)) == expected", "target": "def test_include_exclude_args(params):\n    s = SchemaSerializer(\n        core_schema.typed_dict_schema(\n            {\n                '0': core_schema.typed_dict_field(core_schema.int_schema()),\n                '1': core_schema.typed_dict_field(core_schema.int_schema()),\n                '2': core_schema.typed_dict_field(core_schema.int_schema()),\n                '3': core_schema.typed_dict_field(core_schema.int_schema()),\n            }\n        )\n    )\n    include, exclude, expected = params['include'], params['exclude'], IsStrictDict(params['expected'])\n    value = {'0': 0, '1': 1, '2': 2, '3': 3}\n    assert s.to_python(value, include=include, exclude=exclude) == expected\n    assert s.to_python(value, mode='json', include=include, exclude=exclude) == expected\n    assert json.loads(s.to_json(value, include=include, exclude=exclude)) == expected", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001781", "source": "def quack(self, args, kwargs=None) -> Any:\n        from quack.cross_entropy import cross_entropy\n        assert kwargs is None\n        x, target, dloss = args\n        loss = cross_entropy(x, target)\n        return lambda: torch.autograd.grad(\n            loss, x, grad_outputs=dloss, retain_graph=True\n        )", "target": "def quack(self, args, kwargs) -> Any:\n        from quack.layernorm import layernorm\n        x, w = args\n        return lambda: layernorm(x, w, eps=1e-6)", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001782", "source": "def required_usage_imports(self) -> Generator[str, None, None]:\n        yield \"import numpy\"\n        yield \"import typing as _typing\"", "target": "def required_usage_imports(self) -> Generator[str, None, None]:\n        if TypeNode.compatible_to_runtime_usage:\n            yield \"import typing as _typing\"\n        yield from super().required_usage_imports", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001783", "source": "def outMeta(desc1, desc2, depth):\n            return desc1", "target": "def outMeta(desc):\n            out_desc = desc.withType(desc.depth, 1)\n            return out_desc, out_desc, out_desc", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001784", "source": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "target": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001785", "source": "def f(v: Any, info: core_schema.ValidationInfo) -> Any:\n        calls.append(info.mode)\n        return v", "target": "def test_isinstance():\n    v = SchemaValidator(cs.int_schema())\n    assert v.validate_python(123) == 123\n    assert v.isinstance_python(123) is True\n    assert v.validate_python('123') == 123\n    assert v.isinstance_python('123') is True\n    with pytest.raises(ValidationError, match='Input should be a valid integer'):\n        v.validate_python('foo')\n    assert v.isinstance_python('foo') is False", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001786", "source": "def _prepare_once(self):\n        self.a = torch.ones(1000, device=self.device())\n        self.b = torch.torch.ones(1000, device=self.device())", "target": "def _prepare_once(self):\n        self.m = self.ModuleClass()\n        torch.set_float32_matmul_precision(\"high\")\n        self.input = torch.ones(10, device=self.device())", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001787", "source": "def test_function_wrong_sig():\n    def f(input_value):\n        return input_value + ' Changed'\n    v = SchemaValidator(core_schema.with_info_before_validator_function(f, core_schema.str_schema()))\n    if platform.python_implementation() == 'PyPy':\n        error_message = 'f() takes 1 positional argument but 2 were given'\n    else:\n        error_message = 'f() takes 1 positional argument but 2 were given'\n    with pytest.raises(TypeError, match=re.escape(error_message)):\n        v.validate_python('input value')", "target": "def test_constraints_schema_validation() -> None:\n    val = SchemaValidator(cs.decimal_schema(gt='1'))\n    with pytest.raises(ValidationError):\n        val.validate_python('0')", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001788", "source": "def compiled(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, w, dy = args\n        y = torch.compile(self.rms_norm_ref, mode=self.compile_mode, fullgraph=True)(\n            x, w\n        )\n        return lambda: torch.autograd.grad(\n            y, [x, w], grad_outputs=dy, retain_graph=True\n        )", "target": "def compiled(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, w, dy = args\n        compiled_layernorm = torch.compile(\n            self.layernorm_ref, mode=self.compile_mode, fullgraph=True\n        )\n        y = compiled_layernorm(x, w)\n        return lambda: torch.autograd.grad(\n            y, [x, w], grad_outputs=dy, retain_graph=True\n        )", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001789", "source": "def outMeta(desc):\n                raise NotImplementedError(\"outMeta isn't implemented\")", "target": "def outMeta(desc0, desc1):\n                    raise NotImplementedError(\"outMeta isn't implemented\")", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001790", "source": "def field_name(self) -> str | None:\n        ...", "target": "def field_name(self) -> str | None:\n        return self._generate_schema.field_name_stack.get()", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001791", "source": "def required_usage_imports(self) -> Generator[str, None, None]:\n        yield from ()", "target": "def required_usage_imports(self) -> Generator[str, None, None]:\n        yield \"import typing as _typing\"\n        yield from super().required_usage_imports", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "001792", "source": "def convert_to_jit(gm, gm_args):\n    strip_overloads(gm)\n    try:\n        return torch.jit.script(gm)\n    except Exception:\n        pass\n    return torch.jit.trace(gm, gm_args)", "target": "def test_str_validation_w_strict() -> None:\n    s = SchemaValidator(cs.decimal_schema(strict=True))\n    with pytest.raises(ValidationError):\n        assert s.validate_python('1.23')", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001793", "source": "def test_function_after_config():\n    f_kwargs = None\n    def f(input_value, info):\n        nonlocal f_kwargs\n        f_kwargs = deepcopy_info(info)\n        return input_value + ' Changed'\n    v = SchemaValidator(\n        cs.typed_dict_schema(\n            fields={\n                'test_field': cs.typed_dict_field(\n                    schema={\n                        'type': 'function-after',\n                        'function': {'type': 'with-info', 'function': f, 'field_name': 'test_field'},\n                        'schema': cs.str_schema(),\n                    }\n                )\n            },\n            config=CoreConfig(allow_inf_nan=True),\n        )\n    )\n    assert v.validate_python({'test_field': b'321'}) == {'test_field': '321 Changed'}\n    assert f_kwargs == {'data': {}, 'config': {'allow_inf_nan': True}, 'context': None, 'field_name': 'test_field'}", "target": "def test_alias():\n    s = SchemaSerializer(\n        core_schema.typed_dict_schema(\n            {\n                'cat': core_schema.typed_dict_field(core_schema.int_schema(), serialization_alias='Meow'),\n                'dog': core_schema.typed_dict_field(core_schema.int_schema(), serialization_alias='Woof'),\n                'bird': core_schema.typed_dict_field(core_schema.int_schema()),\n            }\n        )\n    )\n    value = {'cat': 0, 'dog': 1, 'bird': 2}\n    assert s.to_python(value, by_alias=True) == IsStrictDict(Meow=0, Woof=1, bird=2)\n    assert s.to_python(value, exclude={'dog'}, by_alias=True) == IsStrictDict(Meow=0, bird=2)\n    assert s.to_python(value, by_alias=False) == IsStrictDict(cat=0, dog=1, bird=2)\n    assert s.to_python(value, mode='json', by_alias=True) == IsStrictDict(Meow=0, Woof=1, bird=2)\n    assert s.to_python(value, mode='json', include={'cat'}, by_alias=True) == IsStrictDict(Meow=0)\n    assert s.to_python(value, mode='json', by_alias=False) == IsStrictDict(cat=0, dog=1, bird=2)\n    assert json.loads(s.to_json(value, by_alias=True)) == IsStrictDict(Meow=0, Woof=1, bird=2)\n    assert json.loads(s.to_json(value, include={'cat', 'bird'}, by_alias=True)) == IsStrictDict(Meow=0, bird=2)\n    assert json.loads(s.to_json(value, by_alias=False)) == IsStrictDict(cat=0, dog=1, bird=2)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001794", "source": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "target": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "001795", "source": "def test_stateful_multiple_inputs(self):\n            @cv.gapi.kernel(GStatefulSum)\n            class GStatefulSumImpl:\n                @staticmethod\n                def setup(lhs_desc, rhs_desc):\n                    return SumState()\n                @staticmethod\n                def run(lhs, rhs, state):\n                    state.sum+= lhs + rhs\n                    return state.sum\n            g_in1 = cv.GOpaque.Int()\n            g_in2 = cv.GOpaque.Int()\n            g_out = GStatefulSum.on(g_in1, g_in2)\n            comp = cv.GComputation(cv.GIn(g_in1, g_in2), cv.GOut(g_out))\n            pkg  = cv.gapi.kernels(GStatefulSumImpl)\n            lhs_list = [1, 10, 15]\n            rhs_list = [2, 14, 32]\n            ref_out = 0\n            for lhs, rhs in zip(lhs_list, rhs_list):\n                ref_out += lhs + rhs\n                gapi_out = comp.apply(cv.gin(lhs, rhs), cv.gapi.compile_args(pkg))\n                self.assertEqual(ref_out, gapi_out)", "target": "def test_unused_ref():\n    v = SchemaValidator(\n        cs.typed_dict_schema(\n            fields={\n                'name': cs.typed_dict_field(schema=cs.str_schema()),\n                'other': cs.typed_dict_field(schema=cs.int_schema()),\n            },\n            ref='Branch',\n        )\n    )\n    assert v.validate_python({'name': 'root', 'other': '4'}) == {'name': 'root', 'other': 4}", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001796", "source": "def outMeta(desc):\n            out_desc = desc.withType(desc.depth, 1)\n            return out_desc, out_desc, out_desc", "target": "def outMeta(desc, max_corners, quality_lvl,\n                    min_distance, block_sz,\n                    use_harris_detector, k):\n            return cv.empty_array_desc()", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "001797", "source": "def test_multiple_tuple_param(multiple_tuple_schema: SchemaValidator, input_value, expected):\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=expected.message):\n            multiple_tuple_schema.validate_python(input_value)\n    else:\n        assert multiple_tuple_schema.validate_python(input_value) == expected", "target": "def large_channels_last():\n    return (\n        rand(32, 32, 256, 256).to(memory_format=torch.channels_last),\n        rand(32, 32, 256, 256).to(memory_format=torch.channels_last),\n    )", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "001798", "source": "def make_estimator(self, params):\n        representation, solver, n_jobs = params\n        penalty = \"l2\" if solver == \"lbfgs\" else \"l1\"\n        estimator = LogisticRegression(\n            solver=solver,\n            penalty=penalty,\n            tol=0.01,\n            n_jobs=n_jobs,\n            random_state=0,\n        )\n        return estimator", "target": "def make_estimator(self, params):\n        estimator = HistGradientBoostingClassifier(\n            max_iter=100, max_leaf_nodes=15, early_stopping=False, random_state=0\n        )\n        return estimator", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "train", "example_id": "001799", "source": "def benchmark_module(config, module, use_throughput_benchmark=False):\n    if use_throughput_benchmark:\n        return benchmark_using_throughput_benchmark(config, module)\n    module.forward(config.num_warmup_iters)\n    print(f\"Running module for {config.num_iters} iterations\")\n    start = time.time()\n    module.forward(config.num_iters)\n    end = time.time()\n    time_elapsed_s = end - start\n    return secs_to_ms(time_elapsed_s) / config.num_iters / NUM_LOOP_ITERS", "target": "def pick_grad(self, name, is_training):\n        if is_training:\n            return torch.enable_grad()\n        else:\n            return torch.no_grad()", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|opencv/opencv", "data_split": "train", "example_id": "001800", "source": "def pix_to_c(pix):\n        return pix[0] * 256 * 256 + pix[1] * 256 + pix[2]", "target": "def test_garray_type(self):\n            types = [cv.gapi.CV_BOOL   , cv.gapi.CV_INT    , cv.gapi.CV_INT64 , cv.gapi.CV_UINT64,\n                     cv.gapi.CV_DOUBLE , cv.gapi.CV_FLOAT  , cv.gapi.CV_STRING, cv.gapi.CV_POINT ,\n                     cv.gapi.CV_POINT2F, cv.gapi.CV_POINT3F, cv.gapi.CV_SIZE  , cv.gapi.CV_RECT  ,\n                     cv.gapi.CV_SCALAR , cv.gapi.CV_MAT    , cv.gapi.CV_GMAT]\n            for t in types:\n                g_array = cv.GArrayT(t)\n                self.assertEqual(t, g_array.type())", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "001801", "source": "def make_estimator(self, params):\n        (method,) = params\n        estimator = TSNE(random_state=0, method=method)\n        return estimator", "target": "def make_estimator(self, params):\n        estimator = LinearRegression()\n        return estimator", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001802", "source": "def test_by_alias_and_name_config_interaction(\n    config_by_alias: Union[bool, None],\n    config_by_name: Union[bool, None],\n    runtime_by_alias: Union[bool, None],\n    runtime_by_name: Union[bool, None],\n) -> None:\n    if config_by_alias is False and config_by_name is False and runtime_by_alias is False and runtime_by_name is False:\n        pytest.skip(\"Can't have both by_alias and by_name as effectively False\")\n    core_config = {\n        **({'validate_by_alias': config_by_alias} if config_by_alias is not None else {}),\n        **({'validate_by_name': config_by_name} if config_by_name is not None else {}),\n    }\n    @dataclasses.dataclass\n    class MyDataclass:\n        my_field: int\n    schema = core_schema.dataclass_schema(\n        MyDataclass,\n        core_schema.dataclass_args_schema(\n            'MyDataclass',\n            [\n                core_schema.dataclass_field(\n                    name='my_field', schema=core_schema.int_schema(), validation_alias='my_alias'\n                ),\n            ],\n        ),\n        ['my_field'],\n        config=core_schema.CoreConfig(**core_config),\n    )\n    s = SchemaValidator(schema)\n    alias_allowed = next(x for x in (runtime_by_alias, config_by_alias, True) if x is not None)\n    name_allowed = next(x for x in (runtime_by_name, config_by_name, False) if x is not None)\n    if alias_allowed:\n        assert dataclasses.asdict(\n            s.validate_python({'my_alias': 1}, by_alias=runtime_by_alias, by_name=runtime_by_name)\n        ) == {'my_field': 1}\n    if name_allowed:\n        assert dataclasses.asdict(\n            s.validate_python({'my_field': 1}, by_alias=runtime_by_alias, by_name=runtime_by_name)\n        ) == {'my_field': 1}", "target": "def test_by_alias_and_name_config_interaction(\n    config_by_alias: Union[bool, None],\n    config_by_name: Union[bool, None],\n    runtime_by_alias: Union[bool, None],\n    runtime_by_name: Union[bool, None],\n) -> None:\n    if config_by_alias is False and config_by_name is False and runtime_by_alias is False and runtime_by_name is False:\n        pytest.skip(\"Can't have both by_alias and by_name as effectively False\")\n    class Model:\n        def __init__(self, my_field: int) -> None:\n            self.my_field = my_field\n    core_config = {\n        **({'validate_by_alias': config_by_alias} if config_by_alias is not None else {}),\n        **({'validate_by_name': config_by_name} if config_by_name is not None else {}),\n    }\n    schema = core_schema.model_schema(\n        Model,\n        core_schema.model_fields_schema(\n            {\n                'my_field': core_schema.model_field(core_schema.int_schema(), validation_alias='my_alias'),\n            }\n        ),\n        config=core_schema.CoreConfig(**core_config),\n    )\n    s = SchemaValidator(schema)\n    alias_allowed = next(x for x in (runtime_by_alias, config_by_alias, True) if x is not None)\n    name_allowed = next(x for x in (runtime_by_name, config_by_name, False) if x is not None)\n    if alias_allowed:\n        assert s.validate_python({'my_alias': 1}, by_alias=runtime_by_alias, by_name=runtime_by_name).my_field == 1\n    if name_allowed:\n        assert s.validate_python({'my_field': 1}, by_alias=runtime_by_alias, by_name=runtime_by_name).my_field == 1", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|scikit-learn/scikit-learn", "data_split": "train", "example_id": "001803", "source": "def complete_dataclass(\n    cls: type[Any],\n    config_wrapper: _config.ConfigWrapper,\n    *,\n    raise_errors: bool = True,\n    ns_resolver: NsResolver | None = None,\n    _force_build: bool = False,\n) -> bool:\n    original_init = cls.__init__\n    def __init__(__dataclass_self__: PydanticDataclass, *args: Any, **kwargs: Any) -> None:\n        __tracebackhide__ = True\n        s = __dataclass_self__\n        s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)\n    __init__.__qualname__ = f'{cls.__qualname__}.__init__'\n    cls.__init__ = __init__\n    cls.__pydantic_config__ = config_wrapper.config_dict\n    set_dataclass_fields(cls, config_wrapper=config_wrapper, ns_resolver=ns_resolver)\n    if not _force_build and config_wrapper.defer_build:\n        set_dataclass_mocks(cls)\n        return False\n    if hasattr(cls, '__post_init_post_parse__'):\n        warnings.warn(\n            'Support for `__post_init_post_parse__` has been dropped, the method will not be called',\n            PydanticDeprecatedSince20,\n        )\n    typevars_map = get_standard_typevars_map(cls)\n    gen_schema = GenerateSchema(\n        config_wrapper,\n        ns_resolver=ns_resolver,\n        typevars_map=typevars_map,\n    )\n    cls.__signature__ = LazyClassAttribute(\n        '__signature__',\n        partial(\n            generate_pydantic_signature,\n            init=original_init,\n            fields=cls.__pydantic_fields__,\n            validate_by_name=config_wrapper.validate_by_name,\n            extra=config_wrapper.extra,\n            is_dataclass=True,\n        ),\n    )\n    try:\n        schema = gen_schema.generate_schema(cls)\n    except PydanticUndefinedAnnotation as e:\n        if raise_errors:\n            raise\n        set_dataclass_mocks(cls, f'`{e.name}`')\n        return False\n    core_config = config_wrapper.core_config(title=cls.__name__)\n    try:\n        schema = gen_schema.clean_schema(schema)\n    except InvalidSchemaError:\n        set_dataclass_mocks(cls)\n        return False\n    cls = cast('type[PydanticDataclass]', cls)\n    cls.__pydantic_core_schema__ = schema\n    cls.__pydantic_validator__ = create_schema_validator(\n        schema, cls, cls.__module__, cls.__qualname__, 'dataclass', core_config, config_wrapper.plugin_settings\n    )\n    cls.__pydantic_serializer__ = SchemaSerializer(schema, core_config)\n    cls.__pydantic_complete__ = True\n    return True", "target": "def print_outlier_ratio(y):\n    uniq, cnt = np.unique(y, return_counts=True)\n    print(\"----- Target count values: \")\n    for u, c in zip(uniq, cnt):\n        print(\"------ %s -> %d occurrences\" % (str(u), c))\n    print(\"----- Outlier ratio: %.5f\" % (np.min(cnt) / len(y)))", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "001804", "source": "def make_data(self, params):\n        (representation,) = params\n        if representation == \"dense\":\n            data = _synth_regression_dataset(n_samples=100000, n_features=200)\n        else:\n            data = _synth_regression_sparse_dataset(\n                n_samples=100000, n_features=1000, density=0.01\n            )\n        return data", "target": "def make_data(self, params):\n        data = _synth_classification_dataset(\n            n_samples=10000, n_features=100, n_classes=5\n        )\n        return data", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001805", "source": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "target": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|opencv/opencv", "data_split": "train", "example_id": "001806", "source": "def _merge_additional_failures_files(\n    source_pytest_cache: Path, dest_pytest_cache: Path\n) -> None:\n    source_lastfailed_file = (\n        source_pytest_cache / TD_HEURISTIC_PREVIOUSLY_FAILED_ADDITIONAL\n    )\n    dest_lastfailed_file = dest_pytest_cache / TD_HEURISTIC_PREVIOUSLY_FAILED_ADDITIONAL\n    if not source_lastfailed_file.exists():\n        return\n    if not dest_lastfailed_file.exists():\n        copy_file(source_lastfailed_file, dest_lastfailed_file)\n        return\n    from_lastfailed = load_json_file(source_lastfailed_file)\n    to_lastfailed = load_json_file(dest_lastfailed_file)\n    merged_content = list(set(from_lastfailed + to_lastfailed))\n    write_json_file(dest_lastfailed_file, merged_content)", "target": "def find_dnn_file(self, filename):\n            return self.find_file(filename, [os.environ.get('OPENCV_GAPI_ONNX_MODEL_PATH')], False)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "001807", "source": "def test_overload_resolution_can_choose_correct_overload(self):\n        val = 123\n        point = (51, 165)\n        self.assertEqual(cv.utils.testOverloadResolution(val, point),\n                         'overload (int={}, point=(x={}, y={}))'.format(val, *point),\n                         \"Can't select first overload if all arguments are provided as positional\")\n        self.assertEqual(cv.utils.testOverloadResolution(val, point=point),\n                         'overload (int={}, point=(x={}, y={}))'.format(val, *point),\n                         \"Can't select first overload if one of the arguments are provided as keyword\")\n        self.assertEqual(cv.utils.testOverloadResolution(val),\n                         'overload (int={}, point=(x=42, y=24))'.format(val),\n                         \"Can't select first overload if one of the arguments has default value\")\n        rect = (1, 5, 10, 23)\n        self.assertEqual(cv.utils.testOverloadResolution(rect),\n                         'overload (rect=(x={}, y={}, w={}, h={}))'.format(*rect),\n                         \"Can't select second overload if all arguments are provided\")", "target": "def test_list_function_val_error():\n    def f(input_value, info):\n        raise ValueError(f'error {input_value}')\n    v = SchemaValidator(cs.list_schema(items_schema=core_schema.with_info_plain_validator_function(f)))\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python([1, 2])\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'value_error',\n            'loc': (0,),\n            'msg': 'Value error, error 1',\n            'input': 1,\n            'ctx': {'error': HasRepr(repr(ValueError('error 1')))},\n        },\n        {\n            'type': 'value_error',\n            'loc': (1,),\n            'msg': 'Value error, error 2',\n            'input': 2,\n            'ctx': {'error': HasRepr(repr(ValueError('error 2')))},\n        },\n    ]", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001808", "source": "def test_extra_custom_serializer():\n    @dataclasses.dataclass\n    class Model:\n        pass\n    schema = core_schema.dataclass_schema(\n        Model,\n        core_schema.dataclass_args_schema(\n            'Model',\n            [],\n            extra_behavior='allow',\n        ),\n        [],\n    )\n    s = SchemaSerializer(schema)\n    v = SchemaValidator(schema)\n    m = v.validate_python({'extra': 'extra'})\n    assert s.to_python(m) == {'extra': 'extra bam!'}", "target": "def test_extra_custom_serializer():\n    schema = core_schema.typed_dict_schema(\n        {},\n        extra_behavior='allow',\n        extras_schema=core_schema.any_schema(\n            serialization=core_schema.plain_serializer_function_ser_schema(lambda v: v + ' bam!')\n        ),\n    )\n    s = SchemaSerializer(schema)\n    m = {'extra': 'extra'}\n    assert s.to_python(m) == {'extra': 'extra bam!'}", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001809", "source": "def required_usage_imports(self) -> Generator[str, None, None]:\n        for item in self:\n            yield from item.required_usage_imports", "target": "def required_usage_imports(self) -> Generator[str, None, None]:\n        yield \"import os\"", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001810", "source": "def get_shapes(self) -> tuple[tuple[int, ...], ...]:\n        return (\n            (32768, 256),\n            (32768, 512),\n            (32768, 1024),\n            (32768, 2048),\n            (32768, 4096),\n            (32768, 8192),\n            (32768, 16384),\n            (32768, 32768),\n            (32768, 65536),\n            (16384, 131072),\n            (8192, 262144),\n        )", "target": "def get_shapes(self) -> tuple[tuple[int, ...], ...]:\n        return (\n            (32768, 256),\n            (32768, 512),\n            (32768, 1024),\n            (32768, 2048),\n            (32768, 4096),\n            (32768, 8192),\n            (32768, 16384),\n            (32768, 32768),\n            (32768, 65536),\n            (16384, 131072),\n            (8192, 262144),\n        )", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001811", "source": "def test_union():\n    v = SchemaValidator(cs.union_schema(choices=[cs.str_schema(), cs.datetime_schema()]))\n    assert v.validate_python('2022-01-02T00:00') == '2022-01-02T00:00'\n    assert v.validate_python(datetime(2022, 1, 2)) == datetime(2022, 1, 2)\n    v = SchemaValidator(cs.union_schema(choices=[cs.datetime_schema(), cs.str_schema()]))\n    assert v.validate_python('2022-01-02T00:00') == '2022-01-02T00:00'\n    assert v.validate_python(datetime(2022, 1, 2)) == datetime(2022, 1, 2)", "target": "def test_union():\n    v = SchemaValidator(core_schema.union_schema(choices=[core_schema.str_schema(), core_schema.timedelta_schema()]))\n    assert v.validate_python('P2DT1H') == 'P2DT1H'\n    assert v.validate_python(timedelta(days=2, hours=1)) == timedelta(days=2, hours=1)\n    v = SchemaValidator(core_schema.union_schema(choices=[core_schema.timedelta_schema(), core_schema.str_schema()]))\n    assert v.validate_python('P2DT1H') == 'P2DT1H'\n    assert v.validate_python(timedelta(days=2, hours=1)) == timedelta(days=2, hours=1)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "train", "example_id": "001812", "source": "def main() -> None:\n    args = parse_args()\n    repo = GitRepo(get_git_repo_dir(), get_git_remote_name())\n    org, project = repo.gh_owner_and_name()\n    pr = GitHubPR(org, project, args.pr_num)\n    try:\n        if not has_required_labels(pr):\n            print(LABEL_ERR_MSG, flush=True)\n            add_label_err_comment(pr)\n            if args.exit_non_zero:\n                raise RuntimeError(\"PR does not have required labels\")\n        else:\n            delete_all_label_err_comments(pr)\n    except Exception as e:\n        if args.exit_non_zero:\n            raise RuntimeError(f\"Error checking labels: {e}\") from e\n    sys.exit(0)", "target": "def name(self) -> str:\n        prefix = f\"{self.category()}\"\n        return prefix", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "001813", "source": "def make_estimator(self, params):\n        representation, precompute = params\n        estimator = ElasticNet(precompute=precompute, alpha=0.001, random_state=0)\n        return estimator", "target": "def make_estimator(self, params):\n        representation, precompute = params\n        estimator = Lasso(precompute=precompute, alpha=0.001, random_state=0)\n        return estimator", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001814", "source": "def _prepare_once(self):\n        torch._dynamo.config.capture_scalar_outputs = True\n        torch.manual_seed(0)\n        self.splits = torch.randint(10, (self.N,))", "target": "def _prepare_once(self):\n        torch._dynamo.config.capture_scalar_outputs = True\n        torch.manual_seed(0)\n        self.splits = torch.randint(10, (self.N,))\n        sz = self.splits.sum().item()\n        self.input = torch.randn(sz)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001815", "source": "def test_constraints_schema_validation() -> None:\n    val = SchemaValidator(cs.decimal_schema(gt='1'))\n    with pytest.raises(ValidationError):\n        val.validate_python('0')", "target": "def test_constraints_schema_validation() -> None:\n    val = SchemaValidator(cs.int_schema(gt='1'))\n    with pytest.raises(ValidationError):\n        val.validate_python('0')", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "001816", "source": "def test_any_python(input_value, expected):\n    v = SchemaValidator(core_schema.json_schema())\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            v.validate_python(input_value)\n    else:\n        assert v.validate_python(input_value) == expected", "target": "def check_dir(d, create=False, clean=False):\n    d = os.path.abspath(d)\n    log.info(\"Check dir %s (create: %s, clean: %s)\", d, create, clean)\n    if os.path.exists(d):\n        if not os.path.isdir(d):\n            raise Fail(\"Not a directory: %s\" % d)\n        if clean:\n            for x in glob.glob(os.path.join(d, \"*\")):\n                rm_one(x)\n    else:\n        if create:\n            os.makedirs(d)\n    return d", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001817", "source": "def liger(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        from liger_kernel.transformers.cross_entropy import LigerCrossEntropyLoss\n        x, target, dloss = args\n        cross_entropy = LigerCrossEntropyLoss(reduction=\"none\")\n        loss = cross_entropy(x, target)\n        return lambda: torch.autograd.grad(\n            loss, x, grad_outputs=dloss, retain_graph=True\n        )", "target": "def liger(self, args, kwargs) -> Any:\n        from liger_kernel.transformers.layer_norm import LigerLayerNorm\n        x, w = args\n        M, N = x.shape\n        liger_layernorm = LigerLayerNorm(hidden_size=N, eps=1e-6).cuda()\n        liger_layernorm.weight.data.copy_(w)\n        liger_layernorm.bias.data.copy_(\n            torch.zeros(N, device=\"cuda\", dtype=torch.float32)\n        )\n        return lambda: liger_layernorm(x)", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001818", "source": "def full_typename(self) -> str:\n        return \"cv2.typing.\" + self.typename", "target": "def full_typename(self) -> str:\n        return \"cv2.typing.\" + self.typename", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001819", "source": "def test_only_allow_alias(py_and_json) -> None:\n    v = py_and_json(\n        {\n            'type': 'model-fields',\n            'fields': {'field_a': {'validation_alias': 'FieldA', 'type': 'model-field', 'schema': {'type': 'int'}}},\n        },\n        config=CoreConfig(validate_by_name=False, validate_by_alias=True),\n    )\n    assert v.validate_test({'FieldA': '123'}) == ({'field_a': 123}, None, {'field_a'})\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'field_a': '123'})", "target": "def test_only_allow_alias(py_and_json) -> None:\n    v = py_and_json(\n        {\n            'type': 'typed-dict',\n            'fields': {\n                'field_a': {'validation_alias': 'FieldA', 'type': 'typed-dict-field', 'schema': {'type': 'int'}}\n            },\n            'config': {'validate_by_name': False, 'validate_by_alias': True},\n        }\n    )\n    assert v.validate_test({'FieldA': '123'}) == {'field_a': 123}\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'field_a': '123'})", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pytorch/pytorch", "data_split": "train", "example_id": "001820", "source": "def test_MissingFileException(self):\n        try:\n            _res = cv.samples.findFile('non_existed.file', True)\n            self.assertEqual(\"Dead code\", 0)\n        except cv.error as _e:\n            pass", "target": "def relu(a):\n    return (3 * a).relu()", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001821", "source": "def test_empty_string_aliases(py_and_json: PyAndJson):\n    v = py_and_json(\n        {\n            'type': 'model-fields',\n            'fields': {'field_a': {'validation_alias': '', 'type': 'model-field', 'schema': {'type': 'int'}}},\n        }\n    )\n    assert v.validate_test({'': 123}) == ({'field_a': 123}, None, {'field_a'})\n    v = py_and_json(\n        {\n            'type': 'model-fields',\n            'fields': {'field_a': {'validation_alias': ['', ''], 'type': 'model-field', 'schema': {'type': 'int'}}},\n        }\n    )\n    assert v.validate_test({'': {'': 123}}) == ({'field_a': 123}, None, {'field_a'})", "target": "def test_empty_string_aliases(py_and_json: PyAndJson):\n    v = py_and_json(\n        {\n            'type': 'typed-dict',\n            'fields': {'field_a': {'validation_alias': '', 'type': 'typed-dict-field', 'schema': {'type': 'int'}}},\n        }\n    )\n    assert v.validate_test({'': 123}) == {'field_a': 123}\n    v = py_and_json(\n        {\n            'type': 'typed-dict',\n            'fields': {\n                'field_a': {'validation_alias': ['', ''], 'type': 'typed-dict-field', 'schema': {'type': 'int'}}\n            },\n        }\n    )\n    assert v.validate_test({'': {'': 123}}) == {'field_a': 123}", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001822", "source": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "target": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001823", "source": "def benchmark(self):\n        for M, N in self.get_shapes():\n            print(f\"Tensor dimensions: [{M}, {N}]\")\n            torch_dtype = cutlass_torch.dtype(cutlass.BFloat16)\n            x = 0.1 * torch.randn(\n                M, N, device=\"cuda\", dtype=torch_dtype, requires_grad=True\n            )\n            target = torch.randint(0, N, (M,), device=\"cuda\", dtype=torch.int64)\n            dloss = torch.randn(M, device=\"cuda\", dtype=torch.float32)\n            self.benchmark_single_shape(\n                (x, target, dloss), setting=f\"shape: [{M}, {N}]\"\n            )", "target": "def benchmark(self):\n        for M, N in self.get_shapes():\n            print(f\"Tensor dimensions: [{M}, {N}]\")\n            torch_dtype = cutlass_torch.dtype(cutlass.BFloat16)\n            x = torch.randn(M, N, device=\"cuda\", dtype=torch_dtype)\n            w = torch.randn(N, device=\"cuda\", dtype=torch.float32)\n            self.benchmark_single_shape((x, w), setting=f\"shape: [{M}, {N}]\")", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001824", "source": "def name(self):\n        prefix = f\"{self.category()}_{self.mode()}\"\n        if self._subclass:\n            prefix += \"_subclass\"\n        else:\n            prefix += \"_nosubclass\"\n        if self.device() == \"cpu\":\n            prefix += \"_cpu\"\n        return prefix", "target": "def name(self) -> str:\n        prefix = f\"{self.category()}_{self.backend()}\"\n        if self.is_dynamic():\n            prefix += \"_dynamic\"\n        if self.device() == \"cuda\":\n            prefix += \"_gpu\"\n        return prefix", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "001825", "source": "def test_dict_keys():\n    def fmt(value, _info):\n        return f'<{value}>'\n    s = SchemaSerializer(\n        core_schema.dict_schema(\n            core_schema.int_schema(serialization=core_schema.plain_serializer_function_ser_schema(fmt, info_arg=True))\n        )\n    )\n    assert s.to_python({1: True}) == {'<1>': True}", "target": "def test_cuda_runtime_errors_captured() -> None:\n    cuda_exception_missed = True\n    try:\n        print(\"Testing test_cuda_runtime_errors_captured\")\n        torch._assert_async(torch.tensor(0, device=\"cuda\"))\n        torch._assert_async(torch.tensor(0 + 0j, device=\"cuda\"))\n    except RuntimeError as e:\n        if re.search(\"CUDA\", f\"{e}\"):\n            print(f\"Caught CUDA exception with success: {e}\")\n            cuda_exception_missed = False\n        else:\n            raise e\n    if cuda_exception_missed:\n        raise RuntimeError(\"Expected CUDA RuntimeError but have not received!\")", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|opencv/opencv", "data_split": "train", "example_id": "001826", "source": "def resolve(self, root: ASTNode):\n        try:\n            self.value.resolve(root)\n        except TypeResolutionError as e:\n            raise TypeResolutionError(\n                'Failed to resolve alias \"{}\" exposed as \"{}\"'.format(\n                    self.ctype_name, self.typename\n                )\n            ) from e", "target": "def test_render_primitives_on_bgr_function(self):\n            expected = np.zeros(self.size, dtype=np.uint8)\n            actual = np.array(expected, copy=True)\n            self.render_primitives_bgr_ref(expected)\n            cv.gapi.wip.draw.render(actual, self.prims)\n            self.assertEqual(0.0, cv.norm(expected, actual, cv.NORM_INF))", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001827", "source": "def liger(self, args, kwargs=None) -> Any:\n        from liger_kernel.transformers.softmax import LigerSoftmax\n        assert kwargs is None\n        (x,) = args\n        softmax = LigerSoftmax().to(\"cuda\")\n        return lambda: softmax(x)", "target": "def liger(self, args, kwargs=None) -> Any:\n        from liger_kernel.transformers.rms_norm import LigerRMSNorm\n        x, w, dy = args\n        M, N = x.shape\n        liger_rmsnorm = LigerRMSNorm(\n            hidden_size=N, eps=1e-6, casting_mode=\"gemma\"\n        ).cuda()\n        liger_rmsnorm.weight.data.copy_(w)\n        y = liger_rmsnorm(x)\n        return lambda: torch.autograd.grad(\n            y, [x, liger_rmsnorm.weight], grad_outputs=dy, retain_graph=True\n        )", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001828", "source": "def enumerations(self) -> Dict[str, EnumerationNode]:\n        return self._children[ASTNodeType.Enumeration]", "target": "def enumerations(self) -> Dict[str, EnumerationNode]:\n        return self._children[ASTNodeType.Enumeration]", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pytorch/pytorch", "data_split": "train", "example_id": "001829", "source": "def write_base64_json(self, fname):\n        fs = cv.FileStorage(fname, cv.FileStorage_WRITE_BASE64)\n        mats = {'normal_2d_mat': self.get_normal_2d_mat(),\n                'normal_nd_mat': self.get_normal_nd_mat(),\n                'empty_2d_mat': self.get_empty_2d_mat(),\n                'random_mat': self.get_random_mat()}\n        for name, mat in mats.items():\n            fs.write(name, mat)\n        fs.release()\n        data = {}\n        with open(fname) as file:\n            data = json.load(file)\n        for name, mat in mats.items():\n            buffer = b''\n            if mat.size != 0:\n                if hasattr(mat, 'tobytes'):\n                    buffer = mat.tobytes()\n                else:\n                    buffer = mat.tostring()\n            self.assertEqual(buffer, self.decode(data[name]['data']))", "target": "def env_bool_field(\n    name: str,\n    default: bool = False,\n):\n    return field(default_factory=lambda: env_bool(name, default))", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001830", "source": "def test_extra_custom_serializer():\n    class Model:\n        __slots__ = ('__pydantic_extra__', '__dict__')\n        __pydantic_extra__: dict[str, Any]\n    schema = core_schema.model_schema(\n        Model,\n        core_schema.model_fields_schema(\n            {},\n            extra_behavior='allow',\n            extras_schema=core_schema.any_schema(\n                serialization=core_schema.plain_serializer_function_ser_schema(lambda v: v + ' bam!')\n            ),\n        ),\n        extra_behavior='allow',\n    )\n    s = SchemaSerializer(schema)\n    m = Model()\n    m.__pydantic_extra__ = {'extra': 'extra'}\n    assert s.to_python(m) == {'extra': 'extra bam!'}", "target": "def test_extra_custom_serializer():\n    schema = core_schema.typed_dict_schema(\n        {},\n        extra_behavior='allow',\n        extras_schema=core_schema.any_schema(\n            serialization=core_schema.plain_serializer_function_ser_schema(lambda v: v + ' bam!')\n        ),\n    )\n    s = SchemaSerializer(schema)\n    m = {'extra': 'extra'}\n    assert s.to_python(m) == {'extra': 'extra bam!'}", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001831", "source": "def test_generator_error():\n    def gen(error: bool):\n        yield 1\n        yield 2\n        if error:\n            raise RuntimeError('error')\n        yield 3\n    v = SchemaValidator(cs.list_schema(items_schema=cs.int_schema()))\n    assert v.validate_python(gen(False)) == [1, 2, 3]\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python(gen(True))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'iteration_error',\n            'loc': (2,),\n            'msg': 'Error iterating over object, error: RuntimeError: error',\n            'input': HasRepr(IsStr(regex='<generator object test_generator_error.<locals>.gen at 0x[0-9a-fA-F]+>')),\n            'ctx': {'error': 'RuntimeError: error'},\n        }\n    ]", "target": "def test_generator_error():\n    def gen(error: bool):\n        yield 1\n        yield 2\n        if error:\n            raise RuntimeError('my error')\n        yield 3\n    v = SchemaValidator(cs.set_schema(items_schema=cs.int_schema()))\n    r = v.validate_python(gen(False))\n    assert r == {1, 2, 3}\n    assert isinstance(r, set)\n    msg = r'Error iterating over object, error: RuntimeError: my error \\[type=iteration_error,'\n    with pytest.raises(ValidationError, match=msg):\n        v.validate_python(gen(True))", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001832", "source": "def get_memory_bytes(self, args, kwargs) -> int:\n        (x,) = args\n        M, N = x.shape\n        return 2 * M * N * x.dtype.itemsize", "target": "def get_memory_bytes(self, args, kwargs) -> int:\n        x, w, dy = args\n        M, N = x.shape\n        return 3 * M * N * x.dtype.itemsize + 2 * N * w.dtype.itemsize", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001833", "source": "def preproc(ppp):\n                ppp.input().model().set_layout(Layout(\"NCHW\"))\n                ppp.input().tensor().set_element_type(Type.u8)                            \\\n                                    .set_spatial_static_shape(img.shape[0], img.shape[1]) \\\n                                    .set_layout(Layout(\"NHWC\"))\n                ppp.input().preprocess().resize(ResizeAlgorithm.RESIZE_LINEAR)", "target": "def preproc(ppp):\n                ppp.input().model().set_layout(Layout(\"NCHW\"))\n                ppp.input().tensor().set_element_type(Type.u8)                              \\\n                                    .set_spatial_static_shape(img1.shape[0], img2.shape[1]) \\\n                                    .set_layout(Layout(\"NHWC\"))\n                ppp.input().preprocess().resize(ResizeAlgorithm.RESIZE_LINEAR)", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001834", "source": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "target": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "001835", "source": "def _hash_files(files):\n    hashes = {file: _hash_file(file) for file in files}\n    return hashes", "target": "def test_float_repr():\n    v = SchemaValidator(cs.float_schema())\n    assert (\n        plain_repr(v)\n        == 'SchemaValidator(title=\"float\",validator=Float(FloatValidator{strict:false,allow_inf_nan:true}),definitions=[],cache_strings=True)'\n    )\n    v = SchemaValidator(cs.float_schema(strict=True))\n    assert (\n        plain_repr(v)\n        == 'SchemaValidator(title=\"float\",validator=Float(FloatValidator{strict:true,allow_inf_nan:true}),definitions=[],cache_strings=True)'\n    )\n    v = SchemaValidator(cs.float_schema(multiple_of=7))\n    assert plain_repr(v).startswith('SchemaValidator(title=\"constrained-float\",validator=ConstrainedFloat(')", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001836", "source": "def constrain(self: _Pipeline[_InT, _NewOutGe], constraint: annotated_types.Ge) -> _Pipeline[_InT, _NewOutGe]: ...", "target": "def constrain(self: _Pipeline[_InT, _OutT], constraint: _Eq) -> _Pipeline[_InT, _OutT]: ...", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001837", "source": "def type_format(self) -> str:\n        return \"\"", "target": "def type_format(self) -> str:\n        return \"_typing.Sequence[{}]\"", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001838", "source": "def ser_x(self, v: Any, _) -> str:\n            assert self.x == 1_000\n            return f'{v:_}'", "target": "def ser_x(data: Model, v: Any, serializer: core_schema.SerializerFunctionWrapHandler) -> str:\n        assert data['x'] == 1_000\n        x = serializer(v)\n        return f'{x:_}'", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001839", "source": "def get_shapes(self) -> tuple[tuple[int, ...], ...]:\n        return (\n            (32768, 256),\n            (32768, 512),\n            (32768, 1024),\n            (32768, 2048),\n            (32768, 4096),\n            (32768, 8192),\n            (32768, 16384),\n            (32768, 32768),\n            (32768, 65536),\n            (16384, 131072),\n            (8192, 262144),\n        )", "target": "def get_shapes(self) -> tuple[tuple[int, ...], ...]:\n        return (\n            (32768, 256),\n            (32768, 512),\n            (32768, 1024),\n            (32768, 2048),\n            (32768, 4096),\n            (32768, 8192),\n            (32768, 16384),\n            (32768, 32768),\n            (32768, 65536),\n            (16384, 131072),\n            (8192, 262144),\n        )", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "001840", "source": "def make_data(self, params):\n        representation, solver, n_jobs = params\n        if Benchmark.data_size == \"large\":\n            if representation == \"sparse\":\n                data = _20newsgroups_highdim_dataset(n_samples=10000)\n            else:\n                data = _20newsgroups_lowdim_dataset(n_components=1e3)\n        else:\n            if representation == \"sparse\":\n                data = _20newsgroups_highdim_dataset(n_samples=2500)\n            else:\n                data = _20newsgroups_lowdim_dataset()\n        return data", "target": "def make_data(self, params):\n        representation, n_jobs = params\n        if representation == \"sparse\":\n            data = _20newsgroups_highdim_dataset()\n        else:\n            data = _20newsgroups_lowdim_dataset()\n        return data", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001841", "source": "def test_function_wrap_field_serializer_to_python():\n    class Model(RootModel):\n        def ser_root(self, v: Any, serializer: core_schema.SerializerFunctionWrapHandler, _) -> str:\n            root = serializer(v)\n            assert self.root == 1_000\n            return f'{root:_}'\n    s = SchemaSerializer(\n        core_schema.model_schema(\n            Model,\n            core_schema.int_schema(\n                serialization=core_schema.wrap_serializer_function_ser_schema(\n                    Model.ser_root, is_field_serializer=True, info_arg=True, schema=core_schema.any_schema()\n                )\n            ),\n            root_model=True,\n        )\n    )\n    assert s.to_python(Model(1000)) == '1_000'", "target": "def test_function_wrap_field_serializer_to_python():\n    @dataclasses.dataclass\n    class Model:\n        x: int\n        def ser_x(self, v: Any, serializer: core_schema.SerializerFunctionWrapHandler, _) -> str:\n            x = serializer(v)\n            assert self.x == 1_000\n            return f'{x:_}'\n    s = SchemaSerializer(\n        core_schema.model_schema(\n            Model,\n            core_schema.model_fields_schema(\n                {\n                    'x': core_schema.model_field(\n                        core_schema.int_schema(\n                            serialization=core_schema.wrap_serializer_function_ser_schema(\n                                Model.ser_x, is_field_serializer=True, info_arg=True, schema=core_schema.any_schema()\n                            )\n                        )\n                    )\n                }\n            ),\n        )\n    )\n    assert s.to_python(Model(x=1000)) == {'x': '1_000'}", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|opencv/opencv", "data_split": "train", "example_id": "001842", "source": "def with_nnc():\n    torch._C._jit_override_can_fuse_on_cpu(True)\n    torch._C._jit_override_can_fuse_on_gpu(True)\n    torch._C._jit_set_texpr_fuser_enabled(True)\n    torch._C._jit_set_nvfuser_enabled(False)\n    torch._C._jit_set_profiling_executor(True)\n    torch._C._jit_set_profiling_mode(True)", "target": "def build_opencvjs(self):\n        execute([\"make\", \"-j\", str(multiprocessing.cpu_count()), \"opencv.js\"])", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001843", "source": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "target": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001844", "source": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "target": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pytorch/pytorch", "data_split": "train", "example_id": "001845", "source": "def parse_args():\n    parser = argparse.ArgumentParser(description='iColor: deep interactive colorization')\n    parser.add_argument('--input', help='Path to image or video. Skip to capture frames from camera')\n    parser.add_argument('--prototxt', help='Path to colorization_deploy_v2.prototxt', required=True)\n    parser.add_argument('--caffemodel', help='Path to colorization_release_v2.caffemodel', required=True)\n    parser.add_argument('--kernel', help='Path to pts_in_hull.npy', required=True)\n    args = parser.parse_args()\n    return args", "target": "def get_nn_runners(*names):\n    return [nn_runners[name] for name in names]", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "001846", "source": "def remove_prefix(v: str):\n        nonlocal validator_called_count\n        validator_called_count += 1\n        if v.startswith('uuid::'):\n            return v[6:]\n        return v", "target": "def test_aruco_detector_refine(self):\n        aruco_params = cv.aruco.DetectorParameters()\n        aruco_dict = cv.aruco.getPredefinedDictionary(cv.aruco.DICT_4X4_250)\n        aruco_detector = cv.aruco.ArucoDetector(aruco_dict, aruco_params)\n        board_size = (3, 4)\n        board = cv.aruco.GridBoard(board_size, 5.0, 1.0, aruco_dict)\n        board_image = board.generateImage((board_size[0]*50, board_size[1]*50), marginSize=10)\n        corners, ids, rejected = aruco_detector.detectMarkers(board_image)\n        self.assertEqual(board_size[0]*board_size[1], len(ids))\n        part_corners, part_ids, part_rejected = corners[:-1], ids[:-1], list(rejected)\n        part_rejected.append(corners[-1])\n        refine_corners, refine_ids, refine_rejected, recovered_ids = aruco_detector.refineDetectedMarkers(board_image, board, part_corners, part_ids, part_rejected)\n        self.assertEqual(board_size[0] * board_size[1], len(refine_ids))\n        self.assertEqual(1, len(recovered_ids))\n        self.assertEqual(ids[-1], refine_ids[-1])\n        self.assertEqual((1, 4, 2), refine_corners[0].shape)\n        np.testing.assert_array_equal(corners, refine_corners)", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "001847", "source": "def make_data(self, params):\n        (representation,) = params\n        if representation == \"dense\":\n            data = _synth_regression_dataset(n_samples=1000000, n_features=100)\n        else:\n            data = _synth_regression_sparse_dataset(\n                n_samples=10000, n_features=100000, density=0.01\n            )\n        return data", "target": "def make_data(self, params):\n        data = _synth_classification_dataset(\n            n_samples=10000, n_features=100, n_classes=5\n        )\n        return data", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|opencv/opencv", "data_split": "train", "example_id": "001848", "source": "def generate_target(self):\n        return torch.tensor([1] * self.batch_size, dtype=torch.long, device=self.device)", "target": "def test_aruco_detector(self):\n        aruco_params = cv.aruco.DetectorParameters()\n        aruco_dict = cv.aruco.getPredefinedDictionary(cv.aruco.DICT_4X4_250)\n        aruco_detector = cv.aruco.ArucoDetector(aruco_dict, aruco_params)\n        id = 2\n        marker_size = 100\n        offset = 10\n        img_marker = cv.aruco.generateImageMarker(aruco_dict, id, marker_size, aruco_params.markerBorderBits)\n        img_marker = np.pad(img_marker, pad_width=offset, mode='constant', constant_values=255)\n        gold_corners = np.array([[offset, offset],[marker_size+offset-1.0,offset],\n                                 [marker_size+offset-1.0,marker_size+offset-1.0],\n                                 [offset, marker_size+offset-1.0]], dtype=np.float32)\n        corners, ids, rejected = aruco_detector.detectMarkers(img_marker)\n        self.assertEqual(1, len(ids))\n        self.assertEqual(id, ids[0])\n        for i in range(0, len(corners)):\n            np.testing.assert_array_equal(gold_corners, corners[i].reshape(4, 2))", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001849", "source": "def f(value, serializer):\n        return f'result={serializer(len(value))} repr={serializer!r}'", "target": "def f(prefix, value, _info):\n        return f'{prefix}{value}'", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001850", "source": "def test_dict(py_and_json: PyAndJson):\n    v = py_and_json({'type': 'dict', 'keys_schema': {'type': 'date'}, 'values_schema': {'type': 'int'}})\n    assert v.validate_test({'2000-01-01': 2, '2000-01-02': 4}) == {date(2000, 1, 1): 2, date(2000, 1, 2): 4}", "target": "def test_dict(py_and_json: PyAndJson):\n    v = py_and_json({'type': 'dict', 'keys_schema': {'type': 'time'}, 'values_schema': {'type': 'int'}})\n    assert v.validate_test({'12:01:01': 2, '12:01:02': 4}) == {time(12, 1, 1): 2, time(12, 1, 2): 4}", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001851", "source": "def f(a, b):\n            z = torch.mm(a, b)\n            for i in range(200):\n                z = torch.mm(z, b)\n            return z", "target": "def f(a):\n            xs = a.tolist()\n            y = 0\n            if self.use_loop:\n                for i in xs:\n                    y += i\n            else:\n                y = sum(xs)\n            return torch.tensor(y)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001852", "source": "def dec(f: Any) -> _decorators.PydanticDescriptorProxy[Any]:\n        if _decorators.is_instance_method_from_sig(f):\n            raise PydanticUserError(\n                '`@validator` cannot be applied to instance methods', code='validator-instance-method'\n            )\n        f = _decorators.ensure_classmethod_based_on_signature(f)\n        wrap = _decorators_v1.make_generic_v1_field_validator\n        validator_wrapper_info = _decorators.ValidatorDecoratorInfo(\n            fields=fields,\n            mode=mode,\n            each_item=each_item,\n            always=always,\n            check_fields=check_fields,\n        )\n        return _decorators.PydanticDescriptorProxy(f, validator_wrapper_info, shim=wrap)", "target": "def dec(f: FieldSerializer) -> _decorators.PydanticDescriptorProxy[Any]:\n        dec_info = _decorators.FieldSerializerDecoratorInfo(\n            fields=fields,\n            mode=mode,\n            return_type=return_type,\n            when_used=when_used,\n            check_fields=check_fields,\n        )\n        return _decorators.PydanticDescriptorProxy(f, dec_info)", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001853", "source": "def create_class_node(root: NamespaceNode, class_info,\n                      namespaces: Sequence[str]) -> ClassNode:\n    symbol_name = SymbolName.parse(class_info.full_original_name, namespaces)\n    scope = find_scope(root, symbol_name)\n    return create_class_node_in_scope(scope, symbol_name, class_info)", "target": "def create_class_node(self, class_info, namespaces):\n            return create_class_node(self.cv_root, class_info, namespaces)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "train", "example_id": "001854", "source": "def get_inputs(\n    config: ExperimentGroupConfig,\n) -> tuple[torch.Tensor, ...]:\n    op_name = config.op_name\n    M, N, K = config.shape\n    batch_size = config.batch_size\n    dtype = config.dtype\n    device = torch.device(\"cuda\")\n    if op_name == \"mm\":\n        A = torch.randn(M, K, dtype=dtype, device=device)\n        B = torch.randn(N, K, dtype=dtype, device=device).t()\n        return A, B\n    elif op_name == \"addmm\":\n        A = torch.randn(M, K, dtype=dtype, device=device)\n        B = torch.randn(N, K, dtype=dtype, device=device).t()\n        C = torch.randn(N, dtype=dtype, device=device)\n        return C, A, B\n    elif op_name == \"bmm\":\n        A = torch.randn(batch_size, M, K, dtype=dtype, device=device)\n        B = torch.randn(batch_size, N, K, dtype=dtype, device=device).permute(0, 2, 1)\n        return A, B\n    elif op_name == \"_scaled_mm\":\n        if dtype != torch.float8_e4m3fn:\n            raise ValueError(f\"_scaled_mm only supports fp8e4m3, got {dtype}\")\n        input_dtype = torch.bfloat16\n        x = torch.randn(M, K, dtype=input_dtype, device=device)\n        w = torch.randn(N, K, dtype=input_dtype, device=device)\n        w_fp8, w_inverse_scale = _quantize_rowwise(w, dtype)\n        w_t_fp8 = w_fp8.t()\n        w_inverse_scale = w_inverse_scale.t()\n        x_fp8, x_inverse_scale = _quantize_rowwise(x, dtype)\n        return (\n            x_fp8,\n            w_t_fp8,\n            x_inverse_scale,\n            w_inverse_scale,\n            None,\n            None,\n            torch.bfloat16,\n            USE_FAST_ACCUM,\n        )\n    else:\n        raise ValueError(f\"Unknown op {op_name}\")", "target": "def description(self):\n        return \"runtime of a compiled add1 op small input\"", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001855", "source": "def test_custom_error_type(py_and_json: PyAndJson):\n    v = py_and_json(core_schema.custom_error_schema(core_schema.int_schema(), 'recursion_loop'))\n    assert v.validate_test(1) == 1\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_test('X')\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'recursion_loop', 'loc': (), 'msg': 'Recursion error - cyclic reference detected', 'input': 'X'}\n    ]", "target": "def test_custom_error_type():\n    v = SchemaValidator(\n        core_schema.union_schema(\n            choices=[core_schema.str_schema(), core_schema.bytes_schema()], custom_error_type='string_type'\n        )\n    )\n    assert v.validate_python('hello') == 'hello'\n    assert v.validate_python(b'hello') == b'hello'\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python(123)\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'string_type', 'loc': (), 'msg': 'Input should be a valid string', 'input': 123}\n    ]", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001856", "source": "def get_field(csv, model_name: str, field: str):\n    try:\n        return csv.loc[csv[\"name\"] == model_name][field].item()\n    except Exception:\n        return None", "target": "def get_field(csv, model_name: str, field: str):\n    try:\n        return csv.loc[csv[\"name\"] == model_name][field].item()\n    except Exception:\n        return None", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001857", "source": "def _prepare_once(self):\n        self.m = self.ModuleClass()\n        torch.set_float32_matmul_precision(\"high\")\n        self.input = torch.ones(10, device=self.device())", "target": "def _prepare_once(self):\n        self.m = self.ModuleClass()\n        torch.set_float32_matmul_precision(\"high\")\n        self.input = torch.ones(10, device=self.device())", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001858", "source": "def required_usage_imports(self) -> Generator[str, None, None]:\n        if TypeNode.compatible_to_runtime_usage:\n            yield \"import typing as _typing\"\n        yield from super().required_usage_imports", "target": "def required_usage_imports(self) -> Generator[str, None, None]:\n        yield \"import os\"", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001859", "source": "def benchmark(self):\n        for M, N in self.get_shapes():\n            print(f\"\\n Tensor dimensions: [{M}, {N}]\")\n            torch_dtype = cutlass_torch.dtype(cutlass.BFloat16)\n            x = 0.1 * torch.randn(M, N, device=\"cuda\", dtype=torch_dtype)\n            target = torch.randint(0, N, (M,), device=\"cuda\", dtype=torch.int64)\n            self.benchmark_single_shape((x, target), setting=f\"shape: [{M}, {N}]\")", "target": "def benchmark(self):\n        for M, N in self.get_shapes():\n            print(f\"Tensor dimensions: [{M}, {N}]\")\n            torch_dtype = cutlass_torch.dtype(cutlass.BFloat16)\n            x = 0.1 * torch.randn(\n                M, N, device=\"cuda\", dtype=torch_dtype, requires_grad=True\n            )\n            target = torch.randint(0, N, (M,), device=\"cuda\", dtype=torch.int64)\n            dloss = torch.randn(M, device=\"cuda\", dtype=torch.float32)\n            self.benchmark_single_shape(\n                (x, target, dloss), setting=f\"shape: [{M}, {N}]\"\n            )", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001860", "source": "def test_only_validate_by_name(py_and_json) -> None:\n    schema = core_schema.arguments_schema(\n        [\n            core_schema.arguments_parameter(name='a', schema=core_schema.str_schema(), alias='FieldA'),\n        ],\n        validate_by_name=True,\n        validate_by_alias=False,\n    )\n    v = py_and_json(schema)\n    assert v.validate_test(ArgsKwargs((), {'a': 'hello'})) == ((), {'a': 'hello'})\n    with pytest.raises(ValidationError, match=r'a\\n +Missing required argument \\[type=missing_argument,'):\n        assert v.validate_test(ArgsKwargs((), {'FieldA': 'hello'}))", "target": "def test_only_validate_by_name(py_and_json) -> None:\n    schema = core_schema.dataclass_schema(\n        BasicDataclass,\n        core_schema.dataclass_args_schema(\n            'BasicDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), validation_alias='FieldA'),\n            ],\n        ),\n        ['a'],\n        config=core_schema.CoreConfig(validate_by_name=True, validate_by_alias=False),\n    )\n    v = py_and_json(schema)\n    assert v.validate_test({'a': 'hello'}) == BasicDataclass(a='hello')\n    with pytest.raises(ValidationError, match=r'a\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'FieldA': 'hello'})", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001861", "source": "def required_usage_imports(self) -> Generator[str, None, None]:\n        yield \"import cv2.typing\"", "target": "def required_usage_imports(self) -> Generator[str, None, None]:\n        for item in self:\n            yield from item.required_usage_imports", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "001862", "source": "def fit(self, X, y=None):\n        self.clusterer_ = clone(self.clusterer)\n        self.classifier_ = clone(self.classifier)\n        y = self.clusterer_.fit_predict(X)\n        self.classifier_.fit(X, y)\n        return self", "target": "def fit(self, X, y):\n        self.classes_ = sorted(set(y))\n        self._is_fitted = True\n        return self", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001863", "source": "def is_resolved(self) -> bool:\n        return True", "target": "def is_resolved(self) -> bool:\n        return all(item.is_resolved for item in self.items)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001864", "source": "def test_int(input_value, expected):\n    v = SchemaValidator(core_schema.int_schema())\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            v.validate_json(input_value)\n    else:\n        assert v.validate_json(input_value) == expected", "target": "def test_int(input_value, expected):\n    v = SchemaValidator(cs.int_schema())\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            v.validate_python(input_value)\n    else:\n        output = v.validate_python(input_value)\n        assert output == expected\n        assert isinstance(output, int)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001865", "source": "def eager(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, target = args\n        return lambda: F.cross_entropy(x, target, reduction=\"none\")", "target": "def eager(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, w = args\n        return lambda: self.rms_norm_ref(x, w)", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001866", "source": "def typename(self) -> str:\n        return self.type_node.full_typename", "target": "def typename(self) -> str:\n        if self._export_name is not None:\n            return self._export_name\n        return self.ctype_name", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "001867", "source": "def append_42(value, _info):\n        if isinstance(value, list):\n            value.append(42)\n        return value", "target": "def main(filename, min_templates, max_templates, verbose):\n    occurrence_count, benchmark_logs = parse_log_file(filename)\n    times = []\n    for N in range(min_templates, max_templates + 1):\n        selected_templates, total_time = optimize_templates(\n            N, occurrence_count, benchmark_logs, verbose\n        )\n        print(f\"N = {N}\")\n        print(f\"Selected Templates: {selected_templates}\")\n        print(f\"Total Weighted Time: {total_time}\")\n        times.append(total_time)\n    print(times)", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "001868", "source": "def make_scorers(self):\n        self.train_scorer = lambda _, __: neg_mean_inertia(\n            self.X, self.estimator.predict(self.X), self.estimator.cluster_centers_\n        )\n        self.test_scorer = lambda _, __: neg_mean_inertia(\n            self.X_val,\n            self.estimator.predict(self.X_val),\n            self.estimator.cluster_centers_,\n        )", "target": "def make_scorers(self):\n        self.train_scorer = lambda _, __: neg_mean_inertia(\n            self.X, self.estimator.predict(self.X), self.estimator.cluster_centers_\n        )\n        self.test_scorer = lambda _, __: neg_mean_inertia(\n            self.X_val,\n            self.estimator.predict(self.X_val),\n            self.estimator.cluster_centers_,\n        )", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "001869", "source": "def compute_bench(samples_range, features_range):\n    it = 0\n    results = defaultdict(lambda: [])\n    chunk = 100\n    max_it = len(samples_range) * len(features_range)\n    for n_samples in samples_range:\n        for n_features in features_range:\n            it += 1\n            print(\"==============================\")\n            print(\"Iteration %03d of %03d\" % (it, max_it))\n            print(\"==============================\")\n            print()\n            data = nr.randint(-50, 51, (n_samples, n_features))\n            print(\"K-Means\")\n            tstart = time()\n            kmeans = KMeans(init=\"k-means++\", n_clusters=10).fit(data)\n            delta = time() - tstart\n            print(\"Speed: %0.3fs\" % delta)\n            print(\"Inertia: %0.5f\" % kmeans.inertia_)\n            print()\n            results[\"kmeans_speed\"].append(delta)\n            results[\"kmeans_quality\"].append(kmeans.inertia_)\n            print(\"Fast K-Means\")\n            mbkmeans = MiniBatchKMeans(\n                init=\"k-means++\", n_clusters=10, batch_size=chunk\n            )\n            tstart = time()\n            mbkmeans.fit(data)\n            delta = time() - tstart\n            print(\"Speed: %0.3fs\" % delta)\n            print(\"Inertia: %f\" % mbkmeans.inertia_)\n            print()\n            print()\n            results[\"MiniBatchKMeans Speed\"].append(delta)\n            results[\"MiniBatchKMeans Quality\"].append(mbkmeans.inertia_)\n    return results", "target": "def compute_bench(samples_range, features_range):\n    it = 0\n    results = defaultdict(lambda: [])\n    max_it = len(samples_range) * len(features_range)\n    for n_samples in samples_range:\n        for n_features in features_range:\n            it += 1\n            print(\"==============================\")\n            print(\"Iteration %03d of %03d\" % (it, max_it))\n            print(\"n_samples %05d; n_features %02d\" % (n_samples, n_features))\n            print(\"==============================\")\n            print()\n            data = nr.randint(-50, 51, (n_samples, n_features))\n            for linkage in (\"single\", \"average\", \"complete\", \"ward\"):\n                print(linkage.capitalize())\n                tstart = time()\n                AgglomerativeClustering(linkage=linkage, n_clusters=10).fit(data)\n                delta = time() - tstart\n                print(\"Speed: %0.3fs\" % delta)\n                print()\n                results[linkage].append(delta)\n    return results", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001870", "source": "def gen():\n    yield 1\n    yield 2\n    yield 3", "target": "def gen(error: bool):\n        yield 1\n        yield 2\n        if error:\n            raise RuntimeError('error')\n        yield 3", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001871", "source": "def node_type(self) -> ASTNodeType:\n        return ASTNodeType.Function", "target": "def node_type(self) -> ASTNodeType:\n        return ASTNodeType.Enumeration", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|opencv/opencv", "data_split": "train", "example_id": "001872", "source": "def _get_torch_wheel_path_arg(self, torch_whl_dir: Optional[Path]) -> str:\n        if not torch_whl_dir:\n            return \"\"\n        return f\"--build-arg TORCH_WHEELS_PATH={_VLLM_TEMP_FOLDER}\"", "target": "def replace_template_parameters_with_placeholders(string: str) \\\n        -> Tuple[str, Tuple[str, ...]]:\n    template_brackets_indices = []\n    template_instantiations_count = 0\n    template_start_index = 0\n    for i, c in enumerate(string):\n        if c == \"<\":\n            template_instantiations_count += 1\n            if template_instantiations_count == 1:\n                template_start_index = i + 1\n        elif c == \">\":\n            template_instantiations_count -= 1\n            assert template_instantiations_count >= 0, \\\n                \"Provided string is ill-formed. There are more '>' than '<'.\"\n            if template_instantiations_count == 0:\n                template_brackets_indices.append((template_start_index, i))\n    assert template_instantiations_count == 0, \\\n        \"Provided string is ill-formed. There are more '<' than '>'.\"\n    template_args: List[str] = []\n    for i, j in reversed(template_brackets_indices):\n        template_args.insert(0, string[i:j])\n        string = string[:i] + \"{}\" + string[j:]\n    return string, tuple(template_args)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001873", "source": "def _apply_to_root(self, schema: core_schema.CoreSchema) -> core_schema.CoreSchema:\n        if schema['type'] == 'nullable':\n            self._is_nullable = True\n            wrapped = self._apply_to_root(schema['schema'])\n            nullable_wrapper = schema.copy()\n            nullable_wrapper['schema'] = wrapped\n            return nullable_wrapper\n        if schema['type'] == 'definitions':\n            wrapped = self._apply_to_root(schema['schema'])\n            definitions_wrapper = schema.copy()\n            definitions_wrapper['schema'] = wrapped\n            return definitions_wrapper\n        if schema['type'] != 'union':\n            schema = core_schema.union_schema([schema])\n        choices_schemas = [v[0] if isinstance(v, tuple) else v for v in schema['choices'][::-1]]\n        self._choices_to_handle.extend(choices_schemas)\n        while self._choices_to_handle:\n            choice = self._choices_to_handle.pop()\n            self._handle_choice(choice)\n        if self._discriminator_alias is not None and self._discriminator_alias != self.discriminator:\n            discriminator: str | list[list[str | int]] = [[self.discriminator], [self._discriminator_alias]]\n        else:\n            discriminator = self.discriminator\n        return core_schema.tagged_union_schema(\n            choices=self._tagged_union_choices,\n            discriminator=discriminator,\n            custom_error_type=schema.get('custom_error_type'),\n            custom_error_message=schema.get('custom_error_message'),\n            custom_error_context=schema.get('custom_error_context'),\n            strict=False,\n            from_attributes=True,\n            ref=schema.get('ref'),\n            metadata=schema.get('metadata'),\n            serialization=schema.get('serialization'),\n        )", "target": "def can_be_keyword(param: Parameter) -> bool:\n    return param.kind in (Parameter.POSITIONAL_OR_KEYWORD, Parameter.KEYWORD_ONLY)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001874", "source": "def getCMakeArgs(self, arch, target):\n        args = Builder.getCMakeArgs(self, arch, target)\n        args = args + [\n            '-DVISIONOS_ARCH=%s' % arch\n        ]\n        return args", "target": "def getCMakeArgs(self):\n        args = [\n            \"cmake\",\n            \"-GXcode\",\n            \"-DFRAMEWORK_DIR=%s\" % self.framework_dir,\n            \"-DFRAMEWORK_NAME=%s\" % self.framework_name,\n        ]\n        return args", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "001875", "source": "def make_estimator(self, params):\n        estimator = LinearRegression()\n        return estimator", "target": "def make_estimator(self, params):\n        estimator = HistGradientBoostingClassifier(\n            max_iter=100, max_leaf_nodes=15, early_stopping=False, random_state=0\n        )\n        return estimator", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "001876", "source": "def run(img, order):\n                    return np.transpose(img, order)", "target": "def test_serialize_as_any_with_field_serializer(container_schema_builder) -> None:\n    schema = container_schema_builder(\n        {\n            'value': core_schema.int_schema(\n                serialization=core_schema.plain_serializer_function_ser_schema(\n                    lambda model, v: v * 2, is_field_serializer=True\n                )\n            )\n        }\n    )\n    v = SchemaValidator(schema).validate_python({'value': 123})\n    cls = type(v)\n    s = SchemaSerializer(schema)\n    cls.__pydantic_serializer__ = s\n    assert s.to_python(v, serialize_as_any=False) == {'value': 246}\n    assert s.to_python(v, serialize_as_any=True) == {'value': 246}\n    assert s.to_json(v, serialize_as_any=False) == b'{\"value\":246}'\n    assert s.to_json(v, serialize_as_any=True) == b'{\"value\":246}'", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001877", "source": "def dynamic_rnn(\n        sequences: list[Tensor],\n        hiddens: tuple[Tensor, Tensor],\n        w_ih: Tensor,\n        w_hh: Tensor,\n        b_ih: Tensor,\n        b_hh: Tensor,\n    ) -> tuple[list[Tensor], tuple[list[Tensor], list[Tensor]]]:\n        hx, cx = hiddens\n        hxs = hx.unbind(1)\n        cxs = cx.unbind(1)\n        outputs = []\n        hx_outs = []\n        cx_outs = []\n        for batch in range(len(sequences)):\n            output = []\n            hy, cy = hxs[batch], cxs[batch]\n            inputs = sequences[batch].unbind(0)\n            for seq_idx in range(len(inputs)):\n                hy, cy = cell(\n                    inputs[seq_idx].unsqueeze(0), (hy, cy), w_ih, w_hh, b_ih, b_hh\n                )\n                output += [hy]\n            outputs += [torch.stack(output)]\n            hx_outs += [hy.unsqueeze(0)]\n            cx_outs += [cy.unsqueeze(0)]\n        return outputs, (hx_outs, cx_outs)", "target": "def dynamic_rnn(\n        input: Tensor,\n        hidden: tuple[Tensor, Tensor],\n        w_ih: Tensor,\n        w_hh: Tensor,\n        b_ih: Tensor,\n        b_hh: Tensor,\n    ) -> tuple[Tensor, tuple[Tensor, Tensor]]:\n        hx, cx = hidden\n        outputs = []\n        inputs = torch.matmul(input, w_ih.t()).unbind(0)\n        hy, cy = hx[0], cx[0]\n        for seq_idx in range(len(inputs)):\n            hy, cy = premul_cell(inputs[seq_idx], (hy, cy), w_hh, b_ih, b_hh)\n            outputs += [hy]\n        return torch.stack(outputs), (hy.unsqueeze(0), cy.unsqueeze(0))", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001878", "source": "def dynamic_rnn(\n        input: Tensor,\n        hidden: tuple[Tensor, Tensor],\n        w_ih: Tensor,\n        w_hh: Tensor,\n        b_ih: Tensor,\n        b_hh: Tensor,\n    ) -> tuple[Tensor, tuple[Tensor, Tensor]]:\n        hx, cx = hidden\n        outputs = []\n        inputs = input.unbind(0)\n        hy, cy = hx[0], cx[0]\n        for seq_idx in range(len(inputs)):\n            hy, cy = cell(inputs[seq_idx], (hy, cy), w_ih, w_hh, b_ih, b_hh)\n            outputs += [hy]\n        return torch.stack(outputs), (hy.unsqueeze(0), cy.unsqueeze(0))", "target": "def dynamic_rnn(input, hx, cx, w_ih, w_hh, b_ih, b_hh):\n        hy = hx\n        cy = cx\n        inputs = input.unbind(0)\n        for seq_idx in range(len(inputs)):\n            hy, cy = cell(inputs[seq_idx], hy, cy, w_ih, w_hh, b_ih, b_hh)\n        return hy, cy", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "train", "example_id": "001879", "source": "def _merge_additional_failures_files(\n    source_pytest_cache: Path, dest_pytest_cache: Path\n) -> None:\n    source_lastfailed_file = (\n        source_pytest_cache / TD_HEURISTIC_PREVIOUSLY_FAILED_ADDITIONAL\n    )\n    dest_lastfailed_file = dest_pytest_cache / TD_HEURISTIC_PREVIOUSLY_FAILED_ADDITIONAL\n    if not source_lastfailed_file.exists():\n        return\n    if not dest_lastfailed_file.exists():\n        copy_file(source_lastfailed_file, dest_lastfailed_file)\n        return\n    from_lastfailed = load_json_file(source_lastfailed_file)\n    to_lastfailed = load_json_file(dest_lastfailed_file)\n    merged_content = list(set(from_lastfailed + to_lastfailed))\n    write_json_file(dest_lastfailed_file, merged_content)", "target": "def benchmark(self):\n        for M, N in self.get_shapes():\n            print(f\"Tensor dimensions: [{M}, {N}]\")\n            torch_dtype = cutlass_torch.dtype(cutlass.BFloat16)\n            x = torch.randn(M, N, device=\"cuda\", dtype=torch_dtype)\n            w = torch.randn(N, device=\"cuda\", dtype=torch.float32)\n            self.benchmark_single_shape((x, w), setting=f\"shape: [{M}, {N}]\")", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "001880", "source": "def skip(self, params):\n        representation, precompute = params\n        if representation == \"sparse\" and precompute is False:\n            return True\n        return False", "target": "def skip(self, params):\n        representation, precompute = params\n        if representation == \"sparse\" and precompute is False:\n            return True\n        return False", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001881", "source": "def test_constraints_schema_validation_error(constraint: str) -> None:\n    with pytest.raises(SchemaError, match=f\"'{constraint}' must be coercible to a datetime instance\"):\n        SchemaValidator(cs.datetime_schema(**{constraint: 'bad_value'}))", "target": "def test_constraints_schema_validation_error(constraint: str) -> None:\n    with pytest.raises(SchemaError, match=f\"'{constraint}' must be coercible to a timedelta instance\"):\n        SchemaValidator(core_schema.timedelta_schema(**{constraint: 'bad_value'}))", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "001882", "source": "def make_data(self, params):\n        data = _synth_classification_dataset(n_samples=10000, n_features=100)\n        return data", "target": "def make_data(self, params):\n        representation, precompute = params\n        if representation == \"dense\":\n            data = _synth_regression_dataset(n_samples=1000000, n_features=100)\n        else:\n            data = _synth_regression_sparse_dataset(\n                n_samples=50000, n_features=5000, density=0.01\n            )\n        return data", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "001883", "source": "def typename(self) -> str:\n            return self.type_node.full_typename", "target": "def test_args_var_args_only(py_and_json: PyAndJson, input_value, expected):\n    v = py_and_json(\n        {\n            'type': 'arguments',\n            'arguments_schema': [{'name': 'a', 'mode': 'positional_only', 'schema': {'type': 'int'}}],\n            'var_args_schema': {'type': 'int'},\n        }\n    )\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)) as exc_info:\n            v.validate_test(input_value)\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        assert v.validate_test(input_value) == expected", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001884", "source": "def test_bool(input_value, output_value):\n    v = SchemaValidator(core_schema.bool_schema())\n    assert v.validate_json(input_value) == output_value", "target": "def test_bool():\n    v = SchemaValidator(core_schema.bool_schema())\n    assert v.validate_strings('true') is True\n    assert v.validate_strings('true', strict=True) is True\n    assert v.validate_strings('false') is False", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001885", "source": "def test_dict():\n    v = SchemaValidator(core_schema.dict_schema(core_schema.int_schema(), core_schema.date_schema()))\n    assert v.validate_strings({'1': '2017-01-01', '2': '2017-01-02'}) == {1: date(2017, 1, 1), 2: date(2017, 1, 2)}\n    assert v.validate_strings({'1': '2017-01-01', '2': '2017-01-02'}, strict=True) == {\n        1: date(2017, 1, 1),\n        2: date(2017, 1, 2),\n    }", "target": "def test_dict(py_and_json: PyAndJson):\n    v = py_and_json({'type': 'dict', 'keys_schema': {'type': 'date'}, 'values_schema': {'type': 'int'}})\n    assert v.validate_test({'2000-01-01': 2, '2000-01-02': 4}) == {date(2000, 1, 1): 2, date(2000, 1, 2): 4}", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001886", "source": "def area(self) -> int:\n            return self.width * self.height", "target": "def area(self) -> int:\n            raise ValueError('xxx')", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001887", "source": "def liger(self, args, kwargs=None) -> Any:\n        from liger_kernel.transformers.softmax import LigerSoftmax\n        assert kwargs is None\n        (x,) = args\n        softmax = LigerSoftmax().to(\"cuda\")\n        return lambda: softmax(x)", "target": "def liger(self, args, kwargs) -> Any:\n        from liger_kernel.transformers.layer_norm import LigerLayerNorm\n        x, w = args\n        M, N = x.shape\n        liger_layernorm = LigerLayerNorm(hidden_size=N, eps=1e-6).cuda()\n        liger_layernorm.weight.data.copy_(w)\n        liger_layernorm.bias.data.copy_(\n            torch.zeros(N, device=\"cuda\", dtype=torch.float32)\n        )\n        return lambda: liger_layernorm(x)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001888", "source": "def test_all_optional_fields():\n    v = SchemaValidator(\n        core_schema.typed_dict_schema(\n            total=False,\n            fields={\n                'x': core_schema.typed_dict_field(schema=core_schema.str_schema(strict=True)),\n                'y': core_schema.typed_dict_field(schema=core_schema.str_schema()),\n            },\n        )\n    )\n    assert v.validate_python({'x': 'pika', 'y': 'chu'}) == {'x': 'pika', 'y': 'chu'}\n    assert v.validate_python({'x': 'pika'}) == {'x': 'pika'}\n    assert v.validate_python({'y': 'chu'}) == {'y': 'chu'}\n    with pytest.raises(ValidationError) as exc_info:\n        assert v.validate_python({'x': 123})\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'string_type', 'loc': ('x',), 'msg': 'Input should be a valid string', 'input': 123}\n    ]", "target": "def test_no_args_constructor(self):\n        tz = TzInfo()\n        self.assertEqual(tz.utcoffset(None), timedelta(0))\n        self.assertEqual(str(tz), 'UTC')", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "001889", "source": "def make_data(self, params):\n        representation, solver = params\n        if representation == \"dense\":\n            data = _synth_regression_dataset(n_samples=500000, n_features=100)\n        else:\n            data = _synth_regression_sparse_dataset(\n                n_samples=100000, n_features=10000, density=0.005\n            )\n        return data", "target": "def make_data(self, params):\n        (representation,) = params\n        if representation == \"dense\":\n            data = _synth_regression_dataset(n_samples=100000, n_features=200)\n        else:\n            data = _synth_regression_sparse_dataset(\n                n_samples=100000, n_features=1000, density=0.01\n            )\n        return data", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "train", "example_id": "001890", "source": "def foo(x: torch.Tensor) -> torch.Tensor:\n        return torch.sin(x) + torch.cos(x)", "target": "def rms_norm_ref(self, x, w):\n        x_f32 = x.float()\n        return (\n            x_f32\n            * torch.rsqrt(torch.mean(x_f32.square(), dim=-1, keepdim=True) + 1e-6)\n            * w\n        ).to(x.dtype)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001891", "source": "def test_constraints_schema_validation() -> None:\n    val = SchemaValidator(cs.decimal_schema(gt='1'))\n    with pytest.raises(ValidationError):\n        val.validate_python('0')", "target": "def test_constraints_schema_validation() -> None:\n    val = SchemaValidator(cs.datetime_schema(gt='2020-01-01T00:00:00'))\n    with pytest.raises(ValidationError):\n        val.validate_python('2019-01-01T00:00:00')", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "001892", "source": "def cfgPrePostProcessing(self, pp_callback):\n            ppp = PrePostProcessor(self.model)\n            pp_callback(ppp)\n            self.model = ppp.build()", "target": "def invalid_schema(ref: str | None = None, metadata: dict[str, Any] | None = None) -> InvalidSchema:\n    return _dict_not_none(type='invalid', ref=ref, metadata=metadata)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001893", "source": "def for_model(\n        cls,\n        bases: tuple[type[Any], ...],\n        namespace: dict[str, Any],\n        raw_annotations: dict[str, Any],\n        kwargs: dict[str, Any],\n    ) -> Self:\n        config_new = ConfigDict()\n        for base in bases:\n            config = getattr(base, 'model_config', None)\n            if config:\n                config_new.update(config.copy())\n        config_class_from_namespace = namespace.get('Config')\n        config_dict_from_namespace = namespace.get('model_config')\n        if raw_annotations.get('model_config') and config_dict_from_namespace is None:\n            raise PydanticUserError(\n                '`model_config` cannot be used as a model field name. Use `model_config` for model configuration.',\n                code='model-config-invalid-field-name',\n            )\n        if config_class_from_namespace and config_dict_from_namespace:\n            raise PydanticUserError('\"Config\" and \"model_config\" cannot be used together', code='config-both')\n        config_from_namespace = config_dict_from_namespace or prepare_config(config_class_from_namespace)\n        config_new.update(config_from_namespace)\n        for k in list(kwargs.keys()):\n            if k in config_keys:\n                config_new[k] = kwargs.pop(k)\n        return cls(config_new)", "target": "def f(input_value, info):\n        return input_value + f'| context: {info.context}'", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|opencv/opencv", "data_split": "train", "example_id": "001894", "source": "def make_estimator(self, params):\n        estimator = HistGradientBoostingClassifier(\n            max_iter=100, max_leaf_nodes=15, early_stopping=False, random_state=0\n        )\n        return estimator", "target": "def outMeta(mat_desc, scalar_desc, dtype):\n            return mat_desc", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001895", "source": "def fn(x):\n        for _ in range(N):\n            x = x.sin()\n        return x", "target": "def fn():\n        for n in g.graph.nodes:\n            pass", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001896", "source": "def int_schema(\n    *,\n    multiple_of: int | None = None,\n    le: int | None = None,\n    ge: int | None = None,\n    lt: int | None = None,\n    gt: int | None = None,\n    strict: bool | None = None,\n    ref: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> IntSchema:\n    return _dict_not_none(\n        type='int',\n        multiple_of=multiple_of,\n        le=le,\n        ge=ge,\n        lt=lt,\n        gt=gt,\n        strict=strict,\n        ref=ref,\n        metadata=metadata,\n        serialization=serialization,\n    )", "target": "def test_bytes_base64():\n    s = SchemaSerializer(core_schema.bytes_schema(), {'ser_json_bytes': 'base64'})\n    assert s.to_python(b'foobar') == b'foobar'\n    assert s.to_json(b'foobar') == b'\"Zm9vYmFy\"'\n    assert s.to_python(b'foobar', mode='json') == 'Zm9vYmFy'\n    assert base64.b64decode(s.to_python(b'foobar', mode='json').encode()) == b'foobar'\n    assert s.to_json(b'foo bar') == b'\"Zm9vIGJhcg==\"'\n    assert s.to_python(b'foo bar', mode='json') == 'Zm9vIGJhcg=='\n    assert base64.b64decode(s.to_python(b'foo bar', mode='json').encode()) == b'foo bar'", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "001897", "source": "def forward(*new_params: Tensor) -> Tensor:\n        load_weights(model, names, new_params)\n        out = model(inputs)\n        loss = criterion(out, labels)\n        weight_dict = criterion.weight_dict\n        final_loss = cast(\n            Tensor,\n            sum(loss[k] * weight_dict[k] for k in loss.keys() if k in weight_dict),\n        )\n        return final_loss", "target": "def test_url_dict_keys():\n    v = SchemaValidator(core_schema.url_schema())\n    s = SchemaSerializer(core_schema.dict_schema(core_schema.url_schema()))\n    url = v.validate_python('https://example.com')\n    assert s.to_python({url: 'foo'}) == {url: 'foo'}\n    assert s.to_python({url: 'foo'}, mode='json') == {'https://example.com/': 'foo'}\n    assert s.to_json({url: 'foo'}) == b'{\"https://example.com/\":\"foo\"}'", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001898", "source": "def required_usage_imports(self) -> Generator[str, None, None]:\n        yield from ()", "target": "def required_usage_imports(self) -> Generator[str, None, None]:\n        yield \"import typing as _typing\"", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001899", "source": "def test_alias_allow_pop(py_and_json: PyAndJson):\n    schema = core_schema.dataclass_schema(\n        BasicDataclass,\n        core_schema.dataclass_args_schema(\n            'BasicDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), validation_alias='FieldA'),\n            ],\n        ),\n        ['a'],\n        config=core_schema.CoreConfig(validate_by_name=True, validate_by_alias=True),\n    )\n    v = py_and_json(schema)\n    assert v.validate_test({'FieldA': 'hello'}) == BasicDataclass(a='hello')\n    assert v.validate_test({'a': 'hello'}) == BasicDataclass(a='hello')\n    assert v.validate_test(\n        {\n            'FieldA': 'hello',\n            'a': 'world',\n        }\n    ) == BasicDataclass(a='hello')\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'foobar': 'hello'})", "target": "def test_alias_allow_pop(py_and_json: PyAndJson):\n    v = py_and_json(\n        {\n            'type': 'model-fields',\n            'fields': {'field_a': {'validation_alias': 'FieldA', 'type': 'model-field', 'schema': {'type': 'int'}}},\n        },\n        config=CoreConfig(validate_by_name=True),\n    )\n    assert v.validate_test({'FieldA': '123'}) == ({'field_a': 123}, None, {'field_a'})\n    assert v.validate_test({'field_a': '123'}) == ({'field_a': 123}, None, {'field_a'})\n    assert v.validate_test({'FieldA': '1', 'field_a': '2'}) == ({'field_a': 1}, None, {'field_a'})\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'foobar': '123'})", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001900", "source": "def test_bool(input_value, output_value):\n    v = SchemaValidator(core_schema.bool_schema())\n    assert v.validate_json(input_value) == output_value", "target": "def input_data_valid(levels: int = N) -> Any:\n    data = {str(c): 1 for c in range(N)}\n    for _ in range(levels):\n        data = {str(c): data for c in range(N)}\n    return data", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001901", "source": "def test_str_config():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(fields={'field_a': core_schema.model_field(schema=core_schema.str_schema())}),\n        config=CoreConfig(str_max_length=5),\n    )\n    assert v.validate_python({'field_a': 'test'}) == ({'field_a': 'test'}, None, {'field_a'})\n    with pytest.raises(ValidationError, match='String should have at most 5 characters'):\n        v.validate_python({'field_a': 'test long'})", "target": "def test_str_config():\n    v = SchemaValidator(\n        core_schema.typed_dict_schema(\n            fields={'field_a': core_schema.typed_dict_field(schema=core_schema.str_schema())},\n            config=CoreConfig(str_max_length=5),\n        )\n    )\n    assert v.validate_python({'field_a': 'test'}) == {'field_a': 'test'}\n    with pytest.raises(ValidationError, match='String should have at most 5 characters'):\n        v.validate_python({'field_a': 'test long'})", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001902", "source": "def _prepare_once(self):\n        self.m = self.ModuleClass()\n        torch.set_float32_matmul_precision(\"high\")\n        self.input = torch.ones(10, device=self.device())", "target": "def _prepare_once(self):\n        torch._dynamo.config.capture_scalar_outputs = True\n        torch.manual_seed(0)\n        self.splits = torch.randint(10, (self.N,))", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001903", "source": "def train(self, samples, responses):\n        self.model.setMaxDepth(20)\n        self.model.train(samples, cv.ml.ROW_SAMPLE, responses.astype(int))", "target": "def train(self, samples, responses):\n        self.model.setType(cv.ml.SVM_C_SVC)\n        self.model.setC(1)\n        self.model.setKernel(cv.ml.SVM_RBF)\n        self.model.setGamma(.1)\n        self.model.train(samples, cv.ml.ROW_SAMPLE, responses.astype(int))", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001904", "source": "def compiled(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, target, dloss = args\n        compiled_cross_entropy = torch.compile(\n            lambda x, target: F.cross_entropy(x, target, reduction=\"none\"),\n            mode=self.compile_mode,\n            fullgraph=True,\n        )\n        loss = compiled_cross_entropy(x, target)\n        return lambda: torch.autograd.grad(\n            loss, x, grad_outputs=dloss, retain_graph=True\n        )", "target": "def compiled(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        (x,) = args\n        torch._dynamo.mark_dynamic(x, 0)\n        compiled_softmax = torch.compile(\n            lambda x: F.softmax(x, dim=-1), mode=self.compile_mode, fullgraph=True\n        )\n        return lambda: compiled_softmax(x)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001905", "source": "def test_constraints_schema_validation_error(constraint: str) -> None:\n    with pytest.raises(SchemaError, match=f\"'{constraint}' must be coercible to a datetime instance\"):\n        SchemaValidator(cs.datetime_schema(**{constraint: 'bad_value'}))", "target": "def test_bytes_fallback():\n    s = SchemaSerializer(core_schema.bytes_schema())\n    with pytest.warns(\n        UserWarning,\n        match=r'Expected `bytes` - serialized value may not be as expected \\[input_value=123, input_type=int\\]',\n    ):\n        assert s.to_python(123) == 123\n    with pytest.warns(\n        UserWarning,\n        match=r'Expected `bytes` - serialized value may not be as expected \\[input_value=123, input_type=int\\]',\n    ):\n        assert s.to_python(123, mode='json') == 123\n    with pytest.warns(\n        UserWarning,\n        match=r'Expected `bytes` - serialized value may not be as expected \\[input_value=123, input_type=int\\]',\n    ):\n        assert s.to_json(123) == b'123'\n    with pytest.warns(\n        UserWarning,\n        match=r\"Expected `bytes` - serialized value may not be as expected \\[input_value='foo', input_type=str\\]\",\n    ):\n        assert s.to_json('foo') == b'\"foo\"'", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001906", "source": "def typename(self) -> str:\n        return self.alias_export_name", "target": "def typename(self) -> str:\n        if self._export_name is not None:\n            return self._export_name\n        return self.ctype_name", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001907", "source": "def parse_args() -> Any:\n    from argparse import ArgumentParser\n    parser = ArgumentParser(\"Comment on a PR\")\n    parser.add_argument(\"pr_num\", type=int)\n    parser.add_argument(\"action\", type=str)\n    return parser.parse_args()", "target": "def parse_args() -> Any:\n    from argparse import ArgumentParser\n    parser = ArgumentParser(\"Export PR labels\")\n    parser.add_argument(\"org\", type=str)\n    parser.add_argument(\"repo\", type=str)\n    return parser.parse_args()", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001908", "source": "def jacrev(model, inp, strict=None):\n        return functional.jacobian(model, inp, strict=False, vectorize=True)", "target": "def jacrev(model, inp, v=None, strict=None):\n        argnums = tuple(range(len(inp)))\n        return ft.jacrev(model, argnums)(*inp)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001909", "source": "def gen(error: bool):\n        yield 1\n        yield 2\n        if error:\n            raise RuntimeError('my error')\n        yield 3", "target": "def gen(error: bool):\n        yield 1\n        yield 2\n        if error:\n            raise RuntimeError('error')\n        yield 3", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|opencv/opencv", "data_split": "train", "example_id": "001910", "source": "def test_unsupported_numpy_data_types_string_description(self):\n        for dtype in (object, str, np.complex128):\n            test_array = np.zeros((4, 4, 3), dtype=dtype)\n            msg = \".*type = {} is not supported\".format(test_array.dtype)\n            if sys.version_info[0] < 3:\n                self.assertRaisesRegexp(\n                    Exception, msg, cv.utils.dumpInputArray, test_array\n                )\n            else:\n                self.assertRaisesRegex(\n                    Exception, msg, cv.utils.dumpInputArray, test_array\n                )", "target": "def train(self, samples, responses):\n        _sample_n, var_n = samples.shape\n        new_responses = self.unroll_responses(responses).reshape(-1, self.class_n)\n        layer_sizes = np.int32([var_n, 100, 100, self.class_n])\n        self.model.setLayerSizes(layer_sizes)\n        self.model.setTrainMethod(cv.ml.ANN_MLP_BACKPROP)\n        self.model.setBackpropMomentumScale(0)\n        self.model.setBackpropWeightScale(0.001)\n        self.model.setTermCriteria((cv.TERM_CRITERIA_COUNT, 20, 0.01))\n        self.model.setActivationFunction(cv.ml.ANN_MLP_SIGMOID_SYM, 2, 1)\n        self.model.train(samples, cv.ml.ROW_SAMPLE, np.float32(new_responses))", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "001911", "source": "def main():\n    def fn():\n        torch._dynamo.reset()\n        symbolic_convert_overhead_stress_test(x, y, 100000)\n    x = torch.randn(16)\n    y = torch.randn(16)\n    t = min(timeit.repeat(fn, number=1, repeat=3))\n    print(f\"symbolic_convert_overhead_stress_test: {t:.1f}s\")", "target": "def dataclass_field(\n    name: str,\n    schema: CoreSchema,\n    *,\n    kw_only: bool | None = None,\n    init: bool | None = None,\n    init_only: bool | None = None,\n    validation_alias: str | list[str | int] | list[list[str | int]] | None = None,\n    serialization_alias: str | None = None,\n    serialization_exclude: bool | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization_exclude_if: Callable[[Any], bool] | None = None,\n    frozen: bool | None = None,\n) -> DataclassField:\n    return _dict_not_none(\n        type='dataclass-field',\n        name=name,\n        schema=schema,\n        kw_only=kw_only,\n        init=init,\n        init_only=init_only,\n        validation_alias=validation_alias,\n        serialization_alias=serialization_alias,\n        serialization_exclude=serialization_exclude,\n        serialization_exclude_if=serialization_exclude_if,\n        metadata=metadata,\n        frozen=frozen,\n    )", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|pytorch/pytorch", "data_split": "train", "example_id": "001912", "source": "def _get_guide(*refs, is_developer=False):\n    if len(refs) == 1:\n        ref_desc = f\":ref:`{refs[0]}` section\"\n    elif len(refs) == 2:\n        ref_desc = f\":ref:`{refs[0]}` and :ref:`{refs[1]}` sections\"\n    else:\n        ref_desc = \", \".join(f\":ref:`{ref}`\" for ref in refs[:-1])\n        ref_desc += f\", and :ref:`{refs[-1]}` sections\"\n    guide_name = \"Developer\" if is_developer else \"User\"\n    return f\"**{guide_name} guide.** See the {ref_desc} for further details.\"", "target": "def _prepare_once(self):\n        torch._dynamo.config.capture_scalar_outputs = True\n        torch.manual_seed(0)\n        self.splits = torch.randint(10, (self.N,))\n        sz = self.splits.sum().item()\n        self.input = torch.randn(sz)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001913", "source": "def train(self, samples, responses):\n        self.model.train(samples, cv.ml.ROW_SAMPLE, responses)", "target": "def train(self, samples, responses):\n        self.model.train(samples, cv.ml.ROW_SAMPLE, responses)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "001914", "source": "def input_data_wrong():\n    return {\n        'field_str': ['fo'],\n        'field_str_con': 'f',\n        'field_int': 1.5,\n        'field_int_con': 11,\n        'field_float': False,\n        'field_float_con': 10.1,\n        'field_decimal': 'wrong',\n        'field_bool': 4,\n        'field_bytes': 42,\n        'field_bytes_con': b'foo',\n        'field_date': 'wrong',\n        'field_date_con': '2000-01-01',\n        'field_time': 'boom',\n        'field_time_con': '23:00:00',\n        'field_datetime': b'smash',\n        'field_datetime_con': '1900-01-01T00:00:00',\n        'field_uuid': '12345678-1234-5678-1234-567812345678',\n        'field_list_any': {1: 2, 3: 4},\n        'field_list_str': [(i,) for i in range(100)],\n        'field_list_str_con': ['a', 'b'],\n        'field_set_any': {'a': b'b', True: 1.0, None: 5},\n        'field_set_int': {f'x{i}' for i in range(100)},\n        'field_set_int_con': {i for i in range(40)},\n        'field_frozenset_any': 'wrong',\n        'field_frozenset_bytes': frozenset([i for i in range(100)]),\n        'field_frozenset_bytes_con': frozenset({b'a', b'b'}),\n        'field_tuple_var_len_any': b'wrong',\n        'field_tuple_var_len_float': tuple(f'x{i}' for i in range(100)),\n        'field_tuple_var_len_float_con': (1.0, 2.0),\n        'field_tuple_fix_len': ('a', 1, 1.0, True, 'more'),\n        'field_dict_any': {'a', 'b', 1, True, 1.0, 2.0},\n        'field_dict_str_float': {(i,): f'x{i}' for i in range(100)},\n        'field_literal_1_int': 2,\n        'field_literal_1_str': 'bat',\n        'field_literal_mult_int': 42,\n        'field_literal_mult_str': 'wrong',\n        'field_literal_assorted': 'wrong',\n        'field_list_nullable_int': [f'x{i}' for i in range(100)],\n        'field_union': {'field_str': ('foo',), 'field_int': 'x', 'field_float': b'y'},\n        'field_functions_model': {'field_before': 1, 'field_after': 1, 'field_wrap': 1, 'field_plain': 1},\n        'field_recursive': {'name': 'foo', 'sub_branch': {'name': 'bar', 'sub_branch': {}}},\n    }", "target": "def create_class_node(root: NamespaceNode, class_info,\n                      namespaces: Sequence[str]) -> ClassNode:\n    symbol_name = SymbolName.parse(class_info.full_original_name, namespaces)\n    scope = find_scope(root, symbol_name)\n    return create_class_node_in_scope(scope, symbol_name, class_info)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001915", "source": "def add_constant(self, name: str, value: str) -> ConstantNode:\n        return self._add_child(ConstantNode, name, value=value)", "target": "def add_constant(self, name: str, value: str) -> ConstantNode:\n        return self._add_child(ConstantNode, name, value=value)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001916", "source": "def test_only_validate_by_name(py_and_json) -> None:\n    schema = core_schema.dataclass_schema(\n        BasicDataclass,\n        core_schema.dataclass_args_schema(\n            'BasicDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), validation_alias='FieldA'),\n            ],\n        ),\n        ['a'],\n        config=core_schema.CoreConfig(validate_by_name=True, validate_by_alias=False),\n    )\n    v = py_and_json(schema)\n    assert v.validate_test({'a': 'hello'}) == BasicDataclass(a='hello')\n    with pytest.raises(ValidationError, match=r'a\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'FieldA': 'hello'})", "target": "def model_serializer(\n    *,\n    mode: Literal['plain'] = ...,\n    when_used: WhenUsed = 'always',\n    return_type: Any = ...,\n) -> Callable[[_ModelPlainSerializerT], _ModelPlainSerializerT]: ...", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|opencv/opencv", "data_split": "train", "example_id": "001917", "source": "def run_benchmarks(operators, shapes):\n    if operators is None:\n        operators = OPERATORS\n    else:\n        operators = [globals()[k] for k in operators.split(\",\")]\n    if shapes is None:\n        shapes = SHAPES\n    else:\n        shapes = [globals()[k] for k in shapes.split(\",\")]\n    print(\"fuser,device,operator,shape,time\")\n    for shape, operator in itertools.product(shapes, operators):\n        nargs = len(inspect.signature(operator).parameters)\n        args = shape()\n        if nargs > len(args):\n            args = list(args)\n            args += [args[-1]] * (nargs - len(args))\n        args = args[:nargs]\n        args = [arg.to(\"cuda\") for arg in args]\n        result = benchmark(operator, args)\n        print(\n            \",\".join(\n                [\n                    \"eager\",\n                    args[0].device.type,\n                    operator.__name__,\n                    shape.__name__,\n                    micros(result),\n                ]\n            )\n        )\n        def bench(name):\n            nnc_op = torch.jit.trace(operator, args)\n            result = benchmark(nnc_op, args)\n            print(\n                \",\".join(\n                    [\n                        name,\n                        args[0].device.type,\n                        operator.__name__,\n                        shape.__name__,\n                        micros(result),\n                    ]\n                )\n            )\n            sys.stdout.flush()\n        with_nnc()\n        bench(\"nnc\")\n        with_nvfuser()\n        bench(\"nvfuser\")\n        with_legacy()\n        bench(\"legacy\")", "target": "def wrapped_func(*args, **kwargs):\n                    if self.has_failure:\n                        if ret_type_on_failure is None:\n                            return None\n                        return ret_type_on_failure()\n                    try:\n                        ret_type = func(*args, **kwargs)\n                    except Exception:\n                        self.has_failure = True\n                        warnings.warn(\n                            \"Typing stubs generation has failed.\\n{}\".format(\n                                traceback.format_exc()\n                            )\n                        )\n                        if ret_type_on_failure is None:\n                            return None\n                        return ret_type_on_failure()\n                    return ret_type", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001918", "source": "def read_classes(img_classes_file):\n        result = {}\n        with open(img_classes_file) as file:\n            for l in file.readlines():\n                result[l.split()[0]] = int(l.split()[1])\n        return result", "target": "def read_classes(img_classes_file):\n        result = {}\n        with open(img_classes_file) as file:\n            for l in file.readlines():\n                result[l.split()[0]] = int(l.split()[1])\n        return result", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001919", "source": "def benchmark(self):\n        for M, N in self.get_shapes():\n            print(f\"Tensor dimensions: [{M}, {N}]\")\n            torch_dtype = cutlass_torch.dtype(cutlass.BFloat16)\n            x = torch.randn(M, N, device=\"cuda\", dtype=torch_dtype)\n            w = torch.randn(N, device=\"cuda\", dtype=torch.float32)\n            self.benchmark_single_shape((x, w), setting=f\"shape: [{M}, {N}]\")", "target": "def benchmark(self):\n        for M, N in self.get_shapes():\n            print(f\"Tensor dimensions: [{M}, {N}]\")\n            torch_dtype = cutlass_torch.dtype(cutlass.BFloat16)\n            x = torch.randn(M, N, device=\"cuda\", dtype=torch_dtype)\n            w = torch.randn(N, device=\"cuda\", dtype=torch.float32)\n            self.benchmark_single_shape((x, w), setting=f\"shape: [{M}, {N}]\")", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "001920", "source": "def log(event_name):\n            scribe.open_source_signpost(\n                subsystem=\"pr_time_benchmarks\",\n                name=event_name,\n                parameters=json.dumps(\n                    {\n                        \"benchmark_name\": entry.benchmark_name,\n                        \"metric_name\": entry.metric_name,\n                        \"actual_value\": result,\n                        \"expected_value\": entry.expected_value,\n                        \"noise_margin\": entry.noise_margin,\n                        \"change_ratio\": ratio,\n                    }\n                ),\n            )", "target": "def _infer_discriminator_values_for_inner_schema(\n        self, schema: core_schema.CoreSchema, source: str\n    ) -> list[str | int]:\n        if schema['type'] == 'literal':\n            return schema['expected']\n        elif schema['type'] == 'union':\n            values: list[Any] = []\n            for choice in schema['choices']:\n                choice_schema = choice[0] if isinstance(choice, tuple) else choice\n                choice_values = self._infer_discriminator_values_for_inner_schema(choice_schema, source)\n                values.extend(choice_values)\n            return values\n        elif schema['type'] == 'default':\n            return self._infer_discriminator_values_for_inner_schema(schema['schema'], source)\n        elif schema['type'] == 'function-after':\n            return self._infer_discriminator_values_for_inner_schema(schema['schema'], source)\n        elif schema['type'] in {'function-before', 'function-wrap', 'function-plain'}:\n            validator_type = repr(schema['type'].split('-')[1])\n            raise PydanticUserError(\n                f'Cannot use a mode={validator_type} validator in the'\n                f' discriminator field {self.discriminator!r} of {source}',\n                code='discriminator-validator',\n            )\n        else:\n            raise PydanticUserError(\n                f'{source} needs field {self.discriminator!r} to be of type `Literal`',\n                code='discriminator-needs-literal',\n            )", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "001921", "source": "def _work(self):\n        @torch.compile(backend=self.backend(), fullgraph=True)\n        def f(*args):\n            outs = [torch.add(x, x) for x in args]\n            return outs\n        f(*self._args)", "target": "def test_leak_nullable():\n    def fn():\n        def validate(v, info):\n            return v\n        schema = core_schema.with_info_plain_validator_function(validate)\n        schema = core_schema.nullable_schema(schema)\n        validate.__pydantic_validator__ = SchemaValidator(schema)\n        return validate\n    cycle = fn()\n    ref = weakref.ref(cycle)\n    assert ref() is not None\n    del cycle\n    assert_gc(lambda: ref() is None)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001922", "source": "def test_dataclass():\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bytes_schema()),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    s = SchemaSerializer(schema)\n    assert s.to_python(Foo(a='hello', b=b'more')) == IsStrictDict(a='hello', b=b'more')\n    assert s.to_python(Foo(a='hello', b=b'more'), mode='json') == IsStrictDict(a='hello', b='more')\n    j = s.to_json(Foo(a='hello', b=b'more'))\n    if on_pypy:\n        assert json.loads(j) == {'a': 'hello', 'b': 'more'}\n    else:\n        assert j == b'{\"a\":\"hello\",\"b\":\"more\"}'\n    assert s.to_python(Foo(a='hello', b=b'more'), exclude={'b'}) == IsStrictDict(a='hello')\n    assert s.to_json(Foo(a='hello', b=b'more'), include={'a'}) == b'{\"a\":\"hello\"}'", "target": "def test_dataclass():\n    schema = core_schema.call_schema(\n        core_schema.arguments_schema(\n            [\n                core_schema.arguments_parameter('foo', core_schema.int_schema()),\n                core_schema.arguments_parameter('bar', core_schema.str_schema()),\n                core_schema.arguments_parameter('spam', core_schema.bytes_schema(), mode='keyword_only'),\n                core_schema.arguments_parameter('frog', core_schema.int_schema(), mode='keyword_only'),\n            ]\n        ),\n        DataClass,\n        serialization=core_schema.model_ser_schema(\n            DataClass,\n            core_schema.model_fields_schema(\n                {\n                    'foo': core_schema.model_field(core_schema.int_schema()),\n                    'bar': core_schema.model_field(core_schema.str_schema()),\n                    'spam': core_schema.model_field(core_schema.bytes_schema()),\n                }\n            ),\n        ),\n    )\n    v = SchemaValidator(schema)\n    dc = v.validate_python({'foo': 1, 'bar': 'bar-str', 'spam': 'bite', 'frog': 123})\n    assert dc == DataClass(foo=1, bar='bar-str', spam=b'bite', frog=123)\n    dc.class_var = 2\n    assert dataclasses.is_dataclass(dc)\n    s = SchemaSerializer(schema)\n    assert dataclasses.asdict(dc) == IsStrictDict(foo=1, bar='bar-str', spam=b'bite')\n    assert s.to_python(dc) == IsStrictDict(foo=1, bar='bar-str', spam=b'bite')\n    assert s.to_python(dc, mode='json') == {'foo': 1, 'bar': 'bar-str', 'spam': 'bite'}\n    assert json.loads(s.to_json(dc)) == {'foo': 1, 'bar': 'bar-str', 'spam': 'bite'}", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "001923", "source": "def typename(self) -> str:\n        return \"None\"", "target": "def test_union_tuple_fix_len(input_value, expected):\n    v = SchemaValidator(\n        core_schema.union_schema(\n            choices=[\n                core_schema.tuple_schema(\n                    items_schema=[core_schema.int_schema(), core_schema.int_schema(), core_schema.int_schema()],\n                    strict=True,\n                ),\n                core_schema.tuple_schema(\n                    items_schema=[core_schema.str_schema(), core_schema.str_schema(), core_schema.str_schema()],\n                    strict=True,\n                ),\n            ]\n        )\n    )\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)) as exc_info:\n            v.validate_python(input_value)\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        assert v.validate_python(input_value) == expected", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "001924", "source": "def eqFloat(self, input: float) -> float:\n        return input", "target": "def test_model_custom_init_extra():\n    calls = []\n    class ModelInner:\n        __slots__ = '__dict__', '__pydantic_fields_set__', '__pydantic_extra__', '__pydantic_private__'\n        a: int\n        b: int\n        def __getattr__(self, item):\n            return self.__pydantic_extra__[item]\n        def __init__(self, **data):\n            self.__pydantic_validator__.validate_python(data, self_instance=self)\n            calls.append(('inner', self.__dict__, self.__pydantic_fields_set__, self.__pydantic_extra__))\n    inner_schema = core_schema.model_schema(\n        ModelInner,\n        core_schema.model_fields_schema(\n            {\n                'a': core_schema.model_field(core_schema.with_default_schema(core_schema.int_schema(), default=1)),\n                'b': core_schema.model_field(core_schema.int_schema()),\n            }\n        ),\n        config=CoreConfig(extra_fields_behavior='allow'),\n        custom_init=True,\n    )\n    ModelInner.__pydantic_validator__ = SchemaValidator(inner_schema)\n    class ModelOuter:\n        __slots__ = '__dict__', '__pydantic_fields_set__', '__pydantic_extra__', '__pydantic_private__'\n        a: int\n        b: ModelInner\n        def __getattr__(self, item):\n            return self.__pydantic_extra__[item]\n        def __init__(self, **data):\n            data['b']['z'] = 1\n            self.__pydantic_validator__.validate_python(data, self_instance=self)\n            calls.append(('outer', self.__dict__, self.__pydantic_fields_set__, self.__pydantic_extra__))\n    ModelOuter.__pydantic_validator__ = SchemaValidator(\n        core_schema.model_schema(\n            ModelOuter,\n            core_schema.model_fields_schema(\n                {\n                    'a': core_schema.model_field(core_schema.with_default_schema(core_schema.int_schema(), default=1)),\n                    'b': core_schema.model_field(inner_schema),\n                }\n            ),\n            config=CoreConfig(extra_fields_behavior='allow'),\n            custom_init=True,\n        )\n    )\n    m = ModelOuter(a=2, b={'b': 3}, c=1)\n    assert m.__pydantic_fields_set__ == {'a', 'b', 'c'}\n    assert m.a == 2\n    assert m.c == 1\n    assert isinstance(m.b, ModelInner)\n    assert m.b.a == 1\n    assert m.b.b == 3\n    assert m.b.z == 1\n    assert calls == [\n        ('inner', {'a': 1, 'b': 3}, {'b', 'z'}, {'z': 1}),\n        ('outer', {'a': 2, 'b': IsInstance(ModelInner)}, {'c', 'a', 'b'}, {'c': 1}),\n    ]", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "001925", "source": "def setUp(self):\n        cv.setRNGSeed(10)\n        self.image_cache = {}", "target": "def test_extra_behavior_forbid(\n    config: Union[core_schema.CoreConfig, None],\n    schema_extra_behavior_kw: dict[str, Any],\n    validate_fn_extra_kw: Union[ExtraBehavior, None],\n):\n    v = SchemaValidator(\n        core_schema.typed_dict_schema(\n            {'f': core_schema.typed_dict_field(core_schema.str_schema())},\n            **schema_extra_behavior_kw,\n            config=config,\n        )\n    )\n    m: dict[str, Any] = v.validate_python({'f': 'x'}, extra=validate_fn_extra_kw)\n    assert m == {'f': 'x'}\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python({'f': 'x', 'extra_field': 123}, extra=validate_fn_extra_kw)\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'extra_forbidden', 'loc': ('extra_field',), 'msg': 'Extra inputs are not permitted', 'input': 123}\n    ]", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001926", "source": "def ser_x(self, v: Any, _) -> str:\n            assert self.x == 1_000\n            return f'{v:_}'", "target": "def ser_x(data: Model, v: Any, _) -> str:\n        assert data['x'] == 1_000\n        return f'{v:_}'", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001927", "source": "def test_error(self, schema_validator: SchemaValidator):\n        with pytest.raises(ValidationError) as exc_info:\n            schema_validator.validate_python({'a': 2})\n        assert exc_info.value.errors(include_url=False) == [\n            {'type': 'missing', 'loc': ('ModelA', 'b'), 'msg': 'Field required', 'input': {'a': 2}},\n            {'type': 'missing', 'loc': ('ModelB', 'c'), 'msg': 'Field required', 'input': {'a': 2}},\n            {'type': 'missing', 'loc': ('ModelB', 'd'), 'msg': 'Field required', 'input': {'a': 2}},\n        ]", "target": "def resolve_ref_schema(self, maybe_ref_schema: core_schema.CoreSchema) -> core_schema.CoreSchema:\n        if maybe_ref_schema['type'] == 'definition-ref':\n            ref = maybe_ref_schema['schema_ref']\n            definition = self._generate_schema.defs.get_schema_from_ref(ref)\n            if definition is None:\n                raise LookupError(\n                    f'Could not find a ref for {ref}.'\n                    ' Maybe you tried to call resolve_ref_schema from within a recursive model?'\n                )\n            return definition\n        elif maybe_ref_schema['type'] == 'definitions':\n            return self.resolve_ref_schema(maybe_ref_schema['schema'])\n        return maybe_ref_schema", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "train", "example_id": "001928", "source": "def cherry_pick(self, ref: str) -> None:\n        self._run_git(\"cherry-pick\", \"-x\", ref)", "target": "def init_stacked_lstm(num_layers, layer, first_layer_args, other_layer_args):\n    layers = [layer(*first_layer_args)] + [\n        layer(*other_layer_args) for _ in range(num_layers - 1)\n    ]\n    return nn.ModuleList(layers)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "train", "example_id": "001929", "source": "def run_llama2_7b_int8(device: str = \"cuda\"):\n    model = GPTModelConfig(\n        \"Llama-2-7b-chat-hf\",\n        LLaMA,\n        \"int8\",\n        LLaMAWeightOnlyInt8QuantHandler,\n        144,\n        957,\n        136,\n    )\n    token_per_sec, memory_bandwidth, compilation_time = run_experiment(\n        model, device=device\n    )\n    return [\n        Experiment(\n            model.name,\n            \"token_per_sec\",\n            model.token_per_sec,\n            f\"{token_per_sec:.02f}\",\n            model.mode,\n            device,\n            get_arch_name(),\n            True,\n        ),\n        Experiment(\n            model.name,\n            \"memory_bandwidth(GB/s)\",\n            model.memory_bandwidth,\n            f\"{memory_bandwidth:.02f}\",\n            model.mode,\n            device,\n            get_arch_name(),\n            True,\n        ),\n        Experiment(\n            model.name,\n            \"compilation_time(s)\",\n            model.compilation_time,\n            f\"{compilation_time:.02f}\",\n            model.mode,\n            device,\n            get_arch_name(),\n            True,\n        ),\n    ]", "target": "def recurrent_scaleshift(x, scale, shift):\n    y = x\n    for i in range(64):\n        y = scale * y + shift\n    return y", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "001930", "source": "def make_estimator(self, params):\n        (method,) = params\n        estimator = TSNE(random_state=0, method=method)\n        return estimator", "target": "def make_estimator(self, params):\n        representation, solver, n_jobs = params\n        penalty = \"l2\" if solver == \"lbfgs\" else \"l1\"\n        estimator = LogisticRegression(\n            solver=solver,\n            penalty=penalty,\n            tol=0.01,\n            n_jobs=n_jobs,\n            random_state=0,\n        )\n        return estimator", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001931", "source": "def test_dataclass_field_plain_validator():\n    @dataclasses.dataclass\n    class Foo:\n        a: int\n        b: str\n        @classmethod\n        def validate_b(cls, v: bytes, info: core_schema.ValidationInfo) -> str:\n            assert v == b'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return 'hello world!'\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(\n                    name='b', schema=core_schema.with_info_plain_validator_function(Foo.validate_b)\n                ),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 1, 'b': b'hello'})\n    assert dataclasses.asdict(foo) == {'a': 1, 'b': 'hello world!'}", "target": "def test_nested_schema_inlined(benchmark: Callable[..., None]) -> None:\n    v = SchemaValidator(inlined_schema())\n    data = input_data_valid()\n    v.validate_python(data)\n    benchmark(v.validate_python, data)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "train", "example_id": "001932", "source": "def summarize_build_info(vllm_commit: str) -> bool:\n    torch_sha = os.getenv(\"GITHUB_SHA\")\n    md = (\n        _TPL_VLLM_INFO.render(vllm_commit=vllm_commit, torch_sha=torch_sha).strip()\n        + \"\\n\"\n    )\n    return write_gh_step_summary(md)", "target": "def compiled(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, w, dy = args\n        y = torch.compile(self.rms_norm_ref, mode=self.compile_mode, fullgraph=True)(\n            x, w\n        )\n        return lambda: torch.autograd.grad(\n            y, [x, w], grad_outputs=dy, retain_graph=True\n        )", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001933", "source": "def test_validate_assignment_ignore_extra():\n    v = SchemaValidator(\n        core_schema.model_fields_schema(fields={'field_a': core_schema.model_field(schema=core_schema.str_schema())})\n    )\n    assert v.validate_python({'field_a': 'test'}) == ({'field_a': 'test'}, None, {'field_a'})\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_assignment({'field_a': 'test'}, 'other_field', 456)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'no_such_attribute',\n            'loc': ('other_field',),\n            'msg': \"Object has no attribute 'other_field'\",\n            'input': 456,\n            'ctx': {'attribute': 'other_field'},\n        }\n    ]", "target": "def test_one_choice():\n    v = SchemaValidator(core_schema.union_schema(choices=[core_schema.str_schema()]))\n    assert (\n        plain_repr(v)\n        == 'SchemaValidator(title=\"str\",validator=Str(StrValidator{strict:false,coerce_numbers_to_str:false}),definitions=[],cache_strings=True)'\n    )\n    assert v.validate_python('hello') == 'hello'", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001934", "source": "def schema_validator(self) -> SchemaValidator:\n        return SchemaValidator(\n            schema=core_schema.union_schema(\n                choices=[\n                    core_schema.model_schema(\n                        cls=self.ModelA,\n                        schema=core_schema.model_fields_schema(\n                            fields={\n                                'a': core_schema.model_field(schema=core_schema.int_schema()),\n                                'b': core_schema.model_field(schema=core_schema.str_schema()),\n                            }\n                        ),\n                    ),\n                    core_schema.model_schema(\n                        cls=self.ModelB,\n                        schema=core_schema.model_fields_schema(\n                            fields={\n                                'c': core_schema.model_field(schema=core_schema.int_schema()),\n                                'd': core_schema.model_field(schema=core_schema.str_schema()),\n                            }\n                        ),\n                    ),\n                ]\n            )\n        )", "target": "def schema_validator(self) -> SchemaValidator:\n        return SchemaValidator(\n            schema=core_schema.union_schema(\n                choices=[\n                    core_schema.model_schema(\n                        cls=self.ModelA,\n                        schema=core_schema.model_fields_schema(\n                            fields={\n                                'a': core_schema.model_field(schema=core_schema.int_schema()),\n                                'b': core_schema.model_field(schema=core_schema.str_schema()),\n                            }\n                        ),\n                    ),\n                    core_schema.model_schema(\n                        cls=self.ModelB,\n                        schema=core_schema.model_fields_schema(\n                            fields={\n                                'a': core_schema.model_field(schema=core_schema.int_schema()),\n                                'b': core_schema.model_field(schema=core_schema.str_schema()),\n                                'c': core_schema.model_field(\n                                    schema=core_schema.with_default_schema(\n                                        schema=core_schema.float_schema(), default=1.0\n                                    )\n                                ),\n                            }\n                        ),\n                    ),\n                ]\n            )\n        )", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "001935", "source": "def test_any_model():\n    @dataclasses.dataclass\n    class Foo:\n        a: str\n        b: bytes\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo', [core_schema.dataclass_field(name='a', schema=core_schema.str_schema())]\n        ),\n        ['a'],\n    )\n    Foo.__pydantic_validator__ = SchemaValidator(schema)\n    Foo.__pydantic_serializer__ = SchemaSerializer(schema)\n    s = SchemaSerializer(core_schema.any_schema())\n    assert s.to_python(Foo(a='hello', b=b'more')) == IsStrictDict(a='hello')\n    assert s.to_python(Foo(a='hello', b=b'more'), mode='json') == IsStrictDict(a='hello')\n    j = s.to_json(Foo(a='hello', b=b'more'))\n    if on_pypy:\n        assert json.loads(j) == {'a': 'hello'}\n    else:\n        assert j == b'{\"a\":\"hello\"}'\n    assert s.to_python(Foo(a='hello', b=b'more'), exclude={'a'}) == IsStrictDict()\n    assert s.to_json(Foo(a='hello', b=b'more'), exclude={'a'}) == b'{}'\n    assert s.to_python(Foo) == Foo\n    with pytest.raises(PydanticSerializationError, match=r\"Unable to serialize unknown type: <class 'type'>\"):\n        s.to_python(Foo, mode='json')\n    with pytest.raises(PydanticSerializationError, match=r\"Unable to serialize unknown type: <class 'type'>\"):\n        s.to_json(Foo)\n    assert s.to_python(Foo, mode='json', fallback=lambda x: x.__name__) == 'Foo'\n    assert s.to_json(Foo, fallback=lambda x: x.__name__) == b'\"Foo\"'", "target": "def test_custom_op_sizeR(self):\n            roi = (10, 15, 100, 150)\n            expected = (100, 150)\n            g_r  = cv.GOpaque.Rect()\n            g_sz = GSizeR.on(g_r)\n            comp = cv.GComputation(cv.GIn(g_r), cv.GOut(g_sz))\n            pkg = cv.gapi.kernels(GSizeRImpl)\n            actual = comp.apply(cv.gin(roi), args=cv.gapi.compile_args(pkg))\n            self.assertEqual(0.0, cv.norm(expected, actual, cv.NORM_INF))", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001936", "source": "def test_frozenset_from_dict_items(input_value, items_schema, expected):\n    v = SchemaValidator(core_schema.tuple_schema(items_schema=[items_schema], variadic_item_index=0))\n    output = v.validate_python(input_value)\n    assert isinstance(output, tuple)\n    assert output == expected", "target": "def test_use_after():\n    v = SchemaValidator(\n        core_schema.tuple_positional_schema(\n            [\n                core_schema.definitions_schema(\n                    core_schema.definition_reference_schema('foobar'), [core_schema.int_schema(ref='foobar')]\n                ),\n                core_schema.definition_reference_schema('foobar'),\n            ]\n        )\n    )\n    assert v.validate_python(['1', '2']) == (1, 2)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001937", "source": "def test_dataclass_validate_assignment():\n    schema = core_schema.dataclass_schema(\n        FooDataclass,\n        core_schema.dataclass_args_schema(\n            'FooDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), kw_only=False),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), kw_only=False),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 'hello', 'b': 'True'})\n    assert dataclasses.asdict(foo) == {'a': 'hello', 'b': True}\n    v.validate_assignment(foo, 'a', b'world')\n    assert dataclasses.asdict(foo) == {'a': 'world', 'b': True}\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_assignment(foo, 'a', 123)\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'string_type', 'loc': ('a',), 'msg': 'Input should be a valid string', 'input': 123}\n    ]\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_assignment(foo, 'c', '123')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'no_such_attribute',\n            'loc': ('c',),\n            'msg': \"Object has no attribute 'c'\",\n            'input': '123',\n            'ctx': {'attribute': 'c'},\n        }\n    ]\n    assert not hasattr(foo, 'c')\n    with pytest.raises(AttributeError, match=\"'str' object has no attribute 'a'\"):\n        v.validate_assignment('field_a', 'c', 123)", "target": "def test_dataclass_post_init_args_multiple():\n    dc_args = None\n    @dataclasses.dataclass\n    class Foo:\n        a: str\n        b: dataclasses.InitVar[bool]\n        c: dataclasses.InitVar[int]\n        def __post_init__(self, *args):\n            nonlocal dc_args\n            dc_args = args\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.bool_schema(), init_only=True),\n                core_schema.dataclass_field(name='c', schema=core_schema.int_schema(), init_only=True),\n            ],\n            collect_init_only=True,\n        ),\n        ['a', 'b'],\n        post_init=True,\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': b'hello', 'b': 'true', 'c': '42'})\n    assert dataclasses.asdict(foo) == {'a': 'hello'}\n    assert dc_args == (True, 42)", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|pytorch/pytorch", "data_split": "train", "example_id": "001938", "source": "def benchmark(estimator, data):\n    gc.collect()\n    print(\"Benching %s\" % estimator)\n    t0 = time()\n    estimator.fit(data)\n    training_time = time() - t0\n    data_t = estimator.transform(data)\n    data_r = estimator.inverse_transform(data_t)\n    reconstruction_error = np.mean(np.abs(data - data_r))\n    return {\"time\": training_time, \"error\": reconstruction_error}", "target": "def gh_close_pr(org: str, repo: str, pr_num: int, dry_run: bool = False) -> None:\n    url = f\"{GITHUB_API_URL}/repos/{org}/{repo}/pulls/{pr_num}\"\n    if dry_run:\n        print(f\"Dry run closing PR {pr_num}\")\n    else:\n        gh_fetch_url(url, method=\"PATCH\", data={\"state\": \"closed\"})", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "001939", "source": "def forward(self, x):\n                total = sum(t.item() for t in x)\n                return total // 2", "target": "def typed_dict_field(\n    schema: CoreSchema,\n    *,\n    required: bool | None = None,\n    validation_alias: str | list[str | int] | list[list[str | int]] | None = None,\n    serialization_alias: str | None = None,\n    serialization_exclude: bool | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization_exclude_if: Callable[[Any], bool] | None = None,\n) -> TypedDictField:\n    return _dict_not_none(\n        type='typed-dict-field',\n        schema=schema,\n        required=required,\n        validation_alias=validation_alias,\n        serialization_alias=serialization_alias,\n        serialization_exclude=serialization_exclude,\n        serialization_exclude_if=serialization_exclude_if,\n        metadata=metadata,\n    )", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "001940", "source": "def make_data(self, params):\n        representation, solver, n_jobs = params\n        if Benchmark.data_size == \"large\":\n            if representation == \"sparse\":\n                data = _20newsgroups_highdim_dataset(n_samples=10000)\n            else:\n                data = _20newsgroups_lowdim_dataset(n_components=1e3)\n        else:\n            if representation == \"sparse\":\n                data = _20newsgroups_highdim_dataset(n_samples=2500)\n            else:\n                data = _20newsgroups_lowdim_dataset()\n        return data", "target": "def make_data(self, params):\n        (representation,) = params\n        if representation == \"dense\":\n            data = _synth_regression_dataset(n_samples=1000000, n_features=100)\n        else:\n            data = _synth_regression_sparse_dataset(\n                n_samples=10000, n_features=100000, density=0.01\n            )\n        return data", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001941", "source": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "target": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "001942", "source": "def test_config(config: CoreConfig, input_value, expected):\n    v = SchemaValidator(\n        core_schema.typed_dict_schema(\n            fields={\n                'a': core_schema.typed_dict_field(schema=core_schema.int_schema()),\n                'b': core_schema.typed_dict_field(schema=core_schema.float_schema(), required=False),\n            },\n            config=config,\n        )\n    )\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            val = v.validate_python(input_value)\n            print(f'UNEXPECTED OUTPUT: {val!r}')\n    else:\n        output_dict = v.validate_python(input_value)\n        assert output_dict == expected", "target": "def constants(self) -> Dict[str, ConstantNode]:\n        return self._children[ASTNodeType.Constant]", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001943", "source": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "target": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001944", "source": "def liger(self, args, kwargs) -> Any:\n        from liger_kernel.transformers.rms_norm import LigerRMSNorm\n        x, w = args\n        M, N = x.shape\n        liger_rmsnorm = LigerRMSNorm(hidden_size=N, eps=1e-6).cuda()\n        liger_rmsnorm.weight.data.copy_(w)\n        return lambda: liger_rmsnorm(x)", "target": "def liger(self, args, kwargs=None) -> Any:\n        from liger_kernel.transformers.rms_norm import LigerRMSNorm\n        x, w, dy = args\n        M, N = x.shape\n        liger_rmsnorm = LigerRMSNorm(\n            hidden_size=N, eps=1e-6, casting_mode=\"gemma\"\n        ).cuda()\n        liger_rmsnorm.weight.data.copy_(w)\n        y = liger_rmsnorm(x)\n        return lambda: torch.autograd.grad(\n            y, [x, liger_rmsnorm.weight], grad_outputs=dy, retain_graph=True\n        )", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|opencv/opencv", "data_split": "train", "example_id": "001945", "source": "def fake_temp_env(map: dict[str, str]):\n        temp_calls.append(map)\n        return nullcontext()", "target": "def parseFloatMetric(self, xmlnode, name, default = 0):\n        if name in self.properties:\n            self.metrix[name] = float(self.properties[name])\n        elif xmlnode.hasAttribute(name):\n            self.metrix[name] = float(xmlnode.getAttribute(name))\n        else:\n            self.metrix[name] = default", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001946", "source": "def quack(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, target = args\n        from quack.cross_entropy import _cross_entropy\n        return lambda: _cross_entropy(x, target)", "target": "def quack(self, args, kwargs=None) -> Any:\n        from quack.softmax import softmax\n        assert kwargs is None\n        (x,) = args\n        return lambda: softmax(x)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001947", "source": "def f(inp, *weights):\n            x = inp\n            for w in weights:\n                x = torch.matmul(w, x).sin().sin()\n            return x", "target": "def f(a, b):\n            z = torch.mm(a, b)\n            for i in range(200):\n                z = torch.mm(z, b)\n            return z", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001948", "source": "def get_correct_answers(img_list, img_classes, net_output_blob):\n    correct_answers = 0\n    for i in range(len(img_list)):\n        indexes = np.argsort(net_output_blob[i])[-5:]\n        correct_index = img_classes[img_list[i]]\n        if correct_index in indexes:\n            correct_answers += 1\n    return correct_answers", "target": "def get_correct_answers(self, img_list, net_output_blob):\n        correct_answers = 0\n        for i in range(len(img_list)):\n            indexes = np.argsort(net_output_blob[i])[-5:]\n            correct_index = self.img_classes[img_list[i]]\n            if correct_index in indexes:\n                correct_answers += 1\n        return correct_answers", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "001949", "source": "def translate_desired_cuda(gpu_arch_type: str, gpu_arch_version: str) -> str:\n    return {\n        \"cpu\": \"cpu\",\n        \"cpu-aarch64\": \"cpu\",\n        \"cpu-s390x\": \"cpu\",\n        \"cuda\": f\"cu{gpu_arch_version.replace('.', '')}\",\n        \"cuda-aarch64\": f\"cu{gpu_arch_version.replace('-aarch64', '').replace('.', '')}\",\n        \"rocm\": f\"rocm{gpu_arch_version}\",\n        \"xpu\": \"xpu\",\n    }.get(gpu_arch_type, gpu_arch_version)", "target": "def validate_b(cls, v: str, info: core_schema.ValidationInfo) -> str:\n            assert v == 'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return 'hello world!'", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "001950", "source": "def test_timedelta_kwargs_strict():\n    v = SchemaValidator(core_schema.timedelta_schema(strict=True, le=timedelta(days=3)))\n    output = v.validate_python(timedelta(days=2, hours=1))\n    assert output == timedelta(days=2, hours=1)", "target": "def _prepare_once(self):\n        torch._dynamo.config.capture_scalar_outputs = True\n        torch.manual_seed(0)\n        self.splits = torch.randint(10, (self.N,))\n        sz = self.splits.sum().item()\n        self.input = torch.randn(sz)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001951", "source": "def test_dict_py():\n    v = SchemaValidator(cs.dict_schema(keys_schema=cs.datetime_schema(), values_schema=cs.int_schema()))\n    assert v.validate_python({datetime(2000, 1, 1): 2, datetime(2000, 1, 2): 4}) == {\n        datetime(2000, 1, 1): 2,\n        datetime(2000, 1, 2): 4,\n    }", "target": "def test_dict_py():\n    v = SchemaValidator(\n        core_schema.dict_schema(keys_schema=core_schema.timedelta_schema(), values_schema=core_schema.int_schema())\n    )\n    assert v.validate_python({timedelta(days=2, hours=1): 2, timedelta(days=2, hours=2): 4}) == {\n        timedelta(days=2, hours=1): 2,\n        timedelta(days=2, hours=2): 4,\n    }", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001952", "source": "def test_simple(self):\n        img1 = self.get_sample('stitching/a1.png')\n        img2 = self.get_sample('stitching/a2.png')\n        stitcher = cv.Stitcher.create(cv.Stitcher_PANORAMA)\n        stitcher.estimateTransform((img1, img2))\n        result, _ = stitcher.composePanorama()\n        assert result == 0", "target": "def test_simple(self):\n        finder = cv.ORB.create()\n        img1 = self.get_sample('stitching/a1.png')\n        img2 = self.get_sample('stitching/a2.png')\n        img_feat1 = cv.detail.computeImageFeatures2(finder, img1)\n        img_feat2 = cv.detail.computeImageFeatures2(finder, img2)\n        matcher = cv.detail.BestOf2NearestMatcher_create()\n        matches_info = matcher.apply(img_feat1, img_feat2)\n        self.assertIsNotNone(matches_info.matches)\n        self.assertIsNotNone(matches_info.inliers_mask)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001953", "source": "def constrain(self: _Pipeline[_InT, _OutT], constraint: _Eq) -> _Pipeline[_InT, _OutT]: ...", "target": "def constrain(self: _Pipeline[_InT, _OutT], constraint: _In) -> _Pipeline[_InT, _OutT]: ...", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001954", "source": "def get_output(self, input_blob):\n        tensor = torch.FloatTensor(input_blob)\n        out = self.net.forward(tensor).numpy()\n        return out", "target": "def get_output(self, input_blob):\n        self.net.setInput(input_blob, self.in_blob_name)\n        return self.net.forward(self.out_blob_name)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001955", "source": "def f(value, serializer):\n        if value == 42:\n            return 42\n        return f'result={serializer(value)}'", "target": "def f(prefix, value, _info):\n        return f'{prefix}{value}'", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001956", "source": "def predict(self, samples):\n        _ret, resp = self.model.predict(samples)\n        return resp.ravel()", "target": "def predict(self, samples):\n        new_samples = self.unroll_samples(samples)\n        _ret, resp = self.model.predict(new_samples)\n        return resp.ravel().reshape(-1, self.class_n).argmax(1)", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "001957", "source": "def make_estimator(self, params):\n        (method,) = params\n        estimator = TSNE(random_state=0, method=method)\n        return estimator", "target": "def make_estimator(self, params):\n        representation, solver = params\n        estimator = Ridge(solver=solver, fit_intercept=False, random_state=0)\n        return estimator", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001958", "source": "def constrain(self: _Pipeline[_InT, _NewOutGt], constraint: annotated_types.Gt) -> _Pipeline[_InT, _NewOutGt]: ...", "target": "def constrain(self: _Pipeline[_InT, _NewOutLt], constraint: annotated_types.Lt) -> _Pipeline[_InT, _NewOutLt]: ...", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001959", "source": "def my_function(a, b, c):\n        return a + b + c", "target": "def my_function(input_value, info):\n        return input_value + 'x'", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001960", "source": "def liger(self, args, kwargs=None) -> Any:\n        from liger_kernel.transformers.softmax import LigerSoftmax\n        assert kwargs is None\n        (x,) = args\n        softmax = LigerSoftmax().to(\"cuda\")\n        return lambda: softmax(x)", "target": "def liger(self, args, kwargs) -> Any:\n        from liger_kernel.ops.layer_norm import layer_norm_backward\n        x, w, dy = args\n        eps = 1e-6\n        mean, rstd = self.compute_mean_rstd(x, eps)\n        M, N = x.shape\n        return lambda: layer_norm_backward(dy, x, w, None, mean, rstd)[0:2]", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "001961", "source": "def make_estimator(self, params):\n        representation, solver, n_jobs = params\n        penalty = \"l2\" if solver == \"lbfgs\" else \"l1\"\n        estimator = LogisticRegression(\n            solver=solver,\n            penalty=penalty,\n            tol=0.01,\n            n_jobs=n_jobs,\n            random_state=0,\n        )\n        return estimator", "target": "def make_estimator(self, params):\n        representation, precompute = params\n        estimator = ElasticNet(precompute=precompute, alpha=0.001, random_state=0)\n        return estimator", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001962", "source": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "target": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "001963", "source": "def test_model_and_literal_union() -> None:\n    class ModelA:\n        pass\n    validator = SchemaValidator(\n        core_schema.union_schema(\n            choices=[\n                core_schema.model_schema(\n                    cls=ModelA,\n                    schema=core_schema.model_fields_schema(\n                        fields={'a': core_schema.model_field(schema=core_schema.int_schema())}\n                    ),\n                ),\n                core_schema.literal_schema(expected=[True]),\n            ]\n        )\n    )\n    m = validator.validate_python({'a': 42})\n    assert isinstance(m, ModelA)\n    assert m.a == 42\n    assert validator.validate_python(True) is True", "target": "def rnn_tanh_cell(input, hidden, w_ih, w_hh, b_ih, b_hh):\n    igates = torch.mm(input, w_ih.t()) + b_ih\n    hgates = torch.mm(hidden, w_hh.t()) + b_hh\n    return torch.tanh(igates + hgates)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "train", "example_id": "001964", "source": "def forward(\n        self, input: Tensor, states: list[list[tuple[Tensor, Tensor]]]\n    ) -> tuple[Tensor, list[list[tuple[Tensor, Tensor]]]]:\n        output_states = jit.annotate(list[list[tuple[Tensor, Tensor]]], [])\n        output = input\n        i = 0\n        for rnn_layer in self.layers:\n            state = states[i]\n            output, out_state = rnn_layer(output, state)\n            output_states += [out_state]\n            i += 1\n        return output, output_states", "target": "def add_label_err_comment(pr: \"GitHubPR\") -> None:\n    if not any(is_label_err_comment(comment) for comment in pr.get_comments()):\n        gh_post_pr_comment(pr.org, pr.project, pr.pr_num, LABEL_ERR_MSG)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pytorch/pytorch", "data_split": "train", "example_id": "001965", "source": "def full_typename(self) -> str:\n        return '_typing.Callable[[{}], {}]'.format(\n            ', '.join(arg.full_typename for arg in self.arg_types),\n            self.ret_type.full_typename\n        )", "target": "def _read_hashes(hash_file: Path):\n    if not hash_file.exists():\n        return {}\n    with hash_file.open(\"r\") as f:\n        lines = f.readlines()\n    hashes = {}\n    for line in lines:\n        hash = line[:64]\n        file = line[66:].strip()\n        hashes[file] = hash\n    return hashes", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001966", "source": "def check_dir(d, create=False, clean=False):\n    d = os.path.abspath(d)\n    log.info(\"Check dir %s (create: %s, clean: %s)\", d, create, clean)\n    if os.path.exists(d):\n        if not os.path.isdir(d):\n            raise Fail(\"Not a directory: %s\" % d)\n        if clean:\n            for x in glob.glob(os.path.join(d, \"*\")):\n                rm_one(x)\n    else:\n        if create:\n            os.makedirs(d)\n    return d", "target": "def check_dir(d):\n    d = str(d)\n    d = os.path.abspath(d)\n    log.info(\"Check directory: '%s'\", d)\n    if os.path.exists(d):\n        if not os.path.isdir(d):\n            raise Fail(\"Not a directory: %s\" % d)\n    else:\n        raise Fail(\"The directory is missing: %s\" % d)\n    return Path(d)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001967", "source": "def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = F.max_pool2d(x, 2)\n        x = torch.flatten(x, 1)\n        output = self.fc1(x)\n        return output", "target": "def forward(self, x):\n                total = sum(t.item() for t in x)\n                return total // 2", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|pydantic/pydantic", "data_split": "train", "example_id": "001968", "source": "def make_estimator(self, params):\n        representation, precompute = params\n        estimator = Lasso(precompute=precompute, alpha=0.001, random_state=0)\n        return estimator", "target": "def LazyClassAttribute(name: str, get_value: Callable[[], T]) -> T: ...", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001969", "source": "def quack(self, args, kwargs=None) -> Any:\n        from quack.cross_entropy import cross_entropy\n        assert kwargs is None\n        x, target, dloss = args\n        loss = cross_entropy(x, target)\n        return lambda: torch.autograd.grad(\n            loss, x, grad_outputs=dloss, retain_graph=True\n        )", "target": "def quack(self, args, kwargs=None) -> Any:\n        from quack.softmax import softmax\n        assert kwargs is None\n        x, dy = args\n        y = softmax(x)\n        return lambda: torch.autograd.grad(y, x, grad_outputs=dy, retain_graph=True)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001970", "source": "def forward(self, x):\n        for i, l in enumerate(self.linears):\n            x = self.linears[i // 2](x) + l(x)\n        return x", "target": "def forward(self, x):\n                total = sum(t.item() for t in x)\n                return total // 2", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "001971", "source": "def predict(est, data_test, target_test):\n    if args.no_predict:\n        return\n    tic = time()\n    predicted_test = est.predict(data_test)\n    predicted_proba_test = est.predict_proba(data_test)\n    toc = time()\n    roc_auc = roc_auc_score(target_test, predicted_proba_test[:, 1])\n    acc = accuracy_score(target_test, predicted_test)\n    print(f\"predicted in {toc - tic:.3f}s, ROC AUC: {roc_auc:.4f}, ACC: {acc:.4f}\")", "target": "def predict(est, data_test):\n    if args.no_predict:\n        return\n    tic = time()\n    est.predict(data_test)\n    toc = time()\n    print(f\"predicted in {toc - tic:.3f}s\")", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001972", "source": "def test_function_before():\n    s = SchemaSerializer(\n        core_schema.with_info_before_validator_function(lambda v, info: v + 1, core_schema.int_schema())\n    )\n    assert plain_repr(s) == 'SchemaSerializer(serializer=Int(IntSerializer),definitions=[])'", "target": "def tuple_schema(\n    items_schema: list[CoreSchema],\n    *,\n    variadic_item_index: int | None = None,\n    min_length: int | None = None,\n    max_length: int | None = None,\n    fail_fast: bool | None = None,\n    strict: bool | None = None,\n    ref: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: IncExSeqOrElseSerSchema | None = None,\n) -> TupleSchema:\n    return _dict_not_none(\n        type='tuple',\n        items_schema=items_schema,\n        variadic_item_index=variadic_item_index,\n        min_length=min_length,\n        max_length=max_length,\n        fail_fast=fail_fast,\n        strict=strict,\n        ref=ref,\n        metadata=metadata,\n        serialization=serialization,\n    )", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001973", "source": "def compiled(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, dy = args\n        compiled_softmax = torch.compile(\n            lambda x: F.softmax(x, dim=-1), mode=self.compile_mode, fullgraph=True\n        )\n        y = compiled_softmax(x)\n        return lambda: torch.autograd.grad(y, x, grad_outputs=dy, retain_graph=True)", "target": "def compiled(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, w, dy = args\n        y = torch.compile(self.rms_norm_ref, mode=self.compile_mode, fullgraph=True)(\n            x, w\n        )\n        return lambda: torch.autograd.grad(\n            y, [x, w], grad_outputs=dy, retain_graph=True\n        )", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001974", "source": "def test_union():\n    v = SchemaValidator(core_schema.union_schema(choices=[core_schema.str_schema(), core_schema.time_schema()]))\n    assert v.validate_python('12:01:02') == '12:01:02'\n    assert v.validate_python(time(12, 1, 2)) == time(12, 1, 2)\n    v = SchemaValidator(core_schema.union_schema(choices=[core_schema.time_schema(), core_schema.str_schema()]))\n    assert v.validate_python('12:01:02') == '12:01:02'\n    assert v.validate_python(time(12, 1, 2)) == time(12, 1, 2)", "target": "def test_union():\n    v = SchemaValidator(core_schema.union_schema(choices=[core_schema.str_schema(), core_schema.timedelta_schema()]))\n    assert v.validate_python('P2DT1H') == 'P2DT1H'\n    assert v.validate_python(timedelta(days=2, hours=1)) == timedelta(days=2, hours=1)\n    v = SchemaValidator(core_schema.union_schema(choices=[core_schema.timedelta_schema(), core_schema.str_schema()]))\n    assert v.validate_python('P2DT1H') == 'P2DT1H'\n    assert v.validate_python(timedelta(days=2, hours=1)) == timedelta(days=2, hours=1)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "001975", "source": "def test_dataclass_field_wrap_validator1():\n    @dataclasses.dataclass\n    class Foo:\n        a: int\n        b: str\n        @classmethod\n        def validate_b(\n            cls, v: bytes, nxt: core_schema.ValidatorFunctionWrapHandler, info: core_schema.ValidationInfo\n        ) -> str:\n            assert v == b'hello'\n            v = nxt(v)\n            assert v == 'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return 'hello world!'\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(\n                    name='b',\n                    schema=core_schema.with_info_wrap_validator_function(Foo.validate_b, core_schema.str_schema()),\n                ),\n            ],\n        ),\n        ['a', 'b'],\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 1, 'b': b'hello'})\n    assert dataclasses.asdict(foo) == {'a': 1, 'b': 'hello world!'}", "target": "def check_graph_breaks(actual_csv, expected_csv, expected_filename):\n    failed = []\n    improved = []\n    if \"rocm\" in expected_filename:\n        flaky_models.update(\n            {\n                \"alexnet\",\n                \"demucs\",\n                \"densenet121\",\n                \"detectron2_fcos_r_50_fpn\",\n                \"doctr_det_predictor\",\n                \"doctr_reco_predictor\",\n                \"levit_128\",\n                \"llava\",\n                \"microbench_unbacked_tolist_sum\",\n                \"resnet50\",\n                \"resnet152\",\n                \"sam\",\n                \"sam_fast\",\n                \"stable_diffusion_text_encoder\",\n                \"stable_diffusion_unet\",\n                \"timm_efficientdet\",\n                \"torchrec_dlrm\",\n                \"vgg16\",\n                \"meta-llama/Llama-3.2-1B\",\n                \"google/gemma-2-2b\",\n                \"google/gemma-3-4b-it\",\n                \"openai/whisper-tiny\",\n                \"Qwen/Qwen3-0.6B\",\n                \"mistralai/Mistral-7B-Instruct-v0.3\",\n                \"openai/gpt-oss-20b\",\n            }\n        )\n    for model in actual_csv[\"name\"]:\n        graph_breaks = get_field(actual_csv, model, \"graph_breaks\")\n        expected_graph_breaks = get_field(expected_csv, model, \"graph_breaks\")\n        flaky = model in flaky_models\n        if expected_graph_breaks is None:\n            status = \"MISSING:\"\n            improved.append(model)\n        elif graph_breaks == expected_graph_breaks:\n            status = \"PASS_BUT_FLAKY\" if flaky else \"PASS\"\n            print(f\"{model:34}  {status}\")\n            continue\n        elif graph_breaks > expected_graph_breaks:\n            if flaky:\n                status = \"FAIL_BUT_FLAKY:\"\n            else:\n                status = \"FAIL:\"\n                failed.append(model)\n        elif graph_breaks < expected_graph_breaks:\n            if flaky:\n                status = \"IMPROVED_BUT_FLAKY:\"\n            else:\n                status = \"IMPROVED:\"\n                improved.append(model)\n        print(\n            f\"{model:34}  {status:19} graph_breaks={graph_breaks}, expected={expected_graph_breaks}\"\n        )\n    msg = \"\"\n    if failed or improved:\n        if failed:\n            msg += textwrap.dedent(\n                f\n            )\n        if improved:\n            msg += textwrap.dedent(\n                f\n            )\n        sha = os.getenv(\"SHA1\", \"{your CI commit sha}\")\n        msg += textwrap.dedent(\n            f\n        )\n    return failed or improved, msg", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001976", "source": "def node_type(self) -> ASTNodeType:\n        return ASTNodeType.Class", "target": "def node_type(self) -> ASTNodeType:\n        return ASTNodeType.Constant", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001977", "source": "def run(img1, img2, dtype):\n            return cv.add(img1, img2)", "target": "def run(img, sc, dtype):\n            return img + np.array(sc, dtype=np.uint8)[:-1]", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "train", "example_id": "001978", "source": "def fn():\n            result = compiled_fn(a)\n            assert counters[\"inductor\"][\"fxgraph_cache_miss\"] == 0\n            assert counters[\"inductor\"][\"fxgraph_cache_hit\"] == 1\n            return result", "target": "def test_merged_lastfailed_content_with_empty_source(self) -> None:\n        last_failed_source = {\n            \"\": True,\n        }\n        last_failed_dest = {\n            \"tools/tests/test_car.py::test_num1\": True,\n            \"tools/tests/test_car.py::test_num2\": True,\n        }\n        last_failed_merged = {\n            \"tools/tests/test_car.py::test_num1\": True,\n            \"tools/tests/test_car.py::test_num2\": True,\n        }\n        merged = _merged_lastfailed_content(last_failed_source, last_failed_dest)\n        self.assertEqual(merged, last_failed_merged)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001979", "source": "def test_slots_mixed(any_serializer):\n    @dataclasses.dataclass(slots=True)\n    class Model:\n        x: int\n        y: dataclasses.InitVar[str]\n        z: ClassVar[str] = 'z-classvar'\n    @dataclasses.dataclass\n    class SubModel(Model):\n        x2: int\n        y2: dataclasses.InitVar[str]\n        z2: ClassVar[str] = 'z2-classvar'\n    dc = SubModel(x=1, y='a', x2=2, y2='b')\n    assert dataclasses.asdict(dc) == {'x': 1, 'x2': 2}\n    assert any_serializer.to_python(dc) == {'x': 1, 'x2': 2}\n    assert any_serializer.to_json(dc) == b'{\"x\":1,\"x2\":2}'", "target": "def test_slots_mixed():\n    @dataclasses.dataclass(slots=True)\n    class Model:\n        x: int\n        y: dataclasses.InitVar[str]\n        z: ClassVar[str] = 'z-classvar'\n    @dataclasses.dataclass\n    class SubModel(Model):\n        x2: int\n        y2: dataclasses.InitVar[str]\n        z2: ClassVar[str] = 'z2-classvar'\n    schema = core_schema.dataclass_schema(\n        SubModel,\n        core_schema.dataclass_args_schema(\n            'SubModel',\n            [\n                core_schema.dataclass_field(name='x', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(name='y', init_only=True, schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='x2', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(name='y2', init_only=True, schema=core_schema.str_schema()),\n            ],\n        ),\n        ['x'],\n        slots=True,\n    )\n    v = SchemaValidator(schema)\n    dc = v.validate_python({'x': 1, 'y': 'a', 'x2': 2, 'y2': 'b'})\n    assert dc.x == 1\n    assert dc.x2 == 2\n    assert dataclasses.asdict(dc) == {'x': 1, 'x2': 2}", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "001980", "source": "def make_data(self, params):\n        representation, solver = params\n        if representation == \"dense\":\n            data = _synth_regression_dataset(n_samples=500000, n_features=100)\n        else:\n            data = _synth_regression_sparse_dataset(\n                n_samples=100000, n_features=10000, density=0.005\n            )\n        return data", "target": "def make_data(self, params):\n        (representation,) = params\n        if representation == \"dense\":\n            data = _synth_regression_dataset(n_samples=1000000, n_features=100)\n        else:\n            data = _synth_regression_sparse_dataset(\n                n_samples=10000, n_features=100000, density=0.01\n            )\n        return data", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001981", "source": "def test_generator_error():\n    def gen(error: bool):\n        yield 1\n        yield 2\n        if error:\n            raise RuntimeError('my error')\n        yield 3\n    v = SchemaValidator(cs.frozenset_schema(items_schema=cs.int_schema()))\n    r = v.validate_python(gen(False))\n    assert r == {1, 2, 3}\n    assert isinstance(r, frozenset)\n    msg = r'Error iterating over object, error: RuntimeError: my error \\[type=iteration_error,'\n    with pytest.raises(ValidationError, match=msg):\n        v.validate_python(gen(True))", "target": "def test_generator_error():\n    def gen(error: bool):\n        yield 1\n        yield 2\n        if error:\n            raise RuntimeError('error')\n        yield 3\n    v = SchemaValidator(cs.list_schema(items_schema=cs.int_schema()))\n    assert v.validate_python(gen(False)) == [1, 2, 3]\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python(gen(True))\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'iteration_error',\n            'loc': (2,),\n            'msg': 'Error iterating over object, error: RuntimeError: error',\n            'input': HasRepr(IsStr(regex='<generator object test_generator_error.<locals>.gen at 0x[0-9a-fA-F]+>')),\n            'ctx': {'error': 'RuntimeError: error'},\n        }\n    ]", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001982", "source": "def make_v1_generic_root_validator(\n    validator: V1RootValidatorFunction, pre: bool\n) -> V2CoreBeforeRootValidator | V2CoreAfterRootValidator:\n    if pre is True:\n        def _wrapper1(values: RootValidatorValues, _: core_schema.ValidationInfo) -> RootValidatorValues:\n            return validator(values)\n        return _wrapper1\n    def _wrapper2(fields_tuple: RootValidatorFieldsTuple, _: core_schema.ValidationInfo) -> RootValidatorFieldsTuple:\n        if len(fields_tuple) == 2:\n            values, init_vars = fields_tuple\n            values = validator(values)\n            return values, init_vars\n        else:\n            model_dict, model_extra, fields_set = fields_tuple\n            if model_extra:\n                fields = set(model_dict.keys())\n                model_dict.update(model_extra)\n                model_dict_new = validator(model_dict)\n                for k in list(model_dict_new.keys()):\n                    if k not in fields:\n                        model_extra[k] = model_dict_new.pop(k)\n            else:\n                model_dict_new = validator(model_dict)\n            return model_dict_new, model_extra, fields_set\n    return _wrapper2", "target": "def test_constructor(self):\n        for subminute in [timedelta(microseconds=1), timedelta(seconds=1)]:\n            tz = TzInfo(subminute.total_seconds())\n            self.assertNotEqual(tz.utcoffset(None) % timedelta(minutes=1), 0)\n        for invalid in [timedelta(1, 1), timedelta(1)]:\n            self.assertRaises(ValueError, TzInfo, invalid.total_seconds())\n            self.assertRaises(ValueError, TzInfo, -invalid.total_seconds())\n        with self.assertRaises(TypeError):\n            TzInfo(None)\n        with self.assertRaises(TypeError):\n            TzInfo(timedelta(seconds=42))\n        with self.assertRaises(TypeError):\n            TzInfo(ZERO, None)\n        with self.assertRaises(TypeError):\n            TzInfo(ZERO, 42)\n        with self.assertRaises(TypeError):\n            TzInfo(ZERO, 'ABC', 'extra')", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001983", "source": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "target": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|scikit-learn/scikit-learn", "data_split": "train", "example_id": "001984", "source": "def test_repeat_after():\n    with pytest.raises(SchemaError, match='SchemaError: Duplicate ref: `foobar`'):\n        SchemaValidator(\n            schema=core_schema.definitions_schema(\n                core_schema.tuple_positional_schema(\n                    [\n                        core_schema.definitions_schema(\n                            core_schema.definition_reference_schema('foobar'), [core_schema.int_schema(ref='foobar')]\n                        ),\n                        core_schema.definition_reference_schema('foobar'),\n                    ]\n                ),\n                [core_schema.int_schema(ref='foobar')],\n            )\n        )", "target": "def make_estimator(self, params):\n        representation, algorithm, init = params\n        max_iter = 30 if representation == \"sparse\" else 100\n        estimator = KMeans(\n            n_clusters=20,\n            algorithm=algorithm,\n            init=init,\n            n_init=1,\n            max_iter=max_iter,\n            tol=0,\n            random_state=0,\n        )\n        return estimator", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001985", "source": "def wrap_function(input_value, validator, info):\n        return f'Input {validator(input_value)} Changed'", "target": "def wrap_function(input_value, validator, info):\n            try:\n                return validator(input_value)\n            except ValidationError:\n                if isinstance(input_value, list):\n                    return str(len(input_value))\n                else:\n                    return repr(input_value)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001986", "source": "def constrain(\n        self: _Pipeline[_InT, _NewOutDatetime], constraint: annotated_types.Timezone\n    ) -> _Pipeline[_InT, _NewOutDatetime]: ...", "target": "def constrain(self: _Pipeline[_InT, _OutT], constraint: annotated_types.Predicate) -> _Pipeline[_InT, _OutT]: ...", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|pytorch/pytorch", "data_split": "train", "example_id": "001987", "source": "def plot_scatter(X, color, alpha=0.5):\n    return plt.scatter(X[:, 0], X[:, 1], c=color, alpha=alpha, edgecolor=\"k\")", "target": "def eqTensor(self, input: Tensor) -> Tensor:\n        return input", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001988", "source": "def children_types(self) -> Tuple[ASTNodeType, ...]:\n        return (ASTNodeType.Namespace, ASTNodeType.Class, ASTNodeType.Function,\n                ASTNodeType.Enumeration, ASTNodeType.Constant)", "target": "def children_types(self) -> Tuple[ASTNodeType, ...]:\n        return (ASTNodeType.Class, ASTNodeType.Function,\n                ASTNodeType.Enumeration, ASTNodeType.Constant)", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001989", "source": "def typename(self) -> str:\n        return \"\"", "target": "def typename(self) -> str:\n        return self.alias_export_name", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001990", "source": "def test_alias_extra_forbid(py_and_json: PyAndJson):\n    v = py_and_json(\n        {\n            'type': 'model-fields',\n            'extra_behavior': 'forbid',\n            'fields': {'field_a': {'type': 'model-field', 'validation_alias': 'FieldA', 'schema': {'type': 'int'}}},\n        }\n    )\n    assert v.validate_test({'FieldA': 1}) == ({'field_a': 1}, None, {'field_a'})", "target": "def test_alias_extra_forbid(py_and_json: PyAndJson):\n    v = py_and_json(\n        {\n            'type': 'typed-dict',\n            'extra_behavior': 'forbid',\n            'fields': {\n                'field_a': {'type': 'typed-dict-field', 'validation_alias': 'FieldA', 'schema': {'type': 'int'}}\n            },\n        }\n    )\n    assert v.validate_test({'FieldA': 1}) == {'field_a': 1}", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "001991", "source": "def test_non_model_field_before_validator_field_info() -> None:\n    class Model:\n        x: str\n    def f(input_value: Any, info: core_schema.ValidationInfo) -> Any:\n        assert info.field_name == 'x'\n        assert info.data == {}\n        assert isinstance(input_value, bytes)\n        return f'input: {input_value.decode()}'\n    v = SchemaValidator(\n        core_schema.model_schema(\n            Model,\n            core_schema.model_fields_schema(\n                {\n                    'x': core_schema.model_field(\n                        core_schema.with_info_before_validator_function(f, core_schema.str_schema())\n                    )\n                }\n            ),\n        )\n    )\n    assert v.validate_python({'x': b'foo'}).x == 'input: foo'", "target": "def lax_or_strict_schema(\n    lax_schema: CoreSchema,\n    strict_schema: CoreSchema,\n    *,\n    strict: bool | None = None,\n    ref: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> LaxOrStrictSchema:\n    return _dict_not_none(\n        type='lax-or-strict',\n        lax_schema=lax_schema,\n        strict_schema=strict_schema,\n        strict=strict,\n        ref=ref,\n        metadata=metadata,\n        serialization=serialization,\n    )", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001992", "source": "def f(a, b):\n            result = a.clone()\n            for i in range(1000):\n                if i % 3 == 0:\n                    result = result + b\n                elif i % 3 == 1:\n                    result = result + 8 * b\n                else:\n                    result = result.sin()\n            return result", "target": "def f(inp, *weights):\n            x = inp\n            for w in weights:\n                x = torch.matmul(w, x).sin().sin()\n            return x", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001993", "source": "def constrain(self: _Pipeline[_InT, _NewOutGt], constraint: annotated_types.Gt) -> _Pipeline[_InT, _NewOutGt]: ...", "target": "def constrain(self: _Pipeline[_InT, _NewOutLe], constraint: annotated_types.Le) -> _Pipeline[_InT, _NewOutLe]: ...", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "001994", "source": "def resolve(self, root: ASTNode):\n        try:\n            self.value.resolve(root)\n        except TypeResolutionError as e:\n            raise TypeResolutionError(\n                'Failed to resolve alias \"{}\" exposed as \"{}\"'.format(\n                    self.ctype_name, self.typename\n                )\n            ) from e", "target": "def resolve(self, root: ASTNode):\n        if self.is_resolved:\n            return\n        node = _resolve_symbol(root, self.typename)\n        if node is None:\n            raise TypeResolutionError('Failed to resolve \"{}\" exposed as \"{}\"'.format(\n                self.ctype_name, self.typename\n            ))\n        self._ast_node = weakref.proxy(node)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "001995", "source": "def test_alias_allow_pop(py_and_json: PyAndJson):\n    v = py_and_json(\n        {\n            'type': 'model-fields',\n            'fields': {'field_a': {'validation_alias': 'FieldA', 'type': 'model-field', 'schema': {'type': 'int'}}},\n        },\n        config=CoreConfig(validate_by_name=True),\n    )\n    assert v.validate_test({'FieldA': '123'}) == ({'field_a': 123}, None, {'field_a'})\n    assert v.validate_test({'field_a': '123'}) == ({'field_a': 123}, None, {'field_a'})\n    assert v.validate_test({'FieldA': '1', 'field_a': '2'}) == ({'field_a': 1}, None, {'field_a'})\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'foobar': '123'})", "target": "def test_alias_allow_pop(py_and_json: PyAndJson):\n    v = py_and_json(\n        {\n            'type': 'typed-dict',\n            'fields': {\n                'field_a': {'validation_alias': 'FieldA', 'type': 'typed-dict-field', 'schema': {'type': 'int'}}\n            },\n            'config': {'validate_by_name': True, 'validate_by_alias': True},\n        },\n    )\n    assert v.validate_test({'FieldA': '123'}) == {'field_a': 123}\n    assert v.validate_test({'field_a': '123'}) == {'field_a': 123}\n    assert v.validate_test({'FieldA': '1', 'field_a': '2'}) == {'field_a': 1}\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'foobar': '123'})", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|opencv/opencv", "data_split": "train", "example_id": "001996", "source": "def register_test_commands(subparsers: argparse._SubParsersAction) -> None:\n    build_parser = subparsers.add_parser(\n        \"test\",\n        help=\"test related commands\",\n        formatter_class=RichHelp,\n    )\n    build_subparsers = build_parser.add_subparsers(dest=\"test_command\", required=True)\n    overview = \"\\n\".join(\n        f\"  {name:12} {spec.get('help', '')}\" for name, spec in _TARGETS.items()\n    )\n    external_parser = build_subparsers.add_parser(\n        \"external\",\n        help=\"Test external targets\",\n        description=\"Test third-party targets.\\n\\nAvailable targets:\\n\" + overview,\n        formatter_class=RichHelp,\n    )\n    register_targets(external_parser, _TARGETS, common_args=common_args)", "target": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|opencv/opencv", "data_split": "train", "example_id": "001997", "source": "def add_constant(self, name: str, value: str) -> ConstantNode:\n        return self._add_child(ConstantNode, name, value=value)", "target": "def write_data(self, fname):\n        fs = cv.FileStorage(fname, cv.FileStorage_WRITE)\n        R = self.R0\n        T = self.T0\n        m = MyData()\n        fs.write('iterationNr', 100)\n        fs.startWriteStruct('strings', cv.FileNode_SEQ)\n        for elem in self.strings_data:\n            fs.write('', elem)\n        fs.endWriteStruct()\n        fs.startWriteStruct('Mapping', cv.FileNode_MAP)\n        fs.write('One', 1)\n        fs.write('Two', 2)\n        fs.endWriteStruct()\n        fs.write('R_MAT', R)\n        fs.write('T_MAT', T)\n        m.write(fs, 'MyData')\n        fs.release()", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pytorch/pytorch", "data_split": "train", "example_id": "001998", "source": "def delete_on_failure(self, file_path):\n            if not self.has_failure and not file_path.is_file():\n                file_path.parent.mkdir(parents=True, exist_ok=True)\n                file_path.touch()\n            try:\n                yield\n            finally:\n                if self.has_failure and file_path.is_file():\n                    file_path.unlink()", "target": "def script_lstm(\n    input_size,\n    hidden_size,\n    num_layers,\n    bias=True,\n    batch_first=False,\n    dropout=False,\n    bidirectional=False,\n):\n    assert bias\n    assert not batch_first\n    if bidirectional:\n        stack_type = StackedLSTM2\n        layer_type = BidirLSTMLayer\n        dirs = 2\n    elif dropout:\n        stack_type = StackedLSTMWithDropout\n        layer_type = LSTMLayer\n        dirs = 1\n    else:\n        stack_type = StackedLSTM\n        layer_type = LSTMLayer\n        dirs = 1\n    return stack_type(\n        num_layers,\n        layer_type,\n        first_layer_args=[LSTMCell, input_size, hidden_size],\n        other_layer_args=[LSTMCell, hidden_size * dirs, hidden_size],\n    )", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "001999", "source": "def get_shapes(self) -> tuple[tuple[int, ...], ...]:\n        return (\n            (32768, 256),\n            (32768, 512),\n            (32768, 1024),\n            (32768, 2048),\n            (32768, 4096),\n            (32768, 8192),\n            (32768, 16384),\n            (32768, 32768),\n            (32768, 65536),\n            (16384, 131072),\n            (8192, 262144),\n        )", "target": "def get_shapes(self) -> tuple[tuple[int, ...], ...]:\n        return (\n            (32768, 256),\n            (32768, 512),\n            (32768, 1024),\n            (32768, 2048),\n            (32768, 4096),\n            (32768, 8192),\n            (32768, 16384),\n        ) + extra_shapes_for_norm", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "002000", "source": "def ser_x(data: Model, v: Any, _) -> str:\n        assert data['x'] == 1_000\n        return f'{v:_}'", "target": "def ser_x(data: Model, v: Any) -> str:\n        assert data['x'] == 1_000\n        return f'{v:_}'", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "002001", "source": "def test_union():\n    v = SchemaValidator(cs.union_schema(choices=[cs.str_schema(), cs.date_schema()]))\n    assert v.validate_python('2022-01-02') == '2022-01-02'\n    assert v.validate_python(date(2022, 1, 2)) == date(2022, 1, 2)\n    v = SchemaValidator(cs.union_schema(choices=[cs.date_schema(), cs.str_schema()]))\n    assert v.validate_python('2022-01-02') == '2022-01-02'\n    assert v.validate_python(date(2022, 1, 2)) == date(2022, 1, 2)", "target": "def test_union():\n    v = SchemaValidator(cs.union_schema(choices=[cs.str_schema(), cs.datetime_schema()]))\n    assert v.validate_python('2022-01-02T00:00') == '2022-01-02T00:00'\n    assert v.validate_python(datetime(2022, 1, 2)) == datetime(2022, 1, 2)\n    v = SchemaValidator(cs.union_schema(choices=[cs.datetime_schema(), cs.str_schema()]))\n    assert v.validate_python('2022-01-02T00:00') == '2022-01-02T00:00'\n    assert v.validate_python(datetime(2022, 1, 2)) == datetime(2022, 1, 2)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "002002", "source": "def get_shapes(self) -> tuple[tuple[int, ...], ...]:\n        return (\n            (32768, 256),\n            (32768, 512),\n            (32768, 1024),\n            (32768, 2048),\n            (32768, 4096),\n            (32768, 8192),\n            (32768, 16384),\n            (32768, 32768),\n            (32768, 65536),\n            (16384, 131072),\n            (8192, 262144),\n        )", "target": "def get_shapes(self) -> tuple[tuple[int, ...], ...]:\n        return (\n            (32768, 256),\n            (32768, 512),\n            (32768, 1024),\n            (32768, 2048),\n            (32768, 4096),\n            (32768, 8192),\n            (32768, 16384),\n            (32768, 32768),\n            (32768, 65536),\n            (16384, 131072),\n            (8192, 262144),\n        )", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|opencv/opencv", "data_split": "train", "example_id": "002003", "source": "def update(self, op_code, cur, max=None, message=\"\"):\n        msg = self._cur_line or message\n        if max and cur:\n            percent = int(cur / max * 100)\n            if percent != self._last_percent and percent % self._interval == 0:\n                self._last_percent = percent\n                logger.info(\"Progress: %d%% - %s\", percent, msg)\n        elif msg:\n            logger.info(msg)", "target": "def find_function_node(root: NamespaceNode, function_symbol: SymbolName,\n                       create_missing_namespaces: bool = False) -> FunctionNode:\n    scope = find_scope(root, function_symbol, create_missing_namespaces)\n    if function_symbol.name not in scope.functions:\n        raise SymbolNotFoundError(\n            \"Can't find {} in its scope\".format(function_symbol)\n        )\n    return scope.functions[function_symbol.name]", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "002004", "source": "def validate_b(cls, v: str, info: core_schema.ValidationInfo) -> str:\n            assert v == 'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return 'hello world!'", "target": "def validate_b(cls, v: str, info: core_schema.ValidationInfo) -> str:\n            assert v == 'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return 'hello world!'", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "002005", "source": "def eager(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, dy = args\n        y = F.softmax(x, dim=-1)\n        return lambda: torch.autograd.grad(y, x, grad_outputs=dy, retain_graph=True)", "target": "def f(input_value):\n        input_value.more = 'foobar'\n        return input_value", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pytorch/pytorch", "data_split": "train", "example_id": "002006", "source": "def prepare_overload_arguments_and_return_type(variant):\n        arguments = []\n        for i, (_, argno) in enumerate(variant.py_arglist):\n            arg_info = variant.args[argno]\n            type_node = create_type_node(arg_info.tp)\n            if arg_info.pathlike and type_node.typename == \"str\":\n                type_node = PathLikeTypeNode.string_or_pathlike_()\n            default_value = None\n            if len(arg_info.defval):\n                default_value = arg_info.defval\n            if variant.is_arg_optional(i):\n                if arg_info.py_outputarg:\n                    type_node = OptionalTypeNode(type_node)\n                    default_value = \"None\"\n                elif arg_info.isbig() and \"None\" not in type_node.typename:\n                    type_node = OptionalTypeNode(type_node)\n            arguments.append(\n                FunctionNode.Arg(arg_info.export_name, type_node=type_node,\n                                 default_value=default_value)\n            )\n        if func_info.isconstructor:\n            return arguments, None\n        if len(variant.py_outlist) > 1:\n            ret_types = []\n            if variant.py_outlist[0][1] == -1:\n                ret_types.append(create_type_node(variant.rettype))\n                outlist = variant.py_outlist[1:]\n            else:\n                outlist = variant.py_outlist\n            for _, argno in outlist:\n                assert argno >= 0, \\\n                    f\"Logic Error! Outlist contains function return type: {outlist}\"\n                ret_types.append(create_type_node(variant.args[argno].tp))\n            return arguments, FunctionNode.RetType(\n                TupleTypeNode(\"return_type\", ret_types)\n            )\n        if len(variant.py_outlist) == 1:\n            if variant.rettype:\n                return arguments, FunctionNode.RetType(\n                    create_type_node(variant.rettype)\n                )\n            ret_type = variant.args[variant.py_outlist[0][1]].tp\n            return arguments, FunctionNode.RetType(\n                create_type_node(ret_type)\n            )\n        return arguments, None", "target": "def test_params_missing_output_dir(self, _is_path):\n        with self.assertRaises(FileNotFoundError):\n            vllm_build.VllmBuildParameters()", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "train", "example_id": "002007", "source": "def lstm_multilayer_creator(script=True, **kwargs):\n    input, hidden, params, _ = lstm_inputs(return_module=False, **kwargs)\n    inputs = [input, hidden, flatten_list(params)]\n    return ModelDef(\n        inputs=inputs,\n        params=flatten_list(params),\n        forward=lstm_factory_multilayer(lstm_cell, script),\n        backward_setup=lstm_backward_setup,\n        backward=simple_backward,\n    )", "target": "def imagenet_cnn_creator(arch, jit=True):\n    def creator(device=\"cuda\", **kwargs):\n        model = arch().to(device)\n        x = torch.randn(32, 3, 224, 224, device=device)\n        if jit:\n            model = torch.jit.trace(model, x)\n        return ModelDef(\n            inputs=(x,),\n            params=list(model.parameters()),\n            forward=model,\n            backward_setup=simple_backward_setup,\n            backward=simple_backward,\n        )\n    return creator", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "002008", "source": "def f(value, serializer, _info):\n        return f'result={serializer(len(value))}'", "target": "def f(value, serializer):\n        return f'result={serializer(len(value))} repr={serializer!r}'", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "002009", "source": "def get_shapes(self) -> tuple[tuple[int, ...], ...]:\n        return (\n            (32768, 256),\n            (32768, 512),\n            (32768, 1024),\n            (32768, 2048),\n            (32768, 4096),\n            (32768, 8192),\n            (32768, 16384),\n            (32768, 32768),\n            (32768, 65536),\n            (16384, 131072),\n            (8192, 262144),\n        )", "target": "def get_shapes(self) -> tuple[tuple[int, ...], ...]:\n        return (\n            (32768, 256),\n            (32768, 512),\n            (32768, 1024),\n            (32768, 2048),\n            (32768, 4096),\n            (32768, 8192),\n            (32768, 16384),\n            (32768, 32768),\n            (32768, 65536),\n            (16384, 131072),\n            (8192, 262144),\n        )", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pytorch/pytorch", "data_split": "train", "example_id": "002010", "source": "def test_multi_dict_arucodetector(self):\n        aruco_params = cv.aruco.DetectorParameters()\n        aruco_dicts = [\n                cv.aruco.getPredefinedDictionary(cv.aruco.DICT_4X4_250),\n                cv.aruco.getPredefinedDictionary(cv.aruco.DICT_5X5_250)\n            ]\n        aruco_detector = cv.aruco.ArucoDetector(aruco_dicts, aruco_params)\n        id = 2\n        marker_size = 100\n        offset = 10\n        img_marker1 = cv.aruco.generateImageMarker(aruco_dicts[0], id, marker_size, aruco_params.markerBorderBits)\n        img_marker1 = np.pad(img_marker1, pad_width=offset, mode='constant', constant_values=255)\n        img_marker2 = cv.aruco.generateImageMarker(aruco_dicts[1], id, marker_size, aruco_params.markerBorderBits)\n        img_marker2 = np.pad(img_marker2, pad_width=offset, mode='constant', constant_values=255)\n        img_markers = np.concatenate((img_marker1, img_marker2), axis=1)\n        corners, ids, rejected, dictIndices = aruco_detector.detectMarkersMultiDict(img_markers)\n        self.assertEqual(2, len(ids))\n        self.assertEqual(id, ids[0])\n        self.assertEqual(id, ids[1])\n        self.assertEqual(2, len(dictIndices))\n        self.assertEqual(0, dictIndices[0])\n        self.assertEqual(1, dictIndices[1])", "target": "def get_multiheadattn(device: torch.device) -> GetterReturnType:\n    embed_dim, nhead, tgt_len, src_len, bsz = 10, 5, 6, 10, 64\n    in_proj = models.InProjContainer(\n        torch.nn.Linear(embed_dim, embed_dim, bias=False),\n        torch.nn.Linear(embed_dim, embed_dim, bias=False),\n        torch.nn.Linear(embed_dim, embed_dim, bias=False),\n    )\n    model = models.MultiheadAttentionContainer(\n        nhead,\n        in_proj,\n        models.ScaledDotProduct(),\n        torch.nn.Linear(embed_dim, embed_dim, bias=False),\n    )\n    model.to(device)\n    params, names = extract_weights(model)\n    query = torch.rand((tgt_len, bsz, embed_dim), device=device)\n    key = value = torch.rand((src_len, bsz, embed_dim), device=device)\n    attn_mask_2D = torch.randint(0, 2, (tgt_len, src_len), device=device).to(torch.bool)\n    bias_k = bias_v = torch.rand((1, 1, embed_dim), device=device)\n    attn_mask = torch.stack([attn_mask_2D] * (bsz * nhead))\n    bias_k = bias_k.repeat(1, bsz, 1).reshape(1, bsz * nhead, -1)\n    bias_v = bias_v.repeat(1, bsz, 1).reshape(1, bsz * nhead, -1)\n    def forward(*new_params: Tensor) -> Tensor:\n        load_weights(model, names, new_params)\n        mha_output, attn_weights = model(\n            query, key, value, attn_mask=attn_mask, bias_k=bias_k, bias_v=bias_v\n        )\n        loss = mha_output.sum() + attn_weights.sum()\n        return loss\n    return forward, params", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "002011", "source": "def assertGreater(self, a, b, msg=None):\n            if not a > b:\n                self.fail('%s not greater than %s' % (repr(a), repr(b)))", "target": "def test_model_a(self, schema_validator: SchemaValidator):\n        m_a = schema_validator.validate_python({'a': 1, 'b': 'hello'})\n        assert isinstance(m_a, self.ModelA)\n        assert m_a.a == 1\n        assert m_a.b == 'hello'", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "002012", "source": "def dec(f: Callable[..., Any] | classmethod[Any, Any, Any] | staticmethod[Any, Any]) -> Any:\n        if _decorators.is_instance_method_from_sig(f):\n            raise TypeError('`@root_validator` cannot be applied to instance methods')\n        res = _decorators.ensure_classmethod_based_on_signature(f)\n        dec_info = _decorators.RootValidatorDecoratorInfo(mode=mode)\n        return _decorators.PydanticDescriptorProxy(res, dec_info, shim=wrap)", "target": "def dec(f: FieldSerializer) -> _decorators.PydanticDescriptorProxy[Any]:\n        dec_info = _decorators.FieldSerializerDecoratorInfo(\n            fields=fields,\n            mode=mode,\n            return_type=return_type,\n            when_used=when_used,\n            check_fields=check_fields,\n        )\n        return _decorators.PydanticDescriptorProxy(f, dec_info)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "002013", "source": "def eager(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, target, dloss = args\n        loss = F.cross_entropy(x, target, reduction=\"none\")\n        return lambda: torch.autograd.grad(\n            loss, x, grad_outputs=dloss, retain_graph=True\n        )", "target": "def eager(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, dy = args\n        y = F.softmax(x, dim=-1)\n        return lambda: torch.autograd.grad(y, x, grad_outputs=dy, retain_graph=True)", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "002014", "source": "def required_usage_imports(self) -> Generator[str, None, None]:\n        yield \"import typing as _typing\"\n        yield from super().required_usage_imports", "target": "def required_usage_imports(self) -> Generator[str, None, None]:\n        yield \"import os\"", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "002015", "source": "def make_estimator(self, params):\n        (method,) = params\n        estimator = TSNE(random_state=0, method=method)\n        return estimator", "target": "def make_estimator(self, params):\n        (representation,) = params\n        n_estimators = 100 if Benchmark.data_size == \"large\" else 10\n        estimator = GradientBoostingClassifier(\n            n_estimators=n_estimators,\n            max_features=\"log2\",\n            subsample=0.5,\n            random_state=0,\n        )\n        return estimator", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|opencv/opencv", "data_split": "train", "example_id": "002016", "source": "def getAlias(self, fname):\n        return sorted(self.getAliases(fname), key=len)[0]", "target": "def make_circles_pattern(self):\n        spacing = self.square_size\n        r = spacing / self.radius_rate\n        pattern_width = ((self.cols - 1.0) * spacing) + (2.0 * r)\n        pattern_height = ((self.rows - 1.0) * spacing) + (2.0 * r)\n        x_spacing = (self.width - pattern_width) / 2.0\n        y_spacing = (self.height - pattern_height) / 2.0\n        for x in range(0, self.cols):\n            for y in range(0, self.rows):\n                dot = SVG(\"circle\", cx=(x * spacing) + x_spacing + r,\n                          cy=(y * spacing) + y_spacing + r, r=r, fill=\"black\", stroke=\"none\")\n                self.g.append(dot)", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "002017", "source": "def make_estimator(self, params):\n        representation, solver = params\n        estimator = Ridge(solver=solver, fit_intercept=False, random_state=0)\n        return estimator", "target": "def make_estimator(self, params):\n        representation, n_jobs = params\n        n_estimators = 500 if Benchmark.data_size == \"large\" else 100\n        estimator = RandomForestClassifier(\n            n_estimators=n_estimators,\n            min_samples_split=10,\n            max_features=\"log2\",\n            n_jobs=n_jobs,\n            random_state=0,\n        )\n        return estimator", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "002018", "source": "def find_class_node(self, class_info, namespaces):\n            return ClassNode()", "target": "def test_left_to_right_union_strict():\n    choices = [core_schema.int_schema(strict=True), core_schema.float_schema(strict=True)]\n    v = SchemaValidator(core_schema.union_schema(choices, mode='left_to_right'))\n    out = v.validate_python(1)\n    assert out == 1\n    assert isinstance(out, int)\n    out = v.validate_python(1.0)\n    assert out == 1.0\n    assert isinstance(out, float)\n    v = SchemaValidator(\n        core_schema.union_schema(\n            list(reversed(choices)),\n            mode='left_to_right',\n        )\n    )\n    out = v.validate_python(1.0)\n    assert out == 1.0\n    assert isinstance(out, float)\n    out = v.validate_python(1)\n    assert out == 1.0\n    assert isinstance(out, float)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "002019", "source": "def forward_pass(self, mod, inputs, collect_outputs=True):\n        with self.autocast(**self.autocast_arg):\n            return mod(*inputs)", "target": "def forward_pass(self, mod, inputs, collect_outputs=True):\n        with self.autocast(**self.autocast_arg):\n            if isinstance(inputs, dict):\n                return mod(**inputs)\n            else:\n                return mod(*inputs)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "002020", "source": "def _work(self):\n        @torch.compile(\n            backend=self.backend(),\n            fullgraph=True,\n            dynamic=self.is_dynamic(),\n        )\n        def f(a, b):\n            result = a.clone()\n            for i in range(1000):\n                if i % 3 == 0:\n                    result = result + b\n                elif i % 3 == 1:\n                    result = result + 8 * b\n                else:\n                    result = result.sin()\n            return result\n        with fresh_cache():\n            f(self.a, self.b)", "target": "def _work(self):\n        @torch.compile(backend=self.backend(), fullgraph=True)\n        def f(*args):\n            outs = [torch.add(x, x) for x in args]\n            return outs\n        f(*self._args)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "002021", "source": "def test_invalid_schema():\n    with pytest.raises(SchemaError, match='Definitions error: definition `Branch` was never filled'):\n        SchemaValidator(\n            schema=cs.list_schema(\n                items_schema=cs.typed_dict_schema(\n                    fields={\n                        'width': cs.typed_dict_field(schema=cs.int_schema()),\n                        'branch': cs.typed_dict_field(\n                            schema=cs.with_default_schema(\n                                schema=cs.nullable_schema(schema=cs.definition_reference_schema(schema_ref='Branch')),\n                                default=None,\n                            )\n                        ),\n                    }\n                )\n            )\n        )", "target": "def test_aruco_dicts(self):\n        try:\n            import cairosvg\n        except:\n            raise self.skipTest(\"cairosvg library was not found\")\n        else:\n            cols = 3\n            rows = 5\n            square_size = 100\n            aruco_type = [cv.aruco.DICT_4X4_1000, cv.aruco.DICT_5X5_1000, cv.aruco.DICT_6X6_1000,\n                        cv.aruco.DICT_7X7_1000, cv.aruco.DICT_ARUCO_ORIGINAL, cv.aruco.DICT_APRILTAG_16h5,\n                        cv.aruco.DICT_APRILTAG_25h9, cv.aruco.DICT_APRILTAG_36h10, cv.aruco.DICT_APRILTAG_36h11]\n            aruco_type_str = ['DICT_4X4_1000','DICT_5X5_1000', 'DICT_6X6_1000',\n                        'DICT_7X7_1000', 'DICT_ARUCO_ORIGINAL', 'DICT_APRILTAG_16h5',\n                        'DICT_APRILTAG_25h9', 'DICT_APRILTAG_36h10', 'DICT_APRILTAG_36h11']\n            marker_size = 0.8*square_size\n            board_width = cols*square_size\n            board_height = rows*square_size\n            for aruco_type_i in range(len(aruco_type)):\n                aruco_dict = cv.aruco.getPredefinedDictionary(aruco_type[aruco_type_i])\n                board = cv.aruco.CharucoBoard((cols, rows), square_size, marker_size, aruco_dict)\n                charuco_detector = cv.aruco.CharucoDetector(board)\n                from_cv_img = board.generateImage((cols*square_size, rows*square_size))\n                fd1, filesvg = tempfile.mkstemp(prefix=\"out\", suffix=\".svg\")\n                os.close(fd1)\n                fd2, filepng = tempfile.mkstemp(prefix=\"svg_marker\", suffix=\".png\")\n                os.close(fd2)\n                try:\n                    basedir = os.path.abspath(os.path.dirname(__file__))\n                    pm = generate_pattern.PatternMaker(cols, rows, filesvg, \"px\", square_size, 0, board_width,\n                                board_height, \"charuco_checkboard\", marker_size,\n                                os.path.join(basedir, aruco_type_str[aruco_type_i]+'.json.gz'), 0)\n                    pm.make_charuco_board()\n                    pm.save()\n                    cairosvg.svg2png(url=filesvg, write_to=filepng, background_color=\"white\")\n                    from_svg_img = cv.imread(filepng)\n                    _charucoCorners, _charuco_ids_svg, marker_corners_svg, marker_ids_svg = charuco_detector.detectBoard(from_svg_img)\n                    _charucoCorners, _charuco_ids_cv, marker_corners_cv, marker_ids_cv = charuco_detector.detectBoard(from_cv_img)\n                    marker_corners_svg_map, marker_corners_cv_map = {}, {}\n                    for i in range(len(marker_ids_svg)):\n                        marker_corners_svg_map[int(marker_ids_svg[i][0])] = marker_corners_svg[i]\n                    for i in range(len(marker_ids_cv)):\n                        marker_corners_cv_map[int(marker_ids_cv[i][0])] = marker_corners_cv[i]\n                    for key_svg in marker_corners_svg_map.keys():\n                        marker_svg = marker_corners_svg_map[key_svg]\n                        marker_cv = marker_corners_cv_map[key_svg]\n                        np.testing.assert_allclose(marker_svg, marker_cv, 0.1, 0.1)\n                finally:\n                    if os.path.exists(filesvg):\n                        os.remove(filesvg)\n                    if os.path.exists(filepng):\n                        os.remove(filepng)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "002022", "source": "def predict(self, samples):\n        _ret, resp = self.model.predict(samples)\n        return resp.ravel()", "target": "def predict(self, samples):\n        _ret, resp = self.model.predict(samples)\n        return resp.argmax(-1)", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "002023", "source": "def required_usage_imports(self) -> Generator[str, None, None]:\n        yield from ()", "target": "def required_usage_imports(self) -> Generator[str, None, None]:\n        yield \"import cv2.typing\"", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "002024", "source": "def rebuild_dataclass_fields(\n    cls: type[PydanticDataclass],\n    *,\n    config_wrapper: ConfigWrapper,\n    ns_resolver: NsResolver,\n    typevars_map: Mapping[TypeVar, Any],\n) -> dict[str, FieldInfo]:\n    FieldInfo_ = import_cached_field_info()\n    rebuilt_fields: dict[str, FieldInfo] = {}\n    with ns_resolver.push(cls):\n        for f_name, field_info in cls.__pydantic_fields__.items():\n            if field_info._complete:\n                rebuilt_fields[f_name] = field_info\n            else:\n                existing_desc = field_info.description\n                ann = _typing_extra.eval_type(\n                    field_info._original_annotation,\n                    *ns_resolver.types_namespace,\n                )\n                ann = _generics.replace_types(ann, typevars_map)\n                new_field = FieldInfo_.from_annotated_attribute(\n                    ann,\n                    field_info._original_assignment,\n                    _source=AnnotationSource.DATACLASS,\n                )\n                new_field.description = new_field.description if new_field.description is not None else existing_desc\n                update_field_from_config(config_wrapper, f_name, new_field)\n                rebuilt_fields[f_name] = new_field\n    return rebuilt_fields", "target": "def forward(self, x):\n        for i, l in enumerate(self.linears):\n            x = self.linears[i // 2](x) + l(x)\n        return x", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "002025", "source": "def test_core_schema_type_literal():\n    def get_type_value(schema_typeddict) -> str:\n        annotation = get_type_hints(schema_typeddict, include_extras=True)['type']\n        inspected_ann = inspect_annotation(annotation, annotation_source=AnnotationSource.TYPED_DICT)\n        annotation = inspected_ann.type\n        assert annotation is not UNKNOWN\n        assert typing_objects.is_literal(get_origin(annotation)), (\n            f\"The 'type' key of core schemas must be a Literal form, got {get_origin(annotation)}\"\n        )\n        args = get_args(annotation)\n        assert len(args) == 1, (\n            f\"The 'type' key of core schemas must be a Literal form with a single element, got {len(args)} elements\"\n        )\n        type_ = args[0]\n        assert isinstance(type_, str), (\n            f\"The 'type' key of core schemas must be a Literal form with a single string element, got element of type {type(type_)}\"\n        )\n        return type_\n    schema_types = (get_type_value(x) for x in CoreSchema.__args__)\n    schema_types = tuple(dict.fromkeys(schema_types))\n    if get_args(CoreSchemaType) != schema_types:\n        literal = ''.join(f'\\n    {e!r},' for e in schema_types)\n        print(\n            f'python code (near end of python/pydantic_core/core_schema.py):\\n\\nCoreSchemaType = Literal[{literal}\\n]'\n        )\n        pytest.fail('core_schema.CoreSchemaType needs to be updated')", "target": "def test_config(config: CoreConfig, input_value, expected):\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            fields={\n                'a': core_schema.model_field(schema=core_schema.int_schema()),\n                'b': core_schema.model_field(\n                    schema=core_schema.with_default_schema(schema=core_schema.float_schema(), default=4.2)\n                ),\n            }\n        ),\n        config=config,\n    )\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            val = v.validate_python(input_value)\n            print(f'UNEXPECTED OUTPUT: {val!r}')\n    else:\n        result = v.validate_python(input_value)\n        assert result == expected", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "002026", "source": "def test_bad_repr():\n    b = BadRepr()\n    error_msg = '^Unable to serialize unknown type: <unprintable BedReprMeta object>$'\n    with pytest.raises(PydanticSerializationError, match=error_msg):\n        to_jsonable_python(b)\n    assert to_jsonable_python(b, serialize_unknown=True) == '<Unserializable BadRepr object>'\n    with pytest.raises(PydanticSerializationError, match=error_msg):\n        to_json(b)\n    assert to_json(b, serialize_unknown=True) == b'\"<Unserializable BadRepr object>\"'", "target": "def simple_backward_setup(output, seed=None):\n    assert isinstance(output, torch.Tensor)\n    if seed:\n        torch.manual_seed(seed)\n    grad_output = torch.randn_like(output)\n    return output, grad_output", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "002027", "source": "def get_memory_bytes(self, args, kwargs) -> int:\n        (x,) = args\n        M, N = x.shape\n        return 2 * M * N * x.dtype.itemsize", "target": "def get_memory_bytes(self, args, kwargs) -> int:\n        x, w, dy = args\n        M, N = x.shape\n        return (\n            2 * M * N * x.dtype.itemsize\n            + 2 * N * w.dtype.itemsize\n            + M * N * dy.dtype.itemsize\n        )", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "002028", "source": "def getOS(self):\n        return getPlatformVersion() or self.cache.getOS()", "target": "def test_no_args_constructor(self):\n        tz = TzInfo()\n        self.assertEqual(tz.utcoffset(None), timedelta(0))\n        self.assertEqual(str(tz), 'UTC')", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "train", "example_id": "002029", "source": "def compiled(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, w = args\n        torch._dynamo.mark_dynamic(x, 0)\n        compiled_rms_norm = torch.compile(\n            self.rms_norm_ref, mode=self.compile_mode, fullgraph=True\n        )\n        return lambda: compiled_rms_norm(x, w)", "target": "def get_inputs(\n    config: ExperimentGroupConfig,\n) -> tuple[torch.Tensor, ...]:\n    op_name = config.op_name\n    M, N, K = config.shape\n    batch_size = config.batch_size\n    dtype = config.dtype\n    device = torch.device(\"cuda\")\n    if op_name == \"mm\":\n        A = torch.randn(M, K, dtype=dtype, device=device)\n        B = torch.randn(N, K, dtype=dtype, device=device).t()\n        return A, B\n    elif op_name == \"addmm\":\n        A = torch.randn(M, K, dtype=dtype, device=device)\n        B = torch.randn(N, K, dtype=dtype, device=device).t()\n        C = torch.randn(N, dtype=dtype, device=device)\n        return C, A, B\n    elif op_name == \"bmm\":\n        A = torch.randn(batch_size, M, K, dtype=dtype, device=device)\n        B = torch.randn(batch_size, N, K, dtype=dtype, device=device).permute(0, 2, 1)\n        return A, B\n    elif op_name == \"_scaled_mm\":\n        if dtype != torch.float8_e4m3fn:\n            raise ValueError(f\"_scaled_mm only supports fp8e4m3, got {dtype}\")\n        input_dtype = torch.bfloat16\n        x = torch.randn(M, K, dtype=input_dtype, device=device)\n        w = torch.randn(N, K, dtype=input_dtype, device=device)\n        w_fp8, w_inverse_scale = _quantize_rowwise(w, dtype)\n        w_t_fp8 = w_fp8.t()\n        w_inverse_scale = w_inverse_scale.t()\n        x_fp8, x_inverse_scale = _quantize_rowwise(x, dtype)\n        return (\n            x_fp8,\n            w_t_fp8,\n            x_inverse_scale,\n            w_inverse_scale,\n            None,\n            None,\n            torch.bfloat16,\n            USE_FAST_ACCUM,\n        )\n    else:\n        raise ValueError(f\"Unknown op {op_name}\")", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "002030", "source": "def apply_discriminator(\n    schema: core_schema.CoreSchema,\n    discriminator: str | Discriminator,\n    definitions: dict[str, core_schema.CoreSchema] | None = None,\n) -> core_schema.CoreSchema:\n    from ..types import Discriminator\n    if isinstance(discriminator, Discriminator):\n        if isinstance(discriminator.discriminator, str):\n            discriminator = discriminator.discriminator\n        else:\n            return discriminator._convert_schema(schema)\n    return _ApplyInferredDiscriminator(discriminator, definitions or {}).apply(schema)", "target": "def main():\n    parser = argparse.ArgumentParser(description=\"PyTorch distributed benchmark diff\")\n    parser.add_argument(\"file\", nargs=2)\n    args = parser.parse_args()\n    if len(args.file) != 2:\n        raise RuntimeError(\"Must specify 2 files to diff\")\n    ja = load(args.file[0])\n    jb = load(args.file[1])\n    keys = (set(ja.keys()) | set(jb.keys())) - {\"benchmark_results\"}\n    print(f\"{'':20s} {'baseline':>20s}      {'test':>20s}\")\n    print(f\"{'':20s} {'-' * 20:>20s}      {'-' * 20:>20s}\")\n    for key in sorted(keys):\n        va = str(ja.get(key, \"-\"))\n        vb = str(jb.get(key, \"-\"))\n        print(f\"{key + ':':20s} {va:>20s}  vs  {vb:>20s}\")\n    print()\n    ba = ja[\"benchmark_results\"]\n    bb = jb[\"benchmark_results\"]\n    for ra, rb in zip(ba, bb):\n        if ra[\"model\"] != rb[\"model\"]:\n            continue\n        if ra[\"batch_size\"] != rb[\"batch_size\"]:\n            continue\n        model = ra[\"model\"]\n        batch_size = int(ra[\"batch_size\"])\n        name = f\"{model} with batch size {batch_size}\"\n        print(f\"Benchmark: {name}\")\n        print()\n        print(f\"{'':>10s}\", end=\"\")\n        for _ in [75, 95]:\n            print(f\"{'sec/iter':>16s}{'ex/sec':>10s}{'diff':>10s}\", end=\"\")\n        print()\n        for i, (xa, xb) in enumerate(zip(ra[\"result\"], rb[\"result\"])):\n            if i == 0:\n                continue\n            if len(xa[\"ranks\"]) != len(xb[\"ranks\"]):\n                continue\n            ngpus = len(xa[\"ranks\"])\n            ma = sorted(xa[\"measurements\"])\n            mb = sorted(xb[\"measurements\"])\n            print(f\"{ngpus:>4d} GPUs:\", end=\"\")\n            for p in [75, 95]:\n                va = np.percentile(ma, p)\n                vb = np.percentile(mb, p)\n                delta = -100 * ((vb - va) / va)\n                print(\n                    f\"  p{p:02d}: {vb:8.3f}s {int(batch_size / vb):7d}/s {delta:+8.1f}%\",\n                    end=\"\",\n                )\n            print()\n        print()", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "002031", "source": "def test_only_allow_alias(py_and_json) -> None:\n    v = py_and_json(\n        cs.arguments_v3_schema(\n            [\n                cs.arguments_v3_parameter(\n                    name='a', schema=cs.str_schema(), alias='FieldA', mode='positional_or_keyword'\n                ),\n            ],\n            validate_by_name=False,\n            validate_by_alias=True,\n        )\n    )\n    assert v.validate_test(ArgsKwargs((), {'FieldA': 'hello'})) == ((), {'a': 'hello'})\n    assert v.validate_test({'FieldA': 'hello'}) == (('hello',), {})\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Missing required argument \\[type=missing_argument,'):\n        assert v.validate_test(ArgsKwargs((), {'a': 'hello'}))\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Missing required argument \\[type=missing_argument,'):\n        assert v.validate_test({'a': 'hello'})", "target": "def test_only_allow_alias(py_and_json) -> None:\n    schema = core_schema.dataclass_schema(\n        BasicDataclass,\n        core_schema.dataclass_args_schema(\n            'BasicDataclass',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema(), validation_alias='FieldA'),\n            ],\n        ),\n        ['a'],\n        config=core_schema.CoreConfig(validate_by_name=False, validate_by_alias=True),\n    )\n    v = py_and_json(schema)\n    assert v.validate_test({'FieldA': 'hello'}) == BasicDataclass(a='hello')\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'a': 'hello'})", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "002032", "source": "def make_estimator(self, params):\n        (representation,) = params\n        max_iter = 60 if representation == \"dense\" else 300\n        estimator = SGDRegressor(max_iter=max_iter, tol=None, random_state=0)\n        return estimator", "target": "def make_estimator(self, params):\n        (kernel,) = params\n        estimator = SVC(\n            max_iter=100, tol=1e-16, kernel=kernel, random_state=0, gamma=\"scale\"\n        )\n        return estimator", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "002033", "source": "def f(*args):\n            outs = [torch.add(x, x) for x in args]\n            return outs", "target": "def f(a, b):\n            z = torch.mm(a, b)\n            for i in range(200):\n                z = torch.mm(z, b)\n            return z", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "002034", "source": "def required_definition_imports(self) -> Generator[str, None, None]:\n        yield from self.positive_branch_type.required_usage_imports\n        yield from self.negative_branch_type.required_usage_imports\n        yield from self._condition_required_imports", "target": "def required_definition_imports(self) -> Generator[str, None, None]:\n        yield \"import typing as _typing\"\n        yield from super().required_definition_imports", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "002035", "source": "def test_strict():\n    v = SchemaValidator(\n        {\n            'type': 'typed-dict',\n            'fields': {\n                'field_a': {'type': 'typed-dict-field', 'schema': {'type': 'str'}},\n                'field_b': {'type': 'typed-dict-field', 'schema': {'type': 'int'}},\n            },\n            'config': CoreConfig(strict=True),\n        }\n    )\n    assert v.validate_python({'field_a': 'hello', 'field_b': 12}) == {'field_a': 'hello', 'field_b': 12}\n    with pytest.raises(ValidationError) as exc_info:\n        assert v.validate_python({'field_a': 123, 'field_b': '123'})\n    assert exc_info.value.errors(include_url=False) == [\n        {'type': 'string_type', 'loc': ('field_a',), 'msg': 'Input should be a valid string', 'input': 123},\n        {'type': 'int_type', 'loc': ('field_b',), 'msg': 'Input should be a valid integer', 'input': '123'},\n    ]", "target": "def computed_field(\n    property_name: str, return_schema: CoreSchema, *, alias: str | None = None, metadata: dict[str, Any] | None = None\n) -> ComputedField:\n    return _dict_not_none(\n        type='computed-field', property_name=property_name, return_schema=return_schema, alias=alias, metadata=metadata\n    )", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "002036", "source": "def ser_x(data: Model, v: Any, _) -> str:\n        assert data['x'] == 1_000\n        return f'{v:_}'", "target": "def ser_x(\n        data: Model,\n        v: Any,\n        serializer: core_schema.SerializerFunctionWrapHandler,\n        info: core_schema.FieldSerializationInfo,\n    ) -> str:\n        assert data['x'] == 1_000\n        x = serializer(v)\n        return f'{x:_}-{info.field_name}'", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "002037", "source": "def get_output(self, input_blob):\n        tensor = torch.FloatTensor(input_blob)\n        out = self.net.forward(tensor).numpy()\n        return out", "target": "def get_output(self, input_blob):\n        if self.need_reshape:\n            self.net.blobs[self.in_blob_name].reshape(*input_blob.shape)\n        return self.net.forward_all(**{self.in_blob_name: input_blob})[self.out_blob_name]", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "002038", "source": "def _updated_hashes(hash_file, files_to_hash):\n    old_hashes = _read_hashes(hash_file)\n    new_hashes = _hash_files(files_to_hash)\n    if new_hashes != old_hashes:\n        return new_hashes\n    return None", "target": "def is_core_schema(\n    schema: CoreSchemaOrField,\n) -> TypeGuard[CoreSchema]:\n    return schema['type'] not in _CORE_SCHEMA_FIELD_TYPES", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "002039", "source": "def test_union_float_simple(py_and_json: PyAndJson):\n    v = py_and_json({'type': 'union', 'choices': [{'type': 'float'}, {'type': 'list'}]})\n    assert v.validate_test('5') == 5\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_test('xxx')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'float_parsing',\n            'loc': ('float',),\n            'msg': 'Input should be a valid number, unable to parse string as a number',\n            'input': 'xxx',\n        },\n        {\n            'type': 'list_type',\n            'loc': ('list[any]',),\n            'msg': IsStr(regex='Input should be a valid (list|array)'),\n            'input': 'xxx',\n        },\n    ]", "target": "def test_restores_even_on_exception(self):\n        var = \"TEST_TMP_ENV_EXCEPTION\"\n        self.assertNotIn(var, os.environ)\n        with self.assertRaises(RuntimeError):\n            with temp_environ({var: \"x\"}):\n                self.assertEqual(os.environ[var], \"x\")\n                raise RuntimeError(\"boom\")\n        self.assertNotIn(var, os.environ)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "002040", "source": "def training_iter_fn(batch, model, optimizer):\n    outputs = model(**batch)\n    loss = outputs.loss\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n    return loss", "target": "def test_validate_scientific_notation_from_json(input_value, expected):\n    v = SchemaValidator(cs.decimal_schema())\n    assert v.validate_json(input_value) == expected", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "002041", "source": "def eager(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, w = args\n        return lambda: self.rms_norm_ref(x, w)", "target": "def eager(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, w, dy = args\n        y = self.layernorm_ref(x, w)\n        return lambda: torch.autograd.grad(\n            y, [x, w], grad_outputs=dy, retain_graph=True\n        )", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|opencv/opencv", "data_split": "train", "example_id": "002042", "source": "def track_same_transform(self, *args):\n                est_path = get_estimator_path(self, Benchmark.base_commit, args, True)\n                with est_path.open(mode=\"rb\") as f:\n                    estimator_base = pickle.load(f)\n                X_val_t_base = estimator_base.transform(self.X_val)\n                X_val_t = self.estimator.transform(self.X_val)\n                return np.allclose(X_val_t_base, X_val_t)", "target": "def create_class_node(self, class_info, namespaces):\n            return create_class_node(self.cv_root, class_info, namespaces)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "002043", "source": "def quack(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, target = args\n        from quack.cross_entropy import _cross_entropy\n        return lambda: _cross_entropy(x, target)", "target": "def quack(self, args, kwargs=None) -> Any:\n        from quack.softmax import softmax\n        assert kwargs is None\n        x, dy = args\n        y = softmax(x)\n        return lambda: torch.autograd.grad(y, x, grad_outputs=dy, retain_graph=True)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "002044", "source": "def test_extra_behavior_ignore(\n    config: Union[core_schema.CoreConfig, None],\n    schema_extra_behavior_kw: dict[str, Any],\n    validate_fn_extra_kw: Union[ExtraBehavior, None],\n):\n    @dataclasses.dataclass\n    class MyModel:\n        f: str\n    v = SchemaValidator(\n        core_schema.dataclass_schema(\n            MyModel,\n            core_schema.dataclass_args_schema(\n                'MyModel', [core_schema.dataclass_field('f', core_schema.str_schema())], **schema_extra_behavior_kw\n            ),\n            ['f'],\n        ),\n        config=config,\n    )\n    m: MyModel = v.validate_python({'f': 'x', 'extra_field': 123}, extra=validate_fn_extra_kw)\n    assert m.f == 'x'\n    assert not hasattr(m, 'extra_field')\n    v.validate_assignment(m, 'f', 'y', extra=validate_fn_extra_kw)\n    assert m.f == 'y'\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_assignment(m, 'not_f', 'xyz', extra=validate_fn_extra_kw)\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'no_such_attribute',\n            'loc': ('not_f',),\n            'msg': \"Object has no attribute 'not_f'\",\n            'input': 'xyz',\n            'ctx': {'attribute': 'not_f'},\n        }\n    ]\n    assert not hasattr(m, 'not_f')", "target": "def test_extra_behavior_ignore(\n    config: Union[core_schema.CoreConfig, None],\n    schema_extra_behavior_kw: dict[str, Any],\n    validate_fn_extra_kw: Union[ExtraBehavior, None],\n):\n    v = SchemaValidator(\n        core_schema.typed_dict_schema(\n            {'f': core_schema.typed_dict_field(core_schema.str_schema())}, **schema_extra_behavior_kw\n        ),\n        config=config,\n    )\n    m: dict[str, Any] = v.validate_python({'f': 'x', 'extra_field': 123}, extra=validate_fn_extra_kw)\n    assert m == {'f': 'x'}", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "002045", "source": "def generate_schema(self, source_type: Any, /) -> core_schema.CoreSchema:\n        return self._generate_schema.generate_schema(source_type)", "target": "def generate_schema(self, source_type: Any, /) -> core_schema.CoreSchema:\n        raise NotImplementedError", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|opencv/opencv", "data_split": "train", "example_id": "002046", "source": "def pip_install_first_match(pattern: str, extras: Optional[str] = None, pref_uv=False):\n    wheel = first_matching_pkg(pattern)\n    target = f\"{wheel}[{extras}]\" if extras else wheel\n    logger.info(\"Installing %s...\", target)\n    pip_install_packages([target], prefer_uv=pref_uv)", "target": "def test_parse_to_int_convertible(self):\n        try_to_convert = partial(self._try_to_convert, cv.utils.dumpInt)\n        min_int, max_int = get_limits(ctypes.c_int)\n        for convertible in (-10, -1, 2, int(43.2), np.uint8(15), np.int8(33), np.int16(-13),\n                            np.int32(4), np.int64(345), (23), min_int, max_int, np.int_(33)):\n            expected = 'int: {0:d}'.format(convertible)\n            actual = try_to_convert(convertible)\n            self.assertEqual(expected, actual,\n                             msg=get_conversion_error_msg(convertible, expected, actual))", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "002047", "source": "def _work(self) -> None:\n        @torch.compile(\n            backend=\"inductor\",\n            fullgraph=True,\n            dynamic=self._dynamic,\n        )\n        def f(a, b):\n            z = torch.mm(a, b)\n            for i in range(200):\n                z = torch.mm(z, b)\n            return z\n        with fresh_cache(), torch._inductor.config.patch(max_autotune=True):\n            f(self.a, self.b)", "target": "def _work(self):\n        with (\n            fresh_cache(),\n        ):\n            opt_m = torch.compile(backend=self.backend(), dynamic=self.is_dynamic())(\n                self.m.cuda() if self._is_gpu else self.m\n            )\n            opt_m(self.input)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "train", "example_id": "002048", "source": "def forward(self, x, expert_indices):\n        w1_weights = self.w1.to(x.dtype)[expert_indices]\n        w3_weights = self.w3.to(x.dtype)[expert_indices]\n        w2_weights = self.w2.to(x.dtype)[expert_indices]\n        x1 = F.silu(\n            torch.einsum(\"ti,taoi -> tao\", x, w1_weights)\n            * self.scales1[expert_indices].to(x.dtype)\n        )\n        x3 = torch.einsum(\"ti, taoi -> tao\", x, w3_weights) * self.scales3[\n            expert_indices\n        ].to(x.dtype)\n        expert_outs = torch.einsum(\n            \"tao, taio -> tai\", (x1 * x3), w2_weights\n        ) * self.scales2[expert_indices].to(x.dtype)\n        return expert_outs", "target": "def patterns_to_regex(allowed_patterns: list[str]) -> Any:\n    rc = \"(\"\n    for idx, pattern in enumerate(allowed_patterns):\n        if idx > 0:\n            rc += \"|\"\n        pattern_ = PeekableIterator(pattern)\n        assert not any(c in pattern for c in \"{}()[]\\\\\")\n        for c in pattern_:\n            if c == \".\":\n                rc += \"\\\\.\"\n            elif c == \"+\":\n                rc += \"\\\\+\"\n            elif c == \"*\":\n                if pattern_.peek() == \"*\":\n                    next(pattern_)\n                    rc += \".*\"\n                else:\n                    rc += \"[^/]*\"\n            else:\n                rc += c\n    rc += \")\"\n    return re.compile(rc)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "002049", "source": "def lnlstm_creator(script=True, decompose_layernorm=False, **kwargs):\n    assert script is True\n    from .custom_lstms import script_lnlstm\n    input_size = kwargs[\"inputSize\"]\n    hidden_size = kwargs[\"hiddenSize\"]\n    seq_len = kwargs[\"seqLength\"]\n    batch_size = kwargs[\"miniBatch\"]\n    ge = script_lnlstm(\n        input_size, hidden_size, 1, decompose_layernorm=decompose_layernorm\n    ).cuda()\n    input = torch.randn(seq_len, batch_size, input_size, device=\"cuda\")\n    states = [\n        (\n            torch.randn(batch_size, hidden_size, device=\"cuda\"),\n            torch.randn(batch_size, hidden_size, device=\"cuda\"),\n        )\n    ]\n    return ModelDef(\n        inputs=[input, states],\n        params=ge.parameters(),\n        forward=ge,\n        backward_setup=lstm_backward_setup,\n        backward=simple_backward,\n    )", "target": "def predicate_func(v: Any) -> Any:\n            if not func(v):\n                raise PydanticCustomError(\n                    'predicate_failed',\n                    f'Predicate {predicate_name}failed',\n                )\n            return v", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|scikit-learn/scikit-learn", "data_split": "train", "example_id": "002050", "source": "def f_w(v: Any, handler: core_schema.ValidatorFunctionWrapHandler, info: core_schema.ValidationInfo) -> Any:\n        calls.append(info.mode)\n        return handler(v)", "target": "def make_data(self, params):\n        data = _synth_classification_dataset(\n            n_samples=10000, n_features=100, n_classes=5\n        )\n        return data", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "002051", "source": "def eager(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, dy = args\n        y = F.softmax(x, dim=-1)\n        return lambda: torch.autograd.grad(y, x, grad_outputs=dy, retain_graph=True)", "target": "def eager(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, w = args\n        return lambda: self.rms_norm_ref(x, w)", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "002052", "source": "def get_toolchain_file(self):\n        return os.path.join(self.emscripten_dir, \"cmake\", \"Modules\", \"Platform\", \"Emscripten.cmake\")", "target": "def test_alias(py_and_json: PyAndJson):\n    v = py_and_json(\n        {\n            'type': 'typed-dict',\n            'fields': {\n                'field_a': {'validation_alias': 'FieldA', 'type': 'typed-dict-field', 'schema': {'type': 'int'}}\n            },\n        }\n    )\n    assert v.validate_test({'FieldA': '123'}) == {'field_a': 123}\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'foobar': '123'})\n    with pytest.raises(ValidationError, match=r'FieldA\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'field_a': '123'})", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "002053", "source": "def test_simple(self):\n        img1 = self.get_sample('stitching/a1.png')\n        img2 = self.get_sample('stitching/a2.png')\n        stitcher = cv.Stitcher.create(cv.Stitcher_PANORAMA)\n        (_result, pano) = stitcher.stitch((img1, img2))\n        self.assertAlmostEqual(pano.shape[0], 685, delta=100, msg=\"rows: %r\" % list(pano.shape))\n        self.assertAlmostEqual(pano.shape[1], 1025, delta=100, msg=\"cols: %r\" % list(pano.shape))", "target": "def test_timedelta_kwargs_strict():\n    v = SchemaValidator(core_schema.timedelta_schema(strict=True, le=timedelta(days=3)))\n    output = v.validate_python(timedelta(days=2, hours=1))\n    assert output == timedelta(days=2, hours=1)", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "002054", "source": "def make_estimator(self, params):\n        representation, precompute = params\n        estimator = ElasticNet(precompute=precompute, alpha=0.001, random_state=0)\n        return estimator", "target": "def make_estimator(self, params):\n        (representation,) = params\n        n_estimators = 100 if Benchmark.data_size == \"large\" else 10\n        estimator = GradientBoostingClassifier(\n            n_estimators=n_estimators,\n            max_features=\"log2\",\n            subsample=0.5,\n            random_state=0,\n        )\n        return estimator", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "002055", "source": "def wrapper_function(*args, **kwargs):\n            return wrapper(*args, **kwargs)", "target": "def wrapper_function(*args: Any, **kwargs: Any) -> Any:\n            return vd.call(*args, **kwargs)", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "002056", "source": "def keyselector(a):\n    if cvsize_re.match(a):\n        size = [int(d) for d in a.split('x')]\n        return size[0] * size[1]\n    elif cvtype_re.match(a):\n        if a.startswith(\"CV_\"):\n            a = a[3:]\n        depth = 7\n        if a[0] == '8':\n            depth = (0, 1) [a[1] == 'S']\n        elif a[0] == '1':\n            depth = (2, 3) [a[2] == 'S']\n        elif a[2] == 'S':\n            depth = 4\n        elif a[0] == '3':\n            depth = 5\n        elif a[0] == '6':\n            depth = 6\n        cidx = a.find('C')\n        if cidx < 0:\n            channels = 1\n        else:\n            channels = int(a[a.index('C') + 1:])\n        return ((channels-1) & 511) + (depth << 9)\n    return a", "target": "def test_extra_arguments(py_and_json: PyAndJson):\n    v = py_and_json({'type': 'tuple', 'items_schema': [{'type': 'int'}, {'type': 'int'}]})\n    assert v.validate_test([1, 2]) == (1, 2)\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_test([1, 2, 3, 4])\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'too_long',\n            'loc': (),\n            'msg': 'Tuple should have at most 2 items after validation, not 4',\n            'input': [1, 2, 3, 4],\n            'ctx': {'field_type': 'Tuple', 'max_length': 2, 'actual_length': 4},\n        }\n    ]", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "002057", "source": "def test_simple_serializers_fallback(schema_type):\n    s = SchemaSerializer({'type': schema_type})\n    with pytest.warns(\n        UserWarning,\n        match=rf'Expected `{schema_type}` - serialized value may not be as expected \\[input_value=\\[1, 2, 3\\], input_type=list\\]',\n    ):\n        assert s.to_python([1, 2, 3]) == [1, 2, 3]\n    with pytest.warns(\n        UserWarning,\n        match=rf\"Expected `{schema_type}` - serialized value may not be as expected \\[input_value=\\[1, 2, b'bytes'\\], input_type=list\\]\",\n    ):\n        assert s.to_python([1, 2, b'bytes'], mode='json') == [1, 2, 'bytes']\n    with pytest.warns(\n        UserWarning,\n        match=rf'Expected `{schema_type}` - serialized value may not be as expected \\[input_value=\\[1, 2, 3\\], input_type=list\\]',\n    ):\n        assert s.to_json([1, 2, 3]) == b'[1,2,3]'", "target": "def test_property():\n    @dataclasses.dataclass\n    class Model:\n        def __init__(self, **kwargs):\n            for key, value in kwargs.items():\n                setattr(self, key, value)\n        @property\n        def area(self) -> bytes:\n            a = self.width * self.height\n            return b'%d' % a\n    s = SchemaSerializer(\n        core_schema.model_schema(\n            Model,\n            core_schema.model_fields_schema(\n                {\n                    'width': core_schema.model_field(core_schema.int_schema()),\n                    'height': core_schema.model_field(core_schema.int_schema()),\n                },\n                computed_fields=[core_schema.computed_field('area', core_schema.bytes_schema())],\n            ),\n        )\n    )\n    assert s.to_python(Model(width=3, height=4)) == {'width': 3, 'height': 4, 'area': b'12'}\n    assert s.to_python(Model(width=3, height=4), mode='json') == {'width': 3, 'height': 4, 'area': '12'}\n    assert s.to_json(Model(width=3, height=4)) == b'{\"width\":3,\"height\":4,\"area\":\"12\"}'\n    assert s.to_python(Model(width=3, height=4), round_trip=True) == {'width': 3, 'height': 4}\n    assert s.to_json(Model(width=3, height=4), round_trip=True) == b'{\"width\":3,\"height\":4}'\n    assert s.to_python(Model(width=3, height=4), exclude_computed_fields=True) == {'width': 3, 'height': 4}\n    assert s.to_json(Model(width=3, height=4), exclude_computed_fields=True) == b'{\"width\":3,\"height\":4}'", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "002058", "source": "def run(arr0, arr1):\n                    return arr0 + arr1", "target": "def run(img0, img1):\n                    raise Exception('Error')\n                    return img0 + img1", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "002059", "source": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "target": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "002060", "source": "def f(input_value: Any, info: core_schema.ValidationInfo) -> Any:\n        return f'{info.field_name}: {input_value}'", "target": "def test_age_gender_infer_tensor(self):\n            skip_if_openvino_not_available()\n            root_path  = '/omz_intel_models/intel/age-gender-recognition-retail-0013/FP32/age-gender-recognition-retail-0013'\n            model_path = self.find_file(root_path + '.xml',   [os.environ.get('OPENCV_DNN_TEST_DATA_PATH')], required=False)\n            bin_path   = self.find_file(root_path + '.bin',   [os.environ.get('OPENCV_DNN_TEST_DATA_PATH')], required=False)\n            device_id  = 'CPU'\n            img_path = self.find_file('cv/face/david2.jpg', [os.environ.get('OPENCV_TEST_DATA_PATH')])\n            img = cv.imread(img_path)\n            tensor = cv.resize(img, (62, 62)).astype(np.float32)\n            tensor = np.transpose(tensor, (2, 0, 1))\n            tensor = np.expand_dims(tensor, 0)\n            ref = AgeGenderOV(model_path, bin_path, device_id)\n            ov_age, ov_gender = ref.apply(tensor)\n            comp = AgeGenderGAPI(model_path, bin_path, device_id)\n            gapi_age, gapi_gender = comp.apply(tensor)\n            self.assertEqual(0.0, cv.norm(ov_gender, gapi_gender, cv.NORM_INF))\n            self.assertEqual(0.0, cv.norm(ov_age, gapi_age, cv.NORM_INF))", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "002061", "source": "def getCMakeArgs(self, arch, target):\n        args = Builder.getCMakeArgs(self, arch, target)\n        args = args + [\n            '-DVISIONOS_ARCH=%s' % arch\n        ]\n        return args", "target": "def test_utcoffset(self):\n        dummy = self.DT\n        for h in [0, 1.5, 12]:\n            offset = h * HOUR\n            self.assertEqual(timedelta(seconds=offset), TzInfo(offset).utcoffset(dummy))\n            self.assertEqual(timedelta(seconds=-offset), TzInfo(-offset).utcoffset(dummy))\n        self.assertEqual(self.EST.utcoffset(''), timedelta(hours=-5))\n        self.assertEqual(self.EST.utcoffset(5), timedelta(hours=-5))", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|pydantic/pydantic", "data_split": "train", "example_id": "002062", "source": "def get_estimator_and_data():\n    if args.problem == \"classification\":\n        X, y = make_classification(\n            args.n_samples_max * 2,\n            n_features=args.n_features,\n            n_classes=args.n_classes,\n            n_clusters_per_class=1,\n            n_informative=args.n_classes,\n            random_state=0,\n        )\n        return X, y, HistGradientBoostingClassifier\n    elif args.problem == \"regression\":\n        X, y = make_regression(\n            args.n_samples_max * 2, n_features=args.n_features, random_state=0\n        )\n        return X, y, HistGradientBoostingRegressor", "target": "def test_tuple_fix_extra(input_value, expected):\n    v = SchemaValidator(\n        core_schema.tuple_schema(\n            items_schema=[core_schema.int_schema(), core_schema.str_schema(), core_schema.str_schema()],\n            variadic_item_index=2,\n        )\n    )\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)) as exc_info:\n            v.validate_python(input_value)\n        assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        assert v.validate_python(input_value) == expected", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|scikit-learn/scikit-learn", "data_split": "train", "example_id": "002063", "source": "def main():\n    parser = argparse.ArgumentParser(description=\"generate camera-calibration pattern\", add_help=False)\n    parser.add_argument(\"-H\", \"--help\", help=\"show help\", action=\"store_true\", dest=\"show_help\")\n    parser.add_argument(\"-o\", \"--output\", help=\"output file\", default=\"out.svg\", action=\"store\", dest=\"output\")\n    parser.add_argument(\"-c\", \"--columns\", help=\"pattern columns\", default=\"8\", action=\"store\", dest=\"columns\",\n                        type=int)\n    parser.add_argument(\"-r\", \"--rows\", help=\"pattern rows\", default=\"11\", action=\"store\", dest=\"rows\", type=int)\n    parser.add_argument(\"-T\", \"--type\", help=\"type of pattern\", default=\"circles\", action=\"store\", dest=\"p_type\",\n                        choices=[\"circles\", \"acircles\", \"checkerboard\", \"radon_checkerboard\", \"charuco_board\"])\n    parser.add_argument(\"-u\", \"--units\", help=\"length unit\", default=\"mm\", action=\"store\", dest=\"units\",\n                        choices=[\"mm\", \"inches\", \"px\", \"m\"])\n    parser.add_argument(\"-s\", \"--square_size\", help=\"size of squares in pattern\", default=\"20.0\", action=\"store\",\n                        dest=\"square_size\", type=float)\n    parser.add_argument(\"-R\", \"--radius_rate\", help=\"circles_radius = square_size/radius_rate\", default=\"5.0\",\n                        action=\"store\", dest=\"radius_rate\", type=float)\n    parser.add_argument(\"-w\", \"--page_width\", help=\"page width in units\", default=argparse.SUPPRESS, action=\"store\",\n                        dest=\"page_width\", type=float)\n    parser.add_argument(\"-h\", \"--page_height\", help=\"page height in units\", default=argparse.SUPPRESS, action=\"store\",\n                        dest=\"page_height\", type=float)\n    parser.add_argument(\"-a\", \"--page_size\", help=\"page size, superseded if -h and -w are set\", default=\"A4\",\n                        action=\"store\", dest=\"page_size\", choices=[\"A0\", \"A1\", \"A2\", \"A3\", \"A4\", \"A5\"])\n    parser.add_argument(\"-m\", \"--markers\", help=\"list of cells with markers for the radon checkerboard. Marker \"\n                                                \"coordinates as list of numbers: -m 1 2 3 4 means markers in cells \"\n                                                \"[1, 2] and [3, 4]\",\n                        default=argparse.SUPPRESS, action=\"store\", dest=\"markers\", nargs=\"+\", type=int)\n    parser.add_argument(\"-p\", \"--marker_size\", help=\"aruco markers size for ChAruco pattern (default 10.0)\", default=\"10.0\",\n                        action=\"store\", dest=\"aruco_marker_size\", type=float)\n    parser.add_argument(\"-f\", \"--dict_file\", help=\"file name of custom aruco dictionary for ChAruco pattern\", default=\"DICT_ARUCO_ORIGINAL.json\",\n                        action=\"store\", dest=\"dict_file\", type=str)\n    parser.add_argument(\"-do\", \"--dict_offset\", help=\"index of the first ArUco index used\", default=0,\n                        action=\"store\", dest=\"dict_offset\", type=int)\n    args = parser.parse_args()\n    show_help = args.show_help\n    if show_help:\n        parser.print_help()\n        return\n    output = args.output\n    columns = args.columns\n    rows = args.rows\n    p_type = args.p_type\n    units = args.units\n    square_size = args.square_size\n    radius_rate = args.radius_rate\n    aruco_marker_size = args.aruco_marker_size\n    dict_file = args.dict_file\n    dict_offset = args.dict_offset\n    if 'page_width' and 'page_height' in args:\n        page_width = args.page_width\n        page_height = args.page_height\n    else:\n        page_size = args.page_size\n        page_sizes = {\"A0\": [840, 1188], \"A1\": [594, 840], \"A2\": [420, 594], \"A3\": [297, 420], \"A4\": [210, 297],\n                      \"A5\": [148, 210]}\n        page_width = page_sizes[page_size][0]\n        page_height = page_sizes[page_size][1]\n    markers = None\n    if p_type == \"radon_checkerboard\" and \"markers\" in args:\n        if len(args.markers) % 2 == 1:\n            raise ValueError(\"The length of the markers array={} must be even\".format(len(args.markers)))\n        markers = set()\n        for x, y in zip(args.markers[::2], args.markers[1::2]):\n            if x in range(0, columns) and y in range(0, rows):\n                markers.add((x, y))\n            else:\n                raise ValueError(\"The marker {},{} is outside the checkerboard\".format(x, y))\n    if p_type == \"charuco_board\" and aruco_marker_size >= square_size:\n        raise ValueError(\"ArUco markers size must be smaller than square size\")\n    pm = PatternMaker(columns, rows, output, units, square_size, radius_rate, page_width, page_height, markers, aruco_marker_size, dict_file, dict_offset)\n    mp = {\"circles\": pm.make_circles_pattern, \"acircles\": pm.make_acircles_pattern,\n          \"checkerboard\": pm.make_checkerboard_pattern, \"radon_checkerboard\": pm.make_radon_checkerboard_pattern,\n         \"charuco_board\": pm.make_charuco_board}\n    mp[p_type]()\n    pm.save()", "target": "def add_content(self, more_content):\n        sourcename = self.get_sourcename()\n        docstrings = self.get_doc()\n        if docstrings is not None:\n            if not docstrings:\n                docstrings.append([])\n            short_summary = next(\n                (s for s in self.process_doc(docstrings) if s), \"<no summary>\"\n            )\n            self.add_line(short_summary, sourcename, 0)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "002064", "source": "def weight(self) -> int:\n        return 1 + sum(base.weight for base in self.bases)", "target": "def test_json_or_python_enum_dict_key():\n    class MyEnum(str, Enum):\n        A = 'A'\n        B = 'B'\n    print(MyEnum('A'))\n    s = SchemaSerializer(\n        core_schema.dict_schema(\n            core_schema.json_or_python_schema(\n                core_schema.str_schema(), core_schema.no_info_after_validator_function(MyEnum, core_schema.str_schema())\n            ),\n            core_schema.int_schema(),\n        )\n    )\n    assert s.to_json({MyEnum.A: 1, MyEnum.B: 2}) == b'{\"A\":1,\"B\":2}'\n    assert s.to_python({MyEnum.A: 1, MyEnum.B: 2}) == {MyEnum.A: 1, MyEnum.B: 2}", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "002065", "source": "def check_deprecated(config_dict: ConfigDict) -> None:\n    deprecated_removed_keys = V2_REMOVED_KEYS & config_dict.keys()\n    deprecated_renamed_keys = V2_RENAMED_KEYS.keys() & config_dict.keys()\n    if deprecated_removed_keys or deprecated_renamed_keys:\n        renamings = {k: V2_RENAMED_KEYS[k] for k in sorted(deprecated_renamed_keys)}\n        renamed_bullets = [f'* {k!r} has been renamed to {v!r}' for k, v in renamings.items()]\n        removed_bullets = [f'* {k!r} has been removed' for k in sorted(deprecated_removed_keys)]\n        message = '\\n'.join(['Valid config keys have changed in V2:'] + renamed_bullets + removed_bullets)\n        warnings.warn(message, UserWarning)", "target": "def main():\n    args = parse_args()\n    train_dataloader, eval_dataloader = data_processing(\n        args.num_samples, args.batch_size\n    )\n    model = AutoModelForSequenceClassification.from_pretrained(\n        \"bert-base-cased\", num_labels=5\n    )\n    optimizer_cls = getattr(sys.modules[\"torch.optim\"], args.optimizer)\n    if \"capturable\" in inspect.signature(optimizer_cls).parameters.keys():\n        optimizer = optimizer_cls(model.parameters(), lr=args.lr, capturable=True)\n    else:\n        optimizer = optimizer_cls(model.parameters(), lr=args.lr)\n    native_start = time.time()\n    ref_loss, accuracy = model_training_evaluation(\n        None,\n        train_dataloader,\n        eval_dataloader,\n        model,\n        optimizer,\n        args.epochs,\n        args.evaluation,\n    )\n    native_end = time.time()\n    res_loss, accuracy = model_training_evaluation(\n        args.backend,\n        train_dataloader,\n        eval_dataloader,\n        model,\n        optimizer,\n        args.epochs,\n        args.evaluation,\n    )\n    dynamo_end = time.time()\n    if check_loss(ref_loss, res_loss):\n        print(\n            \"[PASSED] TorchDynamo end to end training loss is less than or equal to native PyTorch\"\n        )\n    else:\n        print(\n            \"[FAILED] TorchDynamo end to end training loss is greater than native Pytorch\"\n        )\n    if args.evaluation:\n        print(f\"Model accuracy: {accuracy}\")\n    native_elapsed = native_end - native_start\n    dynamo_elapsed = dynamo_end - native_end\n    print(\n        f\"Train model on {args.epochs} epochs with backend {args.backend} and optimizer {args.optimizer}:\"\n    )\n    print(f\"PyTorch spent {timedelta(seconds=native_elapsed / args.epochs)} per epoch\")\n    print(\n        f\"TorchDynamo spent {timedelta(seconds=dynamo_elapsed / args.epochs)} per epoch\"\n    )", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "002066", "source": "def apply_known_metadata(annotation: Any, schema: CoreSchema) -> CoreSchema | None:\n    import annotated_types as at\n    from ._validators import NUMERIC_VALIDATOR_LOOKUP, forbid_inf_nan_check\n    schema = schema.copy()\n    schema_update, other_metadata = collect_known_metadata([annotation])\n    schema_type = schema['type']\n    chain_schema_constraints: set[str] = {\n        'pattern',\n        'strip_whitespace',\n        'to_lower',\n        'to_upper',\n        'coerce_numbers_to_str',\n    }\n    chain_schema_steps: list[CoreSchema] = []\n    for constraint, value in schema_update.items():\n        if constraint not in CONSTRAINTS_TO_ALLOWED_SCHEMAS:\n            raise ValueError(f'Unknown constraint {constraint}')\n        allowed_schemas = CONSTRAINTS_TO_ALLOWED_SCHEMAS[constraint]\n        if schema_type in {'function-before', 'function-wrap', 'function-after'} and constraint == 'strict':\n            schema['schema'] = apply_known_metadata(annotation, schema['schema'])\n            return schema\n        if schema_type in allowed_schemas:\n            if constraint == 'union_mode' and schema_type == 'union':\n                schema['mode'] = value\n            else:\n                schema[constraint] = value\n            continue\n        if constraint in chain_schema_constraints:\n            def _apply_constraint_with_incompatibility_info(\n                value: Any, handler: cs.ValidatorFunctionWrapHandler\n            ) -> Any:\n                try:\n                    x = handler(value)\n                except ValidationError as ve:\n                    if 'type' in ve.errors()[0]['type']:\n                        raise TypeError(\n                            f\"Unable to apply constraint '{constraint}' to supplied value {value} for schema of type '{schema_type}'\"\n                        )\n                    raise ve\n                return x\n            chain_schema_steps.append(\n                cs.no_info_wrap_validator_function(\n                    _apply_constraint_with_incompatibility_info, cs.str_schema(**{constraint: value})\n                )\n            )\n        elif constraint in NUMERIC_VALIDATOR_LOOKUP:\n            if constraint in LENGTH_CONSTRAINTS:\n                inner_schema = schema\n                while inner_schema['type'] in {'function-before', 'function-wrap', 'function-after'}:\n                    inner_schema = inner_schema['schema']\n                inner_schema_type = inner_schema['type']\n                if inner_schema_type == 'list' or (\n                    inner_schema_type == 'json-or-python' and inner_schema['json_schema']['type'] == 'list'\n                ):\n                    js_constraint_key = 'minItems' if constraint == 'min_length' else 'maxItems'\n                else:\n                    js_constraint_key = 'minLength' if constraint == 'min_length' else 'maxLength'\n            else:\n                js_constraint_key = constraint\n            schema = cs.no_info_after_validator_function(\n                partial(NUMERIC_VALIDATOR_LOOKUP[constraint], **{constraint: value}), schema\n            )\n            metadata = schema.get('metadata', {})\n            if (existing_json_schema_updates := metadata.get('pydantic_js_updates')) is not None:\n                metadata['pydantic_js_updates'] = {\n                    **existing_json_schema_updates,\n                    **{js_constraint_key: as_jsonable_value(value)},\n                }\n            else:\n                metadata['pydantic_js_updates'] = {js_constraint_key: as_jsonable_value(value)}\n            schema['metadata'] = metadata\n        elif constraint == 'allow_inf_nan' and value is False:\n            schema = cs.no_info_after_validator_function(\n                forbid_inf_nan_check,\n                schema,\n            )\n        else:\n            raise RuntimeError(f\"Unable to apply constraint '{constraint}' to schema of type '{schema_type}'\")\n    for annotation in other_metadata:\n        if (annotation_type := type(annotation)) in (at_to_constraint_map := _get_at_to_constraint_map()):\n            constraint = at_to_constraint_map[annotation_type]\n            validator = NUMERIC_VALIDATOR_LOOKUP.get(constraint)\n            if validator is None:\n                raise ValueError(f'Unknown constraint {constraint}')\n            schema = cs.no_info_after_validator_function(\n                partial(validator, {constraint: getattr(annotation, constraint)}), schema\n            )\n            continue\n        elif isinstance(annotation, (at.Predicate, at.Not)):\n            predicate_name = f'{annotation.func.__qualname__!r} ' if hasattr(annotation.func, '__qualname__') else ''\n            if isinstance(annotation, at.Predicate):\n                def val_func(v: Any) -> Any:\n                    predicate_satisfied = annotation.func(v)\n                    if not predicate_satisfied:\n                        raise PydanticCustomError(\n                            'predicate_failed',\n                            f'Predicate {predicate_name}failed',\n                        )\n                    return v\n            else:\n                def val_func(v: Any) -> Any:\n                    predicate_satisfied = annotation.func(v)\n                    if predicate_satisfied:\n                        raise PydanticCustomError(\n                            'not_operation_failed',\n                            f'Not of {predicate_name}failed',\n                        )\n                    return v\n            schema = cs.no_info_after_validator_function(val_func, schema)\n        else:\n            return None\n    if chain_schema_steps:\n        chain_schema_steps = [schema] + chain_schema_steps\n        return cs.chain_schema(chain_schema_steps)\n    return schema", "target": "def test_merged_lastfailed_content_with_empty_source(self) -> None:\n        last_failed_source = {\n            \"\": True,\n        }\n        last_failed_dest = {\n            \"tools/tests/test_car.py::test_num1\": True,\n            \"tools/tests/test_car.py::test_num2\": True,\n        }\n        last_failed_merged = {\n            \"tools/tests/test_car.py::test_num1\": True,\n            \"tools/tests/test_car.py::test_num2\": True,\n        }\n        merged = _merged_lastfailed_content(last_failed_source, last_failed_dest)\n        self.assertEqual(merged, last_failed_merged)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "train", "example_id": "002067", "source": "def test_remove_dir_accepts_str(self):\n        d = self.tmp_path / \"to_rm\"\n        d.mkdir()\n        remove_dir(str(d))\n        self.assertFalse(d.exists())", "target": "def liger(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        from liger_kernel.transformers.cross_entropy import LigerCrossEntropyLoss\n        x, target = args\n        cross_entropy = LigerCrossEntropyLoss(reduction=\"none\")\n        return lambda: cross_entropy(x, target)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "002068", "source": "def get_nccl_wheel_version(arch_version: str) -> str:\n    requirements = map(\n        str.strip, re.split(\"[;|]\", PYTORCH_EXTRA_INSTALL_REQUIREMENTS[arch_version])\n    )\n    return next(x for x in requirements if x.startswith(\"nvidia-nccl\")).split(\"==\")[1]", "target": "def test_union_int_simple(py_and_json: PyAndJson):\n    v = py_and_json({'type': 'union', 'choices': [{'type': 'int'}, {'type': 'list'}]})\n    assert v.validate_test('5') == 5\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_test('xxx')\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'type': 'int_parsing',\n            'loc': ('int',),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'xxx',\n        },\n        {\n            'type': 'list_type',\n            'loc': ('list[any]',),\n            'msg': IsStr(regex='Input should be a valid (list|array)'),\n            'input': 'xxx',\n        },\n    ]", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|scikit-learn/scikit-learn", "data_split": "train", "example_id": "002069", "source": "def pick_grad(self, name, is_training):\n        if is_training:\n            return torch.enable_grad()\n        else:\n            return torch.no_grad()", "target": "def transform(self, X):\n        check_is_fitted(self)\n        H = self.components_\n        W, _, self.n_iter_ = self._fit_transform(X, H=H, update_H=False)\n        return W", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "002070", "source": "def test_type_error():\n    try:\n        PydanticKnownError('foobar')\n    except KeyError as exc:\n        assert str(exc) == '\"Invalid error type: \\'foobar\\'\"'\n    else:\n        raise AssertionError(\"PydanticKnownError('foobar') did not raise KeyError\")\n    e = PydanticKnownError('recursion_loop')\n    assert isinstance(e, PydanticKnownError)", "target": "def test_restores_on_exception(self):\n        start = Path.cwd()\n        with tempfile.TemporaryDirectory() as td:\n            target = Path(td) / \"wd_exc\"\n            target.mkdir()\n            with self.assertRaises(ValueError):\n                with working_directory(str(target)):\n                    self.assertEqual(Path.cwd().resolve(), target.resolve())\n                    raise ValueError(\"boom\")\n        self.assertEqual(Path.cwd().resolve(), start.resolve())", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "002071", "source": "def find_class_node(self, class_info, namespaces):\n            return ClassNode()", "target": "def col_values(self, row: Row) -> list[str]:\n        o = self.open_nowrap_span\n        c = self.close_nowrap_span\n        return [\n            f'{o}`{row.field_type_str}`{c}',\n            f'{o}`{row.input_type_str}`{c}',\n            '' if row.strict else '',\n            f'{o}{row.input_source_str}{c}',\n            row.condition if row.condition else '',\n        ]", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "train", "example_id": "002072", "source": "def _install_test_dependencies(self):\n        logger.info(\"generate test.txt from requirements/test.in with local torch whls\")\n        preprocess_test_in()\n        copy(\"requirements/test.txt\", \"snapshot_constraint.txt\")\n        run_command(\n            f\"{sys.executable} -m uv pip compile requirements/test.in \"\n            \"-o test.txt \"\n            \"--index-strategy unsafe-best-match \"\n            \"--constraint snapshot_constraint.txt \"\n            \"--torch-backend cu128\"\n        )\n        pip_install_packages(requirements=\"test.txt\", prefer_uv=True)\n        logger.info(\"Done. installed requirements for test dependencies\")", "target": "def register_build_commands(subparsers: argparse._SubParsersAction) -> None:\n    build_parser = subparsers.add_parser(\n        \"build\",\n        help=\"Build related commands\",\n        formatter_class=RichHelp,\n    )\n    build_subparsers = build_parser.add_subparsers(dest=\"build_command\", required=True)\n    overview = \"\\n\".join(\n        f\"  {name:12} {spec.get('help', '')}\" for name, spec in _TARGETS.items()\n    )\n    external_parser = build_subparsers.add_parser(\n        \"external\",\n        help=\"Build external targets\",\n        description=\"Build third-party targets.\\n\\nAvailable targets:\\n\" + overview,\n        formatter_class=RichHelp,\n    )\n    register_targets(external_parser, _TARGETS)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "002073", "source": "def ensure_dir_exists(path: Union[str, Path]) -> Path:\n    path_obj = get_path(path)\n    path_obj.mkdir(parents=True, exist_ok=True)\n    return path_obj", "target": "def ensure_dir_exists(dir: Path) -> None:\n    dir.mkdir(parents=True, exist_ok=True)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "002074", "source": "def test_repr() -> None:\n    v = SchemaValidator(\n        cs.arguments_v3_schema(\n            [\n                cs.arguments_v3_parameter(name='a', schema=cs.int_schema(), mode='positional_or_keyword'),\n                cs.arguments_v3_parameter(\n                    name='b',\n                    schema=cs.with_default_schema(schema=cs.int_schema(), default_factory=lambda: 42),\n                    mode='keyword_only',\n                ),\n            ]\n        )\n    )\n    assert 'positional_params_count:1,' in plain_repr(v)", "target": "def test_repr():\n    v = SchemaValidator(\n        cs.arguments_schema(\n            arguments=[\n                {'name': 'b', 'mode': 'positional_or_keyword', 'schema': cs.int_schema()},\n                {\n                    'name': 'a',\n                    'mode': 'keyword_only',\n                    'schema': cs.with_default_schema(schema=cs.int_schema(), default_factory=lambda: 42),\n                },\n            ]\n        )\n    )\n    assert 'positional_params_count:1,' in plain_repr(v)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "002075", "source": "def test_constraints_schema_validation_error(constraint: str) -> None:\n    with pytest.raises(SchemaError, match=f\"'{constraint}' must be coercible to a time instance\"):\n        SchemaValidator(core_schema.time_schema(**{constraint: 'bad_value'}))", "target": "def divide(x, y):\n        if not isinstance(y, (int, float)):\n            dst_dtype = np.result_type(x, y)\n            y = np.array(y).astype(dst_dtype)\n            _, max_value = get_limits(dst_dtype)\n            y[y == 0] = max_value\n        dst = 1.0 * x / y\n        if np.issubdtype(x.dtype, np.integer):\n            dst = np.rint(dst)\n        return dst", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "002076", "source": "def large():\n    return (rand(8192, 8192), rand(8192, 8192))", "target": "def test_function_wrap_field_serializer_to_json():\n    class Model(RootModel):\n        def ser_root(self, v: Any, serializer: core_schema.SerializerFunctionWrapHandler, _) -> str:\n            assert self.root == 1_000\n            root = serializer(v)\n            return f'{root:_}'\n    s = SchemaSerializer(\n        core_schema.model_schema(\n            Model,\n            core_schema.int_schema(\n                serialization=core_schema.wrap_serializer_function_ser_schema(\n                    Model.ser_root, is_field_serializer=True, info_arg=True, schema=core_schema.any_schema()\n                )\n            ),\n            root_model=True,\n        )\n    )\n    assert json.loads(s.to_json(Model(1000))) == '1_000'", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "002077", "source": "def ser_x(self, v: Any, serializer: core_schema.SerializerFunctionWrapHandler, _) -> str:\n            x = serializer(v)\n            assert self.x == 1_000\n            return f'{x:_}'", "target": "def ser_x(data: Model, v: Any, serializer: core_schema.SerializerFunctionWrapHandler, _) -> str:\n        x = serializer(v)\n        assert data['x'] == 1_000\n        return f'{x:_}'", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "002078", "source": "def getTestCommand(self):\n        testcmd = [\n            \"xcodebuild\",\n            \"test\",\n            \"-project\", \"OpenCVTest.xcodeproj\",\n            \"-scheme\", \"OpenCVTestTests\",\n            \"-destination\", \"platform=%s\" % self.platform\n        ]\n        return testcmd", "target": "def test_uuid_json(py_and_json: PyAndJson, input_value, expected):\n    v = py_and_json({'type': 'uuid'})\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            v.validate_test(input_value)\n    else:\n        output = v.validate_test(input_value)\n        assert output == expected\n        assert isinstance(output, UUID)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "002079", "source": "def f(a, b):\n            result = a.clone()\n            for i in range(1000):\n                if i % 3 == 0:\n                    result = result + b\n                elif i % 3 == 1:\n                    result = result + 8 * b\n                else:\n                    result = result.sin()\n            return result", "target": "def f(a):\n            xs = a.tolist()\n            y = 0\n            if self.use_loop:\n                for i in xs:\n                    y += i\n            else:\n                y = sum(xs)\n            return torch.tensor(y)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "002080", "source": "def constrain(self: _Pipeline[_InT, _NewOutLt], constraint: annotated_types.Lt) -> _Pipeline[_InT, _NewOutLt]: ...", "target": "def constrain(self: _Pipeline[_InT, _OutT], constraint: _In) -> _Pipeline[_InT, _OutT]: ...", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "002081", "source": "def test_dict_any_value():\n    v = SchemaValidator(core_schema.dict_schema(keys_schema=core_schema.str_schema()))\n    assert v.validate_json('{\"1\": 1, \"2\": \"a\", \"3\": null}') == {'1': 1, '2': 'a', '3': None}", "target": "def test_dict_any_value():\n    v = SchemaValidator(cs.dict_schema(keys_schema=cs.str_schema()))\n    v = SchemaValidator(cs.dict_schema(keys_schema=cs.str_schema()))\n    assert v.validate_python({'1': 1, '2': 'a', '3': None}) == {'1': 1, '2': 'a', '3': None}", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "002082", "source": "def make_estimator(self, params):\n        representation, solver = params\n        estimator = Ridge(solver=solver, fit_intercept=False, random_state=0)\n        return estimator", "target": "def make_estimator(self, params):\n        estimator = HistGradientBoostingClassifier(\n            max_iter=100, max_leaf_nodes=15, early_stopping=False, random_state=0\n        )\n        return estimator", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "002083", "source": "def test_function_plain_field_serializer_to_python():\n    class Model(RootModel):\n        def ser_root(self, v: Any, _) -> str:\n            assert self.root == 1_000\n            return f'{v:_}'\n    s = SchemaSerializer(\n        core_schema.model_schema(\n            Model,\n            core_schema.int_schema(\n                serialization=core_schema.plain_serializer_function_ser_schema(\n                    Model.ser_root, is_field_serializer=True, info_arg=True\n                )\n            ),\n            root_model=True,\n        )\n    )\n    assert s.to_python(Model(1000)) == '1_000'", "target": "def test_function_plain_field_serializer_to_python():\n    class Model(TypedDict):\n        x: int\n    def ser_x(data: Model, v: Any, _) -> str:\n        assert data['x'] == 1_000\n        return f'{v:_}'\n    s = SchemaSerializer(\n        core_schema.typed_dict_schema(\n            {\n                'x': core_schema.typed_dict_field(\n                    core_schema.int_schema(\n                        serialization=core_schema.plain_serializer_function_ser_schema(\n                            ser_x, is_field_serializer=True, info_arg=True\n                        )\n                    )\n                )\n            }\n        )\n    )\n    assert s.to_python(Model(x=1000)) == {'x': '1_000'}", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "002084", "source": "def setup(app):\n    app.add_directive(\"allow_nan_estimators\", AllowNanEstimators)\n    return {\n        \"version\": \"0.1\",\n        \"parallel_read_safe\": True,\n        \"parallel_write_safe\": True,\n    }", "target": "def setup(app):\n    app.connect(\"builder-inited\", setup_link_role)\n    return {\"version\": \"0.1\", \"parallel_read_safe\": True}", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "002085", "source": "def make_estimator(self, params):\n        representation, solver = params\n        estimator = Ridge(solver=solver, fit_intercept=False, random_state=0)\n        return estimator", "target": "def make_estimator(self, params):\n        representation, precompute = params\n        estimator = ElasticNet(precompute=precompute, alpha=0.001, random_state=0)\n        return estimator", "label": 1}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|opencv/opencv", "data_split": "train", "example_id": "002086", "source": "def make_data(self, params):\n        data = _synth_classification_dataset(n_samples=10000, n_features=100)\n        return data", "target": "def insert_or_replace(element_before, new_element, tag, tag_class):\n    old = element_before.find_next_sibling(tag, class_=tag_class)\n    if old is None:\n        element_before.insert_after(new_element)\n    else:\n        old.replace_with(new_element)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "002087", "source": "def validate_b(cls, v: bytes, info: core_schema.ValidationInfo) -> bytes:\n            assert v == b'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return b'hello world!'", "target": "def validate_b(cls, v: str, info: core_schema.ValidationInfo) -> str:\n            assert v == 'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return 'hello world!'", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "002088", "source": "def t():\n        v.to_python(['a', 'b', 'c', 'd', 'e'], include={-1, -2})", "target": "def required_definition_imports(self) -> Generator[str, None, None]:\n        return self.value.required_usage_imports", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "002089", "source": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "target": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "002090", "source": "def constrain(\n        self: _Pipeline[_InT, _NewOutT], constraint: annotated_types.MultipleOf\n    ) -> _Pipeline[_InT, _NewOutT]: ...", "target": "def constrain(self: _Pipeline[_InT, _OutT], constraint: _NotEq) -> _Pipeline[_InT, _OutT]: ...", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "002091", "source": "def compute_branch_diffs(\n        self, from_branch: str, to_branch: str\n    ) -> tuple[list[str], list[str]]:\n        from_ref = self.rev_parse(from_branch)\n        to_ref = self.rev_parse(to_branch)\n        merge_base = self.get_merge_base(from_ref, to_ref)\n        from_commits = self.revlist(f\"{merge_base}..{from_ref}\")\n        to_commits = self.revlist(f\"{merge_base}..{to_ref}\")\n        from_ids = fuzzy_list_to_dict(self.patch_id(from_commits))\n        to_ids = fuzzy_list_to_dict(self.patch_id(to_commits))\n        for patch_id in set(from_ids).intersection(set(to_ids)):\n            from_values = from_ids[patch_id]\n            to_values = to_ids[patch_id]\n            if len(from_values) != len(to_values):\n                while len(from_values) > 0 and len(to_values) > 0:\n                    frc = self.get_commit(from_values.pop())\n                    toc = self.get_commit(to_values.pop())\n                    if frc.title != toc.title or frc.author_date != toc.author_date:\n                        if (\n                            \"pytorch/pytorch\" not in self.remote_url()\n                            or frc.commit_hash\n                            not in {\n                                \"0a6a1b27a464ba5be5f587cce2ee12ab8c504dbf\",\n                                \"6d0f4a1d545a8f161df459e8d4ccafd4b9017dbe\",\n                                \"edf909e58f06150f7be41da2f98a3b9de3167bca\",\n                                \"a58c6aea5a0c9f8759a4154e46f544c8b03b8db1\",\n                                \"7106d216c29ca16a3504aa2bedad948ebcf4abc2\",\n                            }\n                        ):\n                            raise RuntimeError(\n                                f\"Unexpected differences between {frc} and {toc}\"\n                            )\n                    from_commits.remove(frc.commit_hash)\n                    to_commits.remove(toc.commit_hash)\n                continue\n            for commit in from_values:\n                from_commits.remove(commit)\n            for commit in to_values:\n                to_commits.remove(commit)\n        if \"pytorch/pytorch\" in self.remote_url():\n            for excluded_commit in {\n                \"8e09e20c1dafcdbdb45c2d1574da68a32e54a3a5\",\n                \"5f37e5c2a39c3acb776756a17730b865f0953432\",\n                \"b5222584e6d6990c6585981a936defd1af14c0ba\",\n                \"84d9a2e42d5ed30ec3b8b4140c38dd83abbce88d\",\n                \"f211ec90a6cdc8a2a5795478b5b5c8d7d7896f7e\",\n            }:\n                if excluded_commit in from_commits:\n                    from_commits.remove(excluded_commit)\n        return (from_commits, to_commits)", "target": "def ip_v6_network_validator(input_value: Any, /) -> IPv6Network:\n    if isinstance(input_value, IPv6Network):\n        return input_value\n    try:\n        return IPv6Network(input_value)\n    except ValueError:\n        raise PydanticCustomError('ip_v6_network', 'Input is not a valid IPv6 network')", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "002092", "source": "def benchmark(self):\n        for M, N in self.get_shapes():\n            print(f\"\\n Tensor dimensions: [{M}, {N}]\")\n            torch_dtype = cutlass_torch.dtype(cutlass.BFloat16)\n            x = 0.1 * torch.randn(M, N, device=\"cuda\", dtype=torch_dtype)\n            target = torch.randint(0, N, (M,), device=\"cuda\", dtype=torch.int64)\n            self.benchmark_single_shape((x, target), setting=f\"shape: [{M}, {N}]\")", "target": "def benchmark(self):\n        for M, N in self.get_shapes():\n            print(f\"Tensor dimensions: [{M}, {N}]\")\n            torch_dtype = cutlass_torch.dtype(cutlass.BFloat16)\n            x = torch.randn(M, N, device=\"cuda\", dtype=torch_dtype)\n            w = torch.randn(N, device=\"cuda\", dtype=torch.float32)\n            self.benchmark_single_shape((x, w), setting=f\"shape: [{M}, {N}]\")", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "002093", "source": "def test_function_any():\n    @validate()\n    def foobar(a, b, c):\n        return a, b, c\n    assert foobar(1, 2, 3) == (1, 2, 3)\n    assert foobar(1, 2, 3) == (1, 2, 3)\n    assert foobar(a=1, b=2, c=3) == (1, 2, 3)\n    assert foobar(1, b=2, c=3) == (1, 2, 3)\n    with pytest.raises(ValidationError, match='Unexpected positional argument'):\n        foobar(1, 2, 3, 4)\n    with pytest.raises(ValidationError, match='d\\n  Unexpected keyword argument'):\n        foobar(1, 2, 3, d=4)", "target": "def is_sequence_type(typename: str) -> bool:\n    return typename.startswith(\"vector\")", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|pydantic/pydantic", "data_split": "train", "example_id": "002094", "source": "def compute_bench(samples_range, features_range, n_iter=3, rank=50):\n    it = 0\n    results = defaultdict(lambda: [])\n    max_it = len(samples_range) * len(features_range)\n    for n_samples in samples_range:\n        for n_features in features_range:\n            it += 1\n            print(\"====================\")\n            print(\"Iteration %03d of %03d\" % (it, max_it))\n            print(\"====================\")\n            X = make_low_rank_matrix(\n                n_samples, n_features, effective_rank=rank, tail_strength=0.2\n            )\n            gc.collect()\n            print(\"benchmarking scipy svd: \")\n            tstart = time()\n            svd(X, full_matrices=False)\n            results[\"scipy svd\"].append(time() - tstart)\n            gc.collect()\n            print(\"benchmarking scikit-learn randomized_svd: n_iter=0\")\n            tstart = time()\n            randomized_svd(X, rank, n_iter=0)\n            results[\"scikit-learn randomized_svd (n_iter=0)\"].append(time() - tstart)\n            gc.collect()\n            print(\"benchmarking scikit-learn randomized_svd: n_iter=%d \" % n_iter)\n            tstart = time()\n            randomized_svd(X, rank, n_iter=n_iter)\n            results[\"scikit-learn randomized_svd (n_iter=%d)\" % n_iter].append(\n                time() - tstart\n            )\n    return results", "target": "def test_tuple_json(py_and_json: PyAndJson, variadic_item_index, items, input_value, expected):\n    v = py_and_json(core_schema.tuple_schema(items_schema=items, variadic_item_index=variadic_item_index))\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            v.validate_test(input_value)\n    else:\n        assert v.validate_test(input_value) == expected", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "002095", "source": "def test_gc_schema_validator() -> None:\n    class BaseModel:\n        __validator__: SchemaValidator\n        def __init_subclass__(cls) -> None:\n            cls.__validator__ = SchemaValidator(\n                schema=core_schema.model_schema(cls, GC_TEST_SCHEMA_INNER),\n                config=core_schema.CoreConfig(extra_fields_behavior='allow'),\n            )\n    cache: WeakValueDictionary[int, Any] = WeakValueDictionary()\n    for _ in range(10_000):\n        class MyModel(BaseModel):\n            pass\n        cache[id(MyModel)] = MyModel\n        del MyModel\n    assert_gc(lambda: len(cache) == 0)", "target": "def test_frozenset_ints_python(input_value, expected):\n    v = SchemaValidator(cs.frozenset_schema(items_schema=cs.int_schema()))\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            v.validate_python(input_value)\n    else:\n        output = v.validate_python(input_value)\n        assert output == expected\n        assert isinstance(output, frozenset)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "002096", "source": "def env_str_field(\n    name: str,\n    default: str = \"\",\n) -> str:\n    return field(default_factory=lambda: get_env(name, default))", "target": "def test_empty_literal():\n    with pytest.raises(SchemaError, match='`expected` should have length > 0'):\n        SchemaSerializer(core_schema.literal_schema([]))", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "002097", "source": "def get_memory_bytes(self, args, kwargs) -> int:\n        x, dy = args\n        M, N = x.shape\n        return 3 * M * N * x.dtype.itemsize", "target": "def get_memory_bytes(self, args, kwargs) -> int:\n        x, w, dy = args\n        M, N = x.shape\n        return 3 * M * N * x.dtype.itemsize + 2 * N * w.dtype.itemsize", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "002098", "source": "def s1(v: int) -> int:\n        return v + 1", "target": "def is_instance_schema(\n    cls: Any,\n    *,\n    cls_repr: str | None = None,\n    ref: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> IsInstanceSchema:\n    return _dict_not_none(\n        type='is-instance', cls=cls, cls_repr=cls_repr, ref=ref, metadata=metadata, serialization=serialization\n    )", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "002099", "source": "def test_use_after():\n    v = SchemaSerializer(\n        core_schema.tuple_positional_schema(\n            [\n                core_schema.definitions_schema(\n                    core_schema.definition_reference_schema('foobar'),\n                    [\n                        core_schema.int_schema(\n                            ref='foobar', serialization=core_schema.to_string_ser_schema(when_used='always')\n                        )\n                    ],\n                ),\n                core_schema.definition_reference_schema('foobar'),\n            ]\n        )\n    )\n    assert v.to_python((1, 2)) == ('1', '2')", "target": "def test_use_after():\n    v = SchemaValidator(\n        core_schema.tuple_positional_schema(\n            [\n                core_schema.definitions_schema(\n                    core_schema.definition_reference_schema('foobar'), [core_schema.int_schema(ref='foobar')]\n                ),\n                core_schema.definition_reference_schema('foobar'),\n            ]\n        )\n    )\n    assert v.validate_python(['1', '2']) == (1, 2)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|scikit-learn/scikit-learn", "data_split": "train", "example_id": "002100", "source": "def custom_error_schema(\n    schema: CoreSchema,\n    custom_error_type: str,\n    *,\n    custom_error_message: str | None = None,\n    custom_error_context: dict[str, Any] | None = None,\n    ref: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> CustomErrorSchema:\n    return _dict_not_none(\n        type='custom-error',\n        schema=schema,\n        custom_error_type=custom_error_type,\n        custom_error_message=custom_error_message,\n        custom_error_context=custom_error_context,\n        ref=ref,\n        metadata=metadata,\n        serialization=serialization,\n    )", "target": "def make_estimator(self, params):\n        representation, precompute = params\n        estimator = ElasticNet(precompute=precompute, alpha=0.001, random_state=0)\n        return estimator", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "002101", "source": "def get_cached_generic_type_late(\n    parent: type[BaseModel], typevar_values: Any, origin: type[BaseModel], args: tuple[Any, ...]\n) -> type[BaseModel] | None:\n    cached = _GENERIC_TYPES_CACHE.get(_late_cache_key(origin, args, typevar_values))\n    if cached is not None:\n        set_cached_generic_type(parent, typevar_values, cached, origin, args)\n    return cached", "target": "def test_correct_function_signature() -> None:\n    def my_validator(value: Any, info: Any) -> str:\n        return str(value)\n    v = SchemaValidator(core_schema.with_info_plain_validator_function(my_validator))\n    assert v.validate_python(1) == '1'", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "002102", "source": "def constrain(\n        self: _Pipeline[_InT, _NewOutLen], constraint: annotated_types.Len\n    ) -> _Pipeline[_InT, _NewOutLen]: ...", "target": "def constrain(\n        self: _Pipeline[_InT, _NewOutInterval], constraint: annotated_types.Interval\n    ) -> _Pipeline[_InT, _NewOutInterval]: ...", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "002103", "source": "def compiled(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, dy = args\n        compiled_softmax = torch.compile(\n            lambda x: F.softmax(x, dim=-1), mode=self.compile_mode, fullgraph=True\n        )\n        y = compiled_softmax(x)\n        return lambda: torch.autograd.grad(y, x, grad_outputs=dy, retain_graph=True)", "target": "def compiled(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, w, dy = args\n        compiled_layernorm = torch.compile(\n            self.layernorm_ref, mode=self.compile_mode, fullgraph=True\n        )\n        y = compiled_layernorm(x, w)\n        return lambda: torch.autograd.grad(\n            y, [x, w], grad_outputs=dy, retain_graph=True\n        )", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "002104", "source": "def dynamic_rnn(\n        input: Tensor,\n        hidden: tuple[Tensor, Tensor],\n        w_ih: Tensor,\n        w_hh: Tensor,\n        b_ih: Tensor,\n        b_hh: Tensor,\n    ) -> tuple[Tensor, tuple[Tensor, Tensor]]:\n        hx, cx = hidden\n        outputs = []\n        inputs = torch.matmul(input, w_ih.t()).unbind(0)\n        hy, cy = hx[0], cx[0]\n        for seq_idx in range(len(inputs)):\n            hy, cy = premul_cell(inputs[seq_idx], (hy, cy), w_hh, b_ih, b_hh)\n            outputs += [hy]\n        return torch.stack(outputs), (hy.unsqueeze(0), cy.unsqueeze(0))", "target": "def dynamic_rnn(input, hx, cx, w_ih, w_hh, b_ih, b_hh):\n        hy = hx\n        cy = cx\n        inputs = input.unbind(0)\n        for seq_idx in range(len(inputs)):\n            hy, cy = cell(inputs[seq_idx], hy, cy, w_ih, w_hh, b_ih, b_hh)\n        return hy, cy", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "002105", "source": "def area(self) -> float:\n            return self.side**2", "target": "def isinstance_test(self, py_input, strict: bool | None = None, context: Any = None):\n        if self.validator_type == 'json':\n            try:\n                self.validator.validate_json(json.dumps(py_input), strict=strict, context=context)\n                return True\n            except ValidationError:\n                return False\n        else:\n            assert self.validator_type == 'python', self.validator_type\n            return self.validator.isinstance_python(py_input, strict=strict, context=context)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "002106", "source": "def test_json_bytes_hex_invalid():\n    v = SchemaValidator(core_schema.bytes_schema(), config=CoreConfig(val_json_bytes='hex'))\n    wrong_input = 'a'\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_json(json.dumps(wrong_input))\n    assert exc_info.value.errors(include_url=False, include_context=False) == [\n        {\n            'type': 'bytes_invalid_encoding',\n            'loc': (),\n            'msg': 'Data should be valid hex: Odd number of digits',\n            'input': wrong_input,\n        }\n    ]\n    wrong_input = 'ag'\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_json(json.dumps(wrong_input))\n    assert exc_info.value.errors(include_url=False, include_context=False) == [\n        {\n            'type': 'bytes_invalid_encoding',\n            'loc': (),\n            'msg': \"Data should be valid hex: Invalid character 'g' at position 1\",\n            'input': wrong_input,\n        }\n    ]", "target": "def test_script_rnn_layer(seq_len, batch, input_size, hidden_size):\n    inp = torch.randn(seq_len, batch, input_size)\n    state = LSTMState(torch.randn(batch, hidden_size), torch.randn(batch, hidden_size))\n    rnn = LSTMLayer(LSTMCell, input_size, hidden_size)\n    out, out_state = rnn(inp, state)\n    lstm = nn.LSTM(input_size, hidden_size, 1)\n    lstm_state = LSTMState(state.hx.unsqueeze(0), state.cx.unsqueeze(0))\n    for lstm_param, custom_param in zip(lstm.all_weights[0], rnn.parameters()):\n        assert lstm_param.shape == custom_param.shape\n        with torch.no_grad():\n            lstm_param.copy_(custom_param)\n    lstm_out, lstm_out_state = lstm(inp, lstm_state)\n    assert (out - lstm_out).abs().max() < 1e-5\n    assert (out_state[0] - lstm_out_state[0]).abs().max() < 1e-5\n    assert (out_state[1] - lstm_out_state[1]).abs().max() < 1e-5", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "002107", "source": "def create_class_node(self, class_info, namespaces):\n            return create_class_node(self.cv_root, class_info, namespaces)", "target": "def forbid_inf_nan_check(x: Any) -> Any:\n    if not math.isfinite(x):\n        raise PydanticKnownError('finite_number')\n    return x", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "002108", "source": "def test_by_alias_and_name_config_interaction(\n    config_by_alias: Union[bool, None],\n    config_by_name: Union[bool, None],\n    runtime_by_alias: Union[bool, None],\n    runtime_by_name: Union[bool, None],\n) -> None:\n    if config_by_alias is False and config_by_name is False and runtime_by_alias is False and runtime_by_name is False:\n        pytest.skip(\"Can't have both by_alias and by_name as effectively False\")\n    schema = core_schema.arguments_schema(\n        arguments=[\n            core_schema.arguments_parameter(name='my_field', schema=core_schema.int_schema(), alias='my_alias'),\n        ],\n        **({'validate_by_alias': config_by_alias} if config_by_alias is not None else {}),\n        **({'validate_by_name': config_by_name} if config_by_name is not None else {}),\n    )\n    s = SchemaValidator(schema)\n    alias_allowed = next(x for x in (runtime_by_alias, config_by_alias, True) if x is not None)\n    name_allowed = next(x for x in (runtime_by_name, config_by_name, False) if x is not None)\n    if alias_allowed:\n        assert s.validate_python(\n            ArgsKwargs((), {'my_alias': 1}), by_alias=runtime_by_alias, by_name=runtime_by_name\n        ) == (\n            (),\n            {'my_field': 1},\n        )\n    if name_allowed:\n        assert s.validate_python(\n            ArgsKwargs((), {'my_field': 1}), by_alias=runtime_by_alias, by_name=runtime_by_name\n        ) == (\n            (),\n            {'my_field': 1},\n        )", "target": "def test_by_alias_and_name_config_interaction(\n    config_by_alias: Union[bool, None],\n    config_by_name: Union[bool, None],\n    runtime_by_alias: Union[bool, None],\n    runtime_by_name: Union[bool, None],\n) -> None:\n    if config_by_alias is False and config_by_name is False and runtime_by_alias is False and runtime_by_name is False:\n        pytest.skip(\"Can't have both by_alias and by_name as effectively False\")\n    core_config = {\n        **({'validate_by_alias': config_by_alias} if config_by_alias is not None else {}),\n        **({'validate_by_name': config_by_name} if config_by_name is not None else {}),\n    }\n    schema = core_schema.typed_dict_schema(\n        fields={\n            'my_field': core_schema.typed_dict_field(schema=core_schema.int_schema(), validation_alias='my_alias'),\n        },\n        config=core_schema.CoreConfig(**core_config),\n    )\n    s = SchemaValidator(schema)\n    alias_allowed = next(x for x in (runtime_by_alias, config_by_alias, True) if x is not None)\n    name_allowed = next(x for x in (runtime_by_name, config_by_name, False) if x is not None)\n    if alias_allowed:\n        assert s.validate_python({'my_alias': 1}, by_alias=runtime_by_alias, by_name=runtime_by_name) == {'my_field': 1}\n    if name_allowed:\n        assert s.validate_python({'my_field': 1}, by_alias=runtime_by_alias, by_name=runtime_by_name) == {'my_field': 1}", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "002109", "source": "def test_property_other_error():\n    @dataclasses.dataclass\n    class Model:\n        width: int\n        @property\n        def area(self) -> int:\n            raise ValueError('xxx')\n    s = SchemaSerializer(\n        core_schema.model_schema(\n            Model,\n            core_schema.model_fields_schema(\n                {'width': core_schema.model_field(core_schema.int_schema())},\n                computed_fields=[core_schema.computed_field('area', core_schema.bytes_schema())],\n            ),\n        )\n    )\n    with pytest.raises(ValueError, match='^xxx$'):\n        s.to_python(Model(3))\n    with pytest.raises(ValueError, match='^xxx$'):\n        s.to_python(Model(3), mode='json')\n    e = '^Error serializing to JSON: ValueError: xxx$'\n    with pytest.raises(PydanticSerializationError, match=e):\n        s.to_json(Model(3))", "target": "def name(self) -> str:\n        prefix = f\"{self.category()}_{self.backend()}\"\n        if self.is_dynamic():\n            prefix += \"_dynamic\"\n        if self.device() == \"cuda\":\n            prefix += \"_gpu\"\n        return prefix", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "002110", "source": "def fit(est, data_train, target_train, libname):\n    print(f\"Fitting a {libname} model...\")\n    tic = time()\n    est.fit(data_train, target_train)\n    toc = time()\n    print(f\"fitted in {toc - tic:.3f}s\")", "target": "def fit(est, data_train, target_train, libname, **fit_params):\n    print(f\"Fitting a {libname} model...\")\n    tic = time()\n    est.fit(data_train, target_train, **fit_params)\n    toc = time()\n    print(f\"fitted in {toc - tic:.3f}s\")", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "002111", "source": "def dropoutlstm_creator(script=True, **kwargs):\n    assert script is True\n    from .custom_lstms import LSTMState, script_lstm\n    input_size = kwargs[\"inputSize\"]\n    hidden_size = kwargs[\"hiddenSize\"]\n    seq_len = kwargs[\"seqLength\"]\n    batch_size = kwargs[\"miniBatch\"]\n    num_layers = kwargs[\"numLayers\"]\n    ge = script_lstm(input_size, hidden_size, num_layers, dropout=True).cuda()\n    input = torch.randn(seq_len, batch_size, input_size, device=\"cuda\")\n    states = [\n        LSTMState(\n            torch.randn(batch_size, hidden_size, device=\"cuda\"),\n            torch.randn(batch_size, hidden_size, device=\"cuda\"),\n        )\n        for _ in range(num_layers)\n    ]\n    return ModelDef(\n        inputs=[input, states],\n        params=ge.parameters(),\n        forward=ge,\n        backward_setup=lstm_backward_setup,\n        backward=simple_backward,\n    )", "target": "def _validator(cls, v, info):\n                return v", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "002112", "source": "def test_detect_and_decode(self):\n        img = cv.imread(os.path.join(self.extraTestDataPath, 'cv/qrcode/link_ocv.jpg'))\n        self.assertFalse(img is None)\n        detector = cv.QRCodeDetector()\n        retval, points, straight_qrcode = detector.detectAndDecode(img)\n        self.assertEqual(retval, \"https://opencv.org/\")\n        self.assertEqual(points.shape, (1, 4, 2))", "target": "def test_detect_and_decode(self):\n        img = cv.imread(os.path.join(self.extraTestDataPath, 'cv/barcode/single/book.jpg'))\n        self.assertFalse(img is None)\n        detector = cv.barcode_BarcodeDetector()\n        retval, decoded_info, decoded_type, corners = detector.detectAndDecodeWithType(img)\n        self.assertTrue(retval)\n        self.assertTrue(len(decoded_info) > 0)\n        self.assertTrue(len(decoded_type) > 0)\n        self.assertEqual(decoded_info[0], \"9787115279460\")\n        self.assertEqual(decoded_type[0], \"EAN_13\")\n        self.assertEqual(corners.shape, (1, 4, 2))", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "002113", "source": "def test_dict_fail_fast(fail_fast, expected):\n    v = SchemaValidator(\n        {'type': 'dict', 'keys_schema': {'type': 'int'}, 'values_schema': {'type': 'int'}, 'fail_fast': fail_fast}\n    )\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python({'a': 'b', 'c': 'd'})\n    assert exc_info.value.errors(include_url=False) == expected", "target": "def test_person_detection_retail_0013(self):\n            if not cv.dnn.DNN_TARGET_CPU in cv.dnn.getAvailableTargets(cv.dnn.DNN_BACKEND_INFERENCE_ENGINE):\n                return\n            root_path    = '/omz_intel_models/intel/person-detection-retail-0013/FP32/person-detection-retail-0013'\n            model_path   = self.find_file(root_path + '.xml',   [os.environ.get('OPENCV_DNN_TEST_DATA_PATH')], required=False)\n            weights_path = self.find_file(root_path + '.bin',   [os.environ.get('OPENCV_DNN_TEST_DATA_PATH')], required=False)\n            img_path     = self.find_file('gpu/lbpcascade/er.png', [os.environ.get('OPENCV_TEST_DATA_PATH')])\n            device_id    = 'CPU'\n            img          = cv.resize(cv.imread(img_path), (544, 320))\n            net = cv.dnn.readNetFromModelOptimizer(model_path, weights_path)\n            net.setPreferableBackend(cv.dnn.DNN_BACKEND_INFERENCE_ENGINE)\n            net.setPreferableTarget(cv.dnn.DNN_TARGET_CPU)\n            blob = cv.dnn.blobFromImage(img)\n            def parseSSD(detections, size):\n                h, w = size\n                bboxes = []\n                detections = detections.reshape(-1, 7)\n                for sample_id, class_id, confidence, xmin, ymin, xmax, ymax in detections:\n                    if confidence >= 0.5:\n                        x      = int(xmin * w)\n                        y      = int(ymin * h)\n                        width  = int(xmax * w - x)\n                        height = int(ymax * h - y)\n                        bboxes.append((x, y, width, height))\n                return bboxes\n            net.setInput(blob)\n            dnn_detections = net.forward()\n            dnn_boxes = parseSSD(np.array(dnn_detections), img.shape[:2])\n            g_in   = cv.GMat()\n            inputs = cv.GInferInputs()\n            inputs.setInput('data', g_in)\n            g_sz       = cv.gapi.streaming.size(g_in)\n            outputs    = cv.gapi.infer(\"net\", inputs)\n            detections = outputs.at(\"detection_out\")\n            bboxes     = cv.gapi.parseSSD(detections, g_sz, 0.5, False, False)\n            comp = cv.GComputation(cv.GIn(g_in), cv.GOut(bboxes))\n            pp = cv.gapi.ie.params(\"net\", model_path, weights_path, device_id)\n            gapi_boxes = comp.apply(cv.gin(img.astype(np.float32)),\n                                    args=cv.gapi.compile_args(cv.gapi.networks(pp)))\n            self.assertEqual(0.0, cv.norm(np.array(dnn_boxes).flatten(),\n                                          np.array(gapi_boxes).flatten(),\n                                          cv.NORM_INF))", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "002114", "source": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "target": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "002115", "source": "def name(self):\n        if self.use_loop:\n            return f\"{self.category()}_loop\"\n        return self.category()", "target": "def name(self):\n        prefix = f\"{self.category()}_{self._name}_{self.backend()}\"\n        return prefix", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|opencv/opencv", "data_split": "train", "example_id": "002116", "source": "def test_do_not_delete_tag(\n        self, mock_run_git: \"MagicMock\", mock_delete_tag: \"MagicMock\"\n    ) -> None:\n        for tag in [\n            \"ciflow/doesntseemtomatch\",\n            \"trunk/doesntseemtomatch\",\n            \"doesntseemtomatch\",\n        ]:\n            mock_run_git.side_effect = [\n                tag,\n                str(int(datetime.now().timestamp() - 8 * 24 * 60 * 60)),\n            ]\n            delete_old_tags()\n            mock_delete_tag.assert_not_called()", "target": "def typename(self) -> Optional[str]:\n            return getattr(self.type_node, \"full_typename\", None)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "002117", "source": "def test_union():\n    v = SchemaValidator(cs.union_schema(choices=[cs.str_schema(strict=True), cs.bytes_schema(strict=True)]))\n    assert v.validate_python('oh, a string') == 'oh, a string'\n    assert v.validate_python(b'oh, bytes') == b'oh, bytes'", "target": "def test_union():\n    v = SchemaValidator(core_schema.union_schema(choices=[core_schema.str_schema(), core_schema.timedelta_schema()]))\n    assert v.validate_python('P2DT1H') == 'P2DT1H'\n    assert v.validate_python(timedelta(days=2, hours=1)) == timedelta(days=2, hours=1)\n    v = SchemaValidator(core_schema.union_schema(choices=[core_schema.timedelta_schema(), core_schema.str_schema()]))\n    assert v.validate_python('P2DT1H') == 'P2DT1H'\n    assert v.validate_python(timedelta(days=2, hours=1)) == timedelta(days=2, hours=1)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "002118", "source": "def huge_graph(x):\n    for _ in range(N):\n        x = x.sin()\n    return x", "target": "def huge_graph():\n    def fn(x):\n        for _ in range(N):\n            x = x.sin()\n        return x\n    return torch.fx.symbolic_trace(fn)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "002119", "source": "def test_backward(self, modeldef, benchmark):\n        backward_input = modeldef.forward(*modeldef.inputs)\n        if modeldef.backward_setup is not None:\n            backward_input = modeldef.backward_setup(backward_input)\n        if modeldef.backward is not None:\n            benchmark(cuda_sync, modeldef.backward, *backward_input, retain_graph=True)\n            with torch.no_grad():\n                for param in modeldef.params:\n                    assert param.grad is not None\n                    param.grad.zero_()", "target": "def _dataclass_setstate(self: Any, state: list[Any]) -> None:\n                    for field, value in zip(dataclasses.fields(self), state):\n                        object.__setattr__(self, field.name, value)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "002120", "source": "def ser_x(data: Model, v: Any, _) -> str:\n        assert data['x'] == 1_000\n        return f'{v:_}'", "target": "def ser_x(data: Model, v: Any, info: core_schema.FieldSerializationInfo) -> str:\n        assert data['x'] == 1_000\n        return f'{v:_}-{info.field_name}'", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "002121", "source": "def ser_x(self, v: Any, _) -> str:\n            assert self.x == 1_000\n            return f'{v:_}'", "target": "def ser_x(data: Model, v: Any) -> str:\n        assert data['x'] == 1_000\n        return f'{v:_}'", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "002122", "source": "def ser_x(self, v: Any, _) -> str:\n            assert self.x == 1_000 == v\n            return self.x_formatted", "target": "def ser_x(data: Model, v: Any, serializer: core_schema.SerializerFunctionWrapHandler) -> str:\n        assert data['x'] == 1_000\n        x = serializer(v)\n        return f'{x:_}'", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "002123", "source": "def apply_rotary_emb(x: Tensor, freqs_cis: Tensor) -> Tensor:\n    xshaped = x.float().reshape(*x.shape[:-1], -1, 2)\n    freqs_cis = freqs_cis.view(1, xshaped.size(1), 1, xshaped.size(3), 2)\n    x_out2 = torch.stack(\n        [\n            xshaped[..., 0] * freqs_cis[..., 0] - xshaped[..., 1] * freqs_cis[..., 1],\n            xshaped[..., 1] * freqs_cis[..., 0] + xshaped[..., 0] * freqs_cis[..., 1],\n        ],\n        -1,\n    )\n    x_out2 = x_out2.flatten(3)\n    return x_out2.type_as(x)", "target": "def apply_rotary_emb(x: Tensor, freqs_cis: Tensor) -> Tensor:\n    xshaped = x.float().reshape(*x.shape[:-1], -1, 2)\n    freqs_cis = freqs_cis.view(1, xshaped.size(1), 1, xshaped.size(3), 2)\n    x_out2 = torch.stack(\n        [\n            xshaped[..., 0] * freqs_cis[..., 0] - xshaped[..., 1] * freqs_cis[..., 1],\n            xshaped[..., 1] * freqs_cis[..., 0] + xshaped[..., 0] * freqs_cis[..., 1],\n        ],\n        -1,\n    )\n    x_out2 = x_out2.flatten(3)\n    return x_out2.type_as(x)", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "002124", "source": "def add_enumeration(self, name: str) -> EnumerationNode:\n        return self._add_child(EnumerationNode, name)", "target": "def add_enumeration(self, name: str) -> EnumerationNode:\n        return self._add_child(EnumerationNode, name)", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "002125", "source": "def generate_vector(shape, dtype):\n    if np.issubdtype(dtype, np.integer):\n        return np.random.randint(0, 100, shape).astype(dtype)\n    else:\n        return np.random.normal(10., 12.5, shape).astype(dtype)", "target": "def test_dataclass_slots_field_after_validator():\n    @dataclasses.dataclass(slots=True)\n    class Foo:\n        a: int\n        b: str\n        @classmethod\n        def validate_b(cls, v: str, info: core_schema.ValidationInfo) -> str:\n            assert v == 'hello'\n            assert info.field_name == 'b'\n            assert info.data == {'a': 1}\n            return 'hello world!'\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.int_schema()),\n                core_schema.dataclass_field(\n                    name='b',\n                    schema=core_schema.with_info_after_validator_function(Foo.validate_b, core_schema.str_schema()),\n                ),\n            ],\n        ),\n        ['a', 'b'],\n        slots=True,\n    )\n    v = SchemaValidator(schema)\n    foo = v.validate_python({'a': 1, 'b': b'hello'})\n    assert dataclasses.asdict(foo) == {'a': 1, 'b': 'hello world!'}", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "002126", "source": "def test_include():\n    s = SchemaSerializer(core_schema.dict_schema(serialization=core_schema.filter_dict_schema(include={'a', 'c'})))\n    assert s.to_python({'a': 1, 'b': 2, 'c': 3, 'd': 4}) == {'a': 1, 'c': 3}\n    assert s.to_json({'a': 1, 'b': 2, 'c': 3, 'd': 4}) == b'{\"a\":1,\"c\":3}'\n    assert s.to_python({'a': 1, 'b': 2, 'd': 4}, include={'d'}) == {'a': 1, 'd': 4}\n    assert s.to_python({'a': 1, 'b': 2, 'd': 4}, include={'d': None}) == {'a': 1, 'd': 4}\n    assert s.to_python({'a': 1, 'b': 2, 'd': 4}, include={'d': {1}}) == {'a': 1, 'd': 4}\n    assert s.to_python({'a': 1, 'b': 2, 'd': 4, 5: 6}, include={5}) == {'a': 1, 5: 6}\n    assert s.to_python({'a': 1, 'b': 2, 'd': 4, 5: 6}, mode='json', include={5}) == {'a': 1, '5': 6}\n    assert s.to_json({'a': 1, 'b': 2, 'd': 4, 5: 6}, include={5}) == b'{\"a\":1,\"5\":6}'", "target": "def test_include(schema_func, seq_f):\n    v = SchemaSerializer(\n        schema_func(core_schema.any_schema(), serialization=core_schema.filter_seq_schema(include={1, 3, 5}))\n    )\n    assert v.to_python(seq_f(0, 1, 2, 3)) == seq_f(1, 3)\n    assert v.to_python(seq_f('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h')) == seq_f('b', 'd', 'f')\n    assert v.to_python(seq_f('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'), mode='json') == ['b', 'd', 'f']\n    assert v.to_json(seq_f('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h')) == b'[\"b\",\"d\",\"f\"]'\n    assert v.to_python(seq_f('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'), include={6}) == seq_f('b', 'd', 'f', 'g')\n    assert v.to_python(seq_f('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'), include=[6]) == seq_f('b', 'd', 'f', 'g')\n    assert v.to_json(seq_f('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'), include={6}) == b'[\"b\",\"d\",\"f\",\"g\"]'\n    assert v.to_python(seq_f('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'), include={6: None}) == seq_f('b', 'd', 'f', 'g')\n    assert v.to_python(seq_f('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'), include={-1: None, -2: None}, mode='json') == [\n        'b',\n        'd',\n        'f',\n        'g',\n        'h',\n    ]", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|opencv/opencv", "data_split": "train", "example_id": "002127", "source": "def torchviz_model(args, model, inputs, rank):\n    from torchviz import make_dot\n    outputs = model(*inputs)\n    loss = reduce_to_scalar_loss(outputs)\n    parameter_names = dict(model.named_parameters())\n    dot = make_dot(loss, params=parameter_names, show_attrs=True, show_saved=True)\n    if rank == 0:\n        dot.render(\"torchviz.dot\")", "target": "def constants(self) -> Dict[str, ConstantNode]:\n        return self._children[ASTNodeType.Constant]", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "002128", "source": "def _prepare_once(self):\n        self.a = torch.ones(1000, device=self.device())\n        self.b = torch.torch.ones(1000, device=self.device())", "target": "def _prepare_once(self):\n        self.m = self.ModuleClass()\n        torch.set_float32_matmul_precision(\"high\")\n        self.input = torch.ones(10, device=self.device())", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|scikit-learn/scikit-learn", "data_split": "train", "example_id": "002129", "source": "def constants(self) -> Dict[str, ConstantNode]:\n        return self._children[ASTNodeType.Constant]", "target": "def get_object_members(self, want_all):\n        return (False, [])", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|scikit-learn/scikit-learn", "data_split": "train", "example_id": "002130", "source": "def _work(self):\n        @torch.compile(backend=\"inductor\")\n        def f(x, y):\n            return x + y\n        with fresh_cache():\n            for i in range(8):\n                f(torch.arange(3), i * 2.5)", "target": "def benchmark_influence(conf):\n    prediction_times = []\n    prediction_powers = []\n    complexities = []\n    for param_value in conf[\"changing_param_values\"]:\n        conf[\"tuned_params\"][conf[\"changing_param\"]] = param_value\n        estimator = conf[\"estimator\"](**conf[\"tuned_params\"])\n        print(\"Benchmarking %s\" % estimator)\n        estimator.fit(conf[\"data\"][\"X_train\"], conf[\"data\"][\"y_train\"])\n        conf[\"postfit_hook\"](estimator)\n        complexity = conf[\"complexity_computer\"](estimator)\n        complexities.append(complexity)\n        start_time = time.time()\n        for _ in range(conf[\"n_samples\"]):\n            y_pred = estimator.predict(conf[\"data\"][\"X_test\"])\n        elapsed_time = (time.time() - start_time) / float(conf[\"n_samples\"])\n        prediction_times.append(elapsed_time)\n        pred_score = conf[\"prediction_performance_computer\"](\n            conf[\"data\"][\"y_test\"], y_pred\n        )\n        prediction_powers.append(pred_score)\n        print(\n            \"Complexity: %d | %s: %.4f | Pred. Time: %fs\\n\"\n            % (\n                complexity,\n                conf[\"prediction_performance_label\"],\n                pred_score,\n                elapsed_time,\n            )\n        )\n    return prediction_powers, prediction_times, complexities", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "002131", "source": "def constrain(self: _Pipeline[_InT, _OutT], constraint: _Eq) -> _Pipeline[_InT, _OutT]: ...", "target": "def forward(self, src, has_mask=True):\n        if has_mask:\n            device = src.device\n            if self.src_mask is None or self.src_mask.size(0) != len(src):\n                mask = nn.Transformer.generate_square_subsequent_mask(len(src)).to(\n                    device\n                )\n                self.src_mask = mask\n        else:\n            self.src_mask = None\n        src = self.encoder(src) * math.sqrt(self.ninp)\n        src = self.pos_encoder(src)\n        output = self.transformer_encoder(src, self.src_mask)\n        output = self.decoder(output)\n        return F.log_softmax(output, dim=-1)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|opencv/opencv", "data_split": "train", "example_id": "002132", "source": "def make_gaussians(cluster_n, img_size):\n    points = []\n    ref_distrs = []\n    for _ in xrange(cluster_n):\n        mean = (0.1 + 0.8*random.rand(2)) * img_size\n        a = (random.rand(2, 2)-0.5)*img_size*0.1\n        cov = np.dot(a.T, a) + img_size*0.05*np.eye(2)\n        n = 100 + random.randint(900)\n        pts = random.multivariate_normal(mean, cov, n)\n        points.append( pts )\n        ref_distrs.append( (mean, cov) )\n    points = np.float32( np.vstack(points) )\n    return points, ref_distrs", "target": "def get_xcode_major():\n    ret = check_output([\"xcodebuild\", \"-version\"]).decode('utf-8')\n    m = re.match(r'Xcode\\s+(\\d+)\\..*', ret, flags=re.IGNORECASE)\n    if m:\n        return int(m.group(1))\n    else:\n        raise Exception(\"Failed to parse Xcode version\")", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "002133", "source": "def outMeta(arr_desc0, arr_desc1, arr_desc2):\n        return cv.empty_array_desc()", "target": "def outMeta(arr_desc0, arr_desc1, arr_desc2):\n        return cv.empty_array_desc(), cv.empty_array_desc(), \\\n               cv.empty_array_desc(), cv.empty_array_desc()", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "002134", "source": "def typename(self) -> str:\n        return self.alias_export_name", "target": "def typename(self) -> str:\n        return self._export_name", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "002135", "source": "def add_signature_to_table(soup, table, signature, language, type):\n    row = soup.new_tag('tr')\n    row.append(soup.new_tag('td', style='width: 20px;'))\n    row.append(append(soup.new_tag('td'), signature['name'] + '('))\n    row.append(append(soup.new_tag('td', **{'class': 'paramname'}), signature['arg']))\n    row.append(append(soup.new_tag('td'), ') -> '))\n    row.append(append(soup.new_tag('td'), signature['ret']))\n    table.append(row)", "target": "def f(input_value, validator):\n        return validator(input_value=input_value) + ' Changed'", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "002136", "source": "def compiled(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, target = args\n        torch._dynamo.mark_dynamic(x, 0)\n        torch._dynamo.mark_dynamic(target, 0)\n        compiled_cross_entropy = torch.compile(\n            lambda x, target: F.cross_entropy(x, target, reduction=\"none\"),\n            mode=self.compile_mode,\n            fullgraph=True,\n        )\n        return lambda: compiled_cross_entropy(x, target)", "target": "def compiled(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, target, dloss = args\n        compiled_cross_entropy = torch.compile(\n            lambda x, target: F.cross_entropy(x, target, reduction=\"none\"),\n            mode=self.compile_mode,\n            fullgraph=True,\n        )\n        loss = compiled_cross_entropy(x, target)\n        return lambda: torch.autograd.grad(\n            loss, x, grad_outputs=dloss, retain_graph=True\n        )", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "002137", "source": "def test_reserved_keywords_are_transformed(self):\n        default_lambda_value = 2\n        default_from_value = 3\n        format_str = \"arg={}, lambda={}, from={}\"\n        self.assertEqual(\n            cv.utils.testReservedKeywordConversion(20), format_str.format(20, default_lambda_value, default_from_value)\n        )\n        self.assertEqual(\n            cv.utils.testReservedKeywordConversion(10, lambda_=10), format_str.format(10, 10, default_from_value)\n        )\n        self.assertEqual(\n            cv.utils.testReservedKeywordConversion(10, from_=10), format_str.format(10, default_lambda_value, 10)\n        )\n        self.assertEqual(\n            cv.utils.testReservedKeywordConversion(20, lambda_=-4, from_=12), format_str.format(20, -4, 12)\n        )", "target": "def test_int_kwargs(py_and_json: PyAndJson, kwargs: dict[str, Any], input_value, expected):\n    v = py_and_json({'type': 'int', **kwargs})\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)) as exc_info:\n            v.validate_test(input_value)\n        errors = exc_info.value.errors(include_url=False)\n        assert len(errors) == 1\n        if 'ctx' in errors[0]:\n            assert errors[0]['ctx'] == kwargs\n    else:\n        output = v.validate_test(input_value)\n        assert output == expected\n        assert isinstance(output, int)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "002138", "source": "def benchmark(self):\n        for M, N in self.get_shapes():\n            print(f\"Tensor dimensions: [{M}, {N}]\")\n            torch_dtype = cutlass_torch.dtype(cutlass.BFloat16)\n            x = torch.randn(M, N, device=\"cuda\", dtype=torch_dtype, requires_grad=True)\n            w = torch.randn(N, device=\"cuda\", dtype=torch.float32, requires_grad=True)\n            dy = torch.randn(M, N, device=\"cuda\", dtype=torch_dtype)\n            self.benchmark_single_shape((x, w, dy), setting=f\"shape: [{M}, {N}]\")", "target": "def benchmark(self):\n        for M, N in self.get_shapes():\n            print(f\"Tensor dimensions: [{M}, {N}]\")\n            torch_dtype = cutlass_torch.dtype(cutlass.BFloat16)\n            x = torch.randn(M, N, device=\"cuda\", dtype=torch_dtype)\n            w = torch.randn(N, device=\"cuda\", dtype=torch.float32)\n            self.benchmark_single_shape((x, w), setting=f\"shape: [{M}, {N}]\")", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "002139", "source": "def compiled(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, target, dloss = args\n        compiled_cross_entropy = torch.compile(\n            lambda x, target: F.cross_entropy(x, target, reduction=\"none\"),\n            mode=self.compile_mode,\n            fullgraph=True,\n        )\n        loss = compiled_cross_entropy(x, target)\n        return lambda: torch.autograd.grad(\n            loss, x, grad_outputs=dloss, retain_graph=True\n        )", "target": "def compiled(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, w = args\n        torch._dynamo.mark_dynamic(x, 0)\n        compiled_layernorm = torch.compile(\n            self.layernorm_ref, mode=self.compile_mode, fullgraph=True\n        )\n        return lambda: compiled_layernorm(x, w, eps=1e-6)", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "002140", "source": "def is_resolved(self) -> bool:\n        return True", "target": "def LazyClassAttribute(name: str, get_value: Callable[[], T]) -> T: ...", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "002141", "source": "def required_usage_imports(self) -> Generator[str, None, None]:\n        yield \"import cv2.typing\"", "target": "def required_usage_imports(self) -> Generator[str, None, None]:\n        yield \"import typing as _typing\"\n        yield from super().required_usage_imports", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "002142", "source": "def eager(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, w = args\n        return lambda: self.rms_norm_ref(x, w)", "target": "def eager(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        x, w = args\n        return lambda: self.layernorm_ref(x, w)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "002143", "source": "def parse_args() -> Any:\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"workflow_run_id\", help=\"The id of the workflow run, should be GITHUB_RUN_ID\"\n    )\n    parser.add_argument(\n        \"runner_name\",\n        help=\"The name of the runner to retrieve the job id, should be RUNNER_NAME\",\n    )\n    return parser.parse_args()", "target": "def test_custom_dataclass_names():\n    schema = core_schema.dataclass_schema(\n        FooParentDataclass,\n        core_schema.dataclass_args_schema(\n            'FooParentDataclass',\n            [\n                core_schema.dataclass_field(\n                    name='foo',\n                    schema=core_schema.union_schema(\n                        [\n                            core_schema.dataclass_schema(\n                                FooDataclass,\n                                core_schema.dataclass_args_schema(\n                                    'FooDataclass[dataclass_args_schema]',\n                                    [\n                                        core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                                        core_schema.dataclass_field(name='b', schema=core_schema.bool_schema()),\n                                    ],\n                                ),\n                                ['a', 'b'],\n                                cls_name='FooDataclass[cls_name]',\n                            ),\n                            core_schema.none_schema(),\n                        ]\n                    ),\n                )\n            ],\n        ),\n        ['foo'],\n    )\n    v = SchemaValidator(schema)\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_python({'foo': 123})\n    assert exc_info.value.errors(include_url=False) == [\n        {\n            'ctx': {'class_name': 'FooDataclass[dataclass_args_schema]'},\n            'input': 123,\n            'loc': ('foo', 'FooDataclass[cls_name]'),\n            'msg': 'Input should be a dictionary or an instance of FooDataclass[dataclass_args_schema]',\n            'type': 'dataclass_type',\n        },\n        {'input': 123, 'loc': ('foo', 'none'), 'msg': 'Input should be None', 'type': 'none_required'},\n    ]", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "002144", "source": "def test_only_validate_by_name(py_and_json) -> None:\n    schema = core_schema.arguments_schema(\n        [\n            core_schema.arguments_parameter(name='a', schema=core_schema.str_schema(), alias='FieldA'),\n        ],\n        validate_by_name=True,\n        validate_by_alias=False,\n    )\n    v = py_and_json(schema)\n    assert v.validate_test(ArgsKwargs((), {'a': 'hello'})) == ((), {'a': 'hello'})\n    with pytest.raises(ValidationError, match=r'a\\n +Missing required argument \\[type=missing_argument,'):\n        assert v.validate_test(ArgsKwargs((), {'FieldA': 'hello'}))", "target": "def test_only_validate_by_name(py_and_json) -> None:\n    v = py_and_json(\n        {\n            'type': 'typed-dict',\n            'fields': {\n                'field_a': {'validation_alias': 'FieldA', 'type': 'typed-dict-field', 'schema': {'type': 'int'}}\n            },\n            'config': {'validate_by_name': True, 'validate_by_alias': False},\n        }\n    )\n    assert v.validate_test({'field_a': '123'}) == {'field_a': 123}\n    with pytest.raises(ValidationError, match=r'field_a\\n +Field required \\[type=missing,'):\n        assert v.validate_test({'FieldA': '123'})", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "002145", "source": "def quack(self, args, kwargs=None) -> Any:\n        from quack.softmax import softmax\n        assert kwargs is None\n        (x,) = args\n        return lambda: softmax(x)", "target": "def quack(self, args, kwargs) -> Any:\n        from quack.layernorm import layernorm\n        x, w = args\n        return lambda: layernorm(x, w, eps=1e-6)", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "002146", "source": "def forward(self, inputs):\n        output = self.pool(F.relu(self.conv(inputs)))\n        output = output.view(1)\n        return output", "target": "def forward(self, x):\n        for _ in range(self._n):\n            x = fn9(x)\n        return x", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "002147", "source": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "target": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "002148", "source": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "target": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "002149", "source": "def test_dict():\n    v = SchemaValidator(\n        core_schema.dict_schema(keys_schema=core_schema.int_schema(), values_schema=core_schema.int_schema())\n    )\n    assert v.validate_json('{\"1\": 2, \"3\": 4}') == {1: 2, 3: 4}\n    assert json.loads('{\"1\": 1, \"1\": 2}') == {'1': 2}\n    assert v.validate_json('{\"1\": 1, \"1\": 2}') == {1: 2}", "target": "def test_dict():\n    v = SchemaValidator(core_schema.dict_schema(core_schema.int_schema(), core_schema.date_schema()))\n    assert v.validate_strings({'1': '2017-01-01', '2': '2017-01-02'}) == {1: date(2017, 1, 1), 2: date(2017, 1, 2)}\n    assert v.validate_strings({'1': '2017-01-01', '2': '2017-01-02'}, strict=True) == {\n        1: date(2017, 1, 1),\n        2: date(2017, 1, 2),\n    }", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "002150", "source": "def get_memory_bytes(self, args, kwargs) -> int:\n        x, target = args\n        M, N = x.shape\n        dtype = x.dtype\n        return (M * N + M + M) * dtype.itemsize", "target": "def get_memory_bytes(self, args, kwargs) -> int:\n        x, dy = args\n        M, N = x.shape\n        return 3 * M * N * x.dtype.itemsize", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "002151", "source": "def test_config_time(\n    t: date, expected_to_python, expected_to_json, expected_to_python_dict, expected_to_json_dict, mode\n):\n    s = SchemaSerializer(core_schema.time_schema(), config={'ser_json_temporal': mode})\n    assert s.to_python(t) == t\n    assert s.to_python(t, mode='json') == expected_to_python\n    assert s.to_json(t) == expected_to_json\n    with pytest.warns(\n        UserWarning,\n        match=(\n            r'Expected `time` - serialized value may not be as expected '\n            r\"\\[input_value=\\{datetime\\.time\\([^)]*\\): 'foo'\\}, input_type=dict\\]\"\n        ),\n    ):\n        assert s.to_python({t: 'foo'}) == {t: 'foo'}\n    with pytest.warns(\n        UserWarning,\n        match=(\n            r'Expected `time` - serialized value may not be as expected '\n            r\"\\[input_value=\\{datetime\\.time\\([^)]*\\): 'foo'\\}, input_type=dict\\]\"\n        ),\n    ):\n        assert s.to_python({t: 'foo'}, mode='json') == expected_to_python_dict\n    with pytest.warns(\n        UserWarning,\n        match=(\n            r'Expected `time` - serialized value may not be as expected '\n            r\"\\[input_value=\\{datetime\\.time\\([^)]*\\): 'foo'\\}, input_type=dict\\]\"\n        ),\n    ):\n        assert s.to_json({t: 'foo'}) == expected_to_json_dict", "target": "def resolve_enum_scopes(root: NamespaceNode,\n                        enums: Dict[SymbolName, EnumerationNode]):\n    for symbol_name, enum_node in enums.items():\n        if symbol_name.classes:\n            try:\n                scope = find_scope(root, symbol_name)\n            except ScopeNotFoundError:\n                for i, class_name in enumerate(symbol_name.classes):\n                    scope = find_scope(root,\n                                       SymbolName(symbol_name.namespaces,\n                                                  classes=symbol_name.classes[:i],\n                                                  name=class_name))\n                    if class_name in scope.classes:\n                        continue\n                    class_node = scope.add_class(class_name)\n                    class_node.is_exported = False\n                scope = find_scope(root, symbol_name)\n        else:\n            scope = find_scope(root, symbol_name)\n        enum_node.parent = scope", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "002152", "source": "def test_more_specific_data_matches_subclass(self, choices) -> None:\n        validator = SchemaValidator(core_schema.union_schema(choices))\n        assert isinstance(validator.validate_python({'a': 1}), self.ModelA)\n        assert isinstance(validator.validate_python({'a': 1, 'b': 2}), self.ModelB)\n        assert isinstance(validator.validate_python({'a': 1, 'b': 2}), self.ModelB)\n        assert isinstance(validator.validate_python({'a': '1', 'b': '2'}), self.ModelB)\n        assert isinstance(validator.validate_python({'a': '1', 'b': 2}), self.ModelB)\n        assert isinstance(validator.validate_python({'a': 1, 'b': '2'}), self.ModelB)", "target": "def test_dataclass_args_init(input_value, extra_behavior, expected):\n    @dataclasses.dataclass\n    class Foo:\n        a: str\n        b: str\n        def __post_init__(self):\n            self.b = self.a.upper()\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(name='a', schema=core_schema.str_schema()),\n                core_schema.dataclass_field(name='b', schema=core_schema.str_schema(), init=False),\n            ],\n            extra_behavior=extra_behavior,\n        ),\n        ['a', 'b'],\n        post_init=True,\n    )\n    v = SchemaValidator(schema)\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)) as exc_info:\n            v.validate_python(input_value)\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        assert dataclasses.asdict(v.validate_python(input_value)) == expected", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "002153", "source": "def bootstrap():\n    import sys\n    import copy\n    save_sys_path = copy.copy(sys.path)\n    if hasattr(sys, 'OpenCV_LOADER'):\n        print(sys.path)\n        raise ImportError('ERROR: recursion is detected during loading of \"cv2\" binary extensions. Check OpenCV installation.')\n    sys.OpenCV_LOADER = True\n    DEBUG = False\n    if hasattr(sys, 'OpenCV_LOADER_DEBUG'):\n        DEBUG = True\n    import platform\n    if DEBUG: print('OpenCV loader: os.name=\"{}\"  platform.system()=\"{}\"'.format(os.name, str(platform.system())))\n    LOADER_DIR = os.path.dirname(os.path.abspath(os.path.realpath(__file__)))\n    PYTHON_EXTENSIONS_PATHS = []\n    BINARIES_PATHS = []\n    g_vars = globals()\n    l_vars = locals().copy()\n    if sys.version_info[:2] < (3, 0):\n        from . load_config_py2 import exec_file_wrapper\n    else:\n        from . load_config_py3 import exec_file_wrapper\n    def load_first_config(fnames, required=True):\n        for fname in fnames:\n            fpath = os.path.join(LOADER_DIR, fname)\n            if not os.path.exists(fpath):\n                if DEBUG: print('OpenCV loader: config not found, skip: {}'.format(fpath))\n                continue\n            if DEBUG: print('OpenCV loader: loading config: {}'.format(fpath))\n            exec_file_wrapper(fpath, g_vars, l_vars)\n            return True\n        if required:\n            raise ImportError('OpenCV loader: missing configuration file: {}. Check OpenCV installation.'.format(fnames))\n    load_first_config(['config.py'], True)\n    load_first_config([\n        'config-{}.{}.py'.format(sys.version_info[0], sys.version_info[1]),\n        'config-{}.py'.format(sys.version_info[0])\n    ], True)\n    if DEBUG: print('OpenCV loader: PYTHON_EXTENSIONS_PATHS={}'.format(str(l_vars['PYTHON_EXTENSIONS_PATHS'])))\n    if DEBUG: print('OpenCV loader: BINARIES_PATHS={}'.format(str(l_vars['BINARIES_PATHS'])))\n    applySysPathWorkaround = False\n    if hasattr(sys, 'OpenCV_REPLACE_SYS_PATH_0'):\n        applySysPathWorkaround = True\n    else:\n        try:\n            BASE_DIR = os.path.dirname(LOADER_DIR)\n            if sys.path[0] == BASE_DIR or os.path.realpath(sys.path[0]) == BASE_DIR:\n                applySysPathWorkaround = True\n        except:\n            if DEBUG: print('OpenCV loader: exception during checking workaround for sys.path[0]')\n            pass\n    for p in reversed(l_vars['PYTHON_EXTENSIONS_PATHS']):\n        sys.path.insert(1 if not applySysPathWorkaround else 0, p)\n    if os.name == 'nt':\n        if sys.version_info[:2] >= (3, 8):\n            for p in l_vars['BINARIES_PATHS']:\n                try:\n                    os.add_dll_directory(p)\n                except Exception as e:\n                    if DEBUG: print('Failed os.add_dll_directory(): '+ str(e))\n                    pass\n        os.environ['PATH'] = ';'.join(l_vars['BINARIES_PATHS']) + ';' + os.environ.get('PATH', '')\n        if DEBUG: print('OpenCV loader: PATH={}'.format(str(os.environ['PATH'])))\n    else:\n        os.environ['LD_LIBRARY_PATH'] = ':'.join(l_vars['BINARIES_PATHS']) + ':' + os.environ.get('LD_LIBRARY_PATH', '')\n    if DEBUG: print(\"Relink everything from native cv2 module to cv2 package\")\n    py_module = sys.modules.pop(\"cv2\")\n    native_module = importlib.import_module(\"cv2\")\n    sys.modules[\"cv2\"] = py_module\n    setattr(py_module, \"_native\", native_module)\n    for item_name, item in filter(lambda kv: kv[0] not in (\"__file__\", \"__loader__\", \"__spec__\",\n                                                           \"__name__\", \"__package__\"),\n                                  native_module.__dict__.items()):\n        if item_name not in g_vars:\n            g_vars[item_name] = item\n    sys.path = save_sys_path\n    try:\n        del sys.OpenCV_LOADER\n    except Exception as e:\n        if DEBUG:\n            print(\"Exception during delete OpenCV_LOADER:\", e)\n    if DEBUG: print('OpenCV loader: binary extension... OK')\n    for submodule in __collect_extra_submodules(DEBUG):\n        if __load_extra_py_code_for_module(\"cv2\", submodule, DEBUG):\n            if DEBUG: print(\"Extra Python code for\", submodule, \"is loaded\")\n    if DEBUG: print('OpenCV loader: DONE')", "target": "def visit_AnnAssign(self, node: ast.AnnAssign) -> Any:\n        if isinstance(node.target, ast.Name):\n            self.target = node.target.id", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|scikit-learn/scikit-learn", "data_split": "train", "example_id": "002154", "source": "def LazyClassAttribute(name: str, get_value: Callable[[], T]) -> T: ...", "target": "def plot_results(X, y, label):\n    plt.plot(X, y, label=label, marker=\"o\")", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "002155", "source": "def required_definition_imports(self) -> Generator[str, None, None]:\n        yield \"import typing as _typing\"\n        yield from super().required_definition_imports", "target": "def test_datetime_key():\n    v = SchemaSerializer(core_schema.dict_schema(core_schema.datetime_schema(), core_schema.datetime_schema()))\n    assert v.to_python({datetime(2022, 12, 2, 12, 13, 14): datetime(2022, 12, 2, 12, 13, 14)}) == {\n        datetime(2022, 12, 2, 12, 13, 14): datetime(2022, 12, 2, 12, 13, 14)\n    }\n    assert v.to_python({datetime(2022, 12, 2, 12, 13, 14): datetime(2022, 12, 2, 12, 13, 14)}, mode='json') == {\n        '2022-12-02T12:13:14': '2022-12-02T12:13:14'\n    }\n    assert (\n        v.to_json({datetime(2022, 12, 2, 12, 13, 14): datetime(2022, 12, 2, 12, 13, 14)})\n        == b'{\"2022-12-02T12:13:14\":\"2022-12-02T12:13:14\"}'\n    )", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "002156", "source": "def test_keyword_only(py_and_json: PyAndJson, input_value) -> None:\n    v = py_and_json(\n        cs.arguments_v3_schema(\n            [\n                cs.arguments_v3_parameter(name='a', schema=cs.int_schema(), mode='keyword_only'),\n                cs.arguments_v3_parameter(\n                    name='b', schema=cs.with_default_schema(cs.bool_schema(), default=True), mode='keyword_only'\n                ),\n            ]\n        )\n    )\n    assert v.validate_test(input_value) == ((), {'a': 1, 'b': True})", "target": "def test_datetime_milliseconds(benchmark):\n    v = SchemaSerializer(core_schema.datetime_schema(), config={'ser_json_temporal': 'milliseconds'})\n    d = datetime(2022, 12, 2, 12, 13, 14)\n    assert v.to_python(d, mode='json') == 1669983194000.0\n    @benchmark\n    def r():\n        v.to_python(d, mode='json')", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "002157", "source": "def test_dict(py_and_json: PyAndJson):\n    v = py_and_json({'type': 'dict', 'keys_schema': {'type': 'datetime'}, 'values_schema': {'type': 'int'}})\n    assert v.validate_test({'2000-01-01T00:00': 2, '2000-01-02T00:00': 4}) == {\n        datetime(2000, 1, 1): 2,\n        datetime(2000, 1, 2): 4,\n    }", "target": "def test_dict(py_and_json: PyAndJson):\n    v = py_and_json({'type': 'dict', 'keys_schema': {'type': 'int'}, 'values_schema': {'type': 'int'}})\n    assert v.validate_test({'1': 2, '3': 4}) == {1: 2, 3: 4}\n    v = py_and_json({'type': 'dict', 'strict': True, 'keys_schema': {'type': 'int'}, 'values_schema': {'type': 'int'}})\n    assert v.validate_test({'1': 2, '3': 4}) == {1: 2, 3: 4}\n    assert v.validate_test({}) == {}\n    with pytest.raises(ValidationError, match=re.escape('[type=dict_type, input_value=[], input_type=list]')):\n        v.validate_test([])", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "002158", "source": "def test_cycle_same():\n    def fallback_func_passthrough(obj):\n        return obj\n    f = Foobar()\n    with pytest.raises(ValueError, match=r'Circular reference detected \\(id repeated\\)'):\n        to_jsonable_python(f, fallback=fallback_func_passthrough)\n    with pytest.raises(ValueError, match=r'Circular reference detected \\(id repeated\\)'):\n        to_json(f, fallback=fallback_func_passthrough)", "target": "def test_double_nested():\n    v = SchemaValidator(\n        core_schema.typed_dict_schema(\n            {\n                'a': core_schema.typed_dict_field(core_schema.int_schema(gt=10)),\n                'b': core_schema.typed_dict_field(\n                    core_schema.list_schema(\n                        core_schema.dict_schema(core_schema.str_schema(), core_schema.int_schema(ge=10))\n                    )\n                ),\n            },\n            total=False,\n        )\n    )\n    assert v.validate_python({'a': 11, 'b': [{'a': 10, 'b': 20}, {'a': 30, 'b': 40}]}) == snapshot(\n        {'a': 11, 'b': [{'a': 10, 'b': 20}, {'a': 30, 'b': 40}]}\n    )\n    assert v.validate_python({'a': 11, 'b': [{'a': 10, 'b': 20}, {'a': 30, 'b': 4}]}, allow_partial=True) == snapshot(\n        {'a': 11, 'b': [{'a': 10, 'b': 20}, {'a': 30}]}\n    )\n    assert v.validate_python({'a': 11, 'b': [{'a': 10, 'b': 20}, {'a': 30, 123: 4}]}, allow_partial=True) == snapshot(\n        {'a': 11, 'b': [{'a': 10, 'b': 20}]}\n    )\n    assert v.validate_python({'a': 11, 'b': [{'a': 10, 'b': 2}, {'a': 30}]}, allow_partial=True) == snapshot({'a': 11})\n    with pytest.raises(ValidationError, match=r'b\\.0\\.b\\s+Input should be greater than or equal to 10'):\n        v.validate_python({'b': [{'a': 10, 'b': 2}, {'a': 30}], 'a': 11}, allow_partial=True)\n    with pytest.raises(ValidationError, match=r'b\\.1\\.a\\s+Input should be greater than or equal to 10'):\n        v.validate_python({'b': [{'a': 10, 'b': 20}, {'a': 3}], 'a': 11}, allow_partial=True)\n    assert v.validate_python({'a': 11, 'b': [{'a': 1, 'b': 20}, {'a': 3, 'b': 40}]}, allow_partial=True) == snapshot(\n        {'a': 11}\n    )\n    json = b'{\"a\": 11, \"b\": [{\"a\": 10, \"b\": 20}, {\"a\": 30, \"b\": 40}]}'\n    assert v.validate_json(json, allow_partial=True) == snapshot(\n        {'a': 11, 'b': [{'a': 10, 'b': 20}, {'a': 30, 'b': 40}]}\n    )\n    for i in range(1, len(json)):\n        value = v.validate_json(json[:i], allow_partial=True)\n        assert isinstance(value, dict)", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "002159", "source": "def f(input_value: Any, info: core_schema.ValidationInfo) -> Any:\n        assert info.field_name == 'x'\n        assert info.data == {}\n        assert repr(info) == \"ValidationInfo(config=None, context=None, data={}, field_name='x')\"\n        assert str(info) == \"ValidationInfo(config=None, context=None, data={}, field_name='x')\"\n        assert isinstance(input_value, bytes)\n        return f'input: {input_value.decode()}'", "target": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "train", "example_id": "002160", "source": "def load_json_file(file_path: Path) -> Any:\n    with open(file_path) as f:\n        return json.load(f)", "target": "def forward(\n        self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor\n    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        r\n        return self.query_proj(query), self.key_proj(key), self.value_proj(value)", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn", "data_split": "train", "example_id": "002161", "source": "def skip(self, params):\n        representation, solver = params\n        if representation == \"sparse\" and solver == \"svd\":\n            return True\n        return False", "target": "def skip(self, params):\n        representation, precompute = params\n        if representation == \"sparse\" and precompute is False:\n            return True\n        return False", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "002162", "source": "def name(self) -> str:\n        prefix = f\"{self.category()}\"\n        return prefix", "target": "def name(self):\n        prefix = f\"{self.category()}_{self._name}_{self.backend()}\"\n        return prefix", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "002163", "source": "def sample_line(p1, p2, n, noise=0.0):\n    np.random.seed(10)\n    p1 = np.float32(p1)\n    t = np.random.rand(n,1)\n    return p1 + (p2-p1)*t + np.random.normal(size=(n, 2))*noise", "target": "def inlined_schema() -> cs.CoreSchema:\n    level = N\n    schema: cs.CoreSchema = {\n        'type': 'model',\n        'cls': MyModel,\n        'schema': {\n            'type': 'model-fields',\n            'fields': {str(c): {'type': 'model-field', 'schema': {'type': 'int'}} for c in range(N)},\n        },\n        'ref': f'model_{N}',\n    }\n    for level in reversed(range(N)):\n        schema = {\n            'type': 'model',\n            'cls': MyModel,\n            'schema': {\n                'type': 'model-fields',\n                'fields': {str(c): {'type': 'model-field', 'schema': schema} for c in range(N)},\n            },\n            'ref': f'model_{level}',\n        }\n    return schema", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|opencv/opencv", "data_split": "train", "example_id": "002164", "source": "def regurgitate(depth, use_pyyaml_formatter=False):\n    data = yaml.safe_load(sys.stdin)\n    if use_pyyaml_formatter:\n        output = yaml.dump(data, sort_keys=True)\n        sys.stdout.write(output)\n    else:\n        miniyaml.render(sys.stdout, data, depth)", "target": "def build_argparser():\n    parser = argparse.ArgumentParser(description='This is an OpenCV-based version of Gaze Estimation example')\n    parser.add_argument('--input',\n                        help='Path to the input video file or camera device number')\n    parser.add_argument('--out',\n                        help='Path to the output video file')\n    parser.add_argument('--facem',\n                        default='face-detection-retail-0005.xml',\n                        help='Path to OpenVINO face detection model (.xml)')\n    parser.add_argument('--faced',\n                        default='CPU',\n                        help='Target device for the face detection' +\n                        '(e.g. CPU, GPU, VPU, ...)')\n    parser.add_argument('--headm',\n                        default='head-pose-estimation-adas-0001.xml',\n                        help='Path to OpenVINO head pose estimation model (.xml)')\n    parser.add_argument('--headd',\n                        default='CPU',\n                        help='Target device for the head pose estimation inference ' +\n                        '(e.g. CPU, GPU, VPU, ...)')\n    parser.add_argument('--landm',\n                        default='facial-landmarks-35-adas-0002.xml',\n                        help='Path to OpenVINO landmarks detector model (.xml)')\n    parser.add_argument('--landd',\n                        default='CPU',\n                        help='Target device for the landmarks detector (e.g. CPU, GPU, VPU, ...)')\n    parser.add_argument('--gazem',\n                        default='gaze-estimation-adas-0002.xml',\n                        help='Path to OpenVINO gaze vector estimaiton model (.xml)')\n    parser.add_argument('--gazed',\n                        default='CPU',\n                        help='Target device for the gaze vector estimation inference ' +\n                        '(e.g. CPU, GPU, VPU, ...)')\n    parser.add_argument('--eyem',\n                        default='open-closed-eye-0001.xml',\n                        help='Path to OpenVINO open closed eye model (.xml)')\n    parser.add_argument('--eyed',\n                        default='CPU',\n                        help='Target device for the eyes state inference (e.g. CPU, GPU, VPU, ...)')\n    return parser", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "002165", "source": "def node_type(self) -> ASTNodeType:\n        return ASTNodeType.Enumeration", "target": "def node_type(self) -> ASTNodeType:\n        return ASTNodeType.Constant", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|scikit-learn/scikit-learn", "data_split": "train", "example_id": "002166", "source": "def get_wav2letter(device: torch.device) -> GetterReturnType:\n    N = 10\n    input_frames = 700\n    vocab_size = 28\n    model = models.Wav2Letter(num_classes=vocab_size)\n    criterion = torch.nn.NLLLoss()\n    model.to(device)\n    params, names = extract_weights(model)\n    inputs = torch.rand([N, 1, input_frames], device=device)\n    labels = torch.rand(N, 3, device=device).mul(vocab_size).long()\n    def forward(*new_params: Tensor) -> Tensor:\n        load_weights(model, names, new_params)\n        out = model(inputs)\n        loss = criterion(out, labels)\n        return loss\n    return forward, params", "target": "def make_scorers(self):\n        self.train_scorer = lambda _, __: self.estimator.kl_divergence_\n        self.test_scorer = lambda _, __: self.estimator.kl_divergence_", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|opencv/opencv", "data_split": "train", "example_id": "002167", "source": "def bench_scikit_tree_classifier(X, Y):\n    from sklearn.tree import DecisionTreeClassifier\n    gc.collect()\n    tstart = datetime.now()\n    clf = DecisionTreeClassifier()\n    clf.fit(X, Y).predict(X)\n    delta = datetime.now() - tstart\n    scikit_classifier_results.append(delta.seconds + delta.microseconds / mu_second)", "target": "def test_age_gender_infer2_roi(self):\n            if not cv.dnn.DNN_TARGET_CPU in cv.dnn.getAvailableTargets(cv.dnn.DNN_BACKEND_INFERENCE_ENGINE):\n                return\n            root_path    = '/omz_intel_models/intel/age-gender-recognition-retail-0013/FP32/age-gender-recognition-retail-0013'\n            model_path   = self.find_file(root_path + '.xml',   [os.environ.get('OPENCV_DNN_TEST_DATA_PATH')], required=False)\n            weights_path = self.find_file(root_path + '.bin',   [os.environ.get('OPENCV_DNN_TEST_DATA_PATH')], required=False)\n            device_id    = 'CPU'\n            rois = [(10, 15, 62, 62), (23, 50, 62, 62), (14, 100, 62, 62), (80, 50, 62, 62)]\n            img_path = self.find_file('cv/face/david2.jpg', [os.environ.get('OPENCV_TEST_DATA_PATH')])\n            img = cv.imread(img_path)\n            dnn_age_list    = []\n            dnn_gender_list = []\n            for roi in rois:\n                age, gender = self.infer_reference_network(model_path,\n                                                           weights_path,\n                                                           self.make_roi(img, roi))\n                dnn_age_list.append(age)\n                dnn_gender_list.append(gender)\n            g_in   = cv.GMat()\n            g_rois = cv.GArrayT(cv.gapi.CV_RECT)\n            inputs = cv.GInferListInputs()\n            inputs.setInput('data', g_rois)\n            outputs  = cv.gapi.infer2(\"net\", g_in, inputs)\n            age_g    = outputs.at(\"age_conv3\")\n            gender_g = outputs.at(\"prob\")\n            comp = cv.GComputation(cv.GIn(g_in, g_rois), cv.GOut(age_g, gender_g))\n            pp = cv.gapi.ie.params(\"net\", model_path, weights_path, device_id)\n            gapi_age_list, gapi_gender_list = comp.apply(cv.gin(img, rois),\n                                                         args=cv.gapi.compile_args(cv.gapi.networks(pp)))\n            for gapi_age, gapi_gender, dnn_age, dnn_gender in zip(gapi_age_list,\n                                                                  gapi_gender_list,\n                                                                  dnn_age_list,\n                                                                  dnn_gender_list):\n                self.assertEqual(0.0, cv.norm(dnn_gender, gapi_gender, cv.NORM_INF))\n                self.assertEqual(0.0, cv.norm(dnn_age, gapi_age, cv.NORM_INF))", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "002168", "source": "def test_validate_scientific_notation_from_json(input_value, expected):\n    v = SchemaValidator(cs.decimal_schema())\n    assert v.validate_json(input_value) == expected", "target": "def test_validate_scientific_notation_from_json(input_value, expected):\n    v = SchemaValidator(cs.float_schema())\n    assert v.validate_json(input_value) == expected", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "002169", "source": "def constrain(self: _Pipeline[_InT, _NewOutGe], constraint: annotated_types.Ge) -> _Pipeline[_InT, _NewOutGe]: ...", "target": "def system(command):\n    print(f\"[system] {command}\")\n    p = subprocess.Popen(\n        command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True\n    )\n    output, err = p.communicate()\n    rc = p.returncode\n    output = output.decode(\"ascii\")\n    err = err.decode(\"ascii\")\n    return rc, output, err", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "002170", "source": "def with_config(config: ConfigDict, /) -> Callable[[_TypeT], _TypeT]: ...", "target": "def with_config(**config: Unpack[ConfigDict]) -> Callable[[_TypeT], _TypeT]: ...", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|opencv/opencv", "data_split": "train", "example_id": "002171", "source": "def test_state_in_class(self):\n            @cv.gapi.op('custom.sum', in_types=[cv.GArray.Int], out_types=[cv.GOpaque.Int])\n            class GSum:\n                @staticmethod\n                def outMeta(arr_desc):\n                    return cv.empty_gopaque_desc()\n            @cv.gapi.kernel(GSum)\n            class GSumImpl:\n                last_result = 0\n                @staticmethod\n                def run(arr):\n                    GSumImpl.last_result = sum(arr)\n                    return GSumImpl.last_result\n            g_in  = cv.GArray.Int()\n            comp  = cv.GComputation(cv.GIn(g_in), cv.GOut(GSum.on(g_in)))\n            s = comp.apply(cv.gin([1, 2, 3, 4]), args=cv.gapi.compile_args(cv.gapi.kernels(GSumImpl)))\n            self.assertEqual(10, s)\n            s = comp.apply(cv.gin([1, 2, 8, 7]), args=cv.gapi.compile_args(cv.gapi.kernels(GSumImpl)))\n            self.assertEqual(18, s)\n            self.assertEqual(18, GSumImpl.last_result)", "target": "def test_norm_for_one_array(self):\n        np.random.seed(123)\n        for norm_type, norm in norm_type_under_test.items():\n            element_types = get_element_types(norm_type)\n            for shape, element_type in product(shapes, element_types):\n                array = generate_vector(shape, element_type)\n                expected = norm(array)\n                actual = cv.norm(array, norm_type)\n                self.assertAlmostEqual(\n                    expected, actual, places=2,\n                    msg='Array {0} of {1} and norm {2}'.format(\n                        array, element_type.__name__, norm_name[norm_type]\n                    )\n                )", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|pytorch/pytorch", "data_split": "train", "example_id": "002172", "source": "def fit(self, X, y):\n        super().fit(X, y)\n        df = self.decision_function(X)\n        self.df_min_ = df.min()\n        self.df_max_ = df.max()", "target": "def skip_accuracy_check_as_eager_non_deterministic(self):\n        if self.args.accuracy and self.args.training:\n            return self._accuracy[\"skip\"][\"eager_not_deterministic\"]\n        return set()", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "train", "example_id": "002173", "source": "def populate_family(models):\n        family = {}\n        for model_name in models:\n            family_name = get_family_name(model_name)\n            if family_name not in family:\n                family[family_name] = []\n            family[family_name].append(model_name)\n        return family", "target": "def modeldef(request, net_name, executor, fuser):\n    set_fuser(fuser, executor)\n    name, rnn_creator, context = get_nn_runners(net_name)[0]\n    creator_args = {\n        \"seqLength\": 100,\n        \"numLayers\": 1,\n        \"inputSize\": 512,\n        \"hiddenSize\": 512,\n        \"miniBatch\": 64,\n        \"device\": \"cuda\",\n        \"seed\": None,\n    }\n    return rnn_creator(**creator_args)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pytorch/pytorch", "data_split": "train", "example_id": "002174", "source": "def setUp(self):\n            self.skipTest('Skip tests: ' + message)", "target": "def test_noop_when_empty_path(self):\n        start = Path.cwd()\n        with working_directory(\"\"):\n            self.assertEqual(Path.cwd(), start)\n        self.assertEqual(Path.cwd(), start)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "002175", "source": "def type_format(self) -> str:\n        if TypeNode.compatible_to_runtime_usage:\n            return \"_typing.Union[{}]\"\n        return \"{}\"", "target": "def type_format(self) -> str:\n        return \"_typing.Type[{}]\"", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "002176", "source": "def model_serializer(\n    *, mode: Literal['wrap'], when_used: WhenUsed = 'always', return_type: Any = ...\n) -> Callable[[_ModelWrapSerializerT], _ModelWrapSerializerT]: ...", "target": "def model_serializer(\n    *,\n    mode: Literal['plain'] = ...,\n    when_used: WhenUsed = 'always',\n    return_type: Any = ...,\n) -> Callable[[_ModelPlainSerializerT], _ModelPlainSerializerT]: ...", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|scikit-learn/scikit-learn", "data_split": "train", "example_id": "002177", "source": "def s2(v: int) -> int:\n        return v + 2", "target": "def skip(self, params):\n        representation, precompute = params\n        if representation == \"sparse\" and precompute is False:\n            return True\n        return False", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "002178", "source": "def required_definition_imports(self) -> Generator[str, None, None]:\n        yield from ()", "target": "def required_definition_imports(self) -> Generator[str, None, None]:\n        yield \"import typing as _typing\"\n        yield from super().required_definition_imports", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "002179", "source": "def get_memory_usage():\n    process = psutil.Process()\n    main_memory = process.memory_full_info().pss\n    for child in process.children(recursive=True):\n        try:\n            child_mem = child.memory_full_info().pss\n            main_memory += child_mem\n        except (psutil.NoSuchProcess, psutil.AccessDenied, AttributeError):\n            print(f\"Failed to get PSS for {child}, falling back to USS\")\n            child_mem = child.memory_info().uss\n            main_memory += child_mem\n    return main_memory / (1024 * 1024)", "target": "def no_info_plain_validator_function(\n    function: NoInfoValidatorFunction,\n    *,\n    ref: str | None = None,\n    json_schema_input_schema: CoreSchema | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> PlainValidatorFunctionSchema:\n    return _dict_not_none(\n        type='function-plain',\n        function={'type': 'no-info', 'function': function},\n        ref=ref,\n        json_schema_input_schema=json_schema_input_schema,\n        metadata=metadata,\n        serialization=serialization,\n    )", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "002180", "source": "def input_data_lax():\n    return {\n        'field_str': 'fo',\n        'field_str_con': 'fooba',\n        'field_int': 1,\n        'field_int_con': 8,\n        'field_float': 1.0,\n        'field_float_con': 10.0,\n        'field_decimal': 42.0,\n        'field_bool': True,\n        'field_bytes': b'foobar',\n        'field_bytes_con': b'foobar',\n        'field_date': '2010-02-03',\n        'field_date_con': '2020-01-01',\n        'field_time': '12:00:00',\n        'field_time_con': '12:00:00',\n        'field_datetime': '2020-01-01T12:13:14',\n        'field_datetime_con': '2020-01-01T00:00:00',\n        'field_uuid': '12345678-1234-5678-1234-567812345678',\n        'field_list_any': ['a', b'b', True, 1.0, None] * 10,\n        'field_list_str': ['a', 'b', 'c'] * 10,\n        'field_list_str_con': ['a', 'b', 'c'] * 10,\n        'field_set_any': {'a', b'b', True, 1.0, None},\n        'field_set_int': set(range(100)),\n        'field_set_int_con': set(range(42)),\n        'field_frozenset_any': frozenset({'a', b'b', True, 1.0, None}),\n        'field_frozenset_bytes': frozenset([f'{i}'.encode() for i in range(100)]),\n        'field_frozenset_bytes_con': frozenset([f'{i}'.encode() for i in range(42)]),\n        'field_tuple_var_len_any': ('a', b'b', True, 1.0, None),\n        'field_tuple_var_len_float': tuple(i + 0.5 for i in range(100)),\n        'field_tuple_var_len_float_con': tuple(i + 0.5 for i in range(42)),\n        'field_tuple_fix_len': ('a', 1, 1.0, True),\n        'field_dict_any': {'a': 'b', 1: True, 1.0: 1.0},\n        'field_dict_str_float': {f'{i}': i + 0.5 for i in range(100)},\n        'field_literal_1_int': 1,\n        'field_literal_1_str': 'foobar',\n        'field_literal_mult_int': 3,\n        'field_literal_mult_str': 'foo',\n        'field_literal_assorted': 'foo',\n        'field_list_nullable_int': [1, None, 2, None, 3, None, 4, None],\n        'field_union': {'field_str': 'foo', 'field_int': 1, 'field_float': 1.0},\n        'field_functions_model': {\n            'field_before': 'foo',\n            'field_after': 'foo',\n            'field_wrap': 'foo',\n            'field_plain': 'foo',\n        },\n        'field_recursive': {\n            'name': 'foo',\n            'sub_branch': {'name': 'bar', 'sub_branch': {'name': 'baz', 'sub_branch': None}},\n        },\n    }", "target": "def test_repeated_ref():\n    with pytest.raises(SchemaError, match='SchemaError: Duplicate ref: `foobar`'):\n        SchemaValidator(\n            schema=core_schema.tuple_positional_schema(\n                [\n                    core_schema.definitions_schema(\n                        core_schema.definition_reference_schema('foobar'), [core_schema.int_schema(ref='foobar')]\n                    ),\n                    core_schema.definitions_schema(\n                        core_schema.definition_reference_schema('foobar'), [core_schema.int_schema(ref='foobar')]\n                    ),\n                ]\n            )\n        )", "label": 0}
{"task": "CLONE", "source_repo": "scikit-learn/scikit-learn|pydantic/pydantic", "data_split": "train", "example_id": "002181", "source": "def make_data(random_state, n_samples_per_center, grid_size, scale):\n    random_state = check_random_state(random_state)\n    centers = np.array([[i, j] for i in range(grid_size) for j in range(grid_size)])\n    n_clusters_true, n_features = centers.shape\n    noise = random_state.normal(\n        scale=scale, size=(n_samples_per_center, centers.shape[1])\n    )\n    X = np.concatenate([c + noise for c in centers])\n    y = np.concatenate([[i] * n_samples_per_center for i in range(n_clusters_true)])\n    return shuffle(X, y, random_state=random_state)", "target": "def test_serialize_as_any_with_field_serializer(container_schema_builder) -> None:\n    schema = container_schema_builder(\n        {\n            'value': core_schema.int_schema(\n                serialization=core_schema.plain_serializer_function_ser_schema(\n                    lambda model, v: v * 2, is_field_serializer=True\n                )\n            )\n        }\n    )\n    v = SchemaValidator(schema).validate_python({'value': 123})\n    cls = type(v)\n    s = SchemaSerializer(schema)\n    cls.__pydantic_serializer__ = s\n    assert s.to_python(v, serialize_as_any=False) == {'value': 246}\n    assert s.to_python(v, serialize_as_any=True) == {'value': 246}\n    assert s.to_json(v, serialize_as_any=False) == b'{\"value\":246}'\n    assert s.to_json(v, serialize_as_any=True) == b'{\"value\":246}'", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pytorch/pytorch", "data_split": "train", "example_id": "002182", "source": "def make_optional_arg(*arg_names: str) -> Callable[[NamespaceNode, SymbolName], None]:\n    def _make_optional_arg(root_node: NamespaceNode,\n                           function_symbol_name: SymbolName) -> None:\n        function = find_function_node(root_node, function_symbol_name)\n        for arg_name in arg_names:\n            found_overload_with_arg = False\n            for overload in function.overloads:\n                arg_idx = _find_argument_index(overload.arguments, arg_name)\n                if arg_idx is None:\n                    continue\n                if isinstance(overload.arguments[arg_idx].type_node, OptionalTypeNode):\n                    continue\n                overload.arguments[arg_idx].type_node = OptionalTypeNode(\n                    cast(TypeNode, overload.arguments[arg_idx].type_node)\n                )\n                found_overload_with_arg = True\n            if not found_overload_with_arg:\n                raise RuntimeError(\n                    f\"Failed to find argument with name: '{arg_name}'\"\n                    f\" in '{function_symbol_name.name}' overloads\"\n                )\n    return _make_optional_arg", "target": "def main():\n    result_path = sys.argv[1]\n    Benchmark().enable_compile_time_instruction_count().collect_all().append_results(\n        result_path\n    )", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "002183", "source": "def test_alias_validate_by_name(py_and_json: PyAndJson, input_value, expected):\n    v = py_and_json(\n        {\n            'type': 'arguments',\n            'arguments_schema': [\n                {'name': 'a', 'mode': 'positional_or_keyword', 'schema': {'type': 'int'}, 'alias': 'Foo'}\n            ],\n            'validate_by_name': True,\n        }\n    )\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)):\n            v.validate_test(input_value)\n    else:\n        assert v.validate_test(input_value) == expected", "target": "def test_any_uuid_key():\n    v = SchemaSerializer(core_schema.dict_schema())\n    input_value = {UUID('12345678-1234-5678-1234-567812345678'): 1}\n    assert v.to_python(input_value, mode='json') == {'12345678-1234-5678-1234-567812345678': 1}\n    assert v.to_json(input_value) == b'{\"12345678-1234-5678-1234-567812345678\":1}'", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pydantic/pydantic", "data_split": "train", "example_id": "002184", "source": "def test_union_of_functions():\n    def repr_function(value, _info):\n        if value == 'unexpected':\n            raise PydanticSerializationUnexpectedValue()\n        return f'func: {value!r}'\n    s = SchemaSerializer(\n        core_schema.union_schema(\n            [\n                core_schema.any_schema(\n                    serialization=core_schema.plain_serializer_function_ser_schema(repr_function, info_arg=True)\n                ),\n                core_schema.float_schema(serialization=core_schema.format_ser_schema('_^14')),\n            ]\n        )\n    )\n    assert s.to_python('foobar') == \"func: 'foobar'\"\n    assert s.to_python('foobar', mode='json') == \"func: 'foobar'\"\n    assert s.to_json('foobar') == b'\"func: \\'foobar\\'\"'\n    assert s.to_python('unexpected') == 'unexpected'\n    assert s.to_python('unexpected', mode='json') == '__unexpected__'\n    assert s.to_json('unexpected') == b'\"__unexpected__\"'", "target": "def test_args_var_args_only(py_and_json: PyAndJson, input_value, expected):\n    v = py_and_json(\n        {\n            'type': 'arguments',\n            'arguments_schema': [{'name': 'a', 'mode': 'positional_only', 'schema': {'type': 'int'}}],\n            'var_args_schema': {'type': 'int'},\n        }\n    )\n    if isinstance(expected, Err):\n        with pytest.raises(ValidationError, match=re.escape(expected.message)) as exc_info:\n            v.validate_test(input_value)\n        if expected.errors is not None:\n            assert exc_info.value.errors(include_url=False) == expected.errors\n    else:\n        assert v.validate_test(input_value) == expected", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "002185", "source": "def type_format(self) -> str:\n        if TypeNode.compatible_to_runtime_usage:\n            return \"_typing.Union[{}]\"\n        return \"{}\"", "target": "def type_format(self) -> str:\n        if TypeNode.compatible_to_runtime_usage:\n            return \"_typing.Dict[{}]\"\n        return \"dict[{}]\"", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|pytorch/pytorch", "data_split": "train", "example_id": "002186", "source": "def test_on_error_raise_explicit(self, py_and_json: PyAndJson):\n        v = py_and_json(\n            {\n                'type': 'typed-dict',\n                'fields': {\n                    'x': {\n                        'type': 'typed-dict-field',\n                        'schema': {'type': 'default', 'schema': {'type': 'str'}, 'on_error': 'raise'},\n                    }\n                },\n            }\n        )\n        assert v.validate_test({'x': 'foo'}) == {'x': 'foo'}\n        with pytest.raises(ValidationError) as exc_info:\n            v.validate_test({'x': ['foo']})\n        assert exc_info.value.errors(include_url=False) == [\n            {'input': ['foo'], 'type': 'string_type', 'loc': ('x',), 'msg': 'Input should be a valid string'}\n        ]", "target": "def f(inp, *weights):\n            x = inp\n            for w in weights:\n                x = torch.matmul(w, x).sin().sin()\n            return x", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "002187", "source": "def find_class_node(self, class_info, namespaces):\n            return find_class_node(\n                self.cv_root,\n                SymbolName.parse(class_info.full_original_name, namespaces),\n                create_missing_namespaces=True\n            )", "target": "def find_class_node(self, class_info, namespaces):\n            return ClassNode()", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "002188", "source": "def get_shapes(self) -> tuple[tuple[int, ...], ...]:\n        return (\n            (32768, 256),\n            (32768, 512),\n            (32768, 1024),\n            (32768, 2048),\n            (32768, 4096),\n            (32768, 8192),\n            (32768, 16384),\n            (32768, 32768),\n            (32768, 65536),\n            (16384, 131072),\n            (8192, 262144),\n        )", "target": "def get_shapes(self) -> tuple[tuple[int, ...], ...]:\n        return (\n            (32768, 256),\n            (32768, 512),\n            (32768, 1024),\n            (32768, 2048),\n            (32768, 4096),\n            (32768, 8192),\n            (32768, 16384),\n            (32768, 32768),\n            (32768, 65536),\n            (16384, 131072),\n            (8192, 262144),\n        ) + extra_shapes_for_norm", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "002189", "source": "def types_separator(self) -> str:\n        return \", \"", "target": "def types_separator(self) -> str:\n        return \", \"", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "002190", "source": "def medium():\n    return (rand(32, 12, 64, 64), rand(32, 12, 64, 64))", "target": "def test_int_not_coerced_to_enum():\n    class BinaryEnum(IntEnum):\n        ZERO = 0\n        ONE = 1\n    enum_schema = core_schema.lax_or_strict_schema(\n        core_schema.no_info_after_validator_function(BinaryEnum, core_schema.int_schema()),\n        core_schema.is_instance_schema(BinaryEnum),\n    )\n    schema = core_schema.union_schema([enum_schema, core_schema.int_schema()])\n    validator = SchemaValidator(schema)\n    assert validator.validate_python(0) is not BinaryEnum.ZERO\n    assert validator.validate_python(1) is not BinaryEnum.ONE\n    assert validator.validate_python(BinaryEnum.ZERO) is BinaryEnum.ZERO\n    assert validator.validate_python(BinaryEnum.ONE) is BinaryEnum.ONE", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|scikit-learn/scikit-learn", "data_split": "train", "example_id": "002191", "source": "def test_vector_fast_return(self):\n        expected_shape = (5, 4)\n        rects = cv.utils.generateVectorOfRect(expected_shape[0])\n        self.assertTrue(isinstance(rects, np.ndarray),\n                        \"Vector of rectangles should be returned as numpy array. Got: {}\".format(type(rects)))\n        self.assertEqual(rects.dtype, np.int32, \"Vector of rectangles has wrong elements type\")\n        self.assertEqual(rects.shape, expected_shape, \"Vector of rectangles has wrong shape\")\n        empty_rects = cv.utils.generateVectorOfRect(0)\n        self.assertTrue(isinstance(empty_rects, tuple),\n                        \"Empty vector should be returned as empty tuple. Got: {}\".format(type(empty_rects)))\n        self.assertEqual(len(empty_rects), 0, \"Vector of size 0 should be returned as tuple of length 0\")\n        expected_shape = (10,)\n        ints = cv.utils.generateVectorOfInt(expected_shape[0])\n        self.assertTrue(isinstance(ints, np.ndarray),\n                        \"Vector of integers should be returned as numpy array. Got: {}\".format(type(ints)))\n        self.assertEqual(ints.dtype, np.int32, \"Vector of integers has wrong elements type\")\n        self.assertEqual(ints.shape, expected_shape, \"Vector of integers has wrong shape.\")", "target": "def _weights(x, dx=1, orig=0):\n    x = np.ravel(x)\n    floor_x = np.floor((x - orig) / dx).astype(np.int64)\n    alpha = (x - orig - floor_x * dx) / dx\n    return np.hstack((floor_x, floor_x + 1)), np.hstack((1 - alpha, alpha))", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pytorch/pytorch", "data_split": "train", "example_id": "002192", "source": "def angle_cos(p0, p1, p2):\n    d1, d2 = (p0-p1).astype('float'), (p2-p1).astype('float')\n    return abs( np.dot(d1, d2) / np.sqrt( np.dot(d1, d1)*np.dot(d2, d2) ) )", "target": "def liger(self, args, kwargs) -> Any:\n        from liger_kernel.transformers.rms_norm import LigerRMSNorm\n        x, w = args\n        M, N = x.shape\n        liger_rmsnorm = LigerRMSNorm(hidden_size=N, eps=1e-6).cuda()\n        liger_rmsnorm.weight.data.copy_(w)\n        return lambda: liger_rmsnorm(x)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "002193", "source": "def type_format(self) -> str:\n        return \"\"", "target": "def type_format(self) -> str:\n        if TypeNode.compatible_to_runtime_usage:\n            return \"_typing.Tuple[{}]\"\n        return \"tuple[{}]\"", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|scikit-learn/scikit-learn", "data_split": "train", "example_id": "002194", "source": "def test_var_args_validation_error(py_and_json: PyAndJson, input_value, err_loc) -> None:\n    v = py_and_json(\n        cs.arguments_v3_schema(\n            [\n                cs.arguments_v3_parameter(name='args', schema=cs.int_schema(), mode='var_args'),\n            ]\n        )\n    )\n    with pytest.raises(ValidationError) as exc_info:\n        v.validate_test(input_value)\n    error = exc_info.value.errors()[0]\n    assert error['type'] == 'int_parsing'\n    assert error['loc'] == err_loc", "target": "def setup_impute():\n    try:\n        import pandas\n    except ImportError:\n        raise SkipTest(\"Skipping impute.rst, pandas not installed\")", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|pytorch/pytorch", "data_split": "train", "example_id": "002195", "source": "def preprocess_simple(digits):\n    return np.float32(digits).reshape(-1, SZ*SZ) / 255.0", "target": "def quack(self, args, kwargs) -> Any:\n        from quack.layernorm import layernorm\n        x, w = args\n        return lambda: layernorm(x, w, eps=1e-6)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv|opencv/opencv", "data_split": "train", "example_id": "002196", "source": "def fmt_bool(x):\n    return (\"true\" if x else \"false\")", "target": "def test_age_gender_infer_tensor(self):\n            skip_if_openvino_not_available()\n            root_path  = '/omz_intel_models/intel/age-gender-recognition-retail-0013/FP32/age-gender-recognition-retail-0013'\n            model_path = self.find_file(root_path + '.xml',   [os.environ.get('OPENCV_DNN_TEST_DATA_PATH')], required=False)\n            bin_path   = self.find_file(root_path + '.bin',   [os.environ.get('OPENCV_DNN_TEST_DATA_PATH')], required=False)\n            device_id  = 'CPU'\n            img_path = self.find_file('cv/face/david2.jpg', [os.environ.get('OPENCV_TEST_DATA_PATH')])\n            img = cv.imread(img_path)\n            tensor = cv.resize(img, (62, 62)).astype(np.float32)\n            tensor = np.transpose(tensor, (2, 0, 1))\n            tensor = np.expand_dims(tensor, 0)\n            ref = AgeGenderOV(model_path, bin_path, device_id)\n            ov_age, ov_gender = ref.apply(tensor)\n            comp = AgeGenderGAPI(model_path, bin_path, device_id)\n            gapi_age, gapi_gender = comp.apply(tensor)\n            self.assertEqual(0.0, cv.norm(ov_gender, gapi_gender, cv.NORM_INF))\n            self.assertEqual(0.0, cv.norm(ov_age, gapi_age, cv.NORM_INF))", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "002197", "source": "def area(self) -> int:\n            return self.width * self.height", "target": "def area(self, area: float) -> None:\n            self.side = area**0.5", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pytorch/pytorch", "data_split": "train", "example_id": "002198", "source": "def log(event_name):\n            scribe.open_source_signpost(\n                subsystem=\"pr_time_benchmarks\",\n                name=event_name,\n                parameters=json.dumps(\n                    {\n                        \"benchmark_name\": entry.benchmark_name,\n                        \"metric_name\": entry.metric_name,\n                        \"actual_value\": result,\n                        \"expected_value\": entry.expected_value,\n                        \"noise_margin\": entry.noise_margin,\n                        \"change_ratio\": ratio,\n                    }\n                ),\n            )", "target": "def find_job_id_name(args: Any) -> tuple[str, str]:\n    PYTORCH_REPO = os.environ.get(\"GITHUB_REPOSITORY\", \"pytorch/pytorch\")\n    PYTORCH_GITHUB_API = f\"https://api.github.com/repos/{PYTORCH_REPO}\"\n    GITHUB_TOKEN = os.environ[\"GITHUB_TOKEN\"]\n    REQUEST_HEADERS = {\n        \"Accept\": \"application/vnd.github.v3+json\",\n        \"Authorization\": \"token \" + GITHUB_TOKEN,\n    }\n    url = f\"{PYTORCH_GITHUB_API}/actions/runs/{args.workflow_run_id}/jobs?per_page=100\"\n    jobs = fetch_jobs(url, REQUEST_HEADERS)\n    jobs.sort(key=operator.itemgetter(\"started_at\"), reverse=True)\n    for job in jobs:\n        if job[\"runner_name\"] == args.runner_name:\n            return (job[\"id\"], job[\"name\"])\n    raise RuntimeError(f\"Can't find job id for runner {args.runner_name}\")", "label": 0}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "002199", "source": "def test_multiple_intertwined():\n    v = SchemaValidator(\n        core_schema.definitions_schema(\n            core_schema.definition_reference_schema('Foo'),\n            [\n                core_schema.typed_dict_schema(\n                    {\n                        'height': core_schema.typed_dict_field(core_schema.int_schema()),\n                        'bar': core_schema.typed_dict_field(core_schema.definition_reference_schema('Bar')),\n                    },\n                    ref='Foo',\n                ),\n                core_schema.typed_dict_schema(\n                    {\n                        'width': core_schema.typed_dict_field(core_schema.int_schema()),\n                        'bars': core_schema.typed_dict_field(\n                            core_schema.with_default_schema(\n                                core_schema.list_schema(core_schema.definition_reference_schema('Bar')), default=None\n                            )\n                        ),\n                        'foo': core_schema.typed_dict_field(\n                            core_schema.with_default_schema(\n                                core_schema.union_schema(\n                                    [core_schema.none_schema(), core_schema.definition_reference_schema('Foo')]\n                                ),\n                                default=None,\n                            )\n                        ),\n                    },\n                    ref='Bar',\n                ),\n            ],\n        )\n    )\n    v.validate_python(\n        {\n            'height': 1,\n            'bar': {\n                'width': 2,\n                'bars': [{'width': 3}],\n                'foo': {'height': 4, 'bar': {'width': 5, 'bars': [], 'foo': None}},\n            },\n        }\n    )", "target": "def main(args):\n    opencv_version = get_opencv_version(args.opencv_sdk_path)\n    ndk_version = get_ndk_version(args.ndk_location)\n    print(\"Detected ndk_version:\", ndk_version)\n    abis = os.listdir(path.join(args.opencv_sdk_path, \"sdk/native/libs\"))\n    final_aar_path = FINAL_AAR_PATH_TEMPLATE.replace(\"<OPENCV_VERSION>\", opencv_version)\n    sdk_dir = args.opencv_sdk_path\n    print(\"Removing data from previous runs...\")\n    cleanup([TEMP_DIR, final_aar_path, path.join(FINAL_REPO_PATH, \"org/opencv\", MAVEN_PACKAGE_NAME)])\n    print(\"Preparing Android project...\")\n    shutil.copytree(ANDROID_PROJECT_TEMPLATE_DIR, ANDROID_PROJECT_DIR)\n    fill_template(path.join(ANDROID_PROJECT_DIR, \"OpenCV/build.gradle.template\"),\n                  path.join(ANDROID_PROJECT_DIR, \"OpenCV/build.gradle\"),\n                  {\"LIB_NAME\": \"templib\",\n                   \"LIB_TYPE\": \"c++_static\",\n                   \"PACKAGE_NAME\": MAVEN_PACKAGE_NAME,\n                   \"OPENCV_VERSION\": opencv_version,\n                   \"NDK_VERSION\": ndk_version,\n                   \"COMPILE_SDK\": args.android_compile_sdk,\n                   \"MIN_SDK\": args.android_min_sdk,\n                   \"TARGET_SDK\": args.android_target_sdk,\n                   \"ABI_FILTERS\": \", \".join(['\"' + x + '\"' for x in abis]),\n                   \"JAVA_VERSION\": args.java_version,\n                   })\n    fill_template(path.join(ANDROID_PROJECT_DIR, \"OpenCV/src/main/cpp/CMakeLists.txt.template\"),\n                  path.join(ANDROID_PROJECT_DIR, \"OpenCV/src/main/cpp/CMakeLists.txt\"),\n                  {\"LIB_NAME\": \"templib\", \"LIB_TYPE\": \"STATIC\"})\n    local_props = \"\"\n    if args.ndk_location:\n        local_props += \"ndk.dir=\" + args.ndk_location + \"\\n\"\n    if args.cmake_location:\n        local_props += \"cmake.dir=\" + args.cmake_location + \"\\n\"\n    if local_props:\n        with open(path.join(ANDROID_PROJECT_DIR, \"local.properties\"), \"wt\") as f:\n            f.write(local_props)\n    opencv_libs = get_list_of_opencv_libs(sdk_dir)\n    external_libs = get_list_of_3rdparty_libs(sdk_dir, abis)\n    add_printing_linked_libs(sdk_dir, opencv_libs)\n    print(\"Running gradle assembleRelease...\")\n    cmd = [\"./gradlew\", \"assembleRelease\"]\n    if args.offline:\n        cmd = cmd + [\"--offline\"]\n    subprocess.run(cmd, shell=False, cwd=ANDROID_PROJECT_DIR, check=True)\n    complied_aar_path = get_compiled_aar_path(COMPILED_AAR_PATH_1, COMPILED_AAR_PATH_2)\n    shutil.unpack_archive(complied_aar_path, AAR_UNZIPPED_DIR, \"zip\")\n    print(\"Adding libs to AAR...\")\n    for lib in external_libs:\n        for abi in abis:\n            os.makedirs(path.join(AAR_UNZIPPED_DIR, \"prefab/modules/\" + lib + \"/libs/android.\" + abi))\n            if path.exists(path.join(sdk_dir, \"sdk/native/3rdparty/libs/\" + abi, \"lib\" + lib + \".a\")):\n                shutil.copy(path.join(sdk_dir, \"sdk/native/3rdparty/libs/\" + abi, \"lib\" + lib + \".a\"),\n                            path.join(AAR_UNZIPPED_DIR, \"prefab/modules/\" + lib + \"/libs/android.\" + abi, \"lib\" + lib + \".a\"))\n            else:\n                shutil.copy(path.join(AAR_UNZIPPED_DIR, \"prefab/modules/templib/libs/android.\" + abi, \"libtemplib.a\"),\n                            path.join(AAR_UNZIPPED_DIR, \"prefab/modules/\" + lib + \"/libs/android.\" + abi, \"lib\" + lib + \".a\"))\n            shutil.copy(path.join(AAR_UNZIPPED_DIR, \"prefab/modules/templib/libs/android.\" + abi + \"/abi.json\"),\n                        path.join(AAR_UNZIPPED_DIR, \"prefab/modules/\" + lib + \"/libs/android.\" + abi + \"/abi.json\"))\n        shutil.copy(path.join(AAR_UNZIPPED_DIR, \"prefab/modules/templib/module.json\"),\n                    path.join(AAR_UNZIPPED_DIR, \"prefab/modules/\" + lib + \"/module.json\"))\n    for lib in opencv_libs:\n        for abi in abis:\n            os.makedirs(path.join(AAR_UNZIPPED_DIR, \"prefab/modules/\" + lib + \"/libs/android.\" + abi))\n            shutil.copy(path.join(sdk_dir, \"sdk/native/staticlibs/\" + abi, \"lib\" + lib + \".a\"),\n                        path.join(AAR_UNZIPPED_DIR, \"prefab/modules/\" + lib + \"/libs/android.\" + abi, \"lib\" + lib + \".a\"))\n            shutil.copy(path.join(AAR_UNZIPPED_DIR, \"prefab/modules/templib/libs/android.\" + abi + \"/abi.json\"),\n                        path.join(AAR_UNZIPPED_DIR, \"prefab/modules/\" + lib + \"/libs/android.\" + abi + \"/abi.json\"))\n        os.makedirs(path.join(AAR_UNZIPPED_DIR, \"prefab/modules/\" + lib + \"/include/opencv2\"))\n        shutil.copy(path.join(sdk_dir, \"sdk/native/jni/include/opencv2/\" + lib.replace(\"opencv_\", \"\") + \".hpp\"),\n                    path.join(AAR_UNZIPPED_DIR, \"prefab/modules/\" + lib + \"/include/opencv2/\" + lib.replace(\"opencv_\", \"\") + \".hpp\"))\n        shutil.copytree(path.join(sdk_dir, \"sdk/native/jni/include/opencv2/\" + lib.replace(\"opencv_\", \"\")),\n                        path.join(AAR_UNZIPPED_DIR, \"prefab/modules/\" + lib + \"/include/opencv2/\" + lib.replace(\"opencv_\", \"\")))\n        module_json_text = {\n            \"export_libraries\": convert_deps_list_to_prefab(read_linked_libs(lib, abis), opencv_libs, external_libs),\n            \"android\": {},\n        }\n        with open(path.join(AAR_UNZIPPED_DIR, \"prefab/modules/\" + lib + \"/module.json\"), \"w\") as f:\n            json.dump(module_json_text, f)\n    for h_file in (\"cvconfig.h\", \"opencv.hpp\", \"opencv_modules.hpp\"):\n        shutil.copy(path.join(sdk_dir, \"sdk/native/jni/include/opencv2/\" + h_file),\n                    path.join(AAR_UNZIPPED_DIR, \"prefab/modules/opencv_core/include/opencv2/\" + h_file))\n    shutil.rmtree(path.join(AAR_UNZIPPED_DIR, \"prefab/modules/templib\"))\n    os.makedirs(\"outputs\", exist_ok=True)\n    shutil.make_archive(final_aar_path, \"zip\", AAR_UNZIPPED_DIR, \".\")\n    os.rename(final_aar_path + \".zip\", final_aar_path)\n    print(\"Creating local maven repo...\")\n    shutil.copy(final_aar_path, path.join(ANDROID_PROJECT_DIR, \"OpenCV/opencv-release.aar\"))\n    print(\"Creating a maven repo from project sources (with sources jar and javadoc jar)...\")\n    cmd = [\"./gradlew\", \"publishReleasePublicationToMyrepoRepository\"]\n    if args.offline:\n        cmd = cmd + [\"--offline\"]\n    subprocess.run(cmd, shell=False, cwd=ANDROID_PROJECT_DIR, check=True)\n    os.makedirs(path.join(FINAL_REPO_PATH, \"org/opencv\"), exist_ok=True)\n    shutil.move(path.join(ANDROID_PROJECT_DIR, \"OpenCV/build/repo/org/opencv\", MAVEN_PACKAGE_NAME),\n                path.join(FINAL_REPO_PATH, \"org/opencv\", MAVEN_PACKAGE_NAME))\n    print(\"Creating a maven repo from modified AAR (with cpp libraries)...\")\n    cmd = [\"./gradlew\", \"publishModifiedPublicationToMyrepoRepository\"]\n    if args.offline:\n        cmd = cmd + [\"--offline\"]\n    subprocess.run(cmd, shell=False, cwd=ANDROID_PROJECT_DIR, check=True)\n    shutil.copytree(path.join(ANDROID_PROJECT_DIR, \"OpenCV/build/repo/org/opencv\", MAVEN_PACKAGE_NAME),\n                    path.join(FINAL_REPO_PATH, \"org/opencv\", MAVEN_PACKAGE_NAME),\n                    dirs_exist_ok=True)\n    print(\"Done\")", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "002200", "source": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "target": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv|pydantic/pydantic", "data_split": "train", "example_id": "002201", "source": "def get_conf_mat(gt, prob):\n    assert type(gt) is np.ndarray\n    assert type(prob) is np.ndarray\n    conf_mat = np.zeros((gt.shape[0], gt.shape[0]))\n    for ch_gt in range(conf_mat.shape[0]):\n        gt_channel = gt[ch_gt, ...]\n        for ch_pr in range(conf_mat.shape[1]):\n            prob_channel = prob[ch_pr, ...]\n            conf_mat[ch_gt][ch_pr] = np.count_nonzero(np.multiply(gt_channel, prob_channel))\n    return conf_mat", "target": "def collect_field_or_class_var_from_stmt(\n        self, stmt: AssignmentStmt, model_config: ModelConfigData, class_vars: dict[str, PydanticModelClassVar]\n    ) -> PydanticModelField | PydanticModelClassVar | None:\n        cls = self._cls\n        lhs = stmt.lvalues[0]\n        if not isinstance(lhs, NameExpr) or not _fields.is_valid_field_name(lhs.name) or lhs.name == 'model_config':\n            return None\n        if not stmt.new_syntax:\n            if (\n                isinstance(stmt.rvalue, CallExpr)\n                and isinstance(stmt.rvalue.callee, CallExpr)\n                and isinstance(stmt.rvalue.callee.callee, NameExpr)\n                and stmt.rvalue.callee.callee.fullname in DECORATOR_FULLNAMES\n            ):\n                return None\n            if lhs.name in class_vars:\n                return None\n            error_untyped_fields(self._api, stmt)\n            return None\n        lhs = stmt.lvalues[0]\n        if not isinstance(lhs, NameExpr):\n            return None\n        if not _fields.is_valid_field_name(lhs.name) or lhs.name == 'model_config':\n            return None\n        sym = cls.info.names.get(lhs.name)\n        if sym is None:\n            return None\n        node = sym.node\n        if isinstance(node, PlaceholderNode):\n            return None\n        if isinstance(node, TypeAlias):\n            self._api.fail(\n                'Type aliases inside BaseModel definitions are not supported at runtime',\n                node,\n            )\n            return None\n        if not isinstance(node, Var):\n            return None\n        if node.is_classvar:\n            return PydanticModelClassVar(lhs.name)\n        node_type = get_proper_type(node.type)\n        if isinstance(node_type, Instance) and node_type.type.fullname == 'dataclasses.InitVar':\n            self._api.fail(\n                'InitVar is not supported in BaseModel',\n                node,\n            )\n        has_default = self.get_has_default(stmt)\n        strict = self.get_strict(stmt)\n        if sym.type is None and node.is_final and node.is_inferred:\n            typ = self._api.analyze_simple_literal_type(stmt.rvalue, is_final=True)\n            if typ:\n                node.type = typ\n            else:\n                self._api.fail(\n                    'Need type argument for Final[...] with non-literal default in BaseModel',\n                    stmt,\n                )\n                node.type = AnyType(TypeOfAny.from_error)\n        if node.is_final and has_default:\n            return PydanticModelClassVar(lhs.name)\n        alias, has_dynamic_alias = self.get_alias_info(stmt)\n        if (\n            has_dynamic_alias\n            and not (model_config.validate_by_name or model_config.populate_by_name)\n            and self.plugin_config.warn_required_dynamic_aliases\n        ):\n            error_required_dynamic_aliases(self._api, stmt)\n        is_frozen = self.is_field_frozen(stmt)\n        init_type = self._infer_dataclass_attr_init_type(sym, lhs.name, stmt)\n        return PydanticModelField(\n            name=lhs.name,\n            has_dynamic_alias=has_dynamic_alias,\n            has_default=has_default,\n            strict=strict,\n            alias=alias,\n            is_frozen=is_frozen,\n            line=stmt.line,\n            column=stmt.column,\n            type=init_type,\n            info=cls.info,\n        )", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "002202", "source": "def get_shapes(self) -> tuple[tuple[int, ...], ...]:\n        return (\n            (32768, 256),\n            (32768, 512),\n            (32768, 1024),\n            (32768, 2048),\n            (32768, 4096),\n            (32768, 8192),\n            (32768, 16384),\n            (32768, 32768),\n            (32768, 65536),\n            (16384, 131072),\n            (8192, 262144),\n        ) + extra_shapes_for_norm", "target": "def get_shapes(self) -> tuple[tuple[int, ...], ...]:\n        return (\n            (32768, 256),\n            (32768, 512),\n            (32768, 1024),\n            (32768, 2048),\n            (32768, 4096),\n            (32768, 8192),\n            (32768, 16384),\n        ) + extra_shapes_for_norm", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "002203", "source": "def test_by_alias_and_name_config_interaction(config, runtime, expected) -> None:\n    @dataclasses.dataclass\n    class Foo:\n        my_field: str\n    schema = core_schema.dataclass_schema(\n        Foo,\n        core_schema.dataclass_args_schema(\n            'Foo',\n            [\n                core_schema.dataclass_field(\n                    name='my_field', schema=core_schema.str_schema(), serialization_alias='my_alias'\n                ),\n            ],\n        ),\n        ['my_field'],\n        config=core_schema.CoreConfig(serialize_by_alias=config or False),\n    )\n    s = SchemaSerializer(schema)\n    assert s.to_python(Foo(my_field='hello'), by_alias=runtime) == expected", "target": "def test_parse_vector_int_convertible(self):\n        np.random.seed(123098765)\n        try_to_convert = partial(self._try_to_convert, cv.utils.dumpVectorOfInt)\n        arr = np.random.randint(-20, 20, 40).astype(np.int32).reshape(10, 2, 2)\n        int_min, int_max = get_limits(ctypes.c_int)\n        for convertible in ((int_min, 1, 2, 3, int_max), [40, 50], tuple(),\n                            np.array([int_min, -10, 24, int_max], dtype=np.int32),\n                            np.array([10, 230, 12], dtype=np.uint8), arr[:, 0, 1],):\n            expected = \"[\" + \", \".join(map(str, convertible)) + \"]\"\n            actual = try_to_convert(convertible)\n            self.assertEqual(expected, actual,\n                             msg=get_conversion_error_msg(convertible, expected, actual))", "label": 0}
{"task": "CLONE", "source_repo": "pytorch/pytorch|pydantic/pydantic", "data_split": "train", "example_id": "002204", "source": "def medium_broadcast_channels_last():\n    return (rand(32, 3, 223, 223).to(memory_format=torch.channels_last), rand(3, 1, 1))", "target": "def test_encoding(any_serializer, gen_input, kwargs, expected_json):\n    assert to_json(gen_input(), **kwargs) == expected_json\n    if not kwargs:\n        assert any_serializer.to_python(gen_input(), mode='json') == json.loads(expected_json)", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "002205", "source": "def is_resolved(self) -> bool:\n        return True", "target": "def is_resolved(self) -> bool:\n        return self._ast_node is not None or self._module_name is not None", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "002206", "source": "def validate_as(self, tp: type[_NewOutT], *, strict: bool = ...) -> _Pipeline[_InT, _NewOutT]: ...", "target": "def validate_as(self, tp: EllipsisType, *, strict: bool = ...) -> _Pipeline[_InT, Any]:\n        ...", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|opencv/opencv", "data_split": "train", "example_id": "002207", "source": "def gh_fetch_json_list(\n    url: str,\n    params: Optional[dict[str, Any]] = None,\n    data: Optional[dict[str, Any]] = None,\n) -> list[dict[str, Any]]:\n    return cast(list[dict[str, Any]], _gh_fetch_json_any(url, params, data))", "target": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "002208", "source": "def outMeta(arr_desc0, arr_desc1, arr_desc2):\n        return cv.empty_array_desc()", "target": "def outMeta(arr_desc0, arr_desc1):\n        return cv.empty_array_desc(), cv.empty_array_desc()", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch", "data_split": "train", "example_id": "002209", "source": "def liger(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        from liger_kernel.transformers.cross_entropy import LigerCrossEntropyLoss\n        x, target = args\n        cross_entropy = LigerCrossEntropyLoss(reduction=\"none\")\n        return lambda: cross_entropy(x, target)", "target": "def liger(self, args, kwargs=None) -> Any:\n        assert kwargs is None\n        from liger_kernel.transformers.cross_entropy import LigerCrossEntropyLoss\n        x, target, dloss = args\n        cross_entropy = LigerCrossEntropyLoss(reduction=\"none\")\n        loss = cross_entropy(x, target)\n        return lambda: torch.autograd.grad(\n            loss, x, grad_outputs=dloss, retain_graph=True\n        )", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "002210", "source": "def test_extra_behavior_allow(\n    config: Union[core_schema.CoreConfig, None],\n    schema_extra_behavior_kw: dict[str, Any],\n    validate_fn_extra_kw: Union[ExtraBehavior, None],\n):\n    @dataclasses.dataclass\n    class MyModel:\n        f: str\n    v = SchemaValidator(\n        core_schema.dataclass_schema(\n            MyModel,\n            core_schema.dataclass_args_schema(\n                'MyModel', [core_schema.dataclass_field('f', core_schema.str_schema())], **schema_extra_behavior_kw\n            ),\n            ['f'],\n            config=config,\n        )\n    )\n    m: MyModel = v.validate_python({'f': 'x', 'extra_field': '123'}, extra=validate_fn_extra_kw)\n    assert m.f == 'x'\n    assert getattr(m, 'extra_field') == '123'\n    v.validate_assignment(m, 'f', 'y', extra=validate_fn_extra_kw)\n    assert m.f == 'y'\n    v.validate_assignment(m, 'not_f', '123', extra=validate_fn_extra_kw)\n    assert getattr(m, 'not_f') == '123'", "target": "def test_extra_behavior_allow(\n    config: Union[core_schema.CoreConfig, None],\n    schema_extra_behavior_kw: dict[str, Any],\n    extras_schema_kw: dict[str, Any],\n    expected_extra_value: Any,\n):\n    v = SchemaValidator(\n        core_schema.model_fields_schema(\n            {'f': core_schema.model_field(core_schema.str_schema())}, **schema_extra_behavior_kw, **extras_schema_kw\n        ),\n        config=config,\n    )\n    m, model_extra, fields_set = v.validate_python({'f': 'x', 'extra_field': '123'})\n    assert m == {'f': 'x'}\n    assert model_extra == {'extra_field': expected_extra_value}\n    assert fields_set == {'f', 'extra_field'}\n    v.validate_assignment(m, 'f', 'y')\n    assert m == {'f': 'y'}\n    new_m, new_model_extra, new_fields_set = v.validate_assignment({**m, **model_extra}, 'not_f', '123')\n    assert new_m == {'f': 'y'}\n    assert new_model_extra == {'extra_field': expected_extra_value, 'not_f': expected_extra_value}\n    assert new_fields_set == {'not_f'}", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic", "data_split": "train", "example_id": "002211", "source": "def fn():\n        def validate(v, info):\n            return v\n        schema = core_schema.with_info_plain_validator_function(validate)\n        schema = core_schema.nullable_schema(schema)\n        validate.__pydantic_validator__ = SchemaValidator(schema)\n        return validate", "target": "def fn():\n        def validate(v, info):\n            return v\n        schema = core_schema.with_info_plain_validator_function(validate)\n        schema = core_schema.typed_dict_schema(\n            {'f': core_schema.typed_dict_field(schema)}, extra_behavior='allow', extras_schema=schema\n        )\n        validate.__pydantic_validator__ = SchemaValidator(schema)\n        return validate", "label": 1}
{"task": "CLONE", "source_repo": "pydantic/pydantic|opencv/opencv", "data_split": "train", "example_id": "002212", "source": "def test_bool_literal():\n    s = SchemaSerializer(core_schema.literal_schema([False]))\n    assert 'expected_int:{},expected_str:{},expected_py:Some(Py(' in plain_repr(s)\n    assert s.to_python(False) is False\n    assert s.to_python(False, mode='json') is False\n    assert s.to_python(True) is True\n    assert s.to_json(False) == b'false'", "target": "def test_unsupported_numpy_data_types_string_description(self):\n        for dtype in (object, str, np.complex128):\n            test_array = np.zeros((4, 4, 3), dtype=dtype)\n            msg = \".*type = {} is not supported\".format(test_array.dtype)\n            if sys.version_info[0] < 3:\n                self.assertRaisesRegexp(\n                    Exception, msg, cv.utils.dumpInputArray, test_array\n                )\n            else:\n                self.assertRaisesRegex(\n                    Exception, msg, cv.utils.dumpInputArray, test_array\n                )", "label": 0}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "002213", "source": "def required_usage_imports(self) -> Generator[str, None, None]:\n        yield \"import cv2.typing\"", "target": "def required_usage_imports(self) -> Generator[str, None, None]:\n        yield \"import numpy\"\n        yield \"import typing as _typing\"", "label": 1}
{"task": "CLONE", "source_repo": "opencv/opencv", "data_split": "train", "example_id": "002214", "source": "def required_definition_imports(self) -> Generator[str, None, None]:\n        return self.value.required_usage_imports", "target": "def required_definition_imports(self) -> Generator[str, None, None]:\n        yield from self.positive_branch_type.required_usage_imports\n        yield from self.negative_branch_type.required_usage_imports\n        yield from self._condition_required_imports", "label": 1}
{"task": "CLONE", "source_repo": "pytorch/pytorch|scikit-learn/scikit-learn", "data_split": "train", "example_id": "002215", "source": "def eqDictFloatKeyIntValue(self, input: dict[float, int]) -> dict[float, int]:\n        return input", "target": "def setup(app):\n    app.add_directive(\"allow_nan_estimators\", AllowNanEstimators)\n    return {\n        \"version\": \"0.1\",\n        \"parallel_read_safe\": True,\n        \"parallel_write_safe\": True,\n    }", "label": 0}
